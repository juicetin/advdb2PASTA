/**************************/
/***** Post History *******/
/**************************/

INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve always been interested in machine learning, but I can''t figure out one thing about starting out with a simple "Hello World" example - how can I avoid hard-coding behavior?

For example, if I wanted to "teach" a bot how to avoid randomly placed obstacles, I couldn''t just use relative motion, because the obstacles move around, but I don''t want to hard code, say, distance, because that ruins the whole point of machine learning.

Obviously, randomly generating code would be impractical, so how could I do this?', 5, '2014-05-13 23:58:30.457', '009bca93-fce2-44ed-a277-a8452650a627', 5, 7, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How can I do simple machine learning without hard-coding behavior?', 5, '2014-05-13 23:58:30.457', '009bca93-fce2-44ed-a277-a8452650a627', 5, 8, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 5, '2014-05-13 23:58:30.457', '009bca93-fce2-44ed-a277-a8452650a627', 5, 9, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As a researcher and instructor, I''m looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I''m especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.', 36, '2014-05-14 00:11:06.457', 'ea5a5642-ed30-43ea-9be5-8e8de0e1c660', 7, 12, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What open-source books (or other materials) provide a relatively thorough overview of data science?', 36, '2014-05-14 00:11:06.457', 'ea5a5642-ed30-43ea-9be5-8e8de0e1c660', 7, 13, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 36, '2014-05-14 00:11:06.457', 'ea5a5642-ed30-43ea-9be5-8e8de0e1c660', 7, 14, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Not sure if this fits the scope of this SE, but here''s a stab at an answer anyway.

With all AI approaches you have to decide what it is you''re modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are "fixed" and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.

You rarely hard-code behavior in AI/ML solutions. It''s all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.

In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving.', 51, '2014-05-14 00:36:31.077', '7482c6e8-a216-4648-b725-dd7463e87b7c', 9, 18, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One book that''s freely available is "The Elements of Statistical Learning" by Hastie, Tibshirani, and Friedman (published by Springer): [see Tibshirani''s website][1].

Another fantastic source, although it isn''t a book, is Andrew Ng''s Machine Learning course on Coursera. This has a much more applied-focus than the above book, and Prof. Ng does a great job of explaining the thinking behind several different machine learning algorithms/situations.

  [1]: http://statweb.stanford.edu/~tibs/ElemStatLearn/', 22, '2014-05-14 00:53:43.273', 'f9b92c74-c38f-419e-967a-006663d28a75', 10, 19, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.

My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few years back.  What are the differences between Data Science and Data Mining and in particular what more would I need to look at to become proficient in Data Mining?', 66, '2014-05-14 01:25:59.677', 'bd8a5c03-7143-4cc0-9d50-beafcdffd19a', 14, 29, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is Data Science the Same as Data Mining?', 66, '2014-05-14 01:25:59.677', 'bd8a5c03-7143-4cc0-9d50-beafcdffd19a', 14, 30, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 66, '2014-05-14 01:25:59.677', 'bd8a5c03-7143-4cc0-9d50-beafcdffd19a', 14, 31, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In which situations would one system be preferred over the other? What are the relative advantages and disadvantages of relational databases versus non-relational databases?', 64, '2014-05-14 01:41:23.110', '42817f34-3736-4798-bbed-b07364f263a5', 15, 32, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the advantages and disadvantages of SQL versus NoSQL in data science?', 64, '2014-05-14 01:41:23.110', '42817f34-3736-4798-bbed-b07364f263a5', 15, 33, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<databases>', 64, '2014-05-14 01:41:23.110', '42817f34-3736-4798-bbed-b07364f263a5', 15, 34, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I use [Libsvm][1] to training data and predict classification on **semantic analysis**.

But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** issue.

Last year, [Liblinear][2] was release and it can solve performance issue.
But it cost too much **memory**.

Is **MapReduce** the only way to solve semantic analysis on big data?

Or have any other methods can improve memory issue on **Liblinear**.

Thanks

  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/
  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/', 63, '2014-05-14 01:57:56.880', '3b75652d-4d40-448c-9242-9579560f4634', 16, 36, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Use liblinear on big data', 63, '2014-05-14 01:57:56.880', '3b75652d-4d40-448c-9242-9579560f4634', 16, 37, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><libsvm>', 63, '2014-05-14 01:57:56.880', '3b75652d-4d40-448c-9242-9579560f4634', 16, 38, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I use [Libsvm][1] to training data and predict classification on **semantic analysis** issue.

But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** issue.

Last year, [Liblinear][2] was release and it can solve performance issue.
But it cost too much **memory**.

Is **MapReduce** the only way to solve semantic analysis issue on big data?

Or have any other methods can improve memory issue on **Liblinear**.

Thanks

  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/
  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/', 63, '2014-05-14 02:04:42.460', 'f49ef7e0-7a84-4e16-a28c-520e7948d0f4', 16, 'added 12 characters in body', 40, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-14 02:49:14.580', '52e22583-f290-4b1d-a859-bdecef1cc8e6', 17, 41, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-14 02:49:14.580', '27193173-dce1-4133-9237-25d44e84dc62', 18, 42, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I use [Libsvm][1] to training data and predict classification on **semantic analysis** problem.

But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** problem.

Last year, [Liblinear][2] was release and it can solve performance bottleneck.
But it cost too much **memory**.

Is **MapReduce** the only way to solve semantic analysis problem on big data?

Or have any other methods can improve memory bottleneck on **Liblinear**.

Thanks

  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/
  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/', 63, '2014-05-14 03:40:25.897', 'dfabe8d8-bc68-4f23-8978-86f1d0f44aba', 16, 'added 16 characters in body', 43, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Lots of people use the term *big data* in a rather *comercial* way, as a means of pointing that large datasets are involved in the computation, and therefore upcoming solutions must have nice performance. Of course, *big data* always carry associated terms, like scalability and efficiency, but what exactly defines a problem as a *big data* problem?

Does the computation have to be related to some set of specific purposes, like data mining/information retrieval, or could an algorithm for general graph problems be labeled *big data* if data dataset was *big enough*? Also, how *big* is *big enough*? (In case this is possible to define).', 84, '2014-05-14 03:56:20.963', 'ec94df15-c4ec-44db-aae7-d75b1ad487ef', 19, 44, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How big is big data?', 84, '2014-05-14 03:56:20.963', 'ec94df15-c4ec-44db-aae7-d75b1ad487ef', 19, 45, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><scalability><efficiency><performance>', 84, '2014-05-14 03:56:20.963', 'ec94df15-c4ec-44db-aae7-d75b1ad487ef', 19, 46, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I use [Libsvm][1] to training data and predict classification on **semantic analysis** problem.

But it has **performance** issue on large-scale data because semantic analysis concern ***n-dimension*** problem.

Last year, [Liblinear][2] was release and it can solve performance bottleneck.
But it cost too much **memory**.

Is **MapReduce** the only way to solve semantic analysis problem on big data?

Or have any other methods can improve memory bottleneck on **Liblinear**?

Thanks

  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/
  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/', 63, '2014-05-14 05:15:27.797', 'e9edc983-c391-4af9-b89d-bb37e571010d', 16, 'edited body', 48, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('we created this social network application for eLearning purposes, it''s an experimental thing we are researching on in our lab. it has been used by some case studies for a while and the data on our relational DBMS (SQL Server 2008) is getting big, it''s a few gigabytes now and the tables are highly connected to each other. the performance is still fine, but when should we consider other options? is it the matter of performance?  ', 96, '2014-05-14 05:37:46.780', '7b2b9b15-c532-4414-bd4e-3f9d367b2cf3', 20, 49, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('the data on our relational DBMS is getting big, is it the time to move to NoSQL?', 96, '2014-05-14 05:37:46.780', '7b2b9b15-c532-4414-bd4e-3f9d367b2cf3', 20, 50, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nosql><relational-dbms>', 96, '2014-05-14 05:37:46.780', '7b2b9b15-c532-4414-bd4e-3f9d367b2cf3', 20, 51, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As you rightly note, these days "big data" is something everyone wants to say they''ve got, which entails a certain looseness in how people define the term.  Generally, though, I''d say you''re certainly dealing with big data if the scale is such that it''s no longer feasible to manage with more traditional technologies such as RDBMS, at least without complementing them with big data technologies such as Hadoop.

How big your data has to actually be for that to be the case is debatable.  Here''s a (somewhat provocative) [blog post][1] that claims that it''s not really the case for less than 5 TB of data.  (To be clear, it doesn''t claim "Less than 5 TB isn''t big data", but just "Less than 5 TB isn''t big enough that you need Hadoop".)

But even on smaller datasets, big data technologies like Hadoop can have other advantages, including being well suited to batch operations, playing well with unstructured data (as well as data whose structure isn''t known in advance or could change), horizontal scalability (scaling by adding more nodes instead of beefing up your existing servers), and (as one of the commenters on the above-linked post notes) the ability to integrate your data processing with external data sets (think of a map-reduce where the mapper makes a call to another server).  Other technologies associated with big data, like NoSql databases, emphasize fast performance and consistent availability while dealing with large sets of data, as well also being able to handle semi-unstructured data and to scale horizontally.

Of course, traditional RDBMS have their own advantages including ACID guarantees (Atomicity, Consistency, Isolation, Durability) and better performance for certain operations, as well as being more standardized, more mature, and (for many users) more familiar.  So even for indisputably "big" data, it may make sense to load at least a portion of your data into a traditional SQL database and use that in conjunction with big data technologies.

So, a more generous definition would be that you have big data so long as it''s big enough that big data technologies provide some added value for you.  But as you can see, that can depend not just on the size of your data but on how you want to work with it and what sort of requirements you have in terms of flexibility, consistency, and performance.  *How* you''re using your data is more relevant to the question than what you''re using it *for* (e.g. data mining).  That said, uses like data mining and machine learning are more likely to yield useful results if you have a big enough data set to work with.

  [1]: http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html', 14, '2014-05-14 05:44:29.340', '0ca00e0c-4f01-4ed4-8e63-08a9cbeb6ffe', 21, 52, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My set of data contains a set of numeric attributes and one categorical.

Say, `NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr`,

where `CategoricalAttr` takes one of three possible values: `CategoricalAttrValue1`, `CategoricalAttrValue2` or `CategoricalAttrValue3`.

I''m using default k-means clustering algorithm implementation for Octave https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/.
It works with numeric data only.

So my question: is it correct to split the categorical attribute `CategoricalAttr` into three numeric (binary) variables, like `IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3` ?', 97, '2014-05-14 05:58:21.927', 'deac1274-d6c8-4534-82f2-ce6d40be137b', 22, 53, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('K-Mean clustering for mixed numeric and categorical data', 97, '2014-05-14 05:58:21.927', 'deac1274-d6c8-4534-82f2-ce6d40be137b', 22, 54, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><clustering><octave>', 97, '2014-05-14 05:58:21.927', 'deac1274-d6c8-4534-82f2-ce6d40be137b', 22, 55, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data Science specialization from Johns Hopkins University at Coursera would be a great start.
https://www.coursera.org/specialization/jhudatascience/1', 97, '2014-05-14 06:06:13.603', '50b00a66-20b3-4b36-af40-c372709c61c6', 23, 56, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('K-Means clustering for mixed numeric and categorical data', 97, '2014-05-14 06:11:22.330', 'a6461050-177b-4a17-b842-aaa274e039b1', 22, 'Renamed ''K-Mean'' to ''K-Means'' and added tag', 57, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-mining><clustering><octave><k-means>', 97, '2014-05-14 06:11:22.330', 'a6461050-177b-4a17-b842-aaa274e039b1', 22, 'Renamed ''K-Mean'' to ''K-Means'' and added tag', 58, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The standard k-means algorithm isn''t directly applicable to categorical data, for various reasons.  The sample space for categorical data is discrete, and doesn''t have a natural origin.  A Euclidean distance function on such a space isn''t really meaningful.  As someone put it, "The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs." (from [here][1])

There''s a variation of k-means known as k-modes which is suitable for categorical data, although it has the flaw that the solutions you get are sensitive to initial conditions.  See [here][2] (PDF), for instance.

A Google search for "k-means mix of categorical data" turns up quite a few papers on various algorithms for k-means-like clustering with a mix of categorical and numeric data.  (I haven''t yet read them, so I can''t comment on their merits.)


  [1]: http://www.daylight.com/meetings/mug04/Bradshaw/why_k-modes.html
  [2]: http://arxiv.org/ftp/cs/papers/0603/0603120.pdf', 14, '2014-05-14 06:26:27.163', '73829641-ff19-4c62-a227-95d453a2f312', 24, 59, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Big Data is defined by the volume of data, that''s right, but not only. The particularity of big data is that you need to store a **lots** of **various** and sometimes **unstructured** stuffs **all the times** and from a **tons of sensors**, usually **for years or decade**.

Furthermore you need something scalable, so that it doesn''t take you half a year to find a data back.

So here''s come Big Data, where traditional method won''t work anymore. SQL is not scalable. And SQL works with very structured and linked data (with all those Primary and foreign key mess, innerjoin, imbricated request...).

Basically, because storage becomes cheaper and cheaper and data becomes more and more valuable, big manager ask engineer to records everything. Add to this tons of new sensors with all those mobile, social network, embeded stuff ...etc. So as classic methods won''t work, they have to find new technologies (storing everything in files, in json format, with big index, what we call noSQL).

So Big Data may be very big but can be not so big but complexe unstructured or various data which has to be store quickly and on-the-run in a raw format. We focus and storing at first, and then we look at how to link everything together.', 104, '2014-05-14 07:26:04.390', 'a1e2232d-7803-40a4-87e1-d439cbcfed5d', 25, 60, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A few gigabytes is not very "**big**". It''s more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don''t get TB''s of data a day).

Most professionals working in a big data environment consider **> ~5TB** as the *beginning* of the term big data. But even then it''s not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem.

i.e. if you do alot of searches in you database it would probably be better to run a solr cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it in solr instead of just moving the data from sql to nosql in term of persistence and performance.', 115, '2014-05-14 07:38:31.103', '0a0d04e3-da6d-4e63-9ee8-99daec0b75ad', 26, 61, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The standard k-means algorithm isn''t directly applicable to categorical data, for various reasons.  The sample space for categorical data is discrete, and doesn''t have a natural origin.  A Euclidean distance function on such a space isn''t really meaningful.  As someone put it, "The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs." (from [here][1])

There''s a variation of k-means known as k-modes, introduced in [this paper][2] by Zhexue Huang, which is suitable for categorical data.   Note that the solutions you get are sensitive to initial conditions, as discussed [here][3] (PDF), for instance.

A Google search for "k-means mix of categorical data" turns up quite a few papers on various algorithms for k-means-like clustering with a mix of categorical and numeric data.  (I haven''t yet read them, so I can''t comment on their merits.)  Huang''s paper (linked above) also has a section on a "k-protoypes" which I believe serves this purpose.

---

Actually, what you suggest (converting categorical attributes to binary values, and then doing k-means as if these were numeric values) is another approach that has been tried before (predating k-modes).  (See Ralambondrainy, H. 1995. A conceptual version of the k-means algorithm. Pattern Recognition Letters, 16:11471157.)  But I believe the k-modes approach is preferred for the reasons I indicated above.


  [1]: http://www.daylight.com/meetings/mug04/Bradshaw/why_k-modes.html
  [2]: http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf
  [3]: http://arxiv.org/ftp/cs/papers/0603/0603120.pdf', 14, '2014-05-14 07:40:44.160', '9e8b4e67-db0d-48c2-a91a-e9f73417dfa3', 24, 'added 641 characters in body', 62, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":5,"DisplayName":"Doorknob"},{"Id":51,"DisplayName":"Ansari"},{"Id":63,"DisplayName":"Puffin GDI"},{"Id":101,"DisplayName":"Didzis Elferts"},{"Id":115,"DisplayName":"Johnny000"}]}', -1, '2014-05-14 07:41:49.437', '85163b77-9e22-44b0-b1a5-1ee8422ff63f', 15, '104', 63, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To answer this question you have o answer which kind of compromise you can afford. RDBMs implements [ACID][1]. This is expensive in terms of resources. There are no NoSQL solution which is ACID. See [CAP theorem][2] to dive deep into these ideas.

So you have to understand each compromise given by each solution and choose the one which is the most appropriate for your problem.


  [1]: http://en.wikipedia.org/wiki/ACID
  [2]: http://en.wikipedia.org/wiki/CAP_theorem', 108, '2014-05-14 07:53:02.560', '39dce2b3-e25f-456a-896a-188d64522640', 27, 64, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A few gigabytes is not very "**big**". It''s more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don''t get TB''s of data a day).

Most professionals working in a big data environment consider **> ~5TB** as the *beginning* of the term big data. But even then it''s not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem.

i.e. if you do alot of searches in you database it would probably be better to run a solr cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it into solr instead of just moving the data from sql to nosql in term of persistence and performance.', 115, '2014-05-14 07:53:53.657', 'd5747a83-5139-486b-9be3-9f5468dcc61d', 26, 'added 2 characters in body', 66, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The standard k-means algorithm isn''t directly applicable to categorical data, for various reasons.  The sample space for categorical data is discrete, and doesn''t have a natural origin.  A Euclidean distance function on such a space isn''t really meaningful.  As someone put it, "The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs." (from [here][1])

There''s a variation of k-means known as k-modes, introduced in [this paper][2] by Zhexue Huang, which is suitable for categorical data.   Note that the solutions you get are sensitive to initial conditions, as discussed [here][3] (PDF), for instance.

Huang''s paper (linked above) also has a section on "k-protoypes" which applies to data with a mix of categorical and numeric features.  It uses a distance measure which mixes the Hamming distance for categorical features and the Euclidean distance for numeric features.

A Google search for "k-means mix of categorical data" turns up quite a few more recent papers on various algorithms for k-means-like clustering with a mix of categorical and numeric data.  (I haven''t yet read them, so I can''t comment on their merits.)

---

Actually, what you suggest (converting categorical attributes to binary values, and then doing k-means as if these were numeric values) is another approach that has been tried before (predating k-modes).  (See Ralambondrainy, H. 1995. A conceptual version of the k-means algorithm. Pattern Recognition Letters, 16:11471157.)  But I believe the k-modes approach is preferred for the reasons I indicated above.


  [1]: http://www.daylight.com/meetings/mug04/Bradshaw/why_k-modes.html
  [2]: http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf
  [3]: http://arxiv.org/ftp/cs/papers/0603/0603120.pdf', 14, '2014-05-14 07:55:04.747', 'a6d83474-d91a-4153-9b57-f9dac0d907c2', 24, 'added 182 characters in body', 67, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is free ebook "[Introduction to Data Science][1]" based on [tag:R] language


  [1]: http://jsresearch.net/', 118, '2014-05-14 07:55:40.133', 'd398a744-6316-41a9-b3c6-1a6e509528da', 28, 68, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[@statsRus][statsRus] starts to lay the groundwork for your answer in another question http://datascience.stackexchange.com/questions/1/what-characterises-the-difference-between-data-science-and-statistics:

>  - **Data collection**: web scraping and online surveys
>  - **Data manipulation**: recoding messy data and extracting meaning from linguistic and social network data
>  - **Data scale**: working with extremely large data sets
>  - **Data mining**: finding patterns in large, complex data sets, with an emphasis on algorithmic techniques
>  - **Data communication**: helping turn "machine-readable" data into "human-readable" information via visualization

Definition
----------

[tag:data-mining] can be seen as one item (or set of skills and applications) in the toolkit of the data scientist.  I like how he separates the definition of mining from collection in a sort of trade-specific jargon.

However, I think that *data-mining* would be synonymous with *data-collection* in a US-English colloquial definition.

*As to where to go to become proficient?*  I think that question is too broad as it is currently stated and would receive answers that are primarily opinion based.  Perhaps if you could refine your question, it might be easier to see what you are asking.

  [statsRus]: http://datascience.stackexchange.com/users/36/statsrus', 53, '2014-05-14 07:56:34.437', 'b66aeb2c-28b4-450f-8520-91b8f676999b', 29, 69, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The top-down answer:

Total amount of data in the world: 2.8 zetabytes in 2012, estimated to reach 8 zetabytes by 2015 ([source][1]) and with a doubling time of 40 months. Can''t get bigger than that :)

As an example of a single large organization, Facebook pulls in 500 terabytes per day, into a 100 petabyte warehouse, and runs 70k queries per day on it as of 2012 ([source][2])  Their current warehouse is >300 petabytes.

Big data is probably something that is a good fraction of the Facebook numbers (1/100 probably yes, 1/10000 probably not: it''s a spectrum not a single number).

Some of the features that make it "big" are:

- it is actively analyzed, not just stored  (quote "If you arent taking advantage of big data, then you dont have big data, you have just a pile of data" Jay Parikh @ Facebook)

- building and running a data warehouse is a major infrastructure project

- it is growing at a significant rate

- it is unstructured or has irregular structure

Gartner definition: "Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing" (The 3Vs)  So they also think "bigness" isn''t entirely about the size of the dataset, but also about the velocity and structure and the kind of tools needed.





  [1]: http://siliconangle.com/blog/2012/05/21/when-will-the-world-reach-8-zetabytes-of-stored-data-infographic/
  [2]: http://gigaom.com/2012/08/22/facebook-is-collecting-your-data-500-terabytes-a-day/', 26, '2014-05-14 08:03:28.117', '7f1db063-e957-4964-85d3-e3de3d198e86', 30, 70, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('To answer this question you have to answer which kind of compromise you can afford. RDBMs implements [ACID][1]. This is expensive in terms of resources. There are no NoSQL solutions which are ACID. See [CAP theorem][2] to dive deep into these ideas.

So you have to understand each compromise given by each solution and choose the one which is the most appropriate for your problem.


  [1]: http://en.wikipedia.org/wiki/ACID
  [2]: http://en.wikipedia.org/wiki/CAP_theorem', 14, '2014-05-14 08:03:37.890', '10c74c4b-2467-4513-89c5-6ae7a6752977', 27, 'minor typos/grammar', 71, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-14 08:03:37.890', '10c74c4b-2467-4513-89c5-6ae7a6752977', 27, 'Proposed by 14 approved by 108 edit id of 5', 72, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The top-down answer:

Total amount of data in the world: 2.8 zetabytes in 2012, estimated to reach 8 zetabytes by 2015 ([source][1]) and with a doubling time of 40 months. Can''t get bigger than that :)

As an example of a single large organization, Facebook pulls in 500 terabytes per day, into a 100 petabyte warehouse, and runs 70k queries per day on it as of 2012 ([source][2])  Their current warehouse is >300 petabytes.

Big data is probably something that is a good fraction of the Facebook numbers (1/100 probably yes, 1/10000 probably not: it''s a spectrum not a single number).

In addition to size, some of the features that make it "big" are:

- it is actively analyzed, not just stored  (quote "If you arent taking advantage of big data, then you dont have big data, you have just a pile of data" Jay Parikh @ Facebook)

- building and running a data warehouse is a major infrastructure project

- it is growing at a significant rate

- it is unstructured or has irregular structure

Gartner definition: "Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing" (The 3Vs)  So they also think "bigness" isn''t entirely about the size of the dataset, but also about the velocity and structure and the kind of tools needed.


  [1]: http://siliconangle.com/blog/2012/05/21/when-will-the-world-reach-8-zetabytes-of-stored-data-infographic/
  [2]: http://gigaom.com/2012/08/22/facebook-is-collecting-your-data-500-terabytes-a-day/', 26, '2014-05-14 08:09:20.747', 'e952e344-5431-43bf-aa0e-4f105660d6c6', 30, 'added 15 characters in body', 73, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a bunch of customer profiles stroed in [tag:elasticsearch] cluster. These profiles are now used for creatinon of target groups for our email subscriptions.

Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).

I wonder how can I search for interesting groups automatically - using datascience, machine learning, clustering or somewhat else.

[tag:R] programming langugae seems to be a good tool for this task, but I can''t form a methodology of such group search. One solution is to find somehow largest clusters of customers and use them as target groups, so the question is:

**How can I automatically choose largest clusters of similar customers (similar by parameters that I don''t know at this moment)?**

For example: my program will connect elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.

', 118, '2014-05-14 08:38:07.007', '26fa9a8e-80d6-4965-9725-009a067b91fc', 31, 74, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Clustering customer data stored in ElasticSearch', 118, '2014-05-14 08:38:07.007', '26fa9a8e-80d6-4965-9725-009a067b91fc', 31, 75, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><clustering>', 118, '2014-05-14 08:38:07.007', '26fa9a8e-80d6-4965-9725-009a067b91fc', 31, 76, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":66,"DisplayName":"demongolem"},{"Id":48,"DisplayName":"senshin"},{"Id":78,"DisplayName":"Bill the Lizard"},{"Id":21,"DisplayName":"Sean Owen"},{"Id":118,"DisplayName":"Konstantin V. Salikhov"}]}', -1, '2014-05-14 08:40:54.950', '87bf261d-1e23-4df7-a2d3-fded7c04c34d', 7, '102', 77, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a bunch of customer profiles stroed in [tag:elasticsearch] cluster. These profiles are now used for creatinon of target groups for our email subscriptions.

Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).

I wonder how can I search for interesting groups automatically - using datascience, machine learning, clustering or somewhat else.

[tag:R] programming language seems to be a good tool for this task, but I can''t form a methodology of such group search. One solution is to find somehow largest clusters of customers and use them as target groups, so the question is:

**How can I automatically choose largest clusters of similar customers (similar by parameters that I don''t know at this moment)?**

For example: my program will connect elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.

', 118, '2014-05-14 09:07:44.440', 'd92b7e62-ad68-41e8-bf53-df02eaf2e2c7', 31, 'fix typo', 81, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Depends on 2 things: the nature/structure of your data; and your performance.

SQL databases excel when your data is well structured (e.g. when it can be modeled as a table or Excel spreadsheet). A set of rows with a fixed # of columns. Also, when you need to do a lot of table joins (which it sounds like you do).

NoSQL databases excel when the data is UN-structured beyond key-value pairs.

Performance wise, you gotta ask yourself one question: is your current SQL solution slow?
If it isn''t, go with "IIABDFI" principle. ', 132, '2014-05-14 09:34:15.477', '76691869-e20b-4928-b0b5-408547d8d201', 33, 82, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In working on exploratory data analysis, and developing algorithms, I find that most of my time is spent in a cycle of visualize, write some code, run on small dataset, repeat.   The data I have tends to be computer vision/sensor fusion type stuff, and algorithms are vision-heavy (for example object detection and tracking, etc), and the off the shelf algorithms don''t work in this context.  I find that this takes a lot of iterations (for example, to dial in the type of algorithm or tune the parameters in the algorithm, or to get a visualization right) and also the run times even on a small dataset are quite long, so all together it takes a while.

How can the algorithm development itself be sped up and made more scalable?

Some specific challenges:

How can the number of iterations be reduced?  (Esp. when what kind of algorithm, let alone the specifics of it, does not seem to be easily foreseeable without trying different versions and examining their behavior)

How to run on bigger datasets during development?  (Often going from small to large dataset is when a bunch of new behavior and new issues is seen)

How can algorithm parameters be tuned faster?

How to apply machine learning type tools to algorithm development itself?  (For example, instead of writing the algorithm by hand, write some simple building blocks and combine them in a way learned from the problem, etc)
', 26, '2014-05-14 09:51:54.753', 'de7f9431-9c8c-4f4a-b9e0-546d761b7f67', 35, 86, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to scale up algorithm development?', 26, '2014-05-14 09:51:54.753', 'de7f9431-9c8c-4f4a-b9e0-546d761b7f67', 35, 87, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<algorithms>', 26, '2014-05-14 09:51:54.753', 'de7f9431-9c8c-4f4a-b9e0-546d761b7f67', 35, 88, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To me (coming from a relational database background), "Big Data" is not primarily about the data size (which is the bulk of what the other answers are so far).

"Big Data" and "Bad Data" are closely related. Relational Databases require ''pristine data''. If the data is in the database, it is accurate, clean, and 100% reliable. Relational Databases require "Great Data"  and a huge amount of time, money, and accountability is put on to making sure the data is well prepared before loading it in to the database. If the data is in the database, it is ''gospel'', and it defines the system understanding of reality.

"Big Data" tackles this problem from the other direction. The data is poorly defined, much of it may be inaccurate, and much of it may in fact be missing. The structure and layout of the data is linear as opposed to relational.

Big Data has to have enough volume so that the amount of bad data, or missing data becomes statistically insignificant. When the errors in your data are common enough to cancel each other out, and when the missing data is proportionally small enough to be negligible. When your data access requirements and algorithms are functional even with incomplete and inaccurate data, then you have "Big Data".

"Big Data" is not really about the volume, it is about the characteristics of the data.', 9, '2014-05-14 10:41:23.823', 'd85541b1-1350-411d-b28c-a7915a0d4979', 37, 91, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I heard about many tools / frameworks for helping people to process their data (big data environment).

One is called Hadoop and the other is the noSQL concept. What is the difference in point of processing?

Are they complementary?

', 134, '2014-05-14 10:44:58.933', '8c1b70f8-6111-4299-9d8e-fe376bf25e3f', 38, 92, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the difference between hadoop and noSQL', 134, '2014-05-14 10:44:58.933', '8c1b70f8-6111-4299-9d8e-fe376bf25e3f', 38, 93, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nosql><tools><processing><hadoop>', 134, '2014-05-14 10:44:58.933', '8c1b70f8-6111-4299-9d8e-fe376bf25e3f', 38, 94, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A few gigabytes is not very "**big**". It''s more like the normal size of an enterprise DB. As long as you go over PK when joining tables it should work out really well, even in the future (as long as you don''t get TB''s of data a day).

Most professionals working in a big data environment consider **> ~5TB** as the *beginning* of the term big data. But even then it''s not always the best way to just install the next best nosql database. You should always think about the task that you want to archive with the data (aggregate,read,search,mine,..) to find the best tools for you problem.

i.e. if you do alot of searches in you database it would probably be better to run a solr instance/cluster and denormalize your data from a DBMS like Postgres or your SQL Server from time to time and put it into solr instead of just moving the data from sql to nosql in term of persistence and performance.', 115, '2014-05-14 11:03:51.577', '1ebdfbcc-8a38-4a85-bb37-bf6885081bf6', 26, 'added 2 characters in body', 96, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Big Data is actually not so about the "how big it is".

First, few gigabytes is not big at all, it''s almost nothing. So don''t bother yourself, your system will continu to work efficiently for some time I think.

Then you have to think of how do you use your data.

 - SQL approach: Every data is precious, well collected and selected, and the focus is put on storing high valuable and well structured data. This can be costly, everything is interlink, and it''s good for well stuctured system and functionnal data.
 - Big Data approach: In big data you basically store almost everything, regardless of the value it has, and then do a active analytics process. Things are not linked, they are copied. For example let''s say I have a blog entry. In Big Data there will not be a link to its author, but the author will be embedded inside the blog entry. Way more scalable, but require a different and more complex approach.

If your storing "functionnal" data use by your application, I will suggest you to remain on SQL. If your storing data in order to search on them later or to do reporting, and if this amount of data may increase quickly, I will suggest big data.
In my opinion, big data is useful when you are dealing with real data that have to be collect and analyzed continuously.', 104, '2014-05-14 11:12:03.880', '5f347c00-3376-4c79-a3ff-e3dd9c64bf45', 40, 99, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('My data set contains a number of numeric attributes and one categorical.

Say, `NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr`,

where `CategoricalAttr` takes one of three possible values: `CategoricalAttrValue1`, `CategoricalAttrValue2` or `CategoricalAttrValue3`.

I''m using default k-means clustering algorithm implementation for Octave https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/.
It works with numeric data only.

So my question: is it correct to split the categorical attribute `CategoricalAttr` into three numeric (binary) variables, like `IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3` ?', 97, '2014-05-14 11:13:12.423', '2322a126-81bc-49e2-ab1e-c55af2f9dfa8', 22, 'Cosmetic change in text.', 100, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R has many libraries which are aimed at Data Analysis (e.g. JAGS, BUGS, ARULES etc..), and is mentioned in popular textbooks such as: J.Krusche, Doing Bayesian Data Analysis; B.Lantz, "Machine Learning with R".

I''ve seen a guideline of 5TB for a dataset to be considered as Big Data.

My question is: Is R suitable for the amount of Data typically seen in Big Data problems?
Are there strategies to be employed when using R with this size of dataset?', 136, '2014-05-14 11:15:40.907', '09a9e93c-d137-4792-b869-17e427812538', 41, 101, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is the R language suitable for Big Data', 136, '2014-05-14 11:15:40.907', '09a9e93c-d137-4792-b869-17e427812538', 41, 102, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 136, '2014-05-14 11:15:40.907', '09a9e93c-d137-4792-b869-17e427812538', 41, 103, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('NoSQL is a way to store data that does not require there to be some sort of relation. The simplicity of its design and horizontal scale-ability, one way they store data is the `key : value` pair design. This lends itself to processing that is similar to Hadoop. The use of a NoSQL db really depends on the type of problem that one is after.

Here is a good wikipedia link [NoSQL][1]

Hadoop is a system that is meant to store and process huge chunks of data. It is a distributed file system dfs. The reason it does this is that central to its design it makes the assumption that hardware failures are common, thus making multiple copies of the same piece of information and spreading it across multiple machines and racks, so if one goes down, no problem, we have two more copies. Here is a great link for Hadoop from wikipedia as well, you will see that it is, in my opinion more than just storage, but also processing:
 [Hadoop][2]


  [1]: https://en.wikipedia.org/wiki/NoSQL
  [2]: https://en.wikipedia.org/wiki/Apache_Hadoop', 59, '2014-05-14 11:21:31.500', '78a1f18a-a62d-4c86-8364-409caed1b125', 42, 104, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Hadoop is not a database**, hadoop runs mapreduce jobs while splitting big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset.

i.e. you load into hadoop a set of `<String, Integer>` with the population of some cities and you want to get the average population over the whole dataset.

    [new york, 40394]
    [new york, 134]
    [la, 44]
    [la, 647]
    ...

Now hadoop will first map and reduce the keys, in our example the cities to a new set of integers:

    [new york, [40394,134]]
    [la, [44,647]]
    ...

after this task is completed it will just get like in SQL the average (sum/size) of the interger set for each key:

    [new york, [20264]]
    [la, [346]]
    ...

now hadoop would be done with everything. You can now load the result into the HDFS (hadoop file system) or into any DBMS or file.

Thats just one **very basic** and **simple** example of what hadoop can do. You can run much more complicated tasks in hadoop.', 115, '2014-05-14 11:23:25.913', '7245175c-1974-400a-9a1d-06e390f01e83', 43, 105, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Actually this is coming around. In the book R in a Nutshell there is even a section on using R with Hadoop for big data processing. There are some work arounds that need to be done because R does all it''s work in memory, so you are basically limited to the amount of RAM you have available to you.

A mature project for R and Hadoop is RHadoop', 59, '2014-05-14 11:24:39.530', '1a2acc74-0bab-4f8e-9f30-49abfac53511', 44, 106, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First off, if your data has as many variations (in function of time, context, and others) as to make it hard to apply a single strategy to cope with it, you may be interested in doing a prior temporal/contextual/... characterization of the dataset. Characterizing data, i.e., extracting information about how the volume or specifics of the content varies according to some criteria, usually provides with a better understanding (more consise and precise) than simply inferring algorithms on a brute-force fashion.

So, answering each question:

1. characterization is definitely a means of reducing the number of *iterations* while trying to select proper algorithms for specific data;
2. if you have a discrete set of criterias on which your data varies, it becomes much easier to *scale up* solutions, as will know what information you''d gain/lose if simpler/specific solutions were applied;
3. after a characterization, you should be also easier to select parameters, since you''d know what kind of *specific data* you''d be dealing with;
4. finally, you may use data mining/machine learning algorithms to support this characterization. This includes using:
- clustering algorithms, to reduce the dimensionality of data;
- classification algorithms, to help deciding on specific properties the data in function of time/context/... may present;
- association rules, to predict particular knowledge from the dataset, while also improving/fine-graining the data used for later analysis;
- and other possible strategies and analyses.', 84, '2014-05-14 11:26:40.580', '89a145b1-ee32-4acd-85a2-409b3e7166ee', 45, 107, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**Hadoop is not a database**, hadoop runs mapreduce jobs while splitting big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset.

i.e. you load into hadoop a set of `<String, Integer>` with the population of some cities and you want to get the average population over the whole dataset.

    [new york, 40394]
    [new york, 134]
    [la, 44]
    [la, 647]
    ...

Now hadoop will first map and reduce the keys, in our example the cities to a new set of integers:

    [new york, [40394,134]]
    [la, [44,647]]
    ...

after this task is completed it will just get like in SQL the average (sum/length) of the interger set for each key:

    [new york, [20264]]
    [la, [346]]
    ...

now hadoop would be done with everything. You can now load the result into the HDFS (hadoop file system) or into any DBMS or file.

Thats just one **very basic** and **simple** example of what hadoop can do. You can run much more complicated tasks in hadoop.

As you already mentioned in your question, hadoop and noSQL are complementary. I know a few setups where i.e. billions of datasets from sensors are stored in HBase and get then through hadoop to finally be stored in a DBMS.', 115, '2014-05-14 11:28:54.380', '5bc7bf11-9b69-4072-83ce-dad8b699fd3b', 43, 'added 2 characters in body', 108, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('First off, if your data has as many variations (in function of time, context, and others) as to make it hard to apply a single strategy to cope with it, you may be interested in doing a prior temporal/contextual/... characterization of the dataset. Characterizing data, i.e., extracting information about how the volume or specifics of the content varies according to some criteria, usually provides with a better understanding (more consise and precise) than simply inferring algorithms on a brute-force fashion.

So, answering each question:

1. characterization is definitely a means of reducing the number of *iterations* while trying to select proper algorithms for specific data;
2. if you have a discrete set of criterias on which your data varies, it becomes much easier to *scale up* solutions, as will know what information you''d gain/lose if simpler/specific solutions were applied;
3. after a characterization, you should be also easier to select parameters, since you''d know what kind of *specific data* you''d be dealing with;
4. finally, you may use data mining/machine learning algorithms to support this characterization. This includes using:
- clustering algorithms, to reduce the dimensionality of data;
- classification algorithms, to help deciding on specific properties the data in function of time/context/... may present;
- association rules, to predict particular knowledge from the dataset, while also improving/fine-graining the data used for later analysis;
- and other possible strategies and analyses.

And [here](http://www.cs.cmu.edu/Groups/sage/sagedc.html) is a list of some criterias on which to analyse data, which you may find helpful.', 84, '2014-05-14 11:33:27.770', 'f471a3b8-42db-4a6d-986a-7ac31108c948', 45, 'added 143 characters in body', 109, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**Hadoop is not a database**, hadoop runs mapreduce jobs while splitting big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset.

i.e. you load into hadoop a set of `<String, Integer>` with the population of some cities and you want to get the average population over the whole dataset.

    [new york, 40394]
    [new york, 134]
    [la, 44]
    [la, 647]
    ...

Now hadoop will first map and reduce by using the keys, in our example the cities to a new set of integers:

    [new york, [40394,134]]
    [la, [44,647]]
    ...

after this task is completed it will just get like in SQL the average (sum/length) of the interger set for each key:

    [new york, [20264]]
    [la, [346]]
    ...

now hadoop would be done with everything. You can now load the result into the HDFS (hadoop file system) or into any DBMS or file.

Thats just one **very basic** and **simple** example of what hadoop can do. You can run much more complicated tasks in hadoop.

As you already mentioned in your question, hadoop and noSQL are complementary. I know a few setups where i.e. billions of datasets from sensors are stored in HBase and get then through hadoop to finally be stored in a DBMS.', 115, '2014-05-14 12:08:46.913', '7935a41a-181b-4245-9395-32170abf962c', 43, 'added 9 characters in body', 110, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Note that there is an early version of LIBLINEAR ported to [Apache Spark][1]. See: http://apache-spark-user-list.1001560.n3.nabble.com/Spark-LIBLINEAR-td5546.html


  [1]: http://spark.apache.org', 21, '2014-05-14 12:32:29.503', '38cec2c1-c571-466a-a640-358a3c1348bf', 46, 111, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The main problem with using R for large data sets is the RAM constraint. The reason behind keeping all the data in RAM is that it provides much faster access and data manipulations than would storing on HDDs. If you are willing to take a hit on performance, then yes, it is quite practical to work with [large datasets in R][1].

- RODBC Package: Allows connecting to external DB from R to retrieve and handle data. Hence, the data being *manipulated* is restricted to your RAM. The overall data set can go much larger.
- The ff package allows using larger than RAM data sets by utilising memory-mapped pages.
- BigLM: It builds generalized linear models on big data. It loads data into memory in chunks.
- bigmemory : An R package which allows powerful and memory-efficient parallel
analyses and data mining of massive data sets. It permits storing large objects (matrices etc.) in memory (on the RAM) using external pointer objects to refer to them.


  [1]: http://statistics.org.il/wp-content/uploads/2010/04/Big_Memory%20V0.pdf', 62, '2014-05-14 12:39:41.197', '7992303c-d1a3-40ac-8c8a-2fafacabfdb8', 47, 112, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><r>', 118, '2014-05-14 13:06:28.407', '2a9da2b2-a929-4f6d-baf4-1f346da68021', 41, 'added tag r', 113, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-14 13:06:28.407', '2a9da2b2-a929-4f6d-baf4-1f346da68021', 41, 'Proposed by 118 approved by 136 edit id of 6', 114, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-14 13:08:26.647', 'c69229e9-4232-4319-b59c-fa6ddb624b69', 48, 115, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-14 13:08:26.647', '5b223f01-fc39-41c2-933c-1d047f02f76b', 49, 116, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have an R script that generates a report based on the current contents of a database. This database is constantly in flux with records being added/deleted many times each day. How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning? Or perhaps I want it to re-run once a certain number of new records have been added to the database. How might I go about automating this? I should mention I''m on Windows, but I could easily put this script on my Linux machine if that would simplify the process. ', 151, '2014-05-14 14:26:54.313', '1d4c4cc3-161f-42a3-9c9c-5a344ce7660e', 50, 120, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Running an R script programmatically', 151, '2014-05-14 14:26:54.313', '1d4c4cc3-161f-42a3-9c9c-5a344ce7660e', 50, 121, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><databases><efficiency><tools>', 151, '2014-05-14 14:26:54.313', '1d4c4cc3-161f-42a3-9c9c-5a344ce7660e', 50, 122, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":62,"DisplayName":"AsheeshR"},{"Id":115,"DisplayName":"Johnny000"},{"Id":118,"DisplayName":"Konstantin V. Salikhov"},{"Id":148,"DisplayName":"ProgramFOX"},{"Id":108,"DisplayName":"rapaio"}]}', -1, '2014-05-14 14:40:25.950', '3251bee5-5c96-43ef-bae7-9dd14bd950b1', 5, '102', 123, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('> How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning?

You can set up a cronjob on a Linux system. These are run at the set time, if the computer is on. To do so, open a terminal and type:

    crontab -e

And add:

    00 4 * * * /your/command

Source: [Stack Overflow][1]


  [1]: http://stackoverflow.com/a/14710307/1488917', 62, '2014-05-14 14:48:32.180', '8d8451a2-7e08-488e-98ea-20a3d783ce49', 51, 124, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('> How can I ask my computer to run this every night at 4 am so that I have an up to date report waiting for me in the morning?

You can set up a cronjob on a Linux system. These are run at the set time, if the computer is on. To do so, open a terminal and type:

    crontab -e

And add:

    00 4 * * * r source(/home/FilePath/.../myRscript.R)

Source: [Stack Overflow][1]


  [1]: http://stackoverflow.com/a/14710307/1488917', 151, '2014-05-14 14:57:32.243', 'f12fc059-e868-446f-a45f-dd6eb3bc05e6', 51, 'more explicit code for running an R script rather than just any command', 125, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-14 14:57:32.243', 'f12fc059-e868-446f-a45f-dd6eb3bc05e6', 51, 'Proposed by 151 approved by 62 edit id of 8', 126, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From my limited dabbling with data science using R, I realized that cleaning bad data is a very important part of preparing data for analysis.

Are there any best practices or processes for cleaning data before processing it? If so, are there any automated or semi-automated tools which implement some of these best practices?', 157, '2014-05-14 15:25:21.700', 'c45e9098-e4e9-42bc-a3a7-96197b0e0e4a', 52, 127, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Organized processes to clean data', 157, '2014-05-14 15:25:21.700', 'c45e9098-e4e9-42bc-a3a7-96197b0e0e4a', 52, 128, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-cleaning>', 157, '2014-05-14 15:25:21.700', 'c45e9098-e4e9-42bc-a3a7-96197b0e0e4a', 52, 129, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For windows, use the task scheduler to set the task to run for example daily at 4:00 AM

It gives you many other options regarding frequency etc.
http://en.wikipedia.org/wiki/Windows_Task_Scheduler', 116, '2014-05-14 15:42:02.393', '221a9386-1dd1-4d0a-9f44-cd5318094c6c', 53, 130, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From my point of view, this question is suitable for a two-step answer. The first part, let us call it *soft preprocessing*, could be taken as the usage of different data mining algorithms to preprocess data in such a way that makes it suitable for further analyses. Notice that this could be the analysis itself, in case the goal is simple enough to be tackled in a single shot.

The second part, the *hard preprocessing*, actually comes prior to any other process, and is may be taken as the usage of simple tools or scripts to clean up data, selecting specific contents to be processed. To this problem, POSIX provides us with a wonderous set of magic tools, which can be used to compose concise -- and very powerful -- preprocessing scripts.

For example, for people who deal with data coming from social websites (twitter, facebook, ...), the *data retrieval* usually yields files with very specific format -- although not always nicely structure, as they may contain missing fields, and so. For these cases, a simple `awk` script could clean up the data, producing a *valid* input file for later processing. From the magic set, one may also point out `grep`, `sed`, `cut`, `join`, `paste`, `sort`, and a whole multitude of other tools.

In case simple the source file has too many nitty-gritties, it may also be necessary to produce a bundle of methods to clean up data. In such cases, it is usually better to use scripting languages (other than shell ones), such as Python, Ruby, and Perl. This allows for building up *API*''s to select specific data in a very straightforward and reusable way. Such *API*''s are sometimes made public by their writers, such as [IMDbPY](http://imdbpy.sourceforge.net/), [Stack Exchange API](https://api.stackexchange.com/docs), and many others.

So, answering the question: are there any best practices? It usually depends on your task. If you will always deal with the same data format, it''s commonly best to write an *organized* script to preprocess it; whereas, if you just need a simple and fast clean up on some dataset, count on POSIX tools for concise shell scripts that will do the whole job **much** faster than a Python script, or so. Since the *clean up* depends both on the dataset and on your purposes, it''s hard to have everything already done. Yet, there are lots of API''s that puts you halfway through with the problem.', 84, '2014-05-14 16:29:39.927', '957647d9-0ef5-40f4-aaca-1e4ff9a2f9bb', 57, 140, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can check out [vowpal wabbit][1]. It is quite popular for large-scale learning and includes parallel provisions.

From their website:

> VW is the essence of speed in machine learning, able to learn from terafeature datasets with ease. Via parallel learning, it can exceed the throughput of any single machine network interface when doing linear learning, a first amongst learning algorithms.


  [1]: http://hunch.net/~vw/', 119, '2014-05-14 17:06:33.337', '2c437422-f2f7-4e70-9bdb-22bd10e05265', 58, 145, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In reviewing [Applied Predictive Modeling][1]" a [reviewer states][2]:

> One critique I have of statistical learning (SL) pedagogy is the
> absence of computation performance considerations in the evaluation of
> different modeling techniques. With its emphases on bootstrapping and
> cross-validation to tune/test models, SL is quite compute-intensive.
> Add to that the re-sampling that''s embedded in techniques like bagging
> and boosting, and you have the specter of computation hell for
> supervised learning of large data sets. **In fact, R''s memory
> constraints impose pretty severe limits on the size of models that can
> be fit by top-performing methods like random forests.** Though SL does a
> good job calibrating model performance against small data sets, it''d
> sure be nice to understand performance versus computational cost for
> larger data.

What are R''s memory constraints, and do they impose severe limits on the size of models that can be fit by top-performing methods like [random forests][3]?


  [1]: http://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485
  [2]: http://www.information-management.com/blogs/applied-predictive-modeling-10024771-1.html
  [3]: http://en.wikipedia.org/wiki/Random_forest', 158, '2014-05-14 17:48:21.240', 'c8666588-5a3b-4d49-ba50-3258dc13338e', 59, 147, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are R''s memory constraints?', 158, '2014-05-14 17:48:21.240', 'c8666588-5a3b-4d49-ba50-3258dc13338e', 59, 148, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r>', 158, '2014-05-14 17:48:21.240', 'c8666588-5a3b-4d49-ba50-3258dc13338e', 59, 149, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**Hadoop is not a database**, hadoop runs mapreduce jobs while splitting big datasets in some little chunks of data and spread them over a cluster of nodes to get proceed. In the end the result from each node will be put together again as one dataset.

i.e. you load into hadoop a set of `<String, Integer>` with the population of some cities and you want to get the average population over the whole dataset.

    [new york, 40394]
    [new york, 134]
    [la, 44]
    [la, 647]
    ...

Now hadoop will first map and reduce by using the keys (in our example the cities) to a new set of integers:

    [new york, [40394,134]]
    [la, [44,647]]
    ...

after this task is completed it will just get like in SQL the average (sum/length) of the interger set for each key:

    [new york, [20264]]
    [la, [346]]
    ...

now hadoop would be done with everything. You can now load the result into the HDFS (hadoop file system) or into any DBMS or file.

Thats just one **very basic** and **simple** example of what hadoop can do. You can run much more complicated tasks in hadoop.

As you already mentioned in your question, hadoop and noSQL are complementary. I know a few setups where i.e. billions of datasets from sensors are stored in HBase and get then through hadoop to finally be stored in a DBMS.', 115, '2014-05-14 17:51:58.163', '09e1df24-dfc9-47ee-9ea8-b3ea35f5ab6b', 43, 'added 1 character in body', 150, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R performs all computation in-memory so you can''t perform operation on a dataset that is larger than available RAM amount. However there are some libraries that allow bigdata processing using R and one of popular libraries for bigdata processing like Hadoop.', 118, '2014-05-14 17:58:48.297', '9d921d53-259d-4dcc-aebe-f8dacb8a8f29', 60, 151, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Logic often states that by overfitting a model, it''s capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regardless of the complexity of data, and if so, why?', 158, '2014-05-14 18:09:01.940', 'ab1149f0-cc8b-41da-921e-d6f9ea709f6f', 61, 153, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why Is Overfitting Bad?', 158, '2014-05-14 18:09:01.940', 'ab1149f0-cc8b-41da-921e-d6f9ea709f6f', 61, 154, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 158, '2014-05-14 18:09:01.940', 'ab1149f0-cc8b-41da-921e-d6f9ea709f6f', 61, 155, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Overfitting is *empirically* bad.  Suppose you have a data set which you split in two (test and training).  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on a test dataset than models which are not overfitted.

One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).

Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal.', 26, '2014-05-14 18:27:56.043', '5ddffccf-c5fd-48df-870b-172aba4263bf', 62, 156, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Total amount of data in the world: 2.8 zetabytes in 2012, estimated to reach 8 zetabytes by 2015 ([source][1]) and with a doubling time of 40 months. Can''t get bigger than that :)

As an example of a single large organization, Facebook pulls in 500 terabytes per day, into a 100 petabyte warehouse, and runs 70k queries per day on it as of 2012 ([source][2])  Their current warehouse is >300 petabytes.

Big data is probably something that is a good fraction of the Facebook numbers (1/100 probably yes, 1/10000 probably not: it''s a spectrum not a single number).

In addition to size, some of the features that make it "big" are:

- it is actively analyzed, not just stored  (quote "If you arent taking advantage of big data, then you dont have big data, you have just a pile of data" Jay Parikh @ Facebook)

- building and running a data warehouse is a major infrastructure project

- it is growing at a significant rate

- it is unstructured or has irregular structure

Gartner definition: "Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing" (The 3Vs)  So they also think "bigness" isn''t entirely about the size of the dataset, but also about the velocity and structure and the kind of tools needed.


  [1]: http://siliconangle.com/blog/2012/05/21/when-will-the-world-reach-8-zetabytes-of-stored-data-infographic/
  [2]: http://gigaom.com/2012/08/22/facebook-is-collecting-your-data-500-terabytes-a-day/', 26, '2014-05-14 18:30:59.180', '8bb31230-592a-4a83-991a-caf55c0e6fc6', 30, 'deleted 24 characters in body', 157, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Overfitting, in a nutshell, means take into account **too much** information from your data and/or prior knowledge, and use it in a model. To make it more straightforward, consider the following example: you''re hired by some scientists to provide them with a model to predict the growth of some kind of plants. The scientists have given you information collected from their work
with such plants throughout a whole year, and they shall continuously give you information on the future development of their plantation.

So, you run through the data received, and build up a model out of it. Now suppose that, in your model, you considered just as many characteristics as possible to always find out the exact behavior of the plants you saw in the initial dataset. Now, as the production continues, you''ll always take into account those characteristics, and will produce very *fine-grained* results. However, if the plantation eventually suffer from some seasonal change, the results you will receive may fit your model in such a way that your predictions will begin to fail (either saying that the growth will slow down, while it shall actually speed up, or the opposite).

Apart from being unable to detect such small variations, and to usually classify your entries incorrectly, the *fine-grain* on the model, i.e., the great amount of variables, may cause the processing to be too costly. Now, imagine that your data is already complex. Overfitting your model to the data not only will make the classification/evaluation very complex, but will most probably make you error the prediction over the slightest variation you may have on the input.
', 84, '2014-05-14 18:37:52.333', '5a086527-e70c-4061-8f73-d2705614e26f', 64, 161, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Overfitting is *empirically* bad.  Suppose you have a data set which you split in two (test and training).  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on a test dataset than models which are not overfitted.

One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).  The difference in performance between training and test data indicates how much noise the model picks up; and picking up noise directly translates into worse performance on test data.

Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal.

P.S. On the "ability to generalize" part of the question, it is very possible to have a model which has inherently limited ability to generalize due to the structure of the model (for example linear SVM, ...) but is still prone to overfitting.  In a sense overfitting is just one way that generalization may fail.', 26, '2014-05-14 18:39:59.027', '6e07951a-1131-47e6-b5a7-1220f6f4bdd1', 62, 'added 305 characters in body', 162, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-14 18:45:23.917', 'e762ca7b-c543-42a2-a6aa-20e2d60079ed', 65, 163, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-14 18:45:23.917', '8de39075-4f35-4ca3-ae76-448c5284d426', 66, 164, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-14 18:48:42.263', '34fedb95-2b78-4ed8-9e01-093ec98e1e53', 67, 165, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-14 18:48:42.263', '3696c1bf-49a3-4f38-90a2-2268b13ccf89', 68, 166, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Overfitting is *empirically* bad.  Suppose you have a data set which you split in two (test and training).  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on a test dataset than models which are not overfitted.

One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).  The difference in performance between training and test data indicates how much noise the model picks up; and picking up noise directly translates into worse performance on test data (including future data).

Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal.

P.S. On the "ability to generalize" part of the question, it is very possible to have a model which has inherently limited ability to generalize due to the structure of the model (for example linear SVM, ...) but is still prone to overfitting.  In a sense overfitting is just one way that generalization may fail.', 26, '2014-05-14 19:46:32.213', '15678292-7405-4bee-b0f0-9162a7ab5982', 62, 'added 24 characters in body', 170, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Putting aside how easy it might be to replicate a given build, especially an ad-hoc one, to me replication always possible if it''s planned for and worth doing. That said, it is unclear to me is how to execute a data science workflow that allows for reproducibility without it just being a replication of what has already been done. How do you define the difference between replication and reproducibility, and how to you execute a data science workflow that allows for reproducibility without simply being a replication of what has already been done?', 158, '2014-05-14 20:03:15.233', 'b26e1d4c-681c-4c18-8216-4e7ac96d298d', 69, 171, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is a reproducibile data science workflow?', 158, '2014-05-14 20:03:15.233', 'b26e1d4c-681c-4c18-8216-4e7ac96d298d', 69, 172, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<processing>', 158, '2014-05-14 20:03:15.233', 'b26e1d4c-681c-4c18-8216-4e7ac96d298d', 69, 173, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Note that there is an early version of LIBLINEAR ported to [Apache Spark][1]. See [mailing list comments][2] for some early details, and the [project site][3].


  [1]: http://spark.apache.org
  [2]: http://apache-spark-user-list.1001560.n3.nabble.com/Spark-LIBLINEAR-td5546.html
  [3]: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/distributed-liblinear/', 21, '2014-05-14 21:03:05.313', 'ffcfb3f1-b78e-425d-bff6-ed9f0147dcb0', 46, 'added 162 characters in body', 174, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Putting aside how easy it might be to replicate a given build, especially an ad-hoc one, to me replication always possible if it''s planned for and worth doing. That said, it is unclear to me is how to execute a data science workflow that allows for reproducibility without it just being a replication of what has already been done. How do you define the difference between replication and reproducibility, and how to you execute a data science workflow that allows for reproducibility without simply being a replication of what has already been done?

**UPDATE:** Possible my use of the terms is my own, though I view replication as an exact copy that''s a result of having access to all inputs & processes required to reach the same outcome. Replication to me means that the concepts related to how the input was collected & processed is known, but independently achieves an outcome that expresses an identental outcome semantically speaking.', 158, '2014-05-14 21:34:19.977', 'e1e5b55f-8480-4e5e-b391-eaf60d40577b', 69, 'added 394 characters in body', 175, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To be reproducible without being just a replication, you would need to redo the experiment with new data, following the same technique as before.  The work flow is not as important as the techniques used.  Sample data in the same way, use the same type of models.  It doesn''t matter if you switch from one language to another, so long as the models and the data manipulations are the same.

This type of replication will show that the results you got in the first experiment are less likely to be a fluke than they were earlier.', 178, '2014-05-14 22:03:50.597', '9940efa6-419d-480c-97e5-83b114d7fa59', 70, 176, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the data conditions that we should watch out for, where p-values may not be the best way of deciding statistical significance?  Are there specific problem types that fall into this category?', 179, '2014-05-14 22:12:37.203', 'e02a35c8-d701-4aab-ac60-943550b81965', 71, 177, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When are p-values deceptive?', 179, '2014-05-14 22:12:37.203', 'e02a35c8-d701-4aab-ac60-943550b81965', 71, 178, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><statistics>', 179, '2014-05-14 22:12:37.203', 'e02a35c8-d701-4aab-ac60-943550b81965', 71, 179, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Is it possible to automate generating reproducibility documentation?', 158, '2014-05-14 22:19:52.970', 'a3106baa-327f-4430-b6c0-280ebdeeb0b1', 69, 'edited title', 180, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What is the difference between Hadoop and noSQL', 134, '2014-05-14 22:26:59.453', 'b461a733-4231-456a-aa4f-bfd23207b2f7', 38, 'edited title', 181, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What is a reproducibile data science workflow?', 158, '2014-05-14 22:28:50.800', 'b9210a50-45d3-4029-b2c2-5cd4677be88b', 69, 'Rollback to [e1e5b55f-8480-4e5e-b391-eaf60d40577b]', 182, '7');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One algorithm that can be used for this is the [k-means clustering algorithm](http://en.wikipedia.org/wiki/K-means_clustering).

Basically:

1. Randomly choose k datapoints from your set, m_1, ..., m_k.
2. "Until convergence":

    1. Assign your data points to k clusters, where cluster i is the set of points for which m_i is the closest of your current means
    2. Replace each m_i by the mean of all points assigned to cluster i.

It is good practice to repeat this algorithm several times, then choose the outcome that minimizes distances between the points of each cluster i and the center m_i.

Of course, you have to know k to start here; you can use cross-validation to choose this parameter, though.', 22, '2014-05-14 22:40:40.363', 'dfebb0af-4088-4bec-8dcb-52266bfe44b2', 72, 183, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You shouldn''t consider the p-value out of context.

One rather basic point (as illustrated by [xkcd][1]) is that you need to consider how many tests you''re actually doing.  Obviously, you shouldn''t be shocked to see p < 0.05 for one out of 20 tests, even if the null hypothesis is true every time.

A more subtle example of this occurs in high-energy physics, and is known as the [look-elsewhere effect][2].  The larger the parameter space you search for a signal that might represent a new particle, the more likely you are to see an apparent signal that''s really just due to random fluctuations.

  [1]: http://xkcd.com/882/
  [2]: https://en.wikipedia.org/wiki/Look-elsewhere_effect', 14, '2014-05-14 22:43:23.587', 'c999b644-b087-4c34-8536-c9584f41951f', 73, 184, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One thing you should be aware of is the sample size you are using. Very large samples, such as economists using census data, will lead to deflated p-values. This paper ["Too Big to Fail: Large Samples and the p-Value Problem"][1] covers some of the issues.


  [1]: http://galitshmueli.com/system/files/Print%20Version.pdf "Too Big to Fail: Large Samples and the p-Value Problem"', 64, '2014-05-14 22:58:11.583', 'dfd37513-608e-4c4d-99f8-929c1c3c80f6', 74, 185, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If small p-values are plentiful in big data, what is a comparable replacement for p-values in data with million of samples?', 158, '2014-05-15 00:26:11.387', 'c39875a3-fbd2-4329-b31c-f9eeab2b8195', 75, 187, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is there a replacement for small p-values in big data?', 158, '2014-05-15 00:26:11.387', 'c39875a3-fbd2-4329-b31c-f9eeab2b8195', 75, 188, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics>', 158, '2014-05-15 00:26:11.387', 'c39875a3-fbd2-4329-b31c-f9eeab2b8195', 75, 189, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('*(Note: Pulled this question from the [list of questions in Area51](http://area51.stackexchange.com/proposals/55053/data-science/57398#57398), but believe the question is self explanatory. That said, believe I get the general intent of the question, and as a result likely able to field any questions on the question that might pop-up.)*

**Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?**', 158, '2014-05-15 00:39:33.433', '7d923163-3138-4eba-9910-fd9681c60547', 76, 190, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which Big Data technology stack is most suitable for processing tweets, extracting/expanding URLs and pushing (only) new links into 3rd party system?', 158, '2014-05-15 00:39:33.433', '7d923163-3138-4eba-9910-fd9681c60547', 76, 191, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 158, '2014-05-15 00:39:33.433', '7d923163-3138-4eba-9910-fd9681c60547', 76, 192, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Background:** Following is from the book [Graph Databases][1], which covers a performance test mentioned in the book [Neo4j in Action][2]:

> Relationships in a graph naturally form paths. Querying, or
> traversing, the graph involves following paths. Because of the
> fundamentally path-oriented nature of the datamodel, the majority of
> path-based graph database operations are highly aligned with the way
> in which the data is laid out, making them extremely efficient. In
> their book Neo4j in Action, Partner and Vukotic perform an experiment
> using a relational store and Neo4j.
>
> The comparison shows that the graph database is substantially quicker
> for connected data than a relational store.Partner and Vukotics
> experiment seeks to find friends-of-friends in a social network, to a
> maximum depth of five. Given any two persons chosen at random, is
> there a path that connects them which is at most five relationships
> long? For a social network containing 1,000,000 people, each with
> approximately 50 friends, the results strongly suggest that graph
> databases are the best choice for connected data, as we see in Table
> 2-1.
>
Table 2-1. Finding extended friends in a relational database versus efficient finding in Neo4j

>     Depth   RDBMS Execution time (s)    Neo4j Execution time (s)     Records returned
>     2       0.016                       0.01                        ~2500
>     3       30.267                      0.168                        ~110,000
>     4       1543.505                   1.359                        ~600,000
>     5       Unfinished               2.132                        ~800,000
At depth two (friends-of-friends) both the relational database and the graph database perform well enough for us to consider using them in an online system. While the Neo4j query runs in two-thirds the time of the relational one, an end-user would barely notice the the difference in milliseconds between the two. By the time we reach depth three (friend-of-friend-of-friend), however, its clear that the relational database can no longer deal with the query in a reasonable timeframe: the thirty seconds it takes to complete would be completely unacceptable for an online system. In contrast, Neo4js response time remains relatively flat: just a fraction of a second to perform the querydefinitely quick enough for an online system.
>
> At depth four the relational database exhibits crippling latency,
> making it practically useless for an online system. Neo4js timings
> have deteriorated a little too, but the latency here is at the
> periphery of being acceptable for a responsive online system. Finally,
> at depth five, the relational database simply takes too long to
> complete the query. Neo4j, in contrast, returns a result in around two
> seconds. At depth five, it transpires almost the entire network is our
> friend: for many real-world use cases, wed likely trim the results,
> and the timings.

**Questions are:**

 - Is this a reasonable test to emulate what one might except to find in a social network? *(Meaning do real social networks normally have nodes with approximately 50 friends for example; seems like the "[rich get richer][3]" model would be more natural for social networks, though might be wrong.)*
 - Regardless of the naturalness of the emulation, is there any reason to believe the results are off, or unreproducible?


  [1]: http://www.amazon.com/Graph-Databases-Ian-Robinson/dp/1449356265
  [2]: http://www.amazon.com/Neo4j-Action-Jonas-Partner/dp/1617290769/
  [3]: http://en.wikipedia.org/wiki/The_rich_get_richer_%28statistics%29', 158, '2014-05-15 01:22:35.167', '2b33043e-0e40-47ad-b826-df1b08fd70d8', 77, 193, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is this Neo4j comparison to RDBMS execution time correct?', 158, '2014-05-15 01:22:35.167', '2b33043e-0e40-47ad-b826-df1b08fd70d8', 77, 194, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<databases><nosql>', 158, '2014-05-15 01:22:35.167', '2b33043e-0e40-47ad-b826-df1b08fd70d8', 77, 195, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Is it possible to automate generating reproducibility documentation?', 158, '2014-05-15 01:43:13.557', '037b9d5c-041b-4b57-8190-ddad79760f46', 69, 'Rollback to [a3106baa-327f-4430-b6c0-280ebdeeb0b1]', 196, '7');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is no replacement in the strict sense of the word.  Instead you should look at other measures.

The other measures you look at depend on what you type of problem you are solving.  In general, if you have a small p-value, also consider the magnitude of the effect size.  It may be highly statistically significant but in practice meaningless.  It is also helpful to report the confidence interval of the effect size.

I would consider [this paper](http://galitshmueli.com/system/files/Print%20Version.pdf) as mentoned in DanC''s answer to [this question](http://datascience.stackexchange.com/questions/71/when-are-p-values-deceptive).', 178, '2014-05-15 01:46:28.467', 'ab847ef5-c38c-48de-94f6-1bb15f515356', 78, 197, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('First, think it''s worth me stating what I mean by replication & reproducibility:

- Replication of analysis A results in an exact copy of all inputs and processes that are supply and result in incidental outputs in analysis B.
- Reproducibility of analysis A results in inputs, processes, and outputs that are semantically incidental to analysis A, without access to the exact inputs and processes.

Putting aside how easy it might be to replicate a given build, especially an ad-hoc one, to me replication always possible if it''s planned for and worth doing. That said, it is unclear to me is how to execute a data science workflow that allows for reproducibility.

The closet comparison I''m able to think of is [documentation generators][1] that generates software documentation intended for programmers - though the main difference I see is that in theory, if two sets of analysis ran the "reproducibility documentation generators" the documentation should match.

Another issue, is that while I get the concept of reproducibility documentation, I am having a hard time imagining what it would look like in usable form without just being a guide to replicating the analysis.

Lastly, whole intent of this is to understand if it''s possible to "bake-in" reproducibility documentation as you build out a stack, not after the stack is built.

So, Is it possible to automate generating reproducibility documentation, and if so how, and what would it look like?


----------


***UPDATE:** Please note that this is the second draft of this question and that [Christopher Louden][2] was kind enough to let me edit the question after I realized it was likely the first draft was unclear. Thanks!*


  [1]: http://en.wikipedia.org/wiki/Documentation_generator
  [2]: http://datascience.stackexchange.com/users/178/christopher-louden', 158, '2014-05-15 02:02:08.010', '63d6e665-76ae-4475-b731-6c89c2c9889b', 69, 'added 900 characters in body', 198, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Use liblinear on big data for semantic analysis', 63, '2014-05-15 02:41:27.063', 'c09c6eba-b8ad-452a-bce4-0efaf3f1c8d0', 16, 'edited title', 200, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-15 03:19:40.360', '5dfc7d33-d8a0-4c02-afd0-33a6254c96d5', 79, 201, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-15 03:19:40.360', '2c2e2962-b018-4361-a69d-c4453d4c18a6', 80, 202, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is(are) the difference(s) between parallel and distributed computing? When it comes to scalability and efficiency, it is very common to see solutions dealing with computations in clusters of machines, and sometimes it is referred to as a parallel processing, or as distributed processing.

In a certain way, the computation seems to be always parallel, since there are things running concurrently. But is the distributed computation simply related to the use of more than one machine, or are there any further specificities that distinguishes these two kinds of processing? Wouldn''t it be redundant to say, for example, that a computation is *parallel AND distributed*?', 84, '2014-05-15 04:59:54.317', 'b78b5118-181c-44a3-8643-ef88723e8712', 81, 203, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Parallel and distributed computing', 84, '2014-05-15 04:59:54.317', 'b78b5118-181c-44a3-8643-ef88723e8712', 81, 204, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><scalability><efficiency><parallelism><distributed>', 84, '2014-05-15 04:59:54.317', 'b78b5118-181c-44a3-8643-ef88723e8712', 81, 205, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Simply set, ''parallel'' means running concurrently on distinct resources (CPUs), while ''distributed'' means running accross distinct computers, involving issues related to networks.

Parallel computing using for instance [OpenMP][1] is not distributed, while parallel computing with [Message Passing][2] is often distributed.

Being in a ''distributed but not parallel'' setting would mean under-using resources so it is seldom encoutered.

  [1]: http://en.wikipedia.org/wiki/OpenMP
  [2]: http://en.wikipedia.org/wiki/Message_Passing_Interface', 172, '2014-05-15 05:19:34.757', 'c91ddd3c-baef-4666-aefc-50a3b3905fa2', 82, 206, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Simply set, ''parallel'' means running concurrently on distinct resources (CPUs), while ''distributed'' means running across distinct computers, involving issues related to networks.

Parallel computing using for instance [OpenMP][1] is not distributed, while parallel computing with [Message Passing][2] is often distributed.

Being in a ''distributed but not parallel'' setting would mean under-using resources so it is seldom encountered but it is conceptually possible.

  [1]: http://en.wikipedia.org/wiki/OpenMP
  [2]: http://en.wikipedia.org/wiki/Message_Passing_Interface', 172, '2014-05-15 05:25:39.970', 'bd188207-62ec-4231-8217-c7457e73ad4d', 82, 'added 32 characters in body', 207, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a bunch of customer profiles stored in a [tag:elasticsearch] cluster. These profiles are now used for creation of target groups for our email subscriptions.

Target groups are now formed manually using elasticsearch faceted search capabilities (like  get all male customers of age 23 with one car and 3 children).

How could I search for interesting groups **automatically** - using data science, machine learning, clustering or something else?

[tag:R] programming language seems to be a good tool for this task, but I can''t form a methodology of such group search. One solution is to somehow find the largest clusters of customers and use them as target groups, so the question is:

**How can I automatically choose largest clusters of similar customers (similar by parameters that I don''t know at this moment)?**

For example: my program will connect to elasticsearch, offload customer data to CSV and using R language script will find that large portion of customers are male with no children and another large portion of customers have a car and their eye color is brown.
', 24, '2014-05-15 05:49:39.140', '2f7af5bc-6e56-47a6-acc3-3563efb7c185', 31, 'Just tightening things up a bit to improve readability', 209, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-15 05:49:39.140', '2f7af5bc-6e56-47a6-acc3-3563efb7c185', 31, 'Proposed by 24 approved by 118 edit id of 12', 210, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I posted a pretty detailed answer on stackoverflow about when it is appropriate to use relational vs document (or NoSQL) database: http://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767

Summary:

- for small stuff, go with whatever tools you are familiar with

- a few gigabytes is definitely small stuff: it doesn''t get big until it is too big to fit in a single [MySQL Cluster][1] with a reasonable number of nodes (16-32), which means maybe 8-16TB data and a few million transactions per second.

- if you''re stuck with another database (not MySQL Cluster), get more mileage out of it by throwing in FusionIO hardware.

- once you have data larger than a few TB, it is a good time to look at moving to logical sharding in the application code first and then to NoSQL.

- Cassandra :)

  [1]: http://www.mysql.com/products/cluster/scalability.html', 26, '2014-05-15 07:47:44.710', '9b451f8f-1c75-4ceb-beba-199b5ecb55c3', 83, 211, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I posted a pretty detailed answer on stackoverflow about when it is appropriate to use relational vs document (or NoSQL) database, here:

**[Motivations for using relational database / ORM or document database / ODM][1]**

Summary:

- for small stuff, go with whatever tools you are familiar with

- a few gigabytes is definitely small stuff: it doesn''t get big until it is too big to fit in a single [MySQL Cluster][2] with a reasonable number of nodes (16-32), which means maybe 8-16TB data and a few million transactions per second (or a more conventional hard-drive-based database with up to 100''s of TB data and a few thousand transactions per second).

- if you''re stuck with another database (not MySQL Cluster), get more mileage out of it by throwing in FusionIO hardware.

- once you have data larger than a few TB and faster than thousands of transactions per second, it is a good time to look at moving to logical sharding in the application code first and then to NoSQL.

- [Cassandra][3] :)


  [1]: http://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767
  [2]: http://www.mysql.com/products/cluster/scalability.html
  [3]: http://cassandra.apache.org/', 26, '2014-05-15 07:53:40.170', 'a7bc5315-02ff-4cab-b382-bd2921d1053f', 83, 'added 147 characters in body', 212, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I posted a pretty detailed answer on stackoverflow about when it is appropriate to use relational vs document (or NoSQL) database, here:

**[Motivations for using relational database / ORM or document database / ODM][1]**

Summary:

- for small stuff, go with whatever tools you are familiar with

- a few gigabytes is definitely small stuff: it doesn''t get big until it is too big to fit in a single [MySQL Cluster][2] with a reasonable number of nodes (16-32), which means maybe 8-16TB data and a few million transactions per second (or a more conventional hard-drive-based database with up to 100''s of TB data and a few thousand transactions per second).

- if you''re stuck with another database (not MySQL Cluster), get more mileage out of it by throwing in FusionIO hardware.

- once you have data larger than a few TB *and* faster than thousands of transactions per second, it is a good time to look at moving to logical sharding in the application code first and then to NoSQL.

- [Cassandra][3] :)


  [1]: http://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767
  [2]: http://www.mysql.com/products/cluster/scalability.html
  [3]: http://cassandra.apache.org/', 26, '2014-05-15 07:59:05.497', 'd7deb5de-5a20-4dd9-a23b-245049feb89a', 83, 'added 2 characters in body', 213, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You are asking about [Data Dredging][1], which is what happens when testing a very large number of hypotheses against a data set, or testing hypotheses against a data set that were suggested by the same data.

In particular, check out [Multiple hypothesis hazard][2], and [Testing hypotheses suggested by the data][3].

The solution is to use some kind of correction for [False discovery rate][4] or [Familywise error rate][5], such as [Scheffé''s method][6] or the (very old-school) [Bonferroni correction][7].


  [1]: http://en.wikipedia.org/wiki/Data_dredging
  [2]: http://en.wikipedia.org/wiki/Multiple_comparisons
  [3]: http://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data
  [4]: http://en.wikipedia.org/wiki/False_discovery_rate
  [5]: http://en.wikipedia.org/wiki/Familywise_error_rate
  [6]: http://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method
  [7]: http://en.wikipedia.org/wiki/Bonferroni_correction', 26, '2014-05-15 08:19:40.577', '05ca6d96-2075-4781-a0de-6a2a2fd6d085', 84, 214, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You are asking about [Data Dredging][1], which is what happens when testing a very large number of hypotheses against a data set, or testing hypotheses against a data set that were suggested by the same data.

In particular, check out [Multiple hypothesis hazard][2], and [Testing hypotheses suggested by the data][3].

The solution is to use some kind of correction for [False discovery rate][4] or [Familywise error rate][5], such as [Scheffé''s method][6] or the (very old-school) [Bonferroni correction][7].

In a somewhat less rigorous way, it may help to filter your discoveries by the confidence interval for the odds ratio (OR) for each statistical result.  If the 99% confidence interval for the odds ratio is 10-12, then the OR is <= 1 with some *extremely* small probability, especially if the sample size is also large.  If you find something like this, it is probably a strong effect even if it came out of a test of millions of hypotheses.

  [1]: http://en.wikipedia.org/wiki/Data_dredging
  [2]: http://en.wikipedia.org/wiki/Multiple_comparisons
  [3]: http://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data
  [4]: http://en.wikipedia.org/wiki/False_discovery_rate
  [5]: http://en.wikipedia.org/wiki/Familywise_error_rate
  [6]: http://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method
  [7]: http://en.wikipedia.org/wiki/Bonferroni_correction', 26, '2014-05-15 08:25:47.933', '2279680a-d93e-489a-8ceb-3b7f9a5a8e18', 84, 'added 444 characters in body', 215, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('See http://datascience.stackexchange.com/questions/71/when-are-p-values-deceptive/84#84

The replacement is to use any of the corrections for [False discovery rate][4] (to limit probability that any given discovery is false) or [Familywise error rate][5] (to limit probability of one or more false discoveries).  For example, you might use the HolmBonferroni method.

  [4]: http://en.wikipedia.org/wiki/False_discovery_rate
  [5]: http://en.wikipedia.org/wiki/Familywise_error_rate', 26, '2014-05-15 08:44:47.327', '2bae8f49-b645-4604-a778-3b22a96bf126', 85, 216, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Given website access data in the form `session_id, ip, user_agent`, and optionally timestamp, following the conditions below, how would you best cluster the sessions into unique visitors?

`session_id`: is an id given to every new visitor. It does not expire, however if the user doesn''t accept cookies/clears cookies/changes browser/changes device, he will not be recognised anymore

`IP` can be shared between different users (Imagine a free wi-fi cafe, or your ISP reassigning IPs), and they will often have at least 2, home and work.

`User_agent` is the browser+OS version, allowing to distinguish between devices. For example a user is likely to use both phone and laptop, but is unlikely to use windows+apple laptops. It is unlikely that the same session id has multiple useragents.

Data might look as the fiddle here:
http://sqlfiddle.com/#!2/c4de40/1

Of course, we are talking about assumptions, but it''s about getting as close to reality as possible. For example, if we encounter the same ip and useragent in a limited time frame with a different session_id, it would be a fair assumption that it''s the same user, with some edge case exceptions.

Edit: due to the slow nature of the fiddle, you can alternatively read/run the mysql:

    select session_id, floor(rand()*256*256*256*256) as ip_num , floor(rand()*1000) as user_agent_id
    from
     (select 1+a.nr+10*b.nr as session_id, ceil(rand()*3) as nr
     from
      (select 1 as nr union all select 2 union all select 3 union all select 4 union all select 5
      union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)a
     join
      (select 1 as nr union all select 2 union all select 3 union all select 4 union all select 5
      union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)b
      order by 1
     )d
    inner join
     (select 1 as nr union all select 2 union all select 3 union all select 4 union all select 5
     union all select 6 union all select 7 union all select 8 union all select 9 )e
     on d.nr>=e.nr', 116, '2014-05-15 09:04:09.710', 'c95e75b7-866f-493a-99ed-183c6aa16afb', 86, 219, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Clustering unique visitors by useragent, ip, session_id', 116, '2014-05-15 09:04:09.710', 'c95e75b7-866f-493a-99ed-183c6aa16afb', 86, 220, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering>', 116, '2014-05-15 09:04:09.710', 'c95e75b7-866f-493a-99ed-183c6aa16afb', 86, 221, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Looking at this document called [Anatomy of Facebook][1] I note that the median is 100. Looking at the cumulative function plot I can bet that the average is higher, near 200. So 50 seems to not be the best number here. However I think that this is not the main issue here.

The main issue is the lack of information on how the database was used.

It seems reasonable that a data storage designed specially for graph structures to be more efficient than traditional RDBMs. However, even if the RDBMs are not in the latest trends as a data storage of choice, these systems evolved continuously in a race with the data set dimensions. There are various types of possible designs, various ways of indexing data, improvements related with concurrency and so on.

To conclude I think that regarding reproducibility, the study lack a proper description of how the database schema was designed. I do not expect that a database to dominate on such king of interrogations, however I would expect that with a well-tuned design the differences to not be such massive.


  [1]: https://www.facebook.com/notes/facebook-data-team/anatomy-of-facebook/10150388519243859', 108, '2014-05-15 09:30:36.460', '6b491179-5055-4612-a407-0d2bc4c24d1f', 87, 222, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<definitions><parallelism><distributed>', 118, '2014-05-15 09:31:51.370', '33469ce8-85a3-473b-8e05-b629e511ba29', 81, 'retagged question', 223, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-15 09:31:51.370', '33469ce8-85a3-473b-8e05-b629e511ba29', 81, 'Proposed by 118 approved by 84 edit id of 15', 224, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Given website access data in the form `session_id, ip, user_agent`, and optionally timestamp, following the conditions below, how would you best cluster the sessions into unique visitors?

`session_id`: is an id given to every new visitor. It does not expire, however if the user doesn''t accept cookies/clears cookies/changes browser/changes device, he will not be recognised anymore

`IP` can be shared between different users (Imagine a free wi-fi cafe, or your ISP reassigning IPs), and they will often have at least 2, home and work.

`User_agent` is the browser+OS version, allowing to distinguish between devices. For example a user is likely to use both phone and laptop, but is unlikely to use windows+apple laptops. It is unlikely that the same session id has multiple useragents.

Data might look as the fiddle here:
http://sqlfiddle.com/#!2/c4de40/1

Of course, we are talking about assumptions, but it''s about getting as close to reality as possible. For example, if we encounter the same ip and useragent in a limited time frame with a different session_id, it would be a fair assumption that it''s the same user, with some edge case exceptions.

Edit: Language in which the problem is solved is irellevant, it''s mostly about logic and not implementation. Pseudocode is fine.

Edit: due to the slow nature of the fiddle, you can alternatively read/run the mysql:

    select session_id, floor(rand()*256*256*256*256) as ip_num , floor(rand()*1000) as user_agent_id
    from
     (select 1+a.nr+10*b.nr as session_id, ceil(rand()*3) as nr
     from
      (select 1 as nr union all select 2 union all select 3 union all select 4 union all select 5
      union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)a
     join
      (select 1 as nr union all select 2 union all select 3 union all select 4 union all select 5
      union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)b
      order by 1
     )d
    inner join
     (select 1 as nr union all select 2 union all select 3 union all select 4 union all select 5
     union all select 6 union all select 7 union all select 8 union all select 9 )e
     on d.nr>=e.nr', 116, '2014-05-15 10:06:06.393', '88a7b1c1-97be-4f80-adb3-ab624bf989de', 86, 'added 132 characters in body', 226, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For example, when searching something in Google, results return nigh-instantly.

I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be indexed (and results are personalized, which renders this even more infeasible)?

Moreover, wouldn''t the hardware latency in Google''s hardware be huge? Even if the data in Google were all TB/s SSDs, I imagine the hardware latency to be huge, given the sheer amount of data to process.

Does MapReduce help solve this problem?', 189, '2014-05-15 11:22:27.293', '8173f9be-7747-4ace-bed6-500f3203cdd1', 89, 228, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How do querys into huge database return with negligible latency?', 189, '2014-05-15 11:22:27.293', '8173f9be-7747-4ace-bed6-500f3203cdd1', 89, 229, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 189, '2014-05-15 11:22:27.293', '8173f9be-7747-4ace-bed6-500f3203cdd1', 89, 230, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For example, when searching something in Google, results return nigh-instantly.

I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be indexed (and results are personalized, which renders this even more infeasible)?

Moreover, wouldn''t the hardware latency in Google''s hardware be huge? Even if the data in Google were all stored in TB/s SSDs, I imagine the hardware latency to be huge, given the sheer amount of data to process.

Does MapReduce help solve this problem?', 189, '2014-05-15 11:28:39.880', 'f4a83e57-6dc1-46d3-96f0-a609feb4877d', 89, 'added 10 characters in body', 231, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The terms "parallel computing" and "distributed computing" certainly have a large overlap, but can be differentiated further. Actually, you already did this in your question, by later asking about "parallel processing" and "distributed processing".

One could consider "distributed computing" as the more general term that involves "distributed processing" as well as, for example, "distributed storage". The common term, "distributed", usually refers to some sort of [Message Passing][1] over a network, between machines that are physically separated.

The term "parallel computing" is also in the process of being further defined, e.g. by explicitly differentiating between the terms "parallel" and "concurrent", where - roughly - the first one refers [data parallelism][2] and the latter to [task parallelism][3], although there are hardly really strict and binding defintions.

So one could say that

 - "distributed processing" usually (although not necessarily) means that it also is "parallel processing"
 - "distributed computing" is more general, and also covers aspects that are not related to parallelism
 - and obviously, "parallel computing"/"parallel processing" does not imply that it is "distributed"


  [1]: http://en.wikipedia.org/wiki/Message_Passing_Interface
  [2]: http://en.wikipedia.org/wiki/Data_parallelism
  [3]: http://en.wikipedia.org/wiki/Task_parallelism', 156, '2014-05-15 11:46:38.170', 'f8c762bb-e80e-4585-b914-fa553c08b02d', 90, 232, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Well, I''m not sure if it is MapReduce that solves the problem, but it surely wouldn''t be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it *feasible* to have such low latency on queries from all these TBs of data in different machines:

1. distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);
2. caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;
3. lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.

Considering that, lets try to address your questions:

> but I imagine it infeasible for the results of every single possible query to be indexed

Yes, it would be, and actually is infeasible to have results for *every single possible query*. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number ofqueries from these `n -> inf` terms (`2^n`). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant for the user queries are the ones cached.

> wouldn''t the hardware latency in Google''s hardware be huge? Even if the data in Google were all stored in TB/s SSDs

Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing *ruling* market is money, and the investors are not interested in wasting it. So what is done?

The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of [performance](). But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.

Memory cards are *expensive* for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it''s not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending *specific geographical regions*, which allows for more *specialized* data caching, and even better response times.', 84, '2014-05-15 11:56:43.607', '2eaa478e-591b-4921-bb61-ea93d6da9415', 91, 233, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Well, I''m not sure if it is MapReduce that solves the problem, but it surely wouldn''t be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it *feasible* to have such low latency on queries from all these TBs of data in different machines:

1. distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);
2. caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;
3. lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.

Considering that, lets try to address your questions:

> but I imagine it infeasible for the results of every single possible query to be indexed

Yes, it would be, and actually is infeasible to have results for *every single possible query*. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number of queries from these `n -> inf` terms (`2^n`). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant-for-the-user queries are the ones cached.

> wouldn''t the hardware latency in Google''s hardware be huge? Even if the data in Google were all stored in TB/s SSDs

Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing *ruling* market is money, and the investors are not interested in wasting it. So what is done?

The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of [performance](http://i.stack.imgur.com/Uf6al.gif). But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.

Memory cards are *expensive* for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it''s not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending *specific geographical regions*, which allows for more *specialized* data caching, and even better response times.

  [1]: http://i.stack.imgur.com/Uf6al.gif', 84, '2014-05-15 12:02:22.820', '01a99e57-3c1d-43cb-9500-08927a563520', 91, 'added 1 character in body', 234, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Well, I''m not sure if it is MapReduce that solves the problem, but it surely wouldn''t be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it *feasible* to have such low latency on queries from all these TBs of data in different machines:

1. distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);
2. caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;
3. lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.

Considering that, lets try to address your questions:

> but I imagine it infeasible for the results of every single possible query to be indexed

Yes, it would be, and actually is infeasible to have results for *every single possible query*. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number of queries from these `n -> inf` terms (`2^n`). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant-for-the-user queries are the ones cached.

> wouldn''t the hardware latency in Google''s hardware be huge? Even if the data in Google were all stored in TB/s SSDs

Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing *ruling* market is money, and the investors are not interested in wasting it. So what is done?

The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of [performance](http://i.stack.imgur.com/Uf6al.gif). But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.

Memory cards are *expensive* for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it''s not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending *specific geographical regions*, which allows for more *specialized* data caching, and even better response times.

> Does MapReduce help solve this problem?

Although I don''t think that using or not MapReduce is restricted information inside Google, I''m not conversant about this point. However, Google''s implementation of MapReduce (which is surely *not* Hadoop) must have lots of optimizations, many involving the aspects discussed above. So, the architecture of MapReduce probably helps guiding how the computations are physically distributed, but there are many other points to be considered to justify such speed in querying time.

  [1]: http://i.stack.imgur.com/Uf6al.gif', 84, '2014-05-15 12:25:07.730', '3198603d-6c02-41a0-95e1-085468778bf5', 91, 'added 526 characters in body', 235, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<databases><nosql><neo4j>', 118, '2014-05-15 13:15:02.727', 'b12d3a4f-33fc-4888-9bee-c94c051c1c46', 77, 'added neo4j tag', 236, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-15 13:15:02.727', 'b12d3a4f-33fc-4888-9bee-c94c051c1c46', 77, 'Proposed by 118 approved by 158 edit id of 16', 237, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('MapReduce has nothing to do with real-time anything. It is a batch-oriented processing framework suitable for some offline tasks, like ETL and index building. Google has moved off of MapReduce for most jobs now, and even the Hadoop ecosystem is doing the same.

The answer to low latency is generally to keep precomputed indices in memory. Anything that touches disk is hard to make fast and scale. This is how newer-generation Hadoop-based SQL engines like [Impala][1] get so much speed compared to MapReduce-based infrastructure like [Hive][2], for example.

Search infrastructure can''t cache the results of every single query. But it sure can cache intermediate results, or, more complete results for top queries. With a little caching you can serve results for a significant minority of all queries.

Search is also split across servers. So one machine can delegate to 100 to each get a part of the result and then combine them.

You can also get away with some degree of approximation. Google does not literally form a thousand pages of search results; it just has to get the first page about right.

Keep in mind that Google has _millions_ of computers around the globe. Your queries are going to a data center geographically near to you and that is only serving your geography. This cuts out most of the latency, which is network and not processing time in the data center.


  [1]: http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html
  [2]: http://hive.apache.org/', 21, '2014-05-15 13:18:38.693', 'c3ca33ad-faca-4c65-b977-056dd912bbc7', 92, 238, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''s not much you can do with just this data, but what little you can do does not rely on machine learning.

Yes, sessions from the same IP but different User-Agents are almost certainly distinct users. Sessions with the same IP and User-Agent are usually the same user, except in the case of proxies / wi-fi access points. Those you might identify by looking at the distribution of session count per IP to identify likely ''aggregate'' IPs. Sessions from the same IP / User-Agent that overlap in time are almost surely distinct.

To further distinguish users you would need more info. For example, the sites or IP addresses that the user is connecting to would be a very strong basis for differentiating sessions. Then you could get into more sophisticated learning to figure out when sessions are the same or different users.', 21, '2014-05-15 13:30:04.270', 'da57273c-3a38-4957-bfb8-4cbe642d1228', 93, 239, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('While building a rank, say for a search engine, or a recommendation system, is it valid to rely on click frequency to determine the relevance of an entry?', 84, '2014-05-15 14:41:24.020', 'd9e7cca8-3ee4-4216-a962-52e7c0f51f09', 94, 240, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Does click frequency account for relevance?', 84, '2014-05-15 14:41:24.020', 'd9e7cca8-3ee4-4216-a962-52e7c0f51f09', 94, 241, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation><information-retrieval>', 84, '2014-05-15 14:41:24.020', 'd9e7cca8-3ee4-4216-a962-52e7c0f51f09', 94, 242, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is it valid to *use* click frequency, then **yes**. Is it valid to use **only** the click frequency, then probably **no**.

Search relevance is much more complicated than just one metric. [There are entire books on the subject][1]. Extending this answer beyond a simple yes/no would likely make the answer far too broad (and opinionated)


  [1]: http://www.amazon.ca/s/ref=nb_sb_noss?url=search-alias=aps&field-keywords=search%20ranking', 9, '2014-05-15 15:06:24.600', '140403b0-4119-40f9-b5c7-576c4cc0e61a', 95, 243, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For my part I can say that I use click frequency on i.e. eCommerce products. When you combine it with the days of the year it can even bring you great suggestions.

i.e.: We have historical data from 1 year over 2 products (Snowboots[], Sandalettes[])

    [Snowboots[1024,1253,652,123,50,12,8,4,50,148,345,896]]
    [Sandalettes[23,50,73,100,534,701,1053,1503,1125,453,213,25]]

where [0] = January

As you can see, snowboots are much more searched in January as sandalettes, so you should suggest snowboots to someone searching shoes on your site or /we on january.

You can also see if something is "fresh" at this time, like when people often click a unknown product it could be an insight for a new comming trend or something.

That are just some samples where you could use click frequency as a insight. I think there are **no rules** for what you can use or not in recommendations, **as long as it make sense**.
', 115, '2014-05-15 15:10:30.243', 'fce161d6-601a-47d8-bfd4-52b1332add4c', 96, 244, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Users normally only view the first set of links, which means that unless the link is viewable, it''s not getting clicks; meaning you''d have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. Further, using click frequency to account for relevance is not a direct measure of the resource''s relevance. Also, using click is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter.

That said, if you''re interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see "[YouTube''s head of engineering speaking about clicks vs engagement][3]" for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.

Might be worth noting that historically Google was known for [PageRank algorithm][1] though it''s possible your intent is only to review click-streams, so I won''t delve [Google ranking factors][2].


  [1]: http://en.wikipedia.org/wiki/PageRank
  [2]: https://www.google.com/search?q=google%20ranking%20factors
  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8', 158, '2014-05-15 17:14:36.817', 'f0467fe8-7595-4552-b0da-a1425c38161c', 97, 245, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[Depends on the users intent](http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf), for starters.

[Users normally only view the first set of links](http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm), which means that unless the link is viewable, it''s not getting clicks; meaning you''d have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here''s a [click and attention distribution heat-map][2] for  Google search results:

![Google SEPR Click and Attention distribution heat-map][1]

CAPTION: .

Further, using click frequency to account for relevance is not a direct measure of the resource''s relevance. Also, using click is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter.

That said, if you''re interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see "[YouTube''s head of engineering speaking about clicks vs engagement][3]" for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.

Might be worth noting that historically Google was known for [PageRank algorithm][4] though it''s possible your intent is only to review click-streams, so I won''t delve [Google ranking factors][5].


  [1]: http://i.stack.imgur.com/8kO5S.jpg
  [2]: http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm
  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8
  [4]: http://en.wikipedia.org/wiki/PageRank
  [5]: https://www.google.com/search?q=google%20ranking%20factors', 158, '2014-05-15 17:29:37.650', '8a4582e5-f2ea-434d-807f-fbd741890d10', 97, 'added 213 characters in body', 246, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[Depends on the users intent](http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf), for starters.

[Users normally only view the first set of links](http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm), which means that unless the link is viewable, it''s not getting clicks; meaning you''d have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here''s a [click and attention distribution heat-map][2] for  Google search results:

![Google SEPR Click and Attention distribution heat-map][1]

Further, using click frequency to account for relevance is not a direct measure of the resource''s relevance. Also, using click is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter.

That said, if you''re interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see "[YouTube''s head of engineering speaking about clicks vs engagement][3]" for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.

Might be worth noting that historically Google was known for [PageRank algorithm][4] though it''s possible your intent is only to review click-streams, so I won''t delve [Google ranking factors][5]. Also, you might find a review of [Google''s Search Quality Rating Guidelines](http://static.googleusercontent.com/media/www.google.com/en/us/insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf) of use.


  [1]: http://i.stack.imgur.com/8kO5S.jpg
  [2]: http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm
  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8
  [4]: http://en.wikipedia.org/wiki/PageRank
  [5]: https://www.google.com/search?q=google%20ranking%20factors', 158, '2014-05-15 17:36:45.363', 'a29e97eb-5c5f-4a7d-b50d-5accb8dad367', 97, 'deleted 14 characters in body', 247, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[Depends on the users intent](http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf), for starters.

[Users normally only view the first set of links](http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm), which means that unless the link is viewable, it''s not getting clicks; meaning you''d have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here''s a [click and attention distribution heat-map][2] for  Google search results:

![Google SEPR Click and Attention distribution heat-map][1]

Further, using click frequency to account for relevance is not a direct measure of the resource''s relevance. Also, using clicks is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter.

That said, if you''re interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see "[YouTube''s head of engineering speaking about clicks vs engagement][3]" for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.

Might be worth noting that historically Google was known for [PageRank algorithm][4] though it''s possible your intent is only to review click-streams, so I won''t delve [Google ranking factors][5]; if you are interested in the Google''s approach, you might find a review of [Google''s Search Quality Rating Guidelines](http://static.googleusercontent.com/media/www.google.com/en/us/insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf) of use.


  [1]: http://i.stack.imgur.com/8kO5S.jpg
  [2]: http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm
  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8
  [4]: http://en.wikipedia.org/wiki/PageRank
  [5]: https://www.google.com/search?q=google%20ranking%20factors', 158, '2014-05-15 17:51:20.640', '1357d1be-9193-45dd-8746-aa6f24990710', 97, 'added 1 character in body', 248, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[Depends on the user''s intent](http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf), for starters.

[Users normally only view the first set of links](http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm), which means that unless the link is viewable, it''s not getting clicks; meaning you''d have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here''s a [click and attention distribution heat-map][2] for  Google search results:

![Google SEPR Click and Attention distribution heat-map][1]

Further, using click frequency to account for relevance is not a direct measure of the resource''s relevance. Also, using clicks is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter.

That said, if you''re interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see "[YouTube''s head of engineering speaking about clicks vs engagement][3]" for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.

Might be worth noting that historically Google was known for [PageRank algorithm][4] though it''s possible your intent is only to review click-streams, so I won''t delve [Google ranking factors][5]; if you are interested in the Google''s approach, you might find a review of [Google''s Search Quality Rating Guidelines](http://static.googleusercontent.com/media/www.google.com/en/us/insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf) of use.


  [1]: http://i.stack.imgur.com/8kO5S.jpg
  [2]: http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm
  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8
  [4]: http://en.wikipedia.org/wiki/PageRank
  [5]: https://www.google.com/search?q=google%20ranking%20factors', 158, '2014-05-15 18:06:21.103', '7c00682e-e5a6-4ec6-804c-709f6ecebb00', 97, 'added 1 character in body', 249, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('See also http://datascience.stackexchange.com/questions/71/when-are-p-values-deceptive/84#84

When there are a lot of variables that can be tested for pair-wise correlation (for example), the replacement is to use any of the corrections for [False discovery rate][4] (to limit probability that any given discovery is false) or [Familywise error rate][5] (to limit probability of one or more false discoveries).  For example, you might use the HolmBonferroni method.

In the case of a large sample rather than a lot of variables, something else is needed.  As Christopher said, magnitude of effect a way to treat this.  Combining these two ideas, you might use a confidence interval around your magnitude of effect, and apply a false discovery rate correction to the p-value of the confidence interval.  The effects for which even the lowest bound of the corrected confidence interval is high are likely to be strong effects, regardless of huge data set size.  I am not aware of any published paper that combines confidence intervals with false discovery rate correction in this way, but it seems like a straightforward and intuitively understandable approach.

To make this even better, use a non-parametric way to estimate confidence intervals.  Assuming a distribution is likely to give very optimistic estimates here, and even fitting a distribution to the data is likely to be inaccurate.  Since the information about the shape of the distribution past the edges of the confidence interval comes from a relatively small subsample of the data, this is where it really pays to be careful.  You can use bootstrapping to get a non-parametric confidence interval.


  [4]: http://en.wikipedia.org/wiki/False_discovery_rate
  [5]: http://en.wikipedia.org/wiki/Familywise_error_rate', 26, '2014-05-15 20:32:26.923', 'ea971f59-cae5-4d11-b72a-609bfa4e2b0e', 85, 'added 1302 characters in body', 259, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One possibility here (and this is really an extension of what Sean Owen posted) is to define a "stable user."

For the given info you have you can imagine making a user_id that is a hash of ip and some user agent info (pseudo code):

    uid = MD5Hash(ip + UA.device + UA.model)

Then you flag these ids with "stable" or "unstable" based on usage heuristics you observe for your users.  This can be a threshold of # of visits in a given time window, length of time their cookies persist, some end action on your site (I realize this wasn''t stated in your original log), etc...

The idea here is to separate the users that don''t drop cookies from those that do.

From here you can attribute session_ids to stable uids from your logs.  You will then have "left over" session_ids for unstable users that you are relatively unsure about.  You may be over or under counting sessions, attributing behavior to multiple people when there is only one, etc...  But this is at least limited to the users you are now "less certain" about.

You then perform analytics on your stable group and project that to the unstable group.  Take a user count for example, you know the total # of sessions, but you are unsure of how many users generated those sessions.  You can find the # sessions / unique stable user and use this to project the "estimated" number of unique users in the unstable group since you know the number of sessions attributed to that group.

    projected_num_unstable_users = num_sess_unstable / num_sess_per_stable_uid

This doesn''t help with per user level investigation on unstable users but you can at least get some mileage out of a cohort of stable users that persist for some time.  You can, by various methods, project behavior and counts into the unstable group.  The above is a simple example of something you might want to know.  The general idea is again to define a set of users you are confident persist, measure what you want to measure, and use certain ground truths (num searches, visits, clicks, etc...) to project into the unknown user space and estimate counts for them.

This is a longstanding problem in unique user counting, logging, etc... for services that don''t require log in.', 92, '2014-05-15 21:41:22.703', '5e1fb228-9e1b-44df-8bb8-71727ee6efae', 101, 261, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[Depends on the user''s intent](http://research.microsoft.com/pubs/169639/cikm-clickpatterns.pdf), for starters.

[Users normally only view the first set of links](http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm), which means that unless the link is viewable, it''s not getting clicks; meaning you''d have to be positive those are the best links, otherwise the clicks are most likely going to reflect placement, not relevance. For example, here''s a [click and attention distribution heat-map][2] for  Google search results:

![Google SEPR Click and Attention distribution heat-map][1]

Further, using click frequency to account for relevance is not a direct measure of the resource''s relevance. Also, using clicks is problematic, since issues like click-inflation, click-fraud, etc. will pop-up and are hard to counter.

That said, if you''re interested in using user interaction to model relevance, I would suggest you attempt to measure post-click engagement, not how users respond to search results; see "[YouTube''s head of engineering speaking about clicks vs engagement][3]" for more information, though note that the [size itself of the content is a factor](http://www.orbitmedia.com/blog/ideal-blog-post-length/) too.

Might be worth noting that historically Google was known for [PageRank algorithm][4] though it''s possible your intent is only to review click-streams, so I won''t delve [Google ranking factors][5]; if you are interested in the Google''s approach, you might find a review of [Google''s Search Quality Rating Guidelines](http://static.googleusercontent.com/media/www.google.com/en/us/insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf).


  [1]: http://i.stack.imgur.com/8kO5S.jpg
  [2]: http://www.seoresearcher.com/distribution-of-clicks-on-googles-serps-and-eye-tracking-analysis.htm
  [3]: http://www.youtube.com/watch?v=BsCeNCVb-d8
  [4]: http://en.wikipedia.org/wiki/PageRank
  [5]: https://www.google.com/search?q=google%20ranking%20factors', 158, '2014-05-15 23:08:04.300', '9a735a97-1157-4fb3-9621-9c5c6fd7dcc4', 97, 'deleted 7 characters in body', 262, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Overfitting, in a nutshell, means take into account **too much** information from your data and/or prior knowledge, and use it in a model. To make it more straightforward, consider the following example: you''re hired by some scientists to provide them with a model to predict the growth of some kind of plants. The scientists have given you information collected from their work
with such plants throughout a whole year, and they shall continuously give you information on the future development of their plantation.

So, you run through the data received, and build up a model out of it. Now suppose that, in your model, you considered just as many characteristics as possible to always find out the exact behavior of the plants you saw in the initial dataset. Now, as the production continues, you''ll always take into account those characteristics, and will produce very *fine-grained* results. However, if the plantation eventually suffer from some seasonal change, the results you will receive may fit your model in such a way that your predictions will begin to fail (either saying that the growth will slow down, while it shall actually speed up, or the opposite).

Apart from being unable to detect such small variations, and to usually classify your entries incorrectly, the *fine-grain* on the model, i.e., the great amount of variables, may cause the processing to be too costly. Now, imagine that your data is already complex. Overfitting your model to the data not only will make the classification/evaluation very complex, but will most probably make you error the prediction over the slightest variation you may have on the input.

**Edit**: [This](https://www.youtube.com/watch?v=DQWI1kvmwRg) might as well be of some use, perhaps adding dynamicity to the above explanation :D', 84, '2014-05-15 23:22:39.427', '1eb111d4-5549-4b15-b1b7-932641e8054d', 64, 'added 149 characters in body', 263, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How does a query into a huge database return with negligible latency?', 21, '2014-05-16 02:46:17.830', '1742e878-7034-4015-b8bc-4f4b23905f32', 89, 'Fix typo; add tags', 264, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><google><search>', 21, '2014-05-16 02:46:17.830', '1742e878-7034-4015-b8bc-4f4b23905f32', 89, 'Fix typo; add tags', 265, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-16 02:46:17.830', '1742e878-7034-4015-b8bc-4f4b23905f32', 89, 'Proposed by 21 approved by 189 edit id of 17', 266, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For example, when searching something in Google, results return nigh-instantly.

I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be indexed (and results are personalized, which renders this even more infeasible)?

Moreover, wouldn''t the hardware latency in Google''s hardware be huge? Even if the data in Google were all stored in TB/s SSDs, I imagine the hardware latency to be huge, given the sheer amount of data to process.

Does MapReduce help solve this problem?

EDIT: Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches? Even for the most obscure search I have conducted, I don''t think the search has ever been reported to be larger than 5 seconds. How is this possible?', 189, '2014-05-16 02:46:56.510', '58c6279d-a832-41cf-be47-7568908df195', 89, 'added 263 characters in body', 267, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Well, I''m not sure if it is MapReduce that solves the problem, but it surely wouldn''t be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it *feasible* to have such low latency on queries from all these TBs of data in different machines:

1. distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);
2. caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;
3. lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.

Considering that, lets try to address your questions:

> but I imagine it infeasible for the results of every single possible query to be indexed

Yes, it would be, and actually is infeasible to have results for *every single possible query*. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number of queries from these `n -> inf` terms (`2^n`). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant-for-the-user queries are the ones cached.

> wouldn''t the hardware latency in Google''s hardware be huge? Even if the data in Google were all stored in TB/s SSDs

Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing *ruling* market is money, and the investors are not interested in wasting it. So what is done?

The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of [performance](http://i.stack.imgur.com/Uf6al.gif). But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.

Memory cards are *expensive* for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it''s not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending *specific geographical regions*, which allows for more *specialized* data caching, and even better response times.

> Does MapReduce help solve this problem?

Although I don''t think that using or not MapReduce is restricted information inside Google, I''m not conversant about this point. However, Google''s implementation of MapReduce (which is surely *not* Hadoop) must have lots of optimizations, many involving the aspects discussed above. So, the architecture of MapReduce probably helps guiding how the computations are physically distributed, but there are many other points to be considered to justify such speed in querying time.

> Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches?

The graph below presents a curve of how the *kinds* of queries occur. You can see that there are three main kinds of searches, each of them holding approximately 1/3 of the volume of queries (area below curve). The plot shows power law, and reinforces the fact that smaller queries are the most popular. The second third of queries are still feasible to process, since they hold few words. But the set of so-called *obscure queries*, which usually consist of non-experienced users'' queries, are not a negligible part of the queries.

![Heavy-tailed distribution][1]

And there lies space for novel solutions. Since it''s not just one or two queries (but one third of them), they must have *relevant* results. If you type in something *much too obscure* in a Google search, it shan''t take longer to return a list of results, but will most probably show you something it *inferred* you''d like to say. Or it may simply state that there was no document with such terms -- or even cut down your search to 32 words (which just happened to me in a random test here).

There are dozens of appliable heuristics, which may be either to ignore some words, or try to break the query into smaller ones, and gather the most *popular* results. All these combinations, since they are simply heuristics, can be tailored and tweaked to respect *feasible waiting times* of, say, less then a second? :D

  [1]: http://i.stack.imgur.com/CpcNf.jpg', 84, '2014-05-16 04:28:40.033', 'c7904778-d48f-4ea2-bab6-8bdfabe3d1d3', 91, 'added 1487 characters in body', 268, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Well, I''m not sure if it is MapReduce that solves the problem, but it surely wouldn''t be MapReduce alone to solve all these questions you raised. But here are important things to take into account, and that make it *feasible* to have such low latency on queries from all these TBs of data in different machines:

1. distributed computing: by being distributed does not mean that the indexes are simply distributed in different machines, they are actually replicated along different clusters, which allows for lots of users performing different queries with low retrieval time (yes, huge companies can afford for that much of machines);
2. caching: caches tremendously reduce execution time, be it for the crawling step, for the retrieval of pages, or for the ranking and exihibition of results;
3. lots of tweaking: all the above and very efficient algorithms/solutions can only be effective if the implementation is also efficient. There are tons of (hard coded) optimizations, such as locality of reference, compression, caching; all of them usually appliable to different parts of the processing.

Considering that, lets try to address your questions:

> but I imagine it infeasible for the results of every single possible query to be indexed

Yes, it would be, and actually is infeasible to have results for *every single possible query*. There is an infinite number of terms in the world (even if you assume that only terms properly spelled will be entered), and there is an exponential number of queries from these `n -> inf` terms (`2^n`). So what is done? Caching. But if there are so many queries/results, which ones to cache? Caching policies. The most frequent/popular/relevant-for-the-user queries are the ones cached.

> wouldn''t the hardware latency in Google''s hardware be huge? Even if the data in Google were all stored in TB/s SSDs

Nowdays, with such highly developed processors, people tend to think that every possible task that must finish within a second (or less), and that deals with so much data, must be processed by extremely powerful processors with multiple cores and lots of memory. However, the one thing *ruling* market is money, and the investors are not interested in wasting it. So what is done?

The preference is actually for having lots of machines, each using simple/accessible (in terms of cost) processors, which lowers the price of building up the multitude of clusters there are. And yes, it does work. The main bottleneck always boils down to disk, if you consider simple measurements of [performance](http://i.stack.imgur.com/Uf6al.gif). But once there are so many machines, one can afford to load things up to main memory, instead of working on hard disks.

Memory cards are *expensive* for us, mere human beings, but they are very cheap for enterprises that buy lots of such cards at once. Since it''s not costly, having much memory as needed to load indexes and keep caches at hand is not a problem. And since there are so many machines, there is no need for super fast processors, as you can direct queries to different places, and have clusters of machines responsible for attending *specific geographical regions*, which allows for more *specialized* data caching, and even better response times.

> Does MapReduce help solve this problem?

Although I don''t think that using or not MapReduce is restricted information inside Google, I''m not conversant about this point. However, Google''s implementation of MapReduce (which is surely *not* Hadoop) must have lots of optimizations, many involving the aspects discussed above. So, the architecture of MapReduce probably helps guiding how the computations are physically distributed, but there are many other points to be considered to justify such speed in querying time.

> Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches?

The graph below presents a curve of how the *kinds* of queries occur. You can see that there are three main kinds of searches, each of them holding approximately 1/3 of the volume of queries (area below curve). The plot shows power law, and reinforces the fact that smaller queries are the most popular. The second third of queries are still feasible to process, since they hold few words. But the set of so-called *obscure queries*, which usually consist of non-experienced users'' queries, are not a negligible part of the queries.

![Heavy-tailed distribution][1]

And there lies space for novel solutions. Since it''s not just one or two queries (but one third of them), they must have *relevant* results. If you type in something *much too obscure* in a Google search, it shan''t take longer to return a list of results, but will most probably show you something it *inferred* you''d like to say. Or it may simply state that there was no document with such terms -- or even cut down your search to 32 words (which just happened to me in a random test here).

There are dozens of appliable heuristics, which may be either to ignore some words, or to try to break the query into smaller ones, and gather the most *popular* results. And all these solutions can be tailored and tweaked to respect *feasible waiting times* of, say, less then a second? :D

  [1]: http://i.stack.imgur.com/CpcNf.jpg', 84, '2014-05-16 04:33:52.310', '9058136d-57c7-435f-99c8-7922e7c902fb', 91, 'added 1487 characters in body', 269, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the best noSQL backend to use for a mobile game? Users can make a lot of servers requests, it needs also to retrieve users history (like in app purchase) and analytics of use behavior.', 199, '2014-05-16 05:09:33.557', 'e4db9174-41f4-492b-98eb-bcf39f022eda', 102, 271, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Best noSQL backend for mobile game', 199, '2014-05-16 05:09:33.557', 'e4db9174-41f4-492b-98eb-bcf39f022eda', 102, 272, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nosql>', 199, '2014-05-16 05:09:33.557', 'e4db9174-41f4-492b-98eb-bcf39f022eda', 102, 273, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[LIBSVM][1] is a library for support vector classification (SVM) and regression.
It was created by Chih-Chung Chang and Chih-Jen Lin in 2001.


  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/', 63, '2014-05-16 13:44:53.470', '20a8bb9a-c7d1-4f93-bd5f-d735df0df7c4', 17, 'added 196 characters in body', 274, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-16 13:44:53.470', '20a8bb9a-c7d1-4f93-bd5f-d735df0df7c4', 17, 'Proposed by 63 approved by 50 edit id of 1', 275, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<education><open-source>', 97, '2014-05-16 13:45:00.237', '44b379df-3358-477b-99e6-8a9a34da74ce', 7, 'Added relevant tags', 276, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-16 13:45:00.237', '44b379df-3358-477b-99e6-8a9a34da74ce', 7, 'Proposed by 97 approved by 50 edit id of 3', 277, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<r><data-cleaning>', 136, '2014-05-16 13:45:07.447', 'b37ebd26-025c-4574-99a4-fdcedb82e571', 52, 'added R tag', 278, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-16 13:45:07.447', 'b37ebd26-025c-4574-99a4-fdcedb82e571', 52, 'Proposed by 136 approved by 50 edit id of 9', 279, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Conceptually speaking, *data-mining* can be thought of as one item (or set of skills and applications) in the toolkit of the data scientist.

More specifically, data-mining is an activity that seeks patterns in large, complex data sets. It usually emphasizes algorithmic techniques, but may also involve any set of related skills, applications, or methodologies with that goal.

In US-English colloquial speech, data-mining and data-collection are often used interchangeably.

However, a main difference between these two related activities is *intentionality*.

*Definition inspired mostly by the contributions of @statsRus to Data Science.SE*', 53, '2014-05-16 13:45:25.047', 'e7ca3ea4-8e48-486b-93b6-b0286e44eaa3', 79, 'Cited source for Wiki tag content.', 280, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-16 13:45:25.047', 'e7ca3ea4-8e48-486b-93b6-b0286e44eaa3', 79, 'Proposed by 53 approved by 50 edit id of 13', 281, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-science><data-mining><definitions>', 53, '2014-05-16 13:45:34.440', 'b3c7d2c0-69dd-4dfb-b3f1-02172e2a0a70', 14, 'Propose that definitions tag be added for this question', 282, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-16 13:45:34.440', 'b3c7d2c0-69dd-4dfb-b3f1-02172e2a0a70', 14, 'Proposed by 53 approved by 50 edit id of 4', 283, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Big data is the term for a collection of data sets so large and complex that it becomes difficult to process using on-hand database management tools or traditional data processing applications. The challenges include capture, curation, storage, search, sharing, transfer, analysis and visualization.', 118, '2014-05-16 13:45:57.450', '3f29bc47-4f5c-4b70-bc50-6817e9323191', 66, 'some info about tag', 284, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-16 13:45:57.450', '3f29bc47-4f5c-4b70-bc50-6817e9323191', 66, 'Proposed by 118 approved by 50 edit id of 10', 285, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('An activity that seeks patterns in large, complex data sets. It usually emphasizes algorithmic techniques, but may also involve any set of related skills, applications, or methodologies with that goal.', 53, '2014-05-16 13:46:05.850', '2f2b77af-fd7c-465e-97eb-a4edf055e04e', 80, 'Cited source for Wiki tag content.', 286, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-16 13:46:05.850', '2f2b77af-fd7c-465e-97eb-a4edf055e04e', 80, 'Proposed by 53 approved by 50 edit id of 14', 287, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Assume that we have a set of elements *E* and a similarity (**not distance**) function *sim(ei, ej)* between two elements *ei,ej  E*.

How could we (efficiently) cluster the elements of *E*, using *sim*?

*k*-means, for example, requires a given *k*, Canopy Clustering requires two threshold values. What if we don''t want such predefined parameters?

Note, that *sim* is not neccessarily a metric (i.e. the triangle inequality may, or may not hold). Moreover, it doesn''t matter if the clusters are disjoint (partitions of *E*).
', 113, '2014-05-16 14:26:12.270', '49925465-cc9e-411c-aa0c-5094a6d20f75', 103, 288, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Clustering based on similarity scores', 113, '2014-05-16 14:26:12.270', '49925465-cc9e-411c-aa0c-5094a6d20f75', 103, 289, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering><algorithms><similarity>', 113, '2014-05-16 14:26:12.270', '49925465-cc9e-411c-aa0c-5094a6d20f75', 103, 290, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-16 15:35:51.420', '308637bf-d9b0-47a2-a2f2-a953301309d5', 104, 291, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-16 15:35:51.420', '2fe928f3-e272-46b5-824d-2bb061997bd1', 105, 292, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are many overlaps between data mining and datascience. I would say that people with the role of datamining are concerned with data collection and the extraction of features from unfiltered, unorganised and mostly raw/wild datasets. Some very important data may be difficult to extract, not do to the implementation issues but because it may have foreign artifacts.

Eg. if I needed someone to look at financial data from written tax returns in the 70s which were scanned and machine read to find out if people saved more on car insurance; a dataminer would be the person to get.

If I needed someone to examine the influence Nike''s Twitter profile in the tweets of Brazil and identify key positive features from the profile, I would look for a datascientist.', 34, '2014-05-16 16:25:58.250', '94618ffd-1651-40f1-a2e1-ebb656080903', 106, 293, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Consider a stream containing tuples `(user, new_score)` representing users score in an online game. The stream could have 100-1,000 new elements per second. The game has 200K to 300K unique players.

I would like to have some standing queries like:

1. Which players posted more than x scores in a sliding window of one hour
2. Which players gained x% score in a sliding window of one hour

My question is which open source tools can I employ to jumpstart this project? I am considering [Esper][1] at the moment.

Note: I have just completed reading "Mining Data Streams" (chapter 4 of [Mining of Massive Datasets][2]) and I am quiet new to mining data streams.


  [1]: http://esper.codehaus.org/
  [2]: http://infolab.stanford.edu/~ullman/mmds.html', 200, '2014-05-16 20:07:50.983', '05264096-c5d9-42aa-835c-c90ca0c8408f', 107, 294, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Opensource tools for help in mining stream of leader board scores', 200, '2014-05-16 20:07:50.983', '05264096-c5d9-42aa-835c-c90ca0c8408f', 107, 295, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-stream-mining>', 200, '2014-05-16 20:07:50.983', '05264096-c5d9-42aa-835c-c90ca0c8408f', 107, 296, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-16 20:24:38.980', '010b05ec-6994-4015-8405-273e63c584d0', 108, 297, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-16 20:24:38.980', '8989bab4-3ca4-45b9-ac29-0b3a65e25da4', 109, 298, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Some factors you might consider:

Developer familiarity: go with whatever you or your developers are familiar with.  Mongo, Couch, Riak, DynamoDB etc all have their strengths but all should do ok here, so rather than going for an unfamiliar solution that might be slughtly better go for familiar and save a bunch of development time.

Ease of cloud deployment:  for example, if you are using Amazon AWS, then DynamoDB is likely an excellent choice.  Sure, you could use Mongo on AWS, but why bother?  Other cloud providers have their own preferred db, for example if you are uusing Google AppEngine, it makes sense to use BigTable or Cloud Storage.

Your use case seems both well suited to NoSQL and not very challenging since your data has a natural partition by user.  I think you''d be technically ok with anything, which is why I''m mainly covering other factors.', 26, '2014-05-17 03:07:59.707', '4b50495d-1743-48be-bfe3-665f77a64b92', 111, 304, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This isn''t a full solution, but you may want to look into [OrientDB][1] as part of your stack. Orient is a Graph-Document database server written entirely in Java.

In graph databases, relationships are considered first class citizens and therefore traversing those relationships can be done pretty quickly. Orient is also a document database which would allow you the kind of schema-free architecture it sounds like you would need. The real reason I suggest Orient, however, is because of its extensiblity. It supports streaming via sockets, and the entire database can be embedded into another application. Finally, it can be scaled efficiently and/or can work entirely through memory. So, with some Java expertise, you can actually run your preset queries against the database in memory.

We are doing something similar. In creating an app/site for social science research collaboration, we found ourselves with immensely complex data models. We ended up writing several of the queries using the Gremlin Traversal Language (a subset of Groovy, which is, of course, Java at its heart), and then exposing those queries through the binary connection server of the OrientDB. So, the client opens a TCP socket, sends a short binary message, and the query is executing in Java directly against the in-memory database.

OrientDB also supports writing function queries in Javascript, and you can use Node.js to interact directly with an Orient instance.

For something of this size, I would want to use Orient in conjunction with Hadoop or something like that. You can also use Orient in conjunction with esper.

Consider:
An introduction to orient: http://www.sitepoint.com/a-look-at-orientdb-the-graph-document-nosql/

Complex, real-time queries: http://www.gft-blog.com/business-trends/leveraging-real-time-scoring-through-bigdata-to-detect-insurance-fraud/

A discussion about streaming options with java and orient: https://github.com/orientechnologies/orientdb/issues/1227



  [1]: http://www.orientechnologies.com/', 70, '2014-05-17 04:18:10.020', '823a3c28-c770-4211-83f9-5bc114541157', 112, 305, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When a relational database like mySQL has better performance than a no relational, like mongo?

I saw a question on Quora other day, about why Quora still uses mySQL as their backend. And how their performance is still good.', 199, '2014-05-17 04:53:03.913', '0f316ff9-666b-4b3e-83c6-6b6cfe494f76', 113, 306, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When a relational database has better performance than a no relational', 199, '2014-05-17 04:53:03.913', '0f316ff9-666b-4b3e-83c6-6b6cfe494f76', 113, 307, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><nosql><databases>', 199, '2014-05-17 04:53:03.913', '0f316ff9-666b-4b3e-83c6-6b6cfe494f76', 113, 308, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Consider a stream containing [tuples][tuples] `(user, new_score)` representing users'' scores in an online game. The stream could have 100-1,000 new elements per second. The game has 200K to 300K unique players.

I would like to have some standing queries like:

1. Which players posted more than x scores in a sliding window of one hour
2. Which players gained x% score in a sliding window of one hour

My question is which open source tools can I employ to jumpstart this project? I am considering [Esper][1] at the moment.

Note: I have just completed reading "Mining Data Streams" (chapter 4 of [Mining of Massive Datasets][2]) and I am quite new to mining data streams.


  [1]: http://esper.codehaus.org/
  [2]: http://infolab.stanford.edu/~ullman/mmds.html
  [tuples]: http://en.m.wikipedia.org/wiki/Tuple', 53, '2014-05-17 08:40:59.320', '2d70edb1-2ea1-47a0-8680-c47bc36b432b', 107, 'Corrected spelling and grammar. Added link for usage of a possibly diificult term.', 312, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-17 08:40:59.320', '2d70edb1-2ea1-47a0-8680-c47bc36b432b', 107, 'Proposed by 53 approved by 200 edit id of 23', 313, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I have a very long list of paper names, how could I get abstract of these papers from internet or any database?

The paper names are like "Assessment of Utility in Web Mining for the Domain of Public Health".

Does any one know any API that can give me a solution? I tried to crawl google scholar, however, google blocked my crawler.', 212, '2014-05-17 08:45:08.420', '86da5fdb-3dfb-4960-b07e-00fbe5bf3a8f', 115, 314, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is there any APIs for crawling abstract of paper?', 212, '2014-05-17 08:45:08.420', '86da5fdb-3dfb-4960-b07e-00fbe5bf3a8f', 115, 315, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><machine-learning>', 212, '2014-05-17 08:45:08.420', '86da5fdb-3dfb-4960-b07e-00fbe5bf3a8f', 115, 316, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a database from my Facebook application and I am trying to use machine learning to estimate users'' age based on what Facebook sites they like.

There are two crucial characteristics of my database:

 - the age distribution in my training set (12k of users in sum) is skewed towards younger users (i.e. I have 1157 users aged 27, and 23 users aged 65);

 - many sites have no more than 5 likers (I filtered out the FB sites with less than 5 likers).

 - there''s many more features than samples.

So, my questions are: what strategy would you suggest to prepare the data for further analysis? Should I perform some sort of dimensionality reduction? Which ML method would be most appropriate to use in this case?

I mainly use Python, so Python-specific hints would be greatly appreciated.', 173, '2014-05-17 09:16:18.823', '867300c0-8817-4870-ade0-25429746b9bf', 116, 318, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine learning techniques for estimating users'' age based on Facebook sites they like', 173, '2014-05-17 09:16:18.823', '867300c0-8817-4870-ade0-25429746b9bf', 116, 319, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><dimensionality-reduction><python>', 173, '2014-05-17 09:16:18.823', '867300c0-8817-4870-ade0-25429746b9bf', 116, 320, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-17 13:41:20.283', 'd2c32564-4819-4fa3-a494-edc96100fdbf', 118, 324, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-17 13:41:20.283', '70af58e8-dd3f-4dd1-9d4c-dccaef07d5b0', 119, 325, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I use [Libsvm][1] to train data and predict classification on **semantic analysis** problem. But it has a **performance** issue on large-scale data, because semantic analysis concerns ***n-dimension*** problem.

Last year, [Liblinear][2] was release, and it can solve performance bottleneck.
But it cost too much **memory**. Is **MapReduce** the only way to solve semantic analysis problem on big data? Or are there any other methods that can improve memory bottleneck on **Liblinear**?

  [1]: http://www.csie.ntu.edu.tw/~cjlin/libsvm/
  [2]: http://www.csie.ntu.edu.tw/~cjlin/liblinear/', 84, '2014-05-17 16:24:14.523', '977b326c-9676-4b70-b3aa-3f75ad9d6c49', 16, 'Fixed grammar, and improving formatting.', 327, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-17 16:24:14.523', '977b326c-9676-4b70-b3aa-3f75ad9d6c49', 16, 'Proposed by 84 approved by 63 edit id of 24', 328, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('arXiv has an [API and bulk download](http://arxiv.org/help/bulk_data) but if you want something for paid journals it will be hard to come by without paying an indexer like pubmed or elsevier or the like.', 92, '2014-05-17 18:15:11.937', 'c92d8bc9-2952-437c-bd41-ce67a2ecfd8b', 120, 329, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One thing to start off with would be k-NN.  The idea here is that you have a user/item matrix and for some of the users you have a reported age.  The age for a person in the user item matrix might be well determined by something like the mean or median age of some nearest neighbors in the item space.

So you have each user expressed as a vector in item space, find the k nearest neighbors and assign the vector in question some summary stat of the nearest neighbor ages.  You can choose k on a distance cutoff or more realistically by iteratively assigning ages to a train hold out and choosing the k that minimizes the error in that assignment.

If the dimensionality is a problem you can easily perform reduction in this setup by single value decomposition choosing the m vectors that capture the most variance across the group.

In all cases since each feature is binary it seems that cosine similarity would be your go to distance metric.

I need to think a bit more about other approaches (regression, rf, etc...) given the narrow focus of your feature space (all variants of the same action, liking) I think the user/item approach might be the best.

One note of caution, if the ages you have for train are self reported you might need to correct some of them.  People on facebook tend to report ages in the decade they were born.  Plot a histogram of the birth dates (derived from ages) and see if you have spikes at decades like 70s, 80s, 90s.', 92, '2014-05-17 18:53:30.123', '7470dd79-cb71-48de-852c-bd9a2d41be07', 121, 330, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a database from my Facebook application and I am trying to use machine learning to estimate users'' age based on what Facebook sites they like.

There are three crucial characteristics of my database:

 - the age distribution in my training set (12k of users in sum) is skewed towards younger users (i.e. I have 1157 users aged 27, and 23 users aged 65);

 - many sites have no more than 5 likers (I filtered out the FB sites with less than 5 likers).

 - there''s many more features than samples.

So, my questions are: what strategy would you suggest to prepare the data for further analysis? Should I perform some sort of dimensionality reduction? Which ML method would be most appropriate to use in this case?

I mainly use Python, so Python-specific hints would be greatly appreciated.', 173, '2014-05-17 19:26:53.783', 'dd47aa8d-791d-4338-a66f-c8393c3ab2ca', 116, 'added 2 characters in body', 331, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It depends on your data and what you''re doing with it. For example, if the processing you have to do requires transactions to synchronize across nodes, it will likely be faster to use transactions implemented in an RDBMS rather than implementing it yourself on top of NoSQL databases which don''t support it natively. ', 180, '2014-05-17 20:56:15.577', 'e1bfdaca-8468-4108-a9c2-042d02b85790', 122, 332, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-17 21:10:41.990', '269ed444-397c-4b6e-b3df-f6e1b1cbb076', 123, 333, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-17 21:10:41.990', '3e5293b2-8ca5-48fb-acd7-36525d159223', 124, 334, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want learn about NoSQL and when is better SQL or NoSQL, I know that this question depend of case, because it I ask about a good documentation about NoSQL and that explain when is better a SQL and when is better a NoSQL, cases of example,etc; and also your opinions on NoSQL databases and use cases, and any recommendations for learning about this topic.

Thanks for your help.', 109, '2014-05-17 21:52:34.563', 'aa1b2818-db3c-4213-8ca2-daaf0e7c7283', 125, 336, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('how learn noSQL databases and how know if is better SQL or noSQL', 109, '2014-05-17 21:52:34.563', 'aa1b2818-db3c-4213-8ca2-daaf0e7c7283', 125, 337, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nosql>', 109, '2014-05-17 21:52:34.563', 'aa1b2818-db3c-4213-8ca2-daaf0e7c7283', 125, 338, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Please have a look at my answer here: http://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767

Short version:

- Use NoSQL is data size and number of transactions per second forces it, which typically happens above a few tens of TB and millions of transactions per second (db in memory, running on cluster), *or* at hundreds of TB and thousands of transactions per second (traditional db on disk, transactions per second is *highly* dependent on the usage pattern). Traditional SQL scales up to that point just fine.

- NoSQL is well suited for some problems (data has a natural sharding, schema is flexible, eventual consistency is ok).  You can use there even if scaling doesn''t force you to.

- Developer familiarity with tools and ops ease of deployment are *major* factors, don''t overlook those.  A solution may be technically better but you may have a hard time using it, make sure you need it and make sure you budget for the learning curve.', 26, '2014-05-17 23:53:42.700', 'aecbc0ea-764b-4b8f-92d9-7abe3cfb6610', 126, 339, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Please have a look at my answer here:

**[Motivations for using relational database / ORM or document database / ODM][1]**

Short version:

- Use NoSQL is data size and number of transactions per second forces it, which typically happens above a few tens of TB and millions of transactions per second (db in memory, running on cluster), *or* at hundreds of TB and thousands of transactions per second (traditional db on disk, transactions per second is *highly* dependent on the usage pattern). Traditional SQL scales up to that point just fine.

- NoSQL is well suited for some problems (data has a natural sharding, schema is flexible, eventual consistency is ok).  You can use there even if scaling doesn''t force you to.

- Developer familiarity with tools and ops ease of deployment are *major* factors, don''t overlook those.  A solution may be technically better but you may have a hard time using it, make sure you need it and make sure you budget for the learning curve.

As to how to learn it: fire up a MongoDB image on AWS, or DynamoDB, and have fun!
http://docs.mongodb.org/ecosystem/platforms/amazon-ec2/
http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GettingStartedDynamoDB.html


  [1]: http://stackoverflow.com/questions/13528216/motivations-for-using-relational-database-orm-or-document-database-odm/13599767#13599767', 26, '2014-05-17 23:59:59.580', 'b334174e-2f12-4abf-9939-8dc1ff1ca1fc', 126, 'added 334 characters in body', 340, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Latent Dirichlet Allocation (LDA) is a topic modeling process (http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).

And Hierarchical Dirichlet Process (HDP) is also a topic modeling process (http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process).

**The major difference is LDA requires the specification of the number of topics but HDP doesn''t. Why is that so?**

**What are the differences and pros and cons of both topic modelling methods?**

', 122, '2014-05-18 06:10:52.543', '33a17081-4e0b-485f-9510-524e2d23a7df', 128, 344, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Latent Dirichlet Allocation vs Hierarchical Dirichlet Process', 122, '2014-05-18 06:10:52.543', '33a17081-4e0b-485f-9510-524e2d23a7df', 128, 345, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><topic-model><lda>', 122, '2014-05-18 06:10:52.543', '33a17081-4e0b-485f-9510-524e2d23a7df', 128, 346, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This question asked about generative vs discriminative algorithm, http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm but **can someone give an example of the difference when applied to Natural Language Processing?**

**How are generative and discriminative models use in NLP?**', 122, '2014-05-18 06:17:37.587', '8fb34339-3d0f-469b-b7b5-3e9b5ba175d0', 129, 347, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is generative and discriminative model? How are they used in Natural Language Processing?', 122, '2014-05-18 06:17:37.587', '8fb34339-3d0f-469b-b7b5-3e9b5ba175d0', 129, 348, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><language-model>', 122, '2014-05-18 06:17:37.587', '8fb34339-3d0f-469b-b7b5-3e9b5ba175d0', 129, 349, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From wikipedia,

> dimensionality reduction or dimension reduction is the process of
> reducing the number of random variables under consideration, and
> can be divided into feature selection and feature extraction.

**What is the difference between feature selection and feature extraction?**


**What is an example of dimensionality reduction in a Natural Language Processing task?**

', 122, '2014-05-18 06:26:15.673', '92bf1d2c-0435-4eda-aae3-264ba816940a', 130, 350, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is dimensionality reduction? What is the difference between feature selection and extraction?', 122, '2014-05-18 06:26:15.673', '92bf1d2c-0435-4eda-aae3-264ba816940a', 130, 351, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><dimensionality-reduction><feature-selection>', 122, '2014-05-18 06:26:15.673', '92bf1d2c-0435-4eda-aae3-264ba816940a', 130, 352, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Some factors you might consider:

Developer familiarity: go with whatever you or your developers are familiar with.  Mongo, Couch, Riak, DynamoDB etc all have their strengths but all should do ok here, so rather than going for an unfamiliar solution that might be slightly better go for familiar and save a bunch of development time.

Ease of cloud deployment:  for example, if you are using Amazon AWS, then DynamoDB is likely an excellent choice.  Sure, you could use Mongo on AWS, but why bother?  Other cloud providers have their own preferred db, for example if you are using Google AppEngine, it makes sense to use BigTable or Cloud Datastore.

Your use case seems both well suited to NoSQL and not very challenging since your data has a natural partition by user.  I think you''d be technically ok with anything, which is why I''m mainly covering other factors.', 26, '2014-05-18 06:43:52.453', '1a6ac64c-4db4-4743-b4ee-f2d08b70e94e', 111, 'added 1 character in body', 353, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Look it up on:

* Google Scholar [link][1]
* Citeseer [link][2]

If you get a single exact title match then you have probably found the right article, and can fill in the rest of the info from there.  Both give you download links and bibtex-style output.  What you would likely want to do though to get perfect metadata is download and parse the pdf (if any) and look for DOI-style identifier.

Please be nice and rate-limit your requests if you do this.


  [1]: http://scholar.google.com/scholar?hl=en&q=Assessment%20of%20Utility%20in%20Web%20Mining%20for%20the%20Domain%20of%20Public%20Health&btnG=&as_sdt=1,5&as_sdtp=
  [2]: http://citeseerx.ist.psu.edu/search?q=Assessment%20of%20Utility%20in%20Web%20Mining%20for%20the%20Domain%20of%20Public%20Health&submit=Search&sort=rlv&t=doc', 26, '2014-05-18 06:54:08.560', '02d793c6-1cf6-4823-99bc-98b734e550c9', 131, 354, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Simply put:

- feature selection: you select a subset of the original feature set; while
- feature extraction: you build a new set of features from the original feature set.

Examples of feature extraction: extraction of contours in images, extraction of digrams from a text, extraction of phonemes from recording of spoken text, etc.

Feature extraction involves a transformation of the features, which often is not reversible because some information is lost in the process of dimensionality reduction.', 172, '2014-05-18 07:53:58.203', '54d1fcb3-a466-40e9-a508-22cb01d2003c', 132, 355, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('1. I think a number of clustering algorithms that normally use a metric, do not actually rely on the metric properties (other than commutativity, but I think you''d have that here).  For example, DBSCAN uses epsilon-neighborhoods around a point; there is nothing in there that specifically says the triangle inequality matters.  So you can probably use DBSCAN, although you may have to do some kind of nonstandard spatial index to do efficient lookups in your case.  Your version of epsilon-neighborhood will likely be sim > 1/epsilon rather than the other way around.  Same story with k-means and related algorithms.

2. Can you construct a metric from your similarity?  One possibility: dist(ei, ej) = min( sim(ei, ek) + sim(ek, ej) ) for all k ...  Alternately, can you provide an upper  bound such that sim(ei, ej) < sim(ei, ek) + sim(ek, ej) + d, for all k and some positive constant d?  Intuitively, large sim means closer together: is 1/sim metric-like?  What about 1/(sim + constant)?

3. An alternate construction of a metric is to do an embedding.  As a first step, you can try to map your points ei -> xi, such that xi minimize sum( abs( sim(ei, ej) - f( dist(xi, xj) ) ), for some suitable function f and metric dist.  The function f converts distance in the embedding to a similarity-like value; you''d have to experiment a bit, but 1/dist or exp^-dist are good starting points.  You''d also have to experiment on the best dimension for xi.  From there, you can use conventional clustering on xi.  The idea here is that you can almost (in a best fit sense) convert your distances in the embedding to similarity values, so they would cluster correctly.

4. On the use of predefined parameters, all algorithms have some tuning.  DBSCAN can find the number of clusters, but you still need to give it some parameters.  In general, tuning requires multiple runs of the algorithm with different values for the tunable parameters, together with some function that evaluates goodness-of-clustering (either calculated separately, provided by the clustering algorithm itself, or just eyeballed :)  If the character of your data doesn''t change, you can tune once and then use those fixed parameters; if it changes then you have to tune for each run.  You can find that out by tuning for each run and then comparing how well the parameters from one run work on another, compared to the parameters specifically tuned for that.
', 26, '2014-05-18 09:09:47.780', 'c7af93ab-550a-4356-a365-1d6eb1ef452e', 133, 356, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('1. I think a number of clustering algorithms that normally use a metric, do not actually rely on the metric properties (other than commutativity, but I think you''d have that here).  For example, DBSCAN uses epsilon-neighborhoods around a point; there is nothing in there that specifically says the triangle inequality matters.  So you can probably use DBSCAN, although you may have to do some kind of nonstandard spatial index to do efficient lookups in your case.  Your version of epsilon-neighborhood will likely be sim > 1/epsilon rather than the other way around.  Same story with k-means and related algorithms.

2. Can you construct a metric from your similarity?  One possibility: dist(ei, ej) = min( sim(ei, ek) + sim(ek, ej) ) for all k ...  Alternately, can you provide an upper  bound such that sim(ei, ej) < sim(ei, ek) + sim(ek, ej) + d, for all k and some positive constant d?  Intuitively, large sim values means closer together: is 1/sim metric-like?  What about 1/(sim + constant)?  What about min( 1/sim(ei, ek) + 1/sim(ek, ej) ) for all k? (that last is guaranteed to be a metric, btw)

3. An alternate construction of a metric is to do an embedding.  As a first step, you can try to map your points ei -> xi, such that xi minimize sum( abs( sim(ei, ej) - f( dist(xi, xj) ) ), for some suitable function f and metric dist.  The function f converts distance in the embedding to a similarity-like value; you''d have to experiment a bit, but 1/dist or exp^-dist are good starting points.  You''d also have to experiment on the best dimension for xi.  From there, you can use conventional clustering on xi.  The idea here is that you can almost (in a best fit sense) convert your distances in the embedding to similarity values, so they would cluster correctly.

4. On the use of predefined parameters, all algorithms have some tuning.  DBSCAN can find the number of clusters, but you still need to give it some parameters.  In general, tuning requires multiple runs of the algorithm with different values for the tunable parameters, together with some function that evaluates goodness-of-clustering (either calculated separately, provided by the clustering algorithm itself, or just eyeballed :)  If the character of your data doesn''t change, you can tune once and then use those fixed parameters; if it changes then you have to tune for each run.  You can find that out by tuning for each run and then comparing how well the parameters from one run work on another, compared to the parameters specifically tuned for that.
', 26, '2014-05-18 09:17:15.557', 'b196fe2b-d065-4ad0-8a76-d2e354302de2', 133, 'added 113 characters in body', 357, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In our company, we have a MongoDB database containing a lot of unstructured data, on which we need to run map-reduce algorithms to generate reports and other analyses. We have two approaches to select from for implementing the required analyses:

 1. One approach is to extract the data from MongoDB to a Hadoop cluster and do the analysis completely in Hadoop platform. However, this requires considerable investment on preparing the platform (software and hardware) and educating the team to work with Hadoop and write map-reduce tasks for it.

 2. Another approach is to just put our effort on designing the map-reduce algorithms, and run the algorithms on MongoDB map-reduce functionalities. This way, we can create an initial prototype of final system that can generate the reports. I know that the MongoDB''s map-reduce functionalities are much slower compared to Hadoop, but currently the data is not that big that makes this a bottleneck yet, at least not for the next six months.

The question is, using the second approach and writing the algorithms for MongoDB, can them be later ported to Hadoop with little needed modification and algorithm redesign? MongoDB just supports JavaScript but programming language differences are easy to handle. However, is there any fundamental differences in the map-reduce model of MongoDB and Hadoop that may force us to redesign algorithms substantially for porting to Hadoop?', 227, '2014-05-18 12:03:21.650', '9b507165-272e-4eb6-aa7c-bae5c8394746', 134, 358, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can map-reduce algorithms written for MongoDB be ported to Hadoop later?', 227, '2014-05-18 12:03:21.650', '9b507165-272e-4eb6-aa7c-bae5c8394746', 134, 359, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<scalability><hadoop><map-reduce><mongodb>', 227, '2014-05-18 12:03:21.650', '9b507165-272e-4eb6-aa7c-bae5c8394746', 134, 360, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Considering another criteria, I think that in some cases using Python may be much superior to R for Big Data. I know the wide-spread use of R in data science educational materials and the good data analysis libraries available for it, but sometimes it just depend on the team.

In my experience, for people already familiar with programming, using Python provides much more flexibility and productivity boost compared to a language like R, which is not as well-designed and powerful compared to Python in terms of a programming language. As an evidence, in a data mining course in my university, the best final project was written in Python, although the others has access to R''s rich data analysis library. That is, sometimes the overall productivity (considering learning materials, documentation, etc.) for Python may be better than R even in the lack of special-purpose data analysis libraries for Python. Also, there are some good articles explaining the fast pace of Python in data science: [Python Displacing R][1] and [Rich Scientific Data Structures in Python][2] that may soon fill the gap of available libraries for R.

Another important reason for not using R is when working with real world Big Data problems, contrary to academical only problems, there is much need for other tools and techniques, like data parsing, cleaning, visualization, web scrapping, and a lot of others that are much easier using a general purpose programming language. This may be why the default language used in many Hadoop courses (including the Udacity''s [online course][3]) is Python.


  [1]: http://readwrite.com/2013/11/25/python-displacing-r-as-the-programming-language-for-data-science
  [2]: http://wesmckinney.com/blog/?p=77
  [3]: https://www.udacity.com/course/ud617', 227, '2014-05-18 12:30:06.853', '014f125f-755e-46c0-8e6f-bb777726e0ee', 135, 361, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 12:55:39.657', '76e5eeeb-f4b1-4d62-b4f9-5926d07e0f9d', 136, 362, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 12:55:39.657', '10239549-8176-469a-a3af-ca219aecf11c', 137, 363, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<dimensionality-reduction><feature-selection>', 22, '2014-05-18 13:32:26.123', 'eda67480-4c12-4dbc-9db0-5dc9d422975d', 130, 'This question doesn''t actually involve natural language processing.', 365, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-18 13:32:26.123', 'eda67480-4c12-4dbc-9db0-5dc9d422975d', 130, 'Proposed by 22 approved by 122 edit id of 34', 366, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Any small database processing can be easily tackled by Python/Perl/... scripts, that uses libraries and/or even utilities from the language itself. However, when it comes to performance, everyone reaches out for C/C++. The possibility of tailoring the code to the needs seems to be what makes these languages so appealing for BigData -- be it concerning memory management, parallelism, disk access, or even low-level optimizations (via assembly constructs at C/C++ level).

Of course such set of benefits would not come without a cost: writing the code, and sometimes even rewriting things that dozens of libraries have already implemented, can be quite expensive/tiresome. Although there are lots of implementations available, people are inclined to write the code by themselves whenever they need to *grant* performance. What *disables* performance assertions from using libraries while processing large databases?', 84, '2014-05-18 14:02:51.350', 'ef42bc20-caf8-4c8f-b4c9-dbf915ca6aee', 138, 367, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Granting efficiency on BigData by rewriting C++ code', 84, '2014-05-18 14:02:51.350', 'ef42bc20-caf8-4c8f-b4c9-dbf915ca6aee', 138, 368, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><efficiency><performance>', 84, '2014-05-18 14:02:51.350', 'ef42bc20-caf8-4c8f-b4c9-dbf915ca6aee', 138, 369, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''d suggest [Apache Kafka][1] as message store and any stream processing solution of your choice like [Apache Camel][2] or [Twitter Storm][3]


  [1]: http://kafka.apache.org
  [2]: https://camel.apache.org
  [3]: https://github.com/apache/incubator-storm', 118, '2014-05-18 14:04:37.870', '3441d74f-46c8-4df4-a3ae-b7427bd72a75', 139, 370, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve read very good [article][1] recently that suggests using [Twitter storm][2] for a task that looks pretty similar to yours.


  [1]: http://www.michael-noll.com/blog/2013/01/18/implementing-real-time-trending-topics-in-storm/
  [2]: https://github.com/nathanmarz/storm', 118, '2014-05-18 14:30:10.553', '61a0191e-f112-4499-a474-3f023a96be0a', 140, 371, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 14:36:16.350', '7cfc55a3-b990-4462-90fc-532252ce80c9', 141, 372, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 14:36:16.350', 'b198f2ed-40b4-4942-b7c7-3adbabb32730', 142, 373, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As All we know There are some data indexing techniques  using by well-known indexing apps like Lucene (for java) or Lucene.NET (for .NET), MurMurHash, B+Tree etc.

For a No-Sql / OO Database (which i try to write/play a little around with C#) which technique you suggest ?

I read about MurMurhash-2 and specially v3 comments say Murmur is very fast, also Lucene.Net has good comments..

But what about their memory footprints in general?

Is there any efficient solution which uses less footprint (and of course if faster is preferable) than Lucene or Murmur?

Or should i write a special index structure to get the best results?

If i try to write my own, then is there any accepted scale for a good indexing some like 1% of data-node or 5% of data-node?

Any useful Hint will be appreciated.

Thanks from now..
', 229, '2014-05-18 14:37:20.477', '39a2d7a7-772d-409d-8f68-36ef1dcb80ef', 143, 374, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the most efficient data indexing technique', 229, '2014-05-18 14:37:20.477', '39a2d7a7-772d-409d-8f68-36ef1dcb80ef', 143, 375, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nosql><indexing><data-indexing-techniques>', 229, '2014-05-18 14:37:20.477', '39a2d7a7-772d-409d-8f68-36ef1dcb80ef', 143, 376, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><nosql><indexing><data-indexing-techniques>', 229, '2014-05-18 14:58:05.690', '2018fad7-5c35-42a4-babd-4905526530b4', 143, 'edited tags', 377, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 14:58:34.853', 'a6f4f52a-033d-4ac6-9874-9e2eec1e92f9', 144, 378, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 14:58:34.853', '6446ad63-f799-4559-be4a-bd3f10f3cb3e', 145, 379, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 15:01:24.080', 'f6c5b35f-2b77-467d-a254-1149f8dbff4c', 146, 380, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 15:01:24.080', 'f61d922c-2429-4143-a17a-84b70211fc74', 147, 381, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 15:08:08.913', '5f872d67-c07b-4130-9110-e9cd1a184583', 148, 382, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 15:08:08.913', '7fcde074-4b93-4867-99d1-9ac84d2e876f', 149, 383, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As we all know, there are some data indexing techniques, using by well-known indexing apps, like Lucene (for java) or Lucene.NET (for .NET), MurMurHash, B+Tree etc. For a No-Sql/OO Database (which I try to write/play a little around with C#), which technique you suggest?

I read about MurMurhash-2 and specially v3 comments say Murmur is very fast. Also Lucene.Net has good comments on it. But what about their memory footprints in general? Is there any efficient solution which uses less footprint (and of course if faster is preferable) than Lucene or Murmur? Or should I write a special index structure to get the best results?

If I try to write my own, then is there any accepted scale for a good indexing, something like 1% of data-node, or 5% of data-node? Any useful hint will be appreciated.', 84, '2014-05-18 15:11:00.360', '25f983f3-920b-4ad9-867f-43a6678822e4', 143, 'Fixed grammar, and improving formatting.', 384, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><nosql><efficiency><indexing>', 84, '2014-05-18 15:11:00.360', '25f983f3-920b-4ad9-867f-43a6678822e4', 143, 'Fixed grammar, and improving formatting.', 385, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-18 15:11:00.360', '25f983f3-920b-4ad9-867f-43a6678822e4', 143, 'Proposed by 84 approved by 229 edit id of 41', 386, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><nosql><efficiency><indexing><data-indexing-techniques>', 229, '2014-05-18 15:17:37.167', '9a7b9b70-e8b8-42bb-88f0-624b3f9e67ad', 143, 'edited tags', 387, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><tools><data-stream-mining>', 118, '2014-05-18 15:18:08.050', '2cfaeaf8-6e0b-4b19-b54c-6c847299f712', 76, 'retagged post', 388, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-18 15:18:08.050', '2cfaeaf8-6e0b-4b19-b54c-6c847299f712', 76, 'Proposed by 118 approved by 158 edit id of 37', 389, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 15:34:16.437', '8d5c0b8c-d82f-454e-b048-afa247a64b3a', 151, 393, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-18 15:34:16.437', '8c55f680-9d4a-47c6-8b8e-8885f5cbc541', 152, 394, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As we all know, there are some data indexing techniques, using by well-known indexing apps, like Lucene (for java) or Lucene.NET (for .NET), MurMurHash, B+Tree etc. For a No-Sql / Object Oriented Database (which I try to write/play a little around with C#), which technique you suggest?

I read about MurMurhash-2 and specially v3 comments say Murmur is very fast. Also Lucene.Net has good comments on it. But what about their memory footprints in general? Is there any efficient solution which uses less footprint (and of course if faster is preferable) than Lucene or Murmur? Or should I write a special index structure to get the best results?

If I try to write my own, then is there any accepted scale for a good indexing, something like 1% of data-node, or 5% of data-node? Any useful hint will be appreciated.', 229, '2014-05-18 15:36:24.210', '153292cc-3e2c-4760-af2b-4d4ec6c1f59d', 143, 'added 15 characters in body', 395, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[This question](http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm) asks about generative vs. discriminative algorithm, but can someone give an example of the difference between these forms when applied to Natural Language Processing? **How are generative and discriminative models used in NLP?**', 84, '2014-05-18 15:40:10.727', '4ad8fdfc-eb51-44bf-9711-c707da2467f6', 129, 'Improving formatting.', 396, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-18 15:40:10.727', '4ad8fdfc-eb51-44bf-9711-c707da2467f6', 129, 'Proposed by 84 approved by 122 edit id of 46', 397, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I want learn about NoSQL and when is better to use SQL or NoSQL. I know that this question depends on the case, but I''m asking for a good documentation on NoSQL, and some explanation of when is better to use SQL or NoSQL (use cases, etc). Also, your opinions on NoSQL databases, and any recommendations for learning about this topic are welcome.', 84, '2014-05-18 16:10:50.820', 'cea45259-df79-4e98-ae79-5427ae6d468b', 125, 'Fixed grammar, and improving formatting.', 399, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to learn noSQL databases and how to know when SQL or noSQL is better', 84, '2014-05-18 16:10:50.820', 'cea45259-df79-4e98-ae79-5427ae6d468b', 125, 'Fixed grammar, and improving formatting.', 400, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-18 16:10:50.820', 'cea45259-df79-4e98-ae79-5427ae6d468b', 125, 'Proposed by 84 approved by 26, 109 edit id of 31', 401, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The answers presented so far are very nice, but I was also expecting an emphasis on a particular difference between parallel and distributed processing: the code executed. Considering parallel processes, the code executed is the same, regardless of the level of parallelism (instruction, data, task). You write a *single code*, and it will be executed by different threads/processors, e.g., while computing matrices products, or generating permutations.

On the other hand, distributed computing involves the execution of different algorithms/programs at the same time in different processors (from one or more machines). Such computations are later merged into a intermediate/final results by using the available means of data communication/synchronization (shared memory, network). Further, distributed computing is very appealing for BigData processing, as it allows for exploiting disk parallelism (usually the bottleneck for large databases).

Finally, for the level of parallelism, it may be taken rather as a constraint on the synchronization. For example, in GPGPU, which is single-instruction multiple-data (SIMD), the parallelism occurs by having different inputs for a single instruction, each pair *(data_i, instruction)* being executed by a different thread. Such is the restraint that, in case of divergent branches, it is necessary to discard lots of unnecessary computations, until the threads reconverge. For CPU threads, though, they commonly diverge; yet, one may use synchronization structures to grant concurrent execution of specific sections of the code.', 84, '2014-05-18 17:38:01.383', '55e55168-ffe9-4529-91e6-292f5bf3b276', 153, 403, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Check [Martin Fowler''s Personal website][1] He write good, and specially answer of your question One of his book : "NoSQL Distilled"


  [1]: http://www.martinfowler.com', 229, '2014-05-18 17:53:37.750', 'a61ca912-f857-4806-9576-5b766d47d6c4', 154, 405, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One of the common problems in data science is gathering data from various sources in a somehow cleaned (semi-structured) format and combining metric from various sources for making higher level analysis. Looking at the other people''s effort, especially other questions on this site, it appears that many people in this field are doing somehow repetitive works. For example, analyzing tweets, Facebook posts, Wikipedia articles, etc. is somehow part lots of Big Data problems.

Some of these data sets are accessible using public APIs provided by the provider site, but usually some valuable calculated values are missing from those API outputs, and everyone has to do somehow same analyses over and over again. For example, although clustering users may depend on different use casesa and selection of features, but having a base clustering of twitter/Facebook users can be useful in many Big Data applications, which is neither provided by the API, nor available publicly in independent data sets.

Is there any index or publicly available data set hosting site containing valuable data sets that can be reused in solving other Big Data problems? I mean something like GitHub (or a group of sites/public data sets or at least a comprehensive listing) for the data science. If not, what are the reasons of not having such a platform for data science? Commercial values of data, need to frequent update data sets, ...? Can not a open-source model for sharing data sets devised for data scientists?', 227, '2014-05-18 18:45:38.957', '00d62c57-1a00-4d59-b4fb-d764f8f8ad27', 155, 406, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Publicly available datasets', 227, '2014-05-18 18:45:38.957', '00d62c57-1a00-4d59-b4fb-d764f8f8ad27', 155, 407, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<open-source><dataset>', 227, '2014-05-18 18:45:38.957', '00d62c57-1a00-4d59-b4fb-d764f8f8ad27', 155, 408, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('One of the common problems in data science is gathering data from various sources in a somehow cleaned (semi-structured) format and combining metric from various sources for making higher level analysis. Looking at the other people''s effort, especially other questions on this site, it appears that many people in this field are doing somehow repetitive works. For example, analyzing tweets, Facebook posts, Wikipedia articles, etc. is somehow part lots of Big Data problems.

Some of these data sets are accessible using public APIs provided by the provider site, but usually some valuable calculated values are missing from those API outputs, and everyone has to do somehow same analyses over and over again. For example, although clustering users may depend on different use cases and selection of features, but having a base clustering of Twitter/Facebook users can be useful in many Big Data applications, which is neither provided by the API, nor available publicly in independent data sets.

Is there any index or publicly available data set hosting site containing valuable data sets that can be reused in solving other Big Data problems? I mean something like GitHub (or a group of sites/public data sets or at least a comprehensive listing) for the data science. If not, what are the reasons of not having such a platform for data science? Commercial value of data, need to frequently update data sets, ...? Can not a open-source model for sharing data sets devised for data scientists?', 227, '2014-05-18 19:12:38.843', 'ea894b83-4a12-42c1-afc1-b72597a2320d', 155, 'edited body', 411, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Freebase][1] is a free community driven database that spans many interesting topics and contains about 2,5 billion facts in machine readable format. It is also have good API to perform data queries.


  [1]: https://www.freebase.com', 118, '2014-05-18 19:19:44.240', 'f7a4357c-8d2f-4c77-b676-b707d58da289', 156, 412, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R is great for "big data"! However, you need a workflow since R is limited (with some simplification) by the amount of RAM in the operating system. The approach I take is to interact with a relational database (see the `RSQLite` package for creating and interacting with a SQLite databse), run SQL-style queries to understand the structure of the data, and then extract particular subsets of the data for computationally-intensive statistical analysis.

This just one approach, however: there are packages that allow you to interact with other databases (e.g., Monet) or run analyses in R with fewer memory limitations (e.g., see `pbdR`).', 36, '2014-05-18 19:22:05.160', 'cda8d654-a386-4a69-bc89-324580315738', 157, 413, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is, in fact, a very reasonable list of publicly-available datasets, supported by different enterprises/sources. Here are some of them:

- [Public Datasets on Amazon WebServices](http://aws.amazon.com/publicdatasets/);
- [Frequent Itemset Mining Implementation Repository](http://fimi.ua.ac.be/data/);
- [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets.html);
- [KDnuggets](http://www.kdnuggets.com/datasets/index.html) -- big list of lots of public repositories.

Now, two considerations on your question. First one, regarding policies of database sharing. From personal experience, there are some databases that can''t be made publicly available, either for involving privacy restraints (as for some social network informations), or for concerning government information (like health system databases).

Another point concerns the usage/application of the dataset. Although some bases can be reprocessed to suit the needs of the application, it would be great to have some *nice organization* of the datasets by purpose. The *taxonomy* should involve social graph analysis, itemset mining, classification, and lots of other reasearch areas there may be.', 84, '2014-05-18 19:29:53.530', '8bf75579-956c-4bce-acbd-eb2eb5b07a48', 158, 414, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What is the best noSQL backend to use for a mobile game? Users can make a lot of servers requests, it needs also to retrieve users'' historical records (like app purchasing) and analytics of usage behavior.', 229, '2014-05-18 19:41:19.157', '29eced39-f07a-41a4-8ce7-b94f6c2b06c9', 102, 'correct spellings, added a new thought-related tag', 415, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What is the Best NoSQL backend for a mobile game', 229, '2014-05-18 19:41:19.157', '29eced39-f07a-41a4-8ce7-b94f6c2b06c9', 102, 'correct spellings, added a new thought-related tag', 416, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<nosql><performance>', 229, '2014-05-18 19:41:19.157', '29eced39-f07a-41a4-8ce7-b94f6c2b06c9', 102, 'correct spellings, added a new thought-related tag', 417, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-18 19:41:19.157', '29eced39-f07a-41a4-8ce7-b94f6c2b06c9', 102, 'Proposed by 229 approved by 199 edit id of 47', 418, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Any small database processing can be easily tackled by Python/Perl/... scripts, that uses libraries and/or even utilities from the language itself. However, when it comes to performance, everyone reaches out for C/C++. The possibility of tailoring the code to the needs seems to be what makes these languages so appealing for BigData -- be it concerning memory management, parallelism, disk access, or even low-level optimizations (via assembly constructs at C/C++ level).

Of course such set of benefits would not come without a cost: writing the code, and sometimes even rewriting things that dozens of libraries have already implemented, can be quite expensive/tiresome. Although there are lots of implementations available, people are inclined to write the code by themselves whenever they need to *grant* performance. What *disables* performance assertions from using libraries while processing large databases?

**Edit**: In an attempt to clarify the post, here is a more succint/direct question:

- What makes writing the code by oneself a *guarantee* of performance?
- Why is it *risky* to count on a library implementation when you must **assure** high performance?', 84, '2014-05-18 19:44:39.420', '536423ab-0931-4b92-800a-8dafa6cbe438', 138, 'added 253 characters in body', 419, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I see a lot of courses in Data Science emerging in the last 2 years. Even big universities like Stanford and Columbia offers MS specifically in Data Science. But as long as I see, it looks like data science is just a mix of computer science and statistics techniques.
So I always think about this. If it is just a trend and if in 10 years from now, someone will still mention Data Science as an entire field or just a subject/topic inside CS or stats.
What do you think?', 199, '2014-05-18 19:46:44.653', 'c5683904-1635-432d-abd6-73f1bfccb73a', 159, 420, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is Data Science just a trend or is a long term concept?', 199, '2014-05-18 19:46:44.653', 'c5683904-1635-432d-abd6-73f1bfccb73a', 159, 421, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><machine-learning><databases><statistics><education>', 199, '2014-05-18 19:46:44.653', 'c5683904-1635-432d-abd6-73f1bfccb73a', 159, 422, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The one thing that you can say for sure is: Nobody can say this for sure. And it might indeed be opinion-based to some extent. The introduction of terms like "Big Data" that some people consider as "hypes" or "buzzwords" don''t make it easier to flesh out an appropriate answer here. But I''ll try.

In general, interdisciplinary fields often seem to have the problem of not being taken serious by either of the fields they are spanning. However, the more research is invested into a particular field, the greater is the urge to split this field into several sub-topics. And these sub-topics sonner of later have to be re-combined in new ways, in order to prevent an overspecialization, and to increase and broaden the applicability of techniques that are developed by the (over?)specialized experts in the different fields.

And I consider "Data Science" as such an approach to combine the expertise and findings from different fields. You described it as
> ...a mix of computer science and statistics techniques

And indeed, several questions here aim at the differentiation between data science and statistics. But a pure statistician will most likely not be able to set up a Hadoop cluster and show the results of his analysis in an interactive HTML5 dashboard. And someone who can implement a nice HTML5 dashboard might not be so familiar with the mathematical background of a Chi-Squared-Test.

It is reasonable to assume that giving students enough knowledge to *apply* the most important techniques from the different fields that are covered by data science will lead to new applications of these techniques, and be beneficial - also for the "purists" in these fields. The combination of these techniques is not straightforward in many cases, and can justify an own branch of research.

You also asked whether in 10 years, data science will be considered as "just a topic inside computer science". Again: Nobody can say for sure. But I wonder at which point people stopped asking the question whether "Computer Science" will one day only be considered only as a mix of (or a subject of) Electrical Engineering and Mathematics...





', 156, '2014-05-18 21:05:28.990', '384cfbcf-0580-4bd6-88fe-8f596503483e', 160, 423, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I don''t think that everyone reaches for C/C++ when performance is an issue.

The advantage to writing low-level code is using fewer CPU cycles, or sometimes, less memory. But I''d note that higher-level languages can call down to lower-level languages, and do, to get some of this value. Python and JVM languages can do this.

The data scientist using, for example, scikit-learn on her desktop is already calling heavily optimized native routines to do the number crunching. There is no point in writing new code for speed.

In the distributed "big data" context, you are more typically bottleneck on data movement: network transfer and I/O. Native code does not help. What helps is not writing the same code to run faster, but writing smarter code.

Higher-level languages are going to let you implement more sophisticated distributed algorithms in a given amount of developer time than C/C++. At scale, the smarter algorithm with better data movement will beat dumb native code.

It''s also usually true that developer time, and bugs, cost loads more than new hardware. A year of a senior developer''s time might be $200K fully loaded; over a year that also rents hundreds of servers worth of computation time. It may just not make sense in most cases to bother optimizing over throwing more hardware at it.

_I don''t understand the follow up about "grant" and "disable" and "assert"?_', 21, '2014-05-18 21:25:29.040', 'b6b733bd-f282-4e1e-a8bb-1a517d2c4e42', 161, 424, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are many openly available data sets, one many people often overlook is [data.gov][1]. As mentioned previously Freebase is great, so are all the examples posted by @Rubens


  [1]: http://www.data.gov/', 59, '2014-05-18 22:16:19.300', '8fe23961-09ef-4cff-9f16-325e6280354e', 162, 425, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As all we know, in Digital world there are many ways to do the same work / get expected results..

And responsibilities / risks which comes from the code are on developers'' shoulders..

This is small but i guess a very useful example from .NET world..

So Many .NET developers use the built-in BinaryReader - BinaryWriter on their data serialization for performance / get control over the process..

This is CSharp source code of the FrameWork''s built in BinaryWriter class'' one of the overloaded Write Methods :



    // Writes a boolean to this stream. A single byte is written to the stream
    // with the value 0 representing false or the value 1 representing true.
    //
    public virtual void Write(bool value)
    {
         //_buffer is a byte array which declared in ctor / init codes of the class
        _buffer = ((byte) (value? 1:0));

        //OutStream is the stream instance which BinaryWriter Writes the value(s) into it.
        OutStream.WriteByte(_buffer[0]);
    }


As you see, this method could written without no extra assigning :



    public virtual void Write(bool value)
    {
        OutStream.WriteByte((byte) (value ? 1 : 0));
    }

Without assigning we could gain few milliseconds..This few milliseconds can accept as "almost nothing" but what if there are multi-thousands of writing (i.e. in a server process)?

Lets suppose that "few" is 2 (milliseconds) and multi-Thousands instances are only 2.000..
This means 4 seconds more process time..4 seconds later returning..

If we continue to subject from .NET and if you can check the source codes of BCL - .NET Base Class Library- from MSDN you can see a lot of performance losts from the developer decides..

Any of the point from BCL source It''s normal that you see developer decided to use while() or foreach() loops which could implement a faster for() loop in their code.

This small gains give us the total performance..


And if we return to the BinaryWriter.Write() Method..

Actually extra assigning to a _buffer implementation is not a developer fault..This is exactly decide to "stay in safe" !

Suppose that we decide to not use _buffer and decided to implement the second method..If we try to send multi-thousands bytes over a wire (i.e. upload / download a BLOB or CLOB data) with the second method, it can fail commonly because of connection lost..Cause we try to send all data without any checks and controlling mechanism.When connection lost, Both the server and Client never know the sent data completed or not.

If the developer decides "stay in safe" then normally it means performance costs depends to implemented "stay in safe" mechanism(s).

But if the developer decides "get risky, gain performance" this is not a fault also..Till there are some discussions about "risky" coding.

And as a small note : Commercial library developers always try to stay in safe because they can''t know where their code will use.

', 229, '2014-05-18 23:21:07.220', 'e57d83d8-1c28-4c49-950c-6c4abb32a966', 163, 426, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As all we know, in Digital world there are many ways to do the same work / get expected results..

And responsibilities / risks which comes from the code are on developers'' shoulders..

This is small but i guess a very useful example from .NET world..

So Many .NET developers use the built-in BinaryReader - BinaryWriter on their data serialization for performance / get control over the process..

This is CSharp source code of the FrameWork''s built in BinaryWriter class'' one of the overloaded Write Methods :



    // Writes a boolean to this stream. A single byte is written to the stream
    // with the value 0 representing false or the value 1 representing true.
    //
    public virtual void Write(bool value)
    {
         //_buffer is a byte array which declared in ctor / init codes of the class
        _buffer = ((byte) (value? 1:0));

        //OutStream is the stream instance which BinaryWriter Writes the value(s) into it.
        OutStream.WriteByte(_buffer[0]);
    }


As you see, this method could written without the extra assigning to _buffer variable:



    public virtual void Write(bool value)
    {
        OutStream.WriteByte((byte) (value ? 1 : 0));
    }

Without assigning we could gain few milliseconds..This few milliseconds can accept as "almost nothing" but what if there are multi-thousands of writing (i.e. in a server process)?

Lets suppose that "few" is 2 (milliseconds) and multi-Thousands instances are only 2.000..
This means 4 seconds more process time..4 seconds later returning..

If we continue to subject from .NET and if you can check the source codes of BCL - .NET Base Class Library- from MSDN you can see a lot of performance losts from the developer decides..

Any of the point from BCL source It''s normal that you see developer decided to use while() or foreach() loops which could implement a faster for() loop in their code.

This small gains give us the total performance..


And if we return to the BinaryWriter.Write() Method..

Actually extra assigning to a _buffer implementation is not a developer fault..This is exactly decide to "stay in safe" !

Suppose that we decide to not use _buffer and decided to implement the second method..If we try to send multi-thousands bytes over a wire (i.e. upload / download a BLOB or CLOB data) with the second method, it can fail commonly because of connection lost..Cause we try to send all data without any checks and controlling mechanism.When connection lost, Both the server and Client never know the sent data completed or not.

If the developer decides "stay in safe" then normally it means performance costs depends to implemented "stay in safe" mechanism(s).

But if the developer decides "get risky, gain performance" this is not a fault also..Till there are some discussions about "risky" coding.

And as a small note : Commercial library developers always try to stay in safe because they can''t know where their code will use.

', 229, '2014-05-18 23:29:39.227', 'b7ef4714-758f-41b1-a87b-316c90af77a6', 163, 'added 20 characters in body', 427, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What @Clayton posted seems about right to me, for those terms, and for "data mining" being one tool of the data scientist. However, I haven''t really used the term "data collection," and it doesn''t strike me as synonymous with "data mining."

My own answer to your question: **no**, the terms aren''t the same. Definitions may be loose in this field, but I haven''t seen those terms used interchangeably. In my work, we sometimes use them to differentiate between goals, or methodologies. For us, [tag:data-science] is more about testing a hypothesis, and typically the data have been collected just for that purpose. [tag:data-mining] is more about sifting through existing data, looking for structure, and perhaps generating hypotheses. Data mining can start with a hypothesis, but it''s often very weak or general, and can be difficult to resolve with confidence. (Dig long enough and you''ll find *something*, though it may turn out to be pyrite.)

However, we also have used "data science" as a wider term, to include "data mining." We also talk about "data modeling," which for us is about finding a model for a system of interest, based on data as well as other knowledge and objectives. Sometimes that means trying to find the math that explains the real system, and sometimes it means finding a predictive model that is good enough for a purpose.', 208, '2014-05-19 00:09:17.900', '14034850-865c-45c4-8ab0-3dd82d45855f', 164, 428, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<tools><data-stream-mining>', 118, '2014-05-19 07:33:50.080', '0b5a80cf-c9d0-4ae4-b424-917a23944beb', 107, 'retagged post', 429, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-19 07:33:50.080', '0b5a80cf-c9d0-4ae4-b424-917a23944beb', 107, 'Proposed by 118 approved by 26, 200 edit id of 38', 430, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Considering another criteria, I think that in some cases using Python may be much superior to R for Big Data. I know the wide-spread use of R in data science educational materials and the good data analysis libraries available for it, but sometimes it just depend on the team.

In my experience, for people already familiar with programming, using Python provides much more flexibility and productivity boost compared to a language like R, which is not as well-designed and powerful compared to Python in terms of a programming language. As an evidence, in a data mining course in my university, the best final project was written in Python, although the others has access to R''s rich data analysis library. That is, sometimes the overall productivity (considering learning materials, documentation, etc.) for Python may be better than R even in the lack of special-purpose data analysis libraries for Python. Also, there are some good articles explaining the fast pace of Python in data science: [Python Displacing R][1] and [Rich Scientific Data Structures in Python][2] that may soon fill the gap of available libraries for R.

Another important reason for not using R is when working with real world Big Data problems, contrary to academical only problems, there is much need for other tools and techniques, like data parsing, cleaning, visualization, web scrapping, and a lot of others that are much easier using a general purpose programming language. This may be why the default language used in many Hadoop courses (including the Udacity''s [online course][3]) is Python.

**Edit:**

Recently DARPA has also invested $3 million to help fund Python''s data processing and visualization capabilities for big data jobs, which is clearly a sign of Python''s future in Big Data. ([details][4])


  [1]: http://readwrite.com/2013/11/25/python-displacing-r-as-the-programming-language-for-data-science
  [2]: http://wesmckinney.com/blog/?p=77
  [3]: https://www.udacity.com/course/ud617
  [4]: http://www.computerworld.com/s/article/9236558/Python_gets_a_big_data_boost_from_DARPA', 227, '2014-05-19 08:13:05.037', '3f021d76-d7ca-4a16-bf87-5b105e201c3e', 135, 'added 314 characters in body', 431, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think you messed up some things in your question. Lucene (I know nothing about Lucene,NET, but I suppose is the same) is a library used to analyze, split in tokens, and store documents in order to be able to query and retrieve them later. Lucene has a pretty old but effective model, it uses inverted trees to find and retrieve documents. Without further details, all documents are split in tokens (terms), and for each term is maintained a data structure, which stores all the documents which contains the given term. As a data structure could be used a BTree, a hash table and in the latest major revisions you can even plug in your own data structures.

A BTree (see [Wikipedia page][1] for further details), is a kind of a tree data structure, which is appropriate for working with big chunks of data and is often used for storing tree-like ordered structures on disk. For in-memory other trees performs better.

Murmur hash (see [Wikipedia page][2] for further details), is a family of hash functions used in hash table. The implementation of the hash table is not important, it could be a standard chained implementation or more advanced open hash addressing scheme. The idea is that the hash tables allows one to get fast a key, from an unordered set of keys, and can answer to tasks like: is this key part of this set of keys? which is the value associated with this key?

Now back to your main problem. You have one library (Lucene) and to data structures, both data structures are used in Lucene. Now you see that it is not possible to answer your question in these terms since they are not comparable.

However, regarding you footprint and performance part of the question. First of all you have to know which kind of operations you need to implement.

*Do you need only get value for key, or do you need to find all elements in a range? In other words do you need order or not?* If you do, than a tree can help. If you do not, than a hash table, which is faster could be used instead.

*Do you have a lot of data which does not fit the memory?* If yes than a disk-based solution would help (like BTree). If your data fit the memory, than use the fastest in-memory solution and use disk only as a storage (with a different structure, much simpler).


  [1]: http://en.wikipedia.org/wiki/B-tree
  [2]: http://en.wikipedia.org/wiki/MurmurHash', 108, '2014-05-19 08:46:47.683', '6af5a465-e3b6-4d8b-bf0e-2a4eb8fee574', 165, 432, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":84,"DisplayName":"Rubens"},{"Id":59,"DisplayName":"MCP_infiltrator"},{"Id":63,"DisplayName":"Puffin GDI"},{"Id":62,"DisplayName":"AsheeshR"},{"Id":108,"DisplayName":"rapaio"}]}', -1, '2014-05-19 08:54:23.303', '2baf6166-9101-45e9-b28c-2ecfec1e3b6e', 159, '105', 433, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Let''s say you are predicting the topic of a document given its words.

A generative model describes how likely each topic is, and how likely words are given the topic. This is how it says documents are actually "generated" by the world -- a topic arises according to some distribution, words arise because of the topic, you have a document. Classifying documents of words W into topic T is a matter of maximizing the joint likelihood: P(T,W) = P(W|T)P(T)

A discriminative model operates by only describing how likely a topic is given the words. It says nothing about how likely the words or topic are by themselves. The task is to model P(T|W) directly and find the T that maximizes this. These approaches do not care about P(T) or P(W) directly.', 21, '2014-05-19 11:13:48.067', 'b04f82bd-73bd-41a8-ba33-2de13597e991', 166, 434, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<nosql><efficiency><indexing><data-indexing-techniques><.net>', 229, '2014-05-19 12:05:13.513', '74927709-0a38-49be-a78c-8b81c9a82134', 143, 'edited tags', 435, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-19 12:17:45.960', '1d4cf600-9143-4e0d-82c4-c8c0fe2cb89b', 167, 436, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-19 12:17:45.960', 'f3625077-630c-48cd-a3d2-200272e60ffb', 168, 437, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Assume a set of loosely structured data (e.g. Web tables/Linked Open Data), composed of many data sources. There is no common schema followed by the data and each source can use synonym attributes to describe the values (e.g. "nationality" vs "bornIn").

My goal is to find some "important" attributes that somehow "define" the entities that they describe. So, when I find the same value for such an attribute, I will know that the two descriptions are most likely about the same entity (e.g. the same person).

For example, the attribute "lastName" is more discriminative than the attribute "nationality".

**How could I (statistically) find such attributes that are more important than others?**

A naive solution would be to take the average IDF of the values of each attribute and make this the "importance" factor of the attribute. A similar approach would be to count how many distinct values appear for each attribute.

I have seen the term feature, or attribute selection in machine learning, but I don''t want to discard the remaining attributes, I just want to put higher weights to the most important ones. I am sure the machine learning community has much to offer to me in this problem :)', 113, '2014-05-19 15:55:24.983', 'bb30a00b-b3f7-4658-85af-53562b266c8b', 169, 438, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to specify "discriminative" attributes?', 113, '2014-05-19 15:55:24.983', 'bb30a00b-b3f7-4658-85af-53562b266c8b', 169, 439, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><statistics><feature-selection>', 113, '2014-05-19 15:55:24.983', 'bb30a00b-b3f7-4658-85af-53562b266c8b', 169, 440, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to specify important attributes?', 113, '2014-05-19 16:20:14.243', '65e2d125-bfda-4e3f-8b73-bb0497e468f7', 169, 'edited title', 441, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A possible solution is to calculate the [information gain](http://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain) associated to each attribute:

![Information Gain][1]

Initially you have the whole dataset, and compute the information gain of each item. The one item with the best information gain, you use to partition the dataset, using the values of such item. Then, you perform the same computations for each item (but the ones selected), and always choose the one which best *describes/differentiates* the entries from your dataset.

There are implementations available for such computations. [Decision trees](http://en.wikipedia.org/wiki/Decision_tree_learning) usually base their feature selection on the features with best information gain. You may use the resulting tree structure to find these *important* items.

  [1]: http://i.stack.imgur.com/sUheW.png', 84, '2014-05-19 18:08:32.327', '1188eb0a-063b-40b9-96b8-8f1346a62b9b', 170, 442, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A possible solution is to calculate the [information gain](http://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain) associated to each attribute:

![Information Gain][1]

Initially you have the whole dataset, and compute the information gain of each item. The item with the best information gain is the one you should use to partition the dataset (considering the item''s values). Then, perform the same computations for each item (but the ones selected), and always choose the one which best *describes/differentiates* the entries from your dataset.

There are implementations available for such computations. [Decision trees](http://en.wikipedia.org/wiki/Decision_tree_learning) usually base their feature selection on the features with best information gain. You may use the resulting tree structure to find these *important* items.

  [1]: http://i.stack.imgur.com/sUheW.png', 84, '2014-05-19 18:19:08.730', 'de17a888-1bc1-4911-a69c-d0b26f601fc7', 170, 'added 10 characters in body', 443, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Two things you might find useful:

1. [meta-learning][1] to speedup the search for the right model and the optimal parameters.
Meta learning consists in applying machine learning tools to the problem of finding the right machine learning tool/parameters for the problem at hand. This for instance [this paper][2] for a practical example;

2. [gpucomputing][3] to speedup the algorithm on larger datasets. For instance, [OpenCV can use GPUs][4], which are very effective at processing images/videos and can bring 10 to 100 speedups with respect to CPUs. As your computer most probably has a gpucomputing-able GPU, you could gain lots of time using it.


  [1]: http://en.wikipedia.org/wiki/Meta_learning_%28computer_science%29
  [2]: http://link.springer.com/chapter/10.1007/978-3-642-14464-6_11
  [3]: http://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units
  [4]: http://docs.opencv.org/modules/gpu/doc/introduction.html', 172, '2014-05-19 19:44:48.500', '0bbbb106-477c-46a2-8a95-b7cad3067802', 171, 444, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a modeling and scoring program that makes heavy use of the DataFrame.isin function of pandas, searching through lists of facebook "like" records of individual users for each of a few thousand specific pages. This is the most time-consuming part of the program, more so than the modeling or scoring pieces, simply because it only runs on one core while the rest runs on a few dozen simultaneously.

Though I know I could manually break up the dataframe into chunks and run the operation in parallel, is there any straightforward way to do that automatically? In other words---is there any kind of package out there that will recognize I''m running an easily-delegated operation and automatically distribute it? Perhaps that''s asking for too much, but I''ve been surprised enough in the past by what''s already available in Python, so I figure it''s worth asking.

Any other suggestions about how this might be accomplished (even if not by some magic unicorn package!) would also be appreciated. Mainly, just trying to find a way to shave off 15-20 minutes per run without spending an equal amount of time coding the solution.

Thanks!
-Andrew', 250, '2014-05-19 23:59:58.070', 'b77136c3-041a-4466-87e1-3a4034105a58', 172, 445, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is there a straightforward way to run pandas.DataFrame.isin in parallel?', 250, '2014-05-19 23:59:58.070', 'b77136c3-041a-4466-87e1-3a4034105a58', 172, 446, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<performance><python><pandas><parallel>', 250, '2014-05-19 23:59:58.070', 'b77136c3-041a-4466-87e1-3a4034105a58', 172, 447, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Guessing it''s likely you''ve seen this [YouTube demo][1] and the related [Google Tech Talk][2], which is related to these papers:

 - [P-N Learning: Bootstrapping Binary Classifiers by Structural Constraints][3]
 - [Tracking-Learning-Detection][4]

And this set of code on GitHub for [OpenTLD][5]. If you check the "read me" on GitHub here, you''ll notice that [author''s email (Zdenek Kalal)][6] is listed, so it might be worth sending him an email about your questions, or even inviting him to reply to this question too.


  [1]: http://www.youtube.com/watch?v=1GhNXHCQGsM
  [2]: http://www.youtube.com/watch?v=lmG_FjG4Dy8
  [3]: http://eprints.pascal-network.org/archive/00006951/01/cvpr2010.pdf
  [4]: http://epubs.surrey.ac.uk/713800/1/Kalal-PAMI-2011%281%29.pdf
  [5]: https://github.com/zk00006/OpenTLD/
  [6]: https://github.com/zk00006/OpenTLD/blob/master/README', 158, '2014-05-20 03:56:43.147', '0100aba1-5752-449b-a7a1-2e48d5914f7c', 173, 448, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a modeling and scoring program that makes heavy use of the `DataFrame.isin` function of pandas, searching through lists of facebook "like" records of individual users for each of a few thousand specific pages. This is the most time-consuming part of the program, more so than the modeling or scoring pieces, simply because it only runs on one core while the rest runs on a few dozen simultaneously.

Though I know I could manually break up the dataframe into chunks and run the operation in parallel, is there any straightforward way to do that automatically? In other words, is there any kind of package out there that will recognize I''m running an easily-delegated operation and automatically distribute it? Perhaps that''s asking for too much, but I''ve been surprised enough in the past by what''s already available in Python, so I figure it''s worth asking.

Any other suggestions about how this might be accomplished (even if not by some magic unicorn package!) would also be appreciated. Mainly, just trying to find a way to shave off 15-20 minutes per run without spending an equal amount of time coding the solution.', 84, '2014-05-20 04:47:25.207', 'a750e541-13d8-4dac-9a9e-473bb2b903fb', 172, 'Improving formatting.', 449, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 04:47:25.207', 'a750e541-13d8-4dac-9a9e-473bb2b903fb', 172, 'Proposed by 84 approved by 250 edit id of 50', 450, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Another suggestion is to test the [logistic regression][1]. As an added bonus, the  weights (coefficients) of the model will give you an idea of which sites are age-distriminant.

Sklearn offers the [sklearn.linear_model.LogisticRegression][2] package that is designed to handle sparse data as well.


  [1]: http://en.wikipedia.org/wiki/Logistic_regression
  [2]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html', 172, '2014-05-20 09:24:30.697', '447f1bc4-e685-4ec0-91e3-83476d63cb44', 174, 452, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[Latent Dirichlet Allocation (LDA)](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) and [Hierarchical Dirichlet Process (HDP)](http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process) are both topic modeling processes. The major difference is LDA requires the specification of the number of topics, and HDP doesn''t. Why is that so? And what are the differences, pros, and cons of both topic modelling methods?

', 84, '2014-05-20 13:45:59.373', '32fff684-b72d-404a-9a62-56f4b176ded6', 128, 'Improving formatting.', 453, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Indexing is the almost most important part of data to get an efficient, properly storing and retrieval data from mediums

In different Programming Languages, there are different indexing algorithms and structures can be found.

As an Example, in Java Language the Apache Foundation''s Lucene is very popular.

In years there are very efficient, fast, scalable Indexing Algorithms projectioned to the computer-world such as MurMurHashing Algorithm (using by some NoSQL Databases), B+Tree Algorithm (using by versions of Windows OS itself), or Lucene (very popular in web technologies) and its variations.

Also there can be found problem-specific Indexing within Chemicals or Medical Data Representations in Digital World



', 229, '2014-05-20 13:48:12.163', '2859b19f-57ec-4866-861f-538ab4b4f4e1', 151, 'added 735 characters in body', 454, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:48:12.163', '2859b19f-57ec-4866-861f-538ab4b4f4e1', 151, 'Proposed by 229 approved by 50 edit id of 44', 455, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('R is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (formerly AT&T, now Lucent Technologies) by John Chambers and colleagues. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.

R provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, ...) and graphical techniques, and is highly extensible. The S language is often the vehicle of choice for research in statistical methodology, and R provides an Open Source route to participation in that activity.

One of R''s strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control.

R is available as Free Software under the terms of the Free Software Foundation''s GNU General Public License in source code form. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and MacOS.', 201, '2014-05-20 13:49:04.060', '6f0b10e3-bfd2-4f6c-b98a-fd52f81e0460', 48, 'added 1317 characters in body', 456, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:49:04.060', '6f0b10e3-bfd2-4f6c-b98a-fd52f81e0460', 48, 'Proposed by 201 approved by 50 edit id of 27', 457, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Conceptually speaking, *data-mining* can be thought of as one item (or set of skills and applications) in the toolkit of the data scientist.

More specifically, data-mining is an activity that seeks patterns in large, complex data sets. It usually emphasizes algorithmic techniques, but may also involve any set of related skills, applications, or methodologies with that goal.

In US-English colloquial speech, data-mining and data-collection are often used interchangeably.

However, a main difference between these two related activities is *intentionality*.

*Definition inspired mostly by the contributions of [@statsRus](http://datascience.stackexchange.com/users/36/statsrus) to Data Science.SE*', 53, '2014-05-20 13:49:35.103', 'f3ffb6a7-0916-432e-9e8f-e2dd1ca40a96', 79, 'linkify user statsRus, attempt 4', 458, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:49:35.103', 'f3ffb6a7-0916-432e-9e8f-e2dd1ca40a96', 79, 'Proposed by 53 approved by 50 edit id of 20', 459, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A statistics term used to describe a type of dependence between variables (or data sets). Correlations are often used as an indicator of predictability. However, correlation does NOT imply causation. Different methods of calculating correlation exist to capture more complicated relationships between the variables being studied.', 53, '2014-05-20 13:50:19.543', '7eee99fd-bc9d-4246-8b8b-f8d569e2706f', 124, 'linked to describe relational independence', 460, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:50:19.543', '7eee99fd-bc9d-4246-8b8b-f8d569e2706f', 124, 'Proposed by 53 approved by 50 edit id of 30', 461, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The most basic relationship to describe is a **linear relationship** between variables, *x* and *y*, such that they can be said to be highly-correlated when every increase in *x* results in a proportional increase in *y*. They can also be said to be *inversely proportional* so that when *x* increases, *y* decreases. And finally, the two variables can be said to be [independent](http://en.wikipedia.org/wiki/Independence_%28probability_theory%29) in the event that there is no linear relationship between the two (they are uncorrelated, or have a **Pearson correlation coefficient** of 0. [LaTeX support would be highly desirable at this point.]

Different correlation coefficients and their uses:
--------------------------------------------------

[Pearson correlation coefficient](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) is useful.....
[draft]
', 53, '2014-05-20 13:50:21.763', '31f6d5eb-7a3b-46d2-a9ea-2252a56cc3d1', 123, 'linked to describe relational independence', 462, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:50:21.763', '31f6d5eb-7a3b-46d2-a9ea-2252a56cc3d1', 123, 'Proposed by 53 approved by 50 edit id of 29', 463, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('.NET is a very popular Object Oriented Programming Language Family which includes members such as C# (pronounced CSharp), VB.NET, F# (pronounced FSharp), J# (pronounced JSharp) and much more. The .NET Family offers programming with small effort with well-known high speed of compiled languages such as C and C++

This Tag aims to group Data Science Questions and Answers which users want to operate their processes under .NET Programming Language Family', 229, '2014-05-20 13:50:32.440', 'c528dfb3-f3ea-4ac2-bf64-4aef4fd48c98', 168, 'added 455 characters in body', 464, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:50:32.440', 'c528dfb3-f3ea-4ac2-bf64-4aef4fd48c98', 168, 'Proposed by 229 approved by 50 edit id of 49', 465, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Use the [tag:definitions] tag when:
-----------------------------------

You think we should create an official definition.

An existing Tag Wiki needs a more precise definition to avoid confusion and we need to create consensus before an edit.

(rough draft - needs filling out)', 53, '2014-05-20 13:50:52.447', '9dc0253f-e204-4c0d-a1ef-28095641b189', 104, 'added 286 characters in body', 466, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:50:52.447', '9dc0253f-e204-4c0d-a1ef-28095641b189', 104, 'Proposed by 53 approved by 50 edit id of 18', 467, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**NoSQL** (sometimes expanded to "not only [tag:sql]") is a broad class of database management systems that differ from the classic model of the relational database management system ([tag:rdbms]) in some significant ways.

###NoSQL systems:

- Specifically designed for high load
- Natively support horizontal scalability
- Fault tolerant
- Store data in denormalised manner
- Do not usually enforce strict database schema
- Do not usually store data in a table
- Sometimes provide eventual consistency instead of ACID transactions

###In contrast to RDBMS, NoSQL systems:

- Do not guarantee data consistency
- Usually support a limited query language (subset of SQL or another custom query language)
- May not provide support for transactions/distributed transactions
- Do not usually use some advanced concepts of RDBMS, such as triggers, views, stored procedures

###NoSQL implementations can be categorised by their manner of implementation:

- [Column-oriented][1]
- [Document store][2]
- [Graph][3]
- [Key-value store][4]
- [Multivalue databases][5]
- [Object databases][6]
- [Tripplestore](https://en.wikipedia.org/wiki/Triplestore)
- [Tuple store](http://en.wikipedia.org/wiki/NoSQL#Tuple_store)

### Free NoSQL Books

*   [CouchDB: The Definitive Guide](http://books.couchdb.org/relax/)
*   [The Little MongoDB Book](http://openmymind.net/2011/3/28/The-Little-MongoDB-Book)
*   [The Little Redis Book](http://openmymind.net/2012/1/23/The-Little-Redis-Book/)


  [1]: https://en.wikipedia.org/wiki/Column-oriented_DBMS
  [2]: http://en.wikipedia.org/wiki/Document-oriented_database
  [3]: http://en.wikipedia.org/wiki/Graph_database
  [4]: http://dba.stackexchange.com/questions/607/what-is-a-key-value-store-database
  [5]: https://en.wikipedia.org/wiki/MultiValue
  [6]: https://en.wikipedia.org/wiki/Object_database', 201, '2014-05-20 13:51:16.240', '6785e6ff-09bc-4af3-bfbd-872505aa6715', 118, 'added 1870 characters in body', 468, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:51:16.240', '6785e6ff-09bc-4af3-bfbd-872505aa6715', 118, 'Proposed by 201 approved by 50 edit id of 25', 469, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Efficiency, in algorithmic processing, is usually associated to resource usage. The metrics to evaluate the efficiency of a process are commonly account for execution time, memory/disk or storage requirements, network usage and power consumption.', 84, '2014-05-20 13:51:49.240', 'b005e96e-b45e-4206-a7fb-f0b235980da7', 142, 'added 246 characters in body', 470, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:51:49.240', 'b005e96e-b45e-4206-a7fb-f0b235980da7', 142, 'Proposed by 84 approved by 50 edit id of 36', 471, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('An activity that seeks patterns in a continuous stream of data elements, usually involving summarizing the stream in some way.', 200, '2014-05-20 13:52:00.620', '57345a56-1b97-42b2-b3f1-41d3e1ee5746', 109, 'added 126 characters in body', 472, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:52:00.620', '57345a56-1b97-42b2-b3f1-41d3e1ee5746', 109, 'Proposed by 200 approved by 50 edit id of 22', 473, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In Computer science and Technologies The Data is the most important part.
Since to work with data in an efficient manner to store the data in mediums and reuse, we use some techniques which named in general indexing.

This tag interests with these techniques, efficiency, and anything about using stored data with indexing', 229, '2014-05-20 13:52:19.333', 'c386cfb8-c04f-4a0b-95ec-52ff58ffe13e', 149, 'added 325 characters in body', 474, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:52:19.333', 'c386cfb8-c04f-4a0b-95ec-52ff58ffe13e', 149, 'Proposed by 229 approved by 50 edit id of 43', 475, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**.NET** is a very popular Object Oriented Programming Language Family.This Family includes members such as **C#** (pronounced **CSharp**), **VB.NET**, **F#** (pronounced **Fsharp**), **J#** (pronounced **JSharp**) and much more. The **.NET** Family offers programming with small effort with well-known high speed of compiled languages such as **C** and **C++**

This Tag **aims** to group **Data Science Questions and Answers** which users want to operate their processes **under .NET** Programming Language Family', 229, '2014-05-20 13:52:50.373', '2abe3856-c55b-40fa-a69a-642132b257a2', 167, 'added 517 characters in body', 476, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:52:50.373', '2abe3856-c55b-40fa-a69a-642132b257a2', 167, 'Proposed by 229 approved by 50 edit id of 48', 477, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of humancomputer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.', 118, '2014-05-20 13:52:59.427', 'ef541769-5807-421c-a994-36f865339f34', 147, 'added 449 characters in body', 478, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:52:59.427', 'ef541769-5807-421c-a994-36f865339f34', 147, 'Proposed by 118 approved by 50 edit id of 40', 479, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('a discussion (meta) tag used when there exists *disagreement* or *confusion* about the everyday meaning of a term or phrase.', 53, '2014-05-20 13:53:05.697', '837b4c2b-8756-4461-b392-f5d101010c18', 105, 'added 124 characters in body', 480, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:53:05.697', '837b4c2b-8756-4461-b392-f5d101010c18', 105, 'Proposed by 53 approved by 50 edit id of 19', 481, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('MapReduce is a framework for processing parallelizable problems across huge datasets using a large number of computers (nodes), collectively referred to as a cluster (if all nodes are on the same local network and use similar hardware) or a grid (if the nodes are shared across geographically and administratively distributed systems, and use more heterogenous hardware). Computational processing can occur on data stored either in a filesystem (unstructured) or in a database (structured). MapReduce can take advantage of locality of data, processing it on or near the storage assets in order to reduce the distance over which it must be transmitted.', 227, '2014-05-20 13:53:08.790', '6c9c9fd8-1a2c-40e6-86df-3ca0560af181', 136, 'added 651 characters in body', 482, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:53:08.790', '6c9c9fd8-1a2c-40e6-86df-3ca0560af181', 136, 'Proposed by 227 approved by 50 edit id of 32', 483, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Indexing is the almost most important part of data to get an efficient, properly storing and retrieval data from mediums', 229, '2014-05-20 13:53:17.567', '9c94ad1d-90cf-43eb-a153-47c96d593671', 152, 'added 120 characters in body', 484, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:53:17.567', '9c94ad1d-90cf-43eb-a153-47c96d593671', 152, 'Proposed by 229 approved by 50 edit id of 45', 485, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval etc.', 118, '2014-05-20 13:53:26.907', 'ac8729f8-7707-4822-8e53-8b0ee6f752c6', 145, 'added 448 characters in body', 486, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:53:26.907', 'ac8729f8-7707-4822-8e53-8b0ee6f752c6', 145, 'Proposed by 118 approved by 50 edit id of 39', 487, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In Computer science and Technologies The Data is the most important part.
Since to work with data in an efficient manner to store the data in mediums and reuse, we use some techniques which named in general indexing.

This tag interests with these techniques, efficiency, and anything about using stored data with indexing', 229, '2014-05-20 13:53:31.717', '9090b1e1-06af-45b9-a7ac-1d591b294aeb', 148, 'added 325 characters in body', 488, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:53:31.717', '9090b1e1-06af-45b9-a7ac-1d591b294aeb', 148, 'Proposed by 229 approved by 50 edit id of 42', 489, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('MapReduce is a programming model for processing large data sets with a parallel, distributed algorithm on a cluster.', 227, '2014-05-20 13:53:39.727', '8af71482-ba04-46ee-839e-55d3350b5c15', 137, 'added 116 characters in body', 490, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-05-20 13:53:39.727', '8af71482-ba04-46ee-839e-55d3350b5c15', 137, 'Proposed by 227 approved by 50 edit id of 33', 491, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Another suggestion is to test the [logistic regression][1]. As an added bonus, the  weights (coefficients) of the model will give you an idea of which sites are age-distriminant.

Sklearn offers the [sklearn.linear_model.LogisticRegression][2] package that is designed to handle sparse data as well.

As mentionned in the comments, in the present case, with more input variables than samples, you need to regularize the model (with [sklearn.linear_model.LogisticRegression][2] use the `penalty=''l1''` argument).

  [1]: http://en.wikipedia.org/wiki/Logistic_regression
  [2]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html', 172, '2014-05-20 19:36:16.283', '55c7716a-3a6d-4012-b3f1-f28963abf736', 174, 'added 213 characters in body', 492, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have data coming from a source system that is pipe delimited. Pipe was selected over comma since it was believed no pipes appeared in field, while it was known that commas do occur. After ingesting this data into Hive however it has been discovered that rarely a field does in fact contain a pipe character.

Due to a constraint we are unable to regenerate from source to escape the delimiter or change delimiters in the usual way. However we have the metadata used to create the Hive table. Could we use knowledge of the fields around the problem field to reprocess the file on our side to escape it or to change the file delimiter prior to reloading the data into Hive?', 249, '2014-05-20 22:14:02.927', 'bd8bddfc-182b-4b88-8915-0d5dc7721f63', 175, 493, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can metadata be used to adapt parsing for an unescaped in field use of the delimiter?', 249, '2014-05-20 22:14:02.927', 'bd8bddfc-182b-4b88-8915-0d5dc7721f63', 175, 494, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<metadata><parsing>', 249, '2014-05-20 22:14:02.927', 'bd8bddfc-182b-4b88-8915-0d5dc7721f63', 175, 495, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am seeking for a library/tool to visualize how social network changes when new nodes/edges are added to it.

One of the existing solutions is [SoNIA: Social Network Image Animator][1]. It let''s you make movies like [this one][2].

SoNIA''s documentation says that it''s broken at the moment, and besides this I would prefer JavaScript-based solution instead. So, my question is: are you familiar with any tools or are you able to point me to some libraries which would make this task as easy as possible?

Right after posting this question I''ll dig into [sigma.js][3], so please consider this library covered.

In general, my input data would be something like this:

time_elapsed; node1; node2
1; A; B
2; A; C
3; B; C

So, here we have three points in time (1, 2, 3), three nodes (A, B, C), and three edges, which represent a triadic closure between the three considered nodes.

Moreover, every node will have two attributes (age and gender), so I would like to be able to change the shape/colour of the nodes.

Also, after adding a new node, it would be perfect to have some ForceAtlas2 or similar algorithm to adjust the layout of the graph.

  [1]: http://www.stanford.edu/group/sonia/
  [2]: https://www.youtube.com/watch?v=yGSNCED6mDc
  [3]: http://sigmajs.org/', 173, '2014-05-21 05:29:36.787', '0b5b7037-4ec7-4086-bcee-c0298feaa588', 176, 496, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to animate growth of a social network?', 173, '2014-05-21 05:29:36.787', '0b5b7037-4ec7-4086-bcee-c0298feaa588', 176, 497, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<social-network-analysis><time-series><javascript><visualization>', 173, '2014-05-21 05:29:36.787', '0b5b7037-4ec7-4086-bcee-c0298feaa588', 176, 498, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am seeking for a library/tool to visualize how social network changes when new nodes/edges are added to it.

One of the existing solutions is [SoNIA: Social Network Image Animator][1]. It let''s you make movies like [this one][2].

SoNIA''s documentation says that it''s broken at the moment, and besides this I would prefer JavaScript-based solution instead. So, my question is: are you familiar with any tools or are you able to point me to some libraries which would make this task as easy as possible?

Right after posting this question I''ll dig into [sigma.js][3], so please consider this library covered.

In general, my input data would be something like this:

    time_elapsed; node1; node2
    1; A; B
    2; A; C
    3; B; C

So, here we have three points in time (1, 2, 3), three nodes (A, B, C), and three edges, which represent a triadic closure between the three considered nodes.

Moreover, every node will have two attributes (age and gender), so I would like to be able to change the shape/colour of the nodes.

Also, after adding a new node, it would be perfect to have some ForceAtlas2 or similar algorithm to adjust the layout of the graph.

  [1]: http://www.stanford.edu/group/sonia/
  [2]: https://www.youtube.com/watch?v=yGSNCED6mDc
  [3]: http://sigmajs.org/', 173, '2014-05-21 05:51:58.330', '4de5dbb4-a293-4603-9e76-edb7768422f3', 176, 'added 16 characters in body', 499, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-21 07:05:44.780', 'bba8d982-9b8b-4a4f-b452-e86f2518ad52', 177, 500, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-21 07:05:44.780', '7126fa44-3de9-4899-8a52-34104fc704fb', 178, 501, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My first guess is to [visualize social network in Tableau][1].

And particularly: [building network graphs in Tableau][2].

What you need is to add time dimension to the "Pages" section to be able to see network change dynamics.


  [1]: https://www.google.com/search?q=visualize%20social%20network%20in%20tableau
  [2]: http://www.clearlyandsimply.com/clearly_and_simply/2012/12/build-network-graphs-in-tableau.html', 97, '2014-05-21 07:09:20.093', 'f28d2d21-10bf-42bf-9183-e665d6af7aa5', 179, 502, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-21 07:10:45.827', 'ebe743f1-292b-40a5-aaf7-0c01bc011237', 180, 503, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-05-21 07:10:45.827', '3c14a563-0fb7-41f3-be1c-cf5fb01d86aa', 181, 504, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('My first guess is to [visualize social network in Tableau][1].

And particularly: [building network graphs in Tableau][2].

What you need is to add time dimension to the "Pages" section to be able to see network change dynamics.

This is screen from the link above.
![enter image description here][3]


  [1]: https://www.google.com/search?q=visualize%20social%20network%20in%20tableau
  [2]: http://www.clearlyandsimply.com/clearly_and_simply/2012/12/build-network-graphs-in-tableau.html
  [3]: http://i.stack.imgur.com/Quq8G.png', 97, '2014-05-21 07:18:48.453', '4ea482c2-11b7-4c82-a47d-9af31a79e9d7', 179, 'added screen shot', 505, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('So, a few of your rows will have too many columns by one or more as a result. That''s easy to detect, but harder to infer where the error was -- which two columns are actually one? which delimiter is not a delimiter?

In some cases, you can use the metadata, because it helps you know when an interpretation of the columns can''t be right. For example, if just the one column can have a text value, and all the others must be numeric, it''s unambiguous where the error is. Any additional columns created by this error occur right after the text column.

If they''re all text, this doesn''t work of course.

You might be able to leverage more than the metadata''s column type. For example you may know that some fields are from an enumerated set of values, and use that to determine when a column assignment is wrong.', 21, '2014-05-21 07:19:32.297', '1d2b696f-9008-43b3-bf06-bcd45e477411', 182, 506, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was very impressed when I saw [this animation][1] of the [discourse][2] git repository. They used [Gourse][3] which is specifically for git. But it may give ideas about how to represent the dynamics of growth.

Also, [this stackoverflow answer][4] seems to point at a python/networkx/matplotlib solution.


  [1]: http://youtu.be/T7Ncmq6scck
  [2]: http://www.discourse.org
  [3]: https://code.google.com/p/gource/
  [4]: http://stackoverflow.com/a/13571425/1083707', 262, '2014-05-21 10:53:26.870', 'fe43efff-9ac9-49d8-b1fc-c6d2a139e519', 183, 507, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The details of the Google Prediction API are on this [page][1], but I am not able to find any details about the prediction algorithms running behind the API.

So far I have gathered that they let you provide your preprocessing steps in PMML format.

  [1]: https://developers.google.com/prediction/', 200, '2014-05-21 11:22:34.657', '0e66aa84-4327-4d0a-aa77-7de5db8e4d49', 184, 508, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Google prediction API: What training/prediction methods Google Prediction API employs?', 200, '2014-05-21 11:22:34.657', '0e66aa84-4327-4d0a-aa77-7de5db8e4d49', 184, 509, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<tools>', 200, '2014-05-21 11:22:34.657', '0e66aa84-4327-4d0a-aa77-7de5db8e4d49', 184, 510, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":227,"DisplayName":"Amir Ali Akbari"},{"Id":53,"DisplayName":"Clayton"},{"Id":115,"DisplayName":"Johnny000"},{"Id":113,"DisplayName":"vefthym"},{"Id":148,"DisplayName":"ProgramFOX"}]}', -1, '2014-05-21 14:00:22.100', '96e2e71b-110f-42cb-9c68-e4fd049a43f5', 125, '104', 512, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you take a look over the specifications of PMML which you can find [here][1] you can see on the left menu what options you have (like ModelTree, NaiveBayes, Neural Nets and so on).


  [1]: http://www.dmg.org/v3-0/GeneralStructure.html', 108, '2014-05-21 14:14:38.797', '880f344b-4e2c-4b54-b9c2-d548a131d898', 185, 513, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I was very impressed when I saw [this animation][1] of the [discourse][2] git repository. They used [Gourse][3] which is specifically for git. But it may give ideas about how to represent the dynamics of growth.

Also, [this stackoverflow answer][4] seems to point at a python/networkx/matplotlib solution.

If you''re looking for a web-based solution then d3.js is excellent. See [this][5], [this][6] and [this][7] for example.


  [1]: http://youtu.be/T7Ncmq6scck
  [2]: http://www.discourse.org
  [3]: https://code.google.com/p/gource/
  [4]: http://stackoverflow.com/a/13571425/1083707
  [5]: http://bl.ocks.org/mbostock/4062045
  [6]: http://mbostock.github.io/d3/talk/20111116/force-collapsible.html
  [7]: http://bl.ocks.org/mbostock/929623', 262, '2014-05-21 14:41:35.207', '838c0638-a986-4ff1-88cc-753c87e10e07', 183, 'added 284 characters in body', 514, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am learning [Support Vector Machines][1] and I am unable to understand how a class label is chosen for a data point in a binary classifier.

Is it chosen by consensus with respect to the classification in each dimension of the separating hyperplane?

Thanks


  [1]: http://en.wikipedia.org/wiki/Support_vector_machine', 133, '2014-05-21 15:12:18.980', '17ec82b3-c836-4db2-96aa-7cc9db0a51f2', 186, 515, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using SVM as a binary classifier, is the label for a data point chosen by consensus?', 133, '2014-05-21 15:12:18.980', '17ec82b3-c836-4db2-96aa-7cc9db0a51f2', 186, 516, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<svm><classification><binary>', 133, '2014-05-21 15:12:18.980', '17ec82b3-c836-4db2-96aa-7cc9db0a51f2', 186, 517, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m learning [Support Vector Machines][1], and I''m unable to understand how a class label is chosen for a data point in a binary classifier. Is it chosen by consensus with respect to the classification in each dimension of the separating hyperplane?

  [1]: http://en.wikipedia.org/wiki/Support_vector_machine', 84, '2014-05-21 15:26:02.533', '33b2155d-2002-402e-be45-bc26303f4014', 186, 'Improving formatting.', 518, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I was very impressed when I saw [this animation][1] of the [discourse][2] git repository. They used [Gourse][3] which is specifically for git. But it may give ideas about how to represent the dynamics of growth.

Also, [this stackoverflow answer][4] seems to point at a python/networkx/matplotlib solution.

If you''re looking for a web-based solution then d3.js is excellent. See [this][5], [this][6] and [this][7] for example.

See also [this stackoverflow question][8], the accepted answer points to D3.js again.


  [1]: http://youtu.be/T7Ncmq6scck
  [2]: http://www.discourse.org
  [3]: https://code.google.com/p/gource/
  [4]: http://stackoverflow.com/a/13571425/1083707
  [5]: http://bl.ocks.org/mbostock/4062045
  [6]: http://mbostock.github.io/d3/talk/20111116/force-collapsible.html
  [7]: http://bl.ocks.org/mbostock/929623
  [8]: http://stackoverflow.com/questions/7416659/interactive-graph-visualisation', 262, '2014-05-21 15:29:00.877', '5d946f7a-2906-4fe7-938f-a7a064f3cfd2', 183, 'added 172 characters in body', 519, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The term *consensus*, as far as I''m concerned, is used rather for cases when you have more a than one source of metric/measure/choice from which to make a decision. And, in order to choose a possible result, you perform some *average evaluation/consensus* over the values available.

This is not the case for SVM. The algorithm is based on a [quadratic optimization](http://upload.wikimedia.org/wikipedia/commons/2/2a/Svm_max_sep_hyperplane_with_margin.png), that maximizes the distance from the closest documents of two different classes, using a hyperplane to make the split.

![Hyperplane separating two different classes][1]

So, the only *consensus* here is the resulting hyperplane, computed from the closest documents of each class. In other words, the classes are attributed to each point by calculating the distance from the point to the hyperplane derived. If the distance is positive, it belongs to a certain class, otherwise, it belongs to the other one.

  [1]: http://i.stack.imgur.com/CCO7Z.png', 84, '2014-05-21 15:39:54.830', '949e7b0a-d163-4f8c-91ea-da8ab8f99467', 187, 520, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('###Fancy animations are cool
I was very impressed when I saw [this animation][1] of the [discourse][2] git repository. They used [Gourse][3] which is specifically for git. But it may give ideas about how to represent the dynamics of growth.

###You can create animations with matplotlib
[This stackoverflow answer][4] seems to point at a python/networkx/matplotlib solution.

###But D3.js provides interaction
If you''re looking for a web-based solution then d3.js is excellent. See [this][5], [this][6] and [this][7] for example.
See also [this stackoverflow question][8], the accepted answer points to D3.js again.

##Conclusion

I would be drawn towards the python/networkx options for network analysis (possibly to add attributes to your raw data file for example). Then, for visualisation and dissemination D3.js is perfect. You might be surprised how easy it can be to write d3.js once you get into it. I believe [it even works within an ipython notebook!][9]


  [1]: http://youtu.be/T7Ncmq6scck
  [2]: http://www.discourse.org
  [3]: https://code.google.com/p/gource/
  [4]: http://stackoverflow.com/a/13571425/1083707
  [5]: http://bl.ocks.org/mbostock/4062045
  [6]: http://mbostock.github.io/d3/talk/20111116/force-collapsible.html
  [7]: http://bl.ocks.org/mbostock/929623
  [8]: http://stackoverflow.com/questions/7416659/interactive-graph-visualisation
  [9]: https://www.youtube.com/watch?v=8UtoIR2IEkI', 262, '2014-05-21 15:39:57.380', 'ee5e8890-91e5-4954-b3f5-c23195c0d238', 183, 'more structure and ipython video', 521, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can use map reduce algorithms in Hadoop without programming them in Java. It is called streaming and works like Linux piping. If you believe that you can port your functions to read and write to terminal, it should work nicely. [Here][1] is example blog post which shows how to use map reduce functions written in Python in Hadoop.


  [1]: http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/', 82, '2014-05-21 16:13:25.590', '1bd20c98-88ff-4f8e-bb99-d7d66b2af0d1', 188, 522, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m currently using GAMS (and more specifically CPLEX within GAMS) to solve a very large mixed integer programming problem. This allows me to parallelize the process over 4 cores (although I have more, CPLEX utilizes a maximum of 4 cores) and it finds an optimal solution in a relatively short amount of time. Is there an open source mixed integer programming tool that I could use as an alternative to GAMS and CPLEX? It must be comparable in speed or faster for me to consider it. I have a preference for R based solutions, but I am open to suggestions of all kinds and other users may be interested in different solutions.', 151, '2014-05-21 19:41:19.857', 'f0909f06-5b4e-4e92-ba38-acb871f48488', 189, 523, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Open source solver for large mixed integer programming task?', 151, '2014-05-21 19:41:19.857', 'f0909f06-5b4e-4e92-ba38-acb871f48488', 189, 524, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><open-source><parallelism><optimization>', 151, '2014-05-21 19:41:19.857', 'f0909f06-5b4e-4e92-ba38-acb871f48488', 189, 525, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It turned out that this task was quite easy to accomplish using [vis.js][1]. [This][2] was the best example code which I have found.

The example of what I have built upon this is [here][3] (scroll to the bottom of this post). This graph represents the growth of a subnetwork of Facebook friends. Green dots are females, blue ones are males. The darker the colour, the older the user. By clicking "Dodaj wzy" you can add more nodes and edges to the graph.

Anyway, I am still interested in other ways to accomplish this task, so I won''t accept any answer as for now.

Thanks for your contributions!

  [1]: http://visjs.org/
  [2]: http://visjs.org/examples/graph/20_navigation.html
  [3]: http://laboratoriumdanych.pl/jak-powstaje-siec/', 173, '2014-05-22 12:14:49.727', '987d895e-0679-40f8-a686-a0598bd73a86', 190, 526, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can someone explain me, how to classify a data like MNIST with MLBP-Neural network if I make more than one output (e.g 8), I mean if I just use one output I can easily classify the data, but if I use more than one, which output should I choose ?', 273, '2014-05-22 13:36:24.120', 'e9259adf-4713-43db-8f7f-a98ccd4011b9', 191, 527, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Multi layer back propagation Neural network for classification', 273, '2014-05-22 13:36:24.120', 'e9259adf-4713-43db-8f7f-a98ccd4011b9', 191, 528, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork>', 273, '2014-05-22 13:36:24.120', 'e9259adf-4713-43db-8f7f-a98ccd4011b9', 191, 529, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The most popular use case seem to be recommender systems of different kinds (such as recommending shopping items, users in social networks etc.).

But what are other typical data science applications, which may be used in a different verticals?

For example: customer churn prediction with machine learning, evaluating customer lifetime value, sales forecasting.
', 88, '2014-05-22 15:15:41.133', 'd63c3ff3-97d1-40a7-bd6c-279a42e07b53', 192, 530, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the most popular data science application use cases for consumer web companies', 88, '2014-05-22 15:15:41.133', 'd63c3ff3-97d1-40a7-bd6c-279a42e07b53', 192, 531, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<usecase><consumerweb>', 88, '2014-05-22 15:15:41.133', 'd63c3ff3-97d1-40a7-bd6c-279a42e07b53', 192, 532, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It depends, of course, on the focus of the company: commerce, service, etc.  In adition to the use cases you suggested, some other use cases would be:

 - Funnel analysis: Analyzing the way in which consumers use a website and complete a sale may include data science techniques, especially if the company operates at a large scale.
 - Advertising: Companies that place ads use *a lot* of machine learning techniques to analyze and predict which ads would be most effective or most remunerative give the user''s demographics that would view them.', 178, '2014-05-22 15:43:57.160', 'd6cee457-0656-4c40-9dd1-fa87f20129d5', 193, 533, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Suppose that you need to classify something in K classes, where K > 2. In this case the most often setup I use is one hot encoding. You will have K output columns, and in the training set you will set all values to 0, except the one which has the category index, which could have value 1. Thus, for each training data set instance you will have all outputs with values 0 or 1, all outputs sum to 1 for each instance.

This looks like a probability, which reminds me of a technique used often to connect some outputs which are modeled as probability. This is called softmax function, more details [on Wikipedia][1]. This will allow you to put some constraints on the output values (it is basically a logistic function generalization) so that the output values will be modeled as probabilities.

Finally, with or without softmax you can use the output as a discriminant function to select the proper category.

Another final thought would be to avoid to encode you variables in a connected way. For example you can have the binary representation of the category index. This would induce to the learner an artificial connection between some outputs which are arbitrary. The one hot encoding has the advantage that is neutral to how labels are indexed.


  [1]: http://en.wikipedia.org/wiki/Softmax_activation_function', 108, '2014-05-22 19:20:14.130', 'a18690d0-d38f-4e7b-acee-6fc346a3c0ab', 194, 534, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Satisfaction is a huge one that I run into a lot.

The bottom line is that for very large services (search engines, facebook, linkedin, etc...) your users are simply a collection of log lines.  You have little ability to solicit feed back from them (not a hard and fast rule necessarily).  So you have to infer their positive or negative feedback most of the time.

This means finding ways, even outside of predictive modelling, to truly tell, from a collection of log lines, whether or not someone actually liked something they experienced.  This simple act is even more fundamental (in my biased opinion) than a/b testing since you''re talking about metrics you will eventually track on a test scorecard.

Once you have a handle on good SAT metrics then you can start making predictive models and experimenting.  But even deciding what piece of log instrumentation can tell you about SAT is non-trivial (and often changes).', 92, '2014-05-22 20:48:55.297', 'f926bd04-c9c7-4d20-b0a8-f3a294ad3d0b', 195, 535, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('So we have potential for a machine learning application that fits fairly neatly into the traditional problem domain solved by classifiers, i.e., we have a set of attributes describing an item and a "bucket" that they end up in. However, rather than create models of probabilities like in Naive Bayes or similar classifiers, we want our output to be a set of roughly human-readable rules that can be reviewed and modified by an end user.

Association rule learning looks like the family of algorithms that solves this type of problem, but these algorithms seem to focus on identifying common combinations of features and don''t include the concept of a final bucket that those features might point to. For example, our data set looks something like this:

    Item A { 4-door, small, steel } => { sedan }
    Item B { 2-door, big,   steel } => { truck }
    Item C { 2-door, small, steel } => { coupe }

I just want the rules that say "if it''s big and a 2-door, it''s a truck," not the rules that say "if it''s a 4-door it''s also small."

One workaround I can think of is to simply use association rule learning algorithms and ignore the rules that don''t involve an end bucket, but that seems a bit hacky. Have I missed some family of algorithms out there? Or perhaps I''m approaching the problem incorrectly to begin with?

Thanks in advance!', 275, '2014-05-22 21:47:26.980', '24c0794d-2091-455b-87d3-8a465f0f091a', 196, 536, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Algorithm for generating classification rules', 275, '2014-05-22 21:47:26.980', '24c0794d-2091-455b-87d3-8a465f0f091a', 196, 537, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification>', 275, '2014-05-22 21:47:26.980', '24c0794d-2091-455b-87d3-8a465f0f091a', 196, 538, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('C45 made by Quinlan is able to produce rule for prediction. Check this [Wikipedia][1] page. I know that in [Weka][2] its name is J48. I have no idea which are implementations in R or Python. Anyway, from this kind of decision tree  you should be able to infer rules for prediction.


  [1]: http://en.wikipedia.org/wiki/C4.5_algorithm
  [2]: http://www.cs.waikato.ac.nz/~ml/weka/', 108, '2014-05-22 21:54:05.660', '1ed7d7d3-a296-431c-95b0-a2f341c79c26', 197, 539, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('C45 made by Quinlan is able to produce rule for prediction. Check this [Wikipedia][1] page. I know that in [Weka][2] its name is J48. I have no idea which are implementations in R or Python. Anyway, from this kind of decision tree  you should be able to infer rules for prediction.

*Later edit*

Also you might be interested in algorithms for directly inferring rules for classification. RIPPER is one, which again in Weka it received a different name JRip. See the original paper for RIPPER: [Fast Effective Rule Induction, W.W. Cohen 1995][3]


  [1]: http://en.wikipedia.org/wiki/C4.5_algorithm
  [2]: http://www.cs.waikato.ac.nz/~ml/weka/
  [3]: http://www.google.ro/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0CCYQFjAA&url=http://www.cs.utsa.edu/~bylander/cs6243/cohen95ripper.pdf&ei=-XJ-U-7pGoqtyAOej4Ag&usg=AFQjCNFqLnuJWi3gGXVCrugmv3NTRhHHLA&bvm=bv.67229260,d.bGQ&cad=rja', 108, '2014-05-22 21:59:22.117', '1f78b08a-3fcd-41bd-b491-8cb9d48331fa', 197, 'added 508 characters in body', 540, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Also, there seem to be a very comprehensive list of data science use cases by function and by vertical on Kaggle - ["Data Science Use Cases"][1]


  [1]: http://www.kaggle.com/wiki/DataScienceUseCases', 88, '2014-05-23 03:05:57.990', '942cf03f-a80c-443f-a00e-f18f48d99cbb', 198, 541, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('So we have potential for a machine learning application that fits fairly neatly into the traditional problem domain solved by classifiers, i.e., we have a set of attributes describing an item and a "bucket" that they end up in. However, rather than create models of probabilities like in Naive Bayes or similar classifiers, we want our output to be a set of roughly human-readable rules that can be reviewed and modified by an end user.

Association rule learning looks like the family of algorithms that solves this type of problem, but these algorithms seem to focus on identifying common combinations of features and don''t include the concept of a final bucket that those features might point to. For example, our data set looks something like this:

    Item A { 4-door, small, steel } => { sedan }
    Item B { 2-door, big,   steel } => { truck }
    Item C { 2-door, small, steel } => { coupe }

I just want the rules that say "if it''s big and a 2-door, it''s a truck," not the rules that say "if it''s a 4-door it''s also small."

One workaround I can think of is to simply use association rule learning algorithms and ignore the rules that don''t involve an end bucket, but that seems a bit hacky. Have I missed some family of algorithms out there? Or perhaps I''m approaching the problem incorrectly to begin with?', 84, '2014-05-23 03:27:20.630', 'ec581e41-ca26-422e-9889-beda67bf3a87', 196, 'deleted 22 characters in body', 542, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Satisfaction is a huge one that I run into a lot.  Huge referring to importance/difficulty/complexity.

The bottom line is that for very large services (search engines, facebook, linkedin, etc...) your users are simply a collection of log lines.  You have little ability to solicit feed back from them (not a hard and fast rule necessarily).  So you have to infer their positive or negative feedback most of the time.

This means finding ways, even outside of predictive modelling, to truly tell, from a collection of log lines, whether or not someone actually liked something they experienced.  This simple act is even more fundamental (in my biased opinion) than a/b testing since you''re talking about metrics you will eventually track on a test scorecard.

Once you have a handle on good SAT metrics then you can start making predictive models and experimenting.  But even deciding what piece of log instrumentation can tell you about SAT is non-trivial (and often changes).', 92, '2014-05-23 06:03:40.577', 'da925617-0fa9-4ae2-b06f-cbcc65d2d583', 195, 'added 53 characters in body', 543, '5');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (122, '2014-05-23 06:20:39.747', '231bdeaa-b248-4fba-9ccd-95e9ea7bafeb', 128, '1', 544, '33');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (122, '2014-05-23 06:20:55.193', '80281580-45cb-434f-b8a2-bb4a6d4fd231', 130, '2', 545, '33');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('LDA has two hyperparameters, tuning them changes the induced topics.

What does the alpha and beta hyperparameters contribute to LDA?

How does the topic change if one or the other hyperparameters increase or decrease?

Why are they hyperparamters and not just parameters?', 122, '2014-05-23 06:25:50.480', '2b5594be-56eb-4cd4-aa79-455ae9ae1233', 199, 546, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What does the alpha and beta hyperparameters contribute to in Latent Dirichlet allocation?', 122, '2014-05-23 06:25:50.480', '2b5594be-56eb-4cd4-aa79-455ae9ae1233', 199, 547, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<topic-model><lda><parameter>', 122, '2014-05-23 06:25:50.480', '2b5594be-56eb-4cd4-aa79-455ae9ae1233', 199, 548, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You also can create a MongoDB-Hadoop [connection][1].


  [1]: http://docs.mongodb.org/ecosystem/tutorial/getting-started-with-hadoop/', 278, '2014-05-23 08:34:38.900', '559420b4-78d1-4a89-9897-6da93f4f010f', 200, 549, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES (' In addition to the listed sources: some social network data sets.

 - [Stanford University large network dataset collection (SNAP)][1]
 - [A huge twitter dataset that includes followers][2] + [large collection of twitter datasets here][3]
 - [LastFM data set][4]


  [1]: http://snap.stanford.edu/data/
  [2]: http://blog.infochimps.com/2008/12/29/massive-scrape-of-twitters-friend-graph/
  [3]: http://www.infochimps.com/collections/twitter-census
  [4]: http://mtg.upf.edu/node/1671', 97, '2014-05-23 09:09:44.490', '5209e052-c2f4-442e-b31f-d6a7582b8514', 201, 550, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In addition to the listed sources.

Some social network data sets:

 - [Stanford University large network dataset collection (SNAP)][1]
 - [A huge twitter dataset that includes followers][2] + [large collection of twitter datasets here][3]
 - [LastFM data set][4]

There are plenty of sources listed at Stats SE:

 - [Locating freely available data samples][5]
 - [Data APIs/feeds available as packages in R][6]
 - [Free data set for very high dimensional classification][7]


  [1]: http://snap.stanford.edu/data/
  [2]: http://blog.infochimps.com/2008/12/29/massive-scrape-of-twitters-friend-graph/
  [3]: http://www.infochimps.com/collections/twitter-census
  [4]: http://mtg.upf.edu/node/1671
  [5]: http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/
  [6]: http://stats.stackexchange.com/questions/12670/data-apis-feeds-available-as-packages-in-r
  [7]: http://stats.stackexchange.com/questions/973/free-data-set-for-very-high-dimensional-classification', 97, '2014-05-23 09:19:41.627', '139ebe29-5d52-4bde-b875-7567db44d747', 201, 'added more sources', 551, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The Dirichlet distribution is a multivariate distribution. We can denote the parameters of the Dirichlet as a vector of size K of the form ~ 1/B(a) * Product(x_i ^ (a_i-1)), where a is the vector of size K of the parameters, and sum of x_i = 1.

Now the LDA uses some constructs like:
- a document can have multiple topics (because of this multiplicity, we need the Dirichlet distribution); and there is a Dirichlet distribution which models this relation
- words can also belong to multiple topics, when you consider them outside of a document; so here we need another Dirichlet to model this

The previous two are distributions which you do not really see from data, this is why is called latent, or hidden.

Now, in Bayesian inference you use the Bayes rule to infer the posterior probability. For simplicity, let''s say you have data *x* and you have a model for this data governed by some parameters theta. In order to infer values for this parameters, in full Bayesian inference you will infer the posterior probability of these parameters using Bayes'' rule with *p(theta|x) = p(x|theta)p(theta|alpha)/p(x|alpha)*. In plain words is *posterior probability = likelihood x prior probability / marginal likelihood*. Note that here comes an *alpha*. This is your initial belief about this distribution, and is the parameter of the prior distribution. Usually this is chosen in such a way that will have a conjugate prior (so the distribution of the posterior is the same with the distribution of the prior) and often to encode some knowledge if you have one or to have maximum entropy if you know nothing.

The parameters of the prior are called *hyperparameters*. So, in LDA, both topic distributions, over documents and over words have also correspondent priors, which are denoted usually with alpha and beta, and because are the parameters of the prior distributions are called hyperparameters.

Now about choosing priors. If you plot some Dirichlet distributions you will note that if the individual parameters *alpha_k* have the same value, the pdf is symmetric in the simplex defined by the *x* values, which is the minimum or maximum for pdf is at the center.

If all the alpha_k have values lower than unit the maximum is found at corners

<img src="http://i.stack.imgur.com/5khZE.png" width="200" height="200">

or can if all values alpha_k are the same and greater than 1 the maximum will be found in center like

<img src="http://research.microsoft.com/en-us/um/people/cmbishop/prml/prmlfigs-png/Figure2.5c.png" width="200" height="200">

It is easy to see that if values for alpha_k are not equal the symmetry is broken and the maximum will be found near bigger values.

Additional, please note that values for priors parameters produce smooth pdfs of the distribution as the values of the parameters are near 1. So if you have great confidence that something is clearly distributed in a way you know, with a high degree of confidence, than values far from 1 in absolute value are to be used, if you do not have such kind of knowledge than values near 1 would be encode this lack of knowledge. It is easy to see why 1 plays such a role in Dirichlet distribution from the formula of the distribution itself.

Another way to understand this is to see that prior encode prior-knowledge. In the same time you might think that prior encode some prior seen data. This data was not saw by the algorithm itself, it was saw by you, you learned something, and you can model prior according to what you know (learned). So in the prior parameters (hyperparameters) you encode also how big this data set you apriori saw, because the sum of alpha_k can be that also as the size of this more or less imaginary data set. So the bigger the prior data set, the bigger is the confidence, the bigger the values of alpha_k you can choose, the sharper the surface near maximum value, which means also less doubts.

Hope it helped.

PS: It''s a hell to write something without LaTeX notation. I home moderators/administrators will do something.', 108, '2014-05-23 13:47:54.603', '9d319c11-fd98-4196-be06-15be8d76f19a', 202, 552, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Never done stuff **on that scale**, but as no-one else has jumped in yet have you seen these two papers that discuss non-commercial solutions?  Symphony and COIN-OR seem to be the dominant suggestions.

Linderoth, Jeffrey T., and Andrea Lodi. "MILP software." Wiley encyclopedia of operations research and management science (2010). [PDF version][1]

Linderoth, Jeffrey T., and Ted K. Ralphs. "Noncommercial software for mixed-integer linear programming." Integer programming: theory and practice 3 (2005): 253-303. [Compares performance][2]


  [1]: http://homepages.cae.wisc.edu/~Linderot/papers/Linderoth-Lodi-10.pdf
  [2]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.147.5872', 265, '2014-05-23 14:28:41.563', 'b0f5b49e-8b99-4284-91ff-34b0e69802e8', 203, 553, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I recently did a similar project in Python (predicting opinions using FB like data), and had good results with the following basic process:

 1. Read in the training set (n = N) by iterating over comma-delimited like records line-by-line and use a counter to identify the most popular pages
 2. For each of the K most popular pages (I used about 5000, but you can play around with different values), use pandas.DataFrame.isin to test whether each individual in the training set likes each page, then make a N x K dataframe of the results (I''ll call it xdata_train)
 3. Create a series (I''ll call it ydata_train) containing all of the outcome variables (in my case opinions, in yours age) with the same index as xdata_train
 4. Set up a random forest classifier through scikit-learn to predict
    ydata_train based on xdata_train
 5. Use scikit-learn''s cross-validation testing to tweak parameters and
    refine accuracy (tweaking number of popular pages, number of trees,
    min leaf size, etc.)
 6. Output random forest classifier and list of most popular pages with pickle (or keep in memory if you are doing everything at once)
 7. Load in the rest of your data, load the list of popular pages (if necessary), and repeat step 2 to produce xdata_new
 8. Load the random forest classifier (if necessary) and use it to predict values for the xdata_new data
 9. Output the predicted scores to a new CSV or other output format of your choosing

In your case, you''d need to swap out the classifier for a regressor (so see here: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) but otherwise the same process should work without much trouble.

Also, you should be aware of the most amazing feature of random forests in Python: instant parallelization! Those of us who started out doing this in R and then moved over are always amazed, especially when you get to work on a machine with a few dozen cores (see here: http://blog.yhathq.com/posts/comparing-random-forests-in-python-and-r.html).

Finally, note that this would be a perfect application for network analysis if you have the data on friends as well as the individuals themselves. If you can analyze the ages of a user''s friends, the age of the user will almost certainly be within a year or two of the median among his or her friends, particularly if the users are young enough to have built their friend networks while still in school (since most will be classmates). That prediction would likely trump any you would get from modeling---this is a textbook example of a problem where the right data > the right model every time.

Good luck!
', 250, '2014-05-23 19:28:01.903', 'b2bee134-b885-4070-ac32-b419a2afe85e', 204, 554, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Working on what could often be called "medium data" projects, I''ve been able to parallelize my code (mostly for modeling and prediction in Python) on a single system across anywhere from 4 to 32 cores. Now I''m looking at scaling up to clusters on EC2 (probably with StarCluster/IPython, but open to other suggestions as well), and have been puzzled by how to reconcile distributing work across cores on an instance vs. instances on a cluster.

Is it even practical to parallelize across instances as well as across cores on each instance? If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance?

Bandwidth and RAM are non-trivial concerns in my projects, but it''s easy to spot when those are the bottlenecks and readjust. It''s much harder, I''d imagine, to benchmark the right mix of cores to instances without repeated testing, and my projects vary too much for any single test to apply to all circumstances. Thanks in advance, and if I''ve just failed to google this one properly, feel free to point me to the right answer somewhere else!

', 250, '2014-05-23 19:45:54.283', '23adfc19-f8ac-4163-b0bf-e5501f9176d0', 205, 555, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Instances vs. cores when using EC2', 250, '2014-05-23 19:45:54.283', '23adfc19-f8ac-4163-b0bf-e5501f9176d0', 205, 556, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<parallel><clusters><aws>', 250, '2014-05-23 19:45:54.283', '23adfc19-f8ac-4163-b0bf-e5501f9176d0', 205, 557, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('All things considered equal (cost, CPU perf, etc.) you could choose the smallest instance that can hold all of my dataset in memory and scale out. That way

- you make sure not to induce unnecessary latencies due to network communications, and
- you tend to maximize the overall available memory bandwidth for your processes.

Assuming you are running some sort of [cross-validation scheme][1] to optimize some [meta parameter][2] of your model, assign each core a value to test and choose an many instances as needed to cover all the parameter space in as few rounds as you see fit.

If your data does not fit in the memory of one system, of course you''ll need to distribute across instances. Then it is a matter of balancing memory latency (better with many instances) with network latency (better with fewer instances) but given the nature of EC2 I''d bet you''ll often prefer to work with few fat instances.


  [1]: http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29
  [2]: http://en.wikipedia.org/wiki/Meta-optimization', 172, '2014-05-23 21:01:18.630', 'ae763f36-1ff4-4f38-8c28-8d7072134a8e', 206, 558, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A general rule of thumb is to not distribute until you have to. It''s usually more efficient to have N servers of a certain capacity than 2N servers of half that capacity. More of the data access will be local, and therefore fast in memory versus slow across the network.

At a certain point, scaling up one machine becomes uneconomical because the cost of additional resource scales more than linearly. However this point is still amazingly high.

On Amazon in particular though, the economics of each instance type can vary a lot if you are using spot market instances. The default pricing more or less means that the same amount of resource costs about the same regardless of the instance type, that can vary a lot; large instances can be cheaper than small ones, or N small instances can be much cheaper than one large machine with equivalent resources.

One massive consideration here is that the computation paradigm can change quite a lot when you move from one machine to multiple machines. The tradeoffs that the communication overhead induce may force you to, for example, adopt a data-parallel paradigm to scale. That means a different choice of tools and algorithm. For example, SGD looks quite different in-memory and in Python than on MapReduce. So you would have to consider this before parallelizing.

You may choose to distribute work across a cluster, even if a single node and non-distributed paradigms work for you, for reliability. If a single node fails, you lose all of the computation; a distributed computation can potentially recover and complete just the part of the computation that was lost.', 21, '2014-05-24 10:36:58.987', '156b946a-e3c1-4a59-93e7-ae69ebebfab9', 207, 559, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When using IPython, you very nearly don''t have to worry about it (at the expense of some loss of efficiency/greater communication overhead).  The parallel IPython plugin in StarCluster will by default start one engine per physical core on each node (I believe this is configurable but not sure where).  You just run whatever you want across all engines by using the DirectView api (map_sync, apply_sync, ...) or the %px magic commands.   If you are already using IPython in parallel on one machine, using it on a cluster is no different.

Addressing some of your specific questions:

"how to reconcile distributing work across cores on an instance vs. instances on a cluster" - You get one engine per core (at least); work is automatically distributed across all cores and across all instances.

"Is it even practical to parallelize across instances as well as across cores on each instance?" - Yes :)  If the code you are running is embarrassingly parallel (exact same algo on multiple data sets) then you can mostly ignore where a particular engine is running.  If the core requires a lot of communication between engines, then of course you need to structure it so that engines primarily communicate with other engines on the same physical machine; but that kind of problem is not ideally suited for IPython, I think.

"If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance?" - Use the largest c3 instances for compute-bound, and the smallest for memory-bandwidth-bound problems (or small enough that the problem almost stops being memory-bandwidth-bound); for message-passing-bound problems, also use the largest instances but try to partition the problem so that each partition runs on one physical machine and most message passing is within the same partition.  Problems which run slower on n quadruple c3 than on 2n double c3 are rare (an artificial example may be running multiple simple filters on a large number of images, where you go through all images for each filter rather than all filters for the same image).  Using largest instances is a good rule of thumb.


', 26, '2014-05-24 11:18:26.497', 'c677f842-6942-44bf-b85f-446860ed5658', 208, 560, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('When using IPython, you very nearly don''t have to worry about it (at the expense of some loss of efficiency/greater communication overhead).  The parallel IPython plugin in StarCluster will by default start one engine per physical core on each node (I believe this is configurable but not sure where).  You just run whatever you want across all engines by using the DirectView api (map_sync, apply_sync, ...) or the %px magic commands.   If you are already using IPython in parallel on one machine, using it on a cluster is no different.

Addressing some of your specific questions:

"how to reconcile distributing work across cores on an instance vs. instances on a cluster" - You get one engine per core (at least); work is automatically distributed across all cores and across all instances.

"Is it even practical to parallelize across instances as well as across cores on each instance?" - Yes :)  If the code you are running is embarrassingly parallel (exact same algo on multiple data sets) then you can mostly ignore where a particular engine is running.  If the core requires a lot of communication between engines, then of course you need to structure it so that engines primarily communicate with other engines on the same physical machine; but that kind of problem is not ideally suited for IPython, I think.

"If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance?" - Use the largest c3 instances for compute-bound, and the smallest for memory-bandwidth-bound problems (or small enough that the problem almost stops being memory-bandwidth-bound); for message-passing-bound problems, also use the largest instances but try to partition the problem so that each partition runs on one physical machine and most message passing is within the same partition.  Problems which run significantly slower on N quadruple c3 than on 2N double c3 are rare (an artificial example may be running multiple simple filters on a large number of images, where you go through all images for each filter rather than all filters for the same image).  Using largest instances is a good rule of thumb.


', 26, '2014-05-24 12:12:31.857', '0751f7cf-edc8-4185-9d29-24865ef39d12', 208, 'added 14 characters in body', 561, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A recommendation system keeps a log of what recommendations have been made to a particular user and whether that user accepts the recommendation. It''s like

    user_id item_id result
    1       4       1
    1       7       -1
    5       19      1
    5       80      1

where 1 means the user accepted the recommendation while -1 means the user did not respond to the recommendation.

**Question:** If I am going to make recommendations to a bunch of users based on the kind of log described above, and I want to maximize MAP@3 scores, how should I deal with the implicit data (1 or -1)?

My idea is to treat 1 and -1 as ratings, and predict the rating using factorization machines-type algorithms. But this does not seem right, given the asymmetry of the implicit data (-1 does not mean the user does not like the recommendation).', 71, '2014-05-25 13:57:52.657', '5c239016-baf9-4c0b-86e0-1613cff2356a', 209, 562, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How should one deal with implicit data in recommendation', 71, '2014-05-25 13:57:52.657', '5c239016-baf9-4c0b-86e0-1613cff2356a', 209, 563, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation>', 71, '2014-05-25 13:57:52.657', '5c239016-baf9-4c0b-86e0-1613cff2356a', 209, 564, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Assuming symmetric Dirichlet distributions (for simplicity), a low alpha value places more weight on having each document composed of only a few dominant topics (whereas a high value will return many more relatively dominant topics). Similarly, a low beta value places more weight on having each topic composed of only a few dominant words.', 283, '2014-05-26 04:07:32.390', '5e1b3be6-9cd8-4d0d-98b0-36e7b4634125', 210, 565, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A recommendation system keeps a log of what recommendations have been made to a particular user and whether that user accepts the recommendation. It''s like

    user_id item_id result
    1       4       1
    1       7       -1
    5       19      1
    5       80      1

where 1 means the user accepted the recommendation while -1 means the user did not respond to the recommendation.

**Question:** If I am going to make recommendations to a bunch of users based on the kind of log described above, and I want to maximize MAP@3 scores, how should I deal with the implicit data (1 or -1)?

My idea is to treat 1 and -1 as ratings, and predict the rating using factorization machines-type algorithms. But this does not seem right, given the asymmetry of the implicit data (-1 does not mean the user does not like the recommendation).

**Edit 1**
Let us think about it in the context of a matrix factorization approach. If we treat -1 and 1 as ratings, there will be some problem. For example, user 1 likes movie A which scores high in one factor (e.g. having glorious background music) in the latent factor space. The system recommends movie B which also scores high in "glorious background music", but for some reason user 1 is too busy to look into the recommendation, and we have a -1 rating movie B. If we just treat 1 or -1 equally, then the system might be discouraged to recommend movie with glorious BGM to user 1 while user 1 still loves movie with glorious BGM. I think this situation is to be avoided.', 71, '2014-05-26 05:12:32.653', '782e8050-c987-4a5f-b91c-c6fcdeca23c3', 209, 'added 682 characters in body', 566, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m new to this community and hopefully my question will well fit in here.
As part of my undergraduate data analytics course I have choose to do the project on human activity recognition using smartphone data sets. As far as I''m concern this topic relates to Machine Learning and Support Vector Machines. I''m not well familiar with this technologies yet so I will need some help.

I have decided to follow this project idea http://www.inf.ed.ac.uk/teaching/courses/dme/2014/datasets.html (first project on the top)
The project goal is determine what activity a person is engaging in (e.g., WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) from data recorded by a smartphone (Samsung Galaxy S II) on the subject''s waist. Using its embedded accelerometer and gyroscope, the data includes 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz.

All the data set is given in one folder with some description and feature labels. The data is divided for ''test'' and ''train'' files in which data is represented in this format:

      2.5717778e-001 -2.3285230e-002 -1.4653762e-002 -9.3840400e-001 -9.2009078e-001 -6.6768331e-001 -9.5250112e-001 -9.2524867e-001 -6.7430222e-001 -8.9408755e-001 -5.5457721e-001 -4.6622295e-001  7.1720847e-001  6.3550240e-001  7.8949666e-001 -8.7776423e-001 -9.9776606e-001 -9.9841381e-001 -9.3434525e-001 -9.7566897e-001 -9.4982365e-001 -8.3047780e-001 -1.6808416e-001 -3.7899553e-001  2.4621698e-001  5.2120364e-001 -4.8779311e-001  4.8228047e-001 -4.5462113e-002  2.1195505e-001 -1.3489443e-001  1.3085848e-001 -1.4176313e-002 -1.0597085e-001  7.3544013e-002 -1.7151642e-001  4.0062978e-002  7.6988933e-002 -4.9054573e-001 -7.0900265e-001

And that''s only a vary small sample of what the file contain.

I don''t really know what this data represents and how can be interpreted. Also for analyzing, classification and clustering of the data, what tools will I need to use?
Is there any way I can put this data into excel with labels included and for example use R or python to extract sample data and work on this?

Any hints/tips would be much appreciated.

Thank you!', 295, '2014-05-27 10:41:33.220', '0e20cd8f-34d0-44ca-9656-fb127e4cbf2e', 211, 567, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Human activity recognition using smartphone data set problem', 295, '2014-05-27 10:41:33.220', '0e20cd8f-34d0-44ca-9656-fb127e4cbf2e', 211, 568, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><machine-learning><databases><clustering><data-mining>', 295, '2014-05-27 10:41:33.220', '0e20cd8f-34d0-44ca-9656-fb127e4cbf2e', 211, 569, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Your system isn''t just trained on items that are recommended right? if so you have a big feedback loop here. You want to learn from all clicks/views, I hope.

You suggest that not-looking at an item is a negative signal. I strongly suggest you do not treat it that way. Not interacting with something is almost always best treated as no information. If you have an explicit signal that indicates a dislike, like a down vote (or, maybe watched 10 seconds of a video and stopped), maybe that''s valid.

I would not construe this input as rating-like data. (Although in your case, you may get away with it.) Instead think of them as weights, which is exactly the treatment in the Hu Koren Volinsky paper on ALS that @Trey mentions in a comment. This lets you record relative strength of positive/negative interactions.

Finally I would note that this paper, while is very likely to be what you''re looking for, does not provide for negative weights. It is simple to extend in this way. If you get that far I can point you to the easy extension, which exists already in two implementations that I know of, in [Spark][1] and [Oryx][2].


  [1]: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/recommendation/ALS.scala
  [2]: https://github.com/cloudera/oryx/blob/master/als-common/src/main/java/com/cloudera/oryx/als/common/factorizer/als/AlternatingLeastSquares.java', 21, '2014-05-27 10:58:00.620', 'e2633786-6b37-4632-933c-6e981de9961c', 212, 570, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The data set definitions are on the page here:

[Attribute Information at the bottom][1]

or you can see inside the ZIP folder the file named activity_labels, that has your column headings inside of it, make sure you read the README carefully, it has some good info in it. You can easily bring in a `.csv` file in R using the `read.csv` command.

For example if you name you file `samsungdata` you can open R and run this command:

    data <- read.csv("directory/where/file/is/located/samsungdata.csv", header = TRUE)

Or if you are already inside of the working directory in R you can just run the following

    data <- read.csv("samsungdata.csv", header = TRUE)

Where the name `data` can be changed to whatever you want to call your data set.

  [1]: http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones#', 59, '2014-05-27 12:07:45.920', 'e0ca2a3a-da57-4e61-b42c-e405ea2039f0', 213, 571, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m new to this community and hopefully my question will well fit in here.
As part of my undergraduate data analytics course I have choose to do the project on human activity recognition using smartphone data sets. As far as I''m concern this topic relates to Machine Learning and Support Vector Machines. I''m not well familiar with this technologies yet so I will need some help.

I have decided to follow this project idea http://www.inf.ed.ac.uk/teaching/courses/dme/2014/datasets.html (first project on the top)
The project goal is determine what activity a person is engaging in (e.g., WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) from data recorded by a smartphone (Samsung Galaxy S II) on the subject''s waist. Using its embedded accelerometer and gyroscope, the data includes 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz.

All the data set is given in one folder with some description and feature labels. The data is divided for ''test'' and ''train'' files in which data is represented in this format:

      2.5717778e-001 -2.3285230e-002 -1.4653762e-002 -9.3840400e-001 -9.2009078e-001 -6.6768331e-001 -9.5250112e-001 -9.2524867e-001 -6.7430222e-001 -8.9408755e-001 -5.5457721e-001 -4.6622295e-001  7.1720847e-001  6.3550240e-001  7.8949666e-001 -8.7776423e-001 -9.9776606e-001 -9.9841381e-001 -9.3434525e-001 -9.7566897e-001 -9.4982365e-001 -8.3047780e-001 -1.6808416e-001 -3.7899553e-001  2.4621698e-001  5.2120364e-001 -4.8779311e-001  4.8228047e-001 -4.5462113e-002  2.1195505e-001 -1.3489443e-001  1.3085848e-001 -1.4176313e-002 -1.0597085e-001  7.3544013e-002 -1.7151642e-001  4.0062978e-002  7.6988933e-002 -4.9054573e-001 -7.0900265e-001

And that''s only a very small sample of what the file contain.

I don''t really know what this data represents and how can be interpreted. Also for analyzing, classification and clustering of the data, what tools will I need to use?
Is there any way I can put this data into excel with labels included and for example use R or python to extract sample data and work on this?

Any hints/tips would be much appreciated.', 84, '2014-05-27 14:57:34.150', '7ce1dc45-e93b-4713-95c0-5647f929b02f', 211, 'Fixed grammar, and improving formatting.', 574, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Public Data Sets

 -

https://www.opensciencedatacloud.org/publicdata/

Google Public Data Sets

 -
http://www.google.com/publicdata/directory

Amazon Web Services

 -

https://aws.amazon.com/publicdatasets/

Finding Data on the Internet

 -

http://www.inside-r.org/howto/finding-data-internet', 295, '2014-05-27 16:05:02.883', 'af834d70-2dbe-42eb-a6e8-9bbdacbcdb40', 214, 575, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m building a workflow for creating machine learning models (in my case, using Python''s `pandas` and `sklearn` packages) from data pulled from a very large database (here, Vertica by way of SQL and `pyodbc`), and a critical step in that process involves imputing missing values of the predictors. This is straightforward within a single analytics or stats platform---be it Python, R, Stata, etc.---but I''m curious where best to locate this step in a multi-platform workflow.

It''s simple enough to do this in Python, either with the <a href="http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values">`sklearn.preprocessing.Imputer`</a> class, using the <a href="http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.fillna.html">`pandas.DataFrame.fillna`</a> method, or by hand (depending upon the complexity of the imputation method used). But since I''m going to be using this for dozens or hundreds of columns across hundreds of millions of records, I wonder if there''s a more efficient way to do this directly through SQL ahead of time. Aside from the potential efficiencies of doing this in a distributed platform like Vertica, this would have the added benefit of allowing us to create an automated pipeline for building "complete" versions of tables, so we don''t need to fill in a new set of missing values from scratch every time we want to run a model.

I haven''t been able to find much guidance about this, but I imagine that we could:

 1. create a table of substitute values (e.g., mean/median/mode, either overall or by group) for each incomplete column
 2. join the substitute value table with the original table to assign a substitute value for each row and incomplete column
 3. use a series of case statements to take the original value if available and the substitute value otherwise

Is this a reasonable thing to do in Vertica/SQL, or is there a good reason not to bother and just handle it in Python instead? And if the latter, is there a strong case for doing this in pandas rather than sklearn or vice-versa? Thanks!', 250, '2014-05-27 21:07:48.973', '7435ff22-915f-4106-b77c-58bab667b4f8', 215, 576, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where in the workflow should we deal with missing data?', 250, '2014-05-27 21:07:48.973', '7435ff22-915f-4106-b77c-58bab667b4f8', 215, 577, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><python><pandas><sklearn>', 250, '2014-05-27 21:07:48.973', '7435ff22-915f-4106-b77c-58bab667b4f8', 215, 578, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My strong opinion regarding automated tasks like imputation (but, here I can include also scaling, centering, feature selection, etc) is to avoid in any way do such things without carefully inspecting your data.

Of course, after deciding what kind of imputation to apply it can be automated (under the assumption that the new data has the same shape/problems).

So, before anything, take a wise decision. I often wasted time trying to automate this things, destroying my data. I will give you some examples:
- a marketplace encoded as N/A, which I missed and considered to be North/America
- numbers like -999.0, because the data producer could not find a better replacement for missing data
- number like 0 for blood pressure or body temperature, instead of missing data (it is hard to imagine a living human with 0 blood pressure)
- multiple placeholders for missing data, due to the fact that the data was collected from various sources

After that you need to understand what kind of imputation would resemble better the information from your data for a given task. This is often much harder to do it right than it seems.

After all those things, my advice is to delay your imputation task to an upper layer where you have tools to reproduce on new data and to inspect if the assumptions for the new data are not violated (if it is possible).', 108, '2014-05-28 07:08:05.393', '04cffb36-5fb5-45d9-a1f5-0e1777c646ec', 216, 580, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It looks like this (or very similar data set) is used for Coursera courses. Cleaning this dataset is task for [Getting and Cleaning Data][1], but it is also used for case study for [Exploratory Data analysis][2]. Video from this case study is available in videos for week 4 of EDA course-ware. It might help you with starting with this data.


  [1]: https://www.coursera.org/course/getdata
  [2]: https://class.coursera.org/exdata-002', 82, '2014-05-28 09:43:54.197', '1637bff0-8316-4b9e-ba2c-d541508da5a0', 217, 581, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('So, I have a dataset with 39.949 variables and 180 rows. dataset is successfully
saved in DataFrame but when I try to find cov() it result an error.
here is the code

      import pandas as pd
      cov_data=pd.DataFrame(dataset).cov()

Here is the error

    File "/home/syahdeini/Desktop/FP/pca_2.py", line 44, in find_eagen
    cov_data=pd.DataFrame(data_mat).cov()
    File "/usr/lib/python2.7/dist-packages/pandas/core/frame.py", line 3716, in cov
    baseCov = np.cov(mat.T)
    File "/usr/lib/python2.7/dist-packages/numpy/lib/function_base.py", line 1766, in cov
    return (dot(X, X.T.conj()) / fact).squeeze()
    ValueError: array is too big.
', 273, '2014-05-29 13:08:09.060', '7bd05da6-4c4b-4639-a077-436919934475', 218, 582, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('built-in cov in pandas DataFrame results ValueError array is too big', 273, '2014-05-29 13:08:09.060', '7bd05da6-4c4b-4639-a077-436919934475', 218, 583, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><pandas>', 273, '2014-05-29 13:08:09.060', '7bd05da6-4c4b-4639-a077-436919934475', 218, 584, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Since you have 39,949 variables, the covariance matrix would have about 1.6 billion elements (39,949 * 39,949 = 1,595,922,601).  That is likely why you are getting that error.', 178, '2014-05-29 13:51:48.820', '39774ccc-d4d0-43f0-8929-ab52b3ebdad9', 219, 585, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Agreed. To be simplistic about it, if this translates to 1.6B floats, at 16 bytes per float (32-bit version; 64-bit is bigger), then you''re trying to create an array of about 26 GB. Even if you have the RAM for that, I''d imagine that it''s probably going to overload something else along the way.

(Maybe not, but generally speaking, any operations that are that computationally intensive should first raise the question of whether you are doing the right calculation in the first place. And if you do need to do something of that magnitude, you should then try to break it down into more manageable chunks that can be run in parallel or distributed across machines.)

But given that you are describing a very, very wide dataset (~40k columns x 180 rows), I wonder whether you really want to take the covariance matrix of the transposed dataset (so 180x180 = 32,400 covariances)? That would be a far more tractable problem, and it''s easier to see how it might be useful.

In any case, you''re probably far better off calculating each pairwise covariance (or at least, the vector of cov(x_i,x_k) for all x_k != x_i) at the point where you''ll actually use it, rather than calculating a giant matrix initially then referring back to it later. Memory issues aside, it''ll make your life much easier if you start running things in parallel, and will help ensure you don''t waste resources on unnecessary calculations.', 250, '2014-05-29 14:13:00.317', '9882f334-d988-4b68-958f-193aaec026ee', 220, 586, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It''s actually even simpler than that, from what you describe---you''re just looking for a basic classification tree algorithm (so no need for slightly more complex variants like C4.5 which are optimized for prediction accuracy). The canonical text is:

http://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418

This is readily implemented in R:

http://cran.r-project.org/web/packages/tree/tree.pdf

and Python:

http://scikit-learn.org/stable/modules/tree.html', 250, '2014-05-29 14:30:21.357', '56420626-767c-4446-8037-dea50b7530be', 221, 587, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Christopher is right about the size of the array. To be simplistic about it, if this translates to 1.6B floats, at 16 bytes per float (32-bit version; 64-bit is bigger), then you''re trying to create an array of about 26 GB. Even if you have the RAM for that, I''d imagine that it''s probably going to overload something else along the way.

(Maybe not, but generally speaking, any operations that are that computationally intensive should first raise the question of whether you are doing the right calculation in the first place. And if you do need to do something of that magnitude, you should then try to break it down into more manageable chunks that can be run in parallel or distributed across machines.)

But given that you are describing a very, very wide dataset (~40k columns x 180 rows), I wonder whether you really want to take the covariance matrix of the transposed dataset (so 180x180 = 32,400 covariances)? That would be a far more tractable problem, and it''s easier to see how it might be useful.

In any case, you''re probably far better off calculating each pairwise covariance (or at least, the vector of cov(x_i,x_k) for all x_k != x_i) at the point where you''ll actually use it, rather than calculating a giant matrix initially then referring back to it later. Memory issues aside, it''ll make your life much easier if you start running things in parallel, and will help ensure you don''t waste resources on unnecessary calculations.', 250, '2014-05-29 14:31:51.913', '07f04cd9-29af-43dc-a72c-b22c44b1b8ee', 220, 'Clarified what I''m agreeing with at the outset.', 588, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Any small database processing can be easily tackled by Python/Perl/... scripts, that uses libraries and/or even utilities from the language itself. However, when it comes to performance, people tend to reach out for C/C++/low-level languages. The possibility of tailoring the code to the needs seems to be what makes these languages so appealing for BigData -- be it concerning memory management, parallelism, disk access, or even low-level optimizations (via assembly constructs at C/C++ level).

Of course such set of benefits would not come without a cost: writing the code, and sometimes even *reinventing the wheel*, can be quite expensive/tiresome. Although there are lots of libraries available, people are inclined to write the code by themselves whenever they need to *grant* performance. What *disables* performance assertions from using libraries while processing large databases?

For example, consider an entreprise that continuously crawls webpages and parses the data collected. For each sliding-window, different data mining algorithms are run upon the data extracted. Why would the developers ditch off using available libraries/frameworks (be it for crawling, text processing, and data mining)? Using stuff already implemented would not only ease the burden of coding the whole process, but also would save a lot of time.

**In a single shot**:

- what makes writing the code by oneself a *guarantee* of performance?
- why is it *risky* to rely on a frameworks/libraries when you must **assure** high performance?', 84, '2014-05-29 15:01:17.840', '8dc644fb-677f-4b51-beb1-7fc1959f5e90', 138, 'Improving question.', 589, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Why is it hard to grant efficiency while using libraries?', 84, '2014-05-29 15:01:17.840', '8dc644fb-677f-4b51-beb1-7fc1959f5e90', 138, 'Improving question.', 590, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Enigma][1] is a repository of public available datasets. Its free plan offers public data search, with 10k API calls per month. Not all public databases are listed, but the list is enough for common cases.


I used it for academic research and it saved me a lot of time.

  [1]: http://enigma.io', 43, '2014-05-29 19:02:13.210', '5d6279bf-6fac-47e9-8ad7-527ecbad9cb2', 222, 591, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Having a lot of text documents (in natural language, unstructured), what are the possible ways of annotating them with some semantic meta-data? For example, consider a short document:

    I saw the company''s manager last day.

To be able to extract information from it, it must be annotated with additional data to be less ambiguous. The process of finding such meta-data is not in question, so assume it is done manually. The question is how to store these data in a way that further analysis on it can be done more conveniently/efficiently?

A possible approach is to use XML tags (see below), but it seems too verbose, and maybe there are better approaches/guidelines for storing such meta-data on text documents.

    <Person name="John">I</Person> saw the <Organization name="ACME">company</Organization>''s
    manager <Time value="2014-5-29">last day</Time>.
', 227, '2014-05-29 20:11:16.327', '92a3929a-0987-4d88-b32d-55e344e971d5', 223, 592, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to annotate text documents with meta-data?', 227, '2014-05-29 20:11:16.327', '92a3929a-0987-4d88-b32d-55e344e971d5', 223, 593, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><metadata><data-cleaning><text-mining>', 227, '2014-05-29 20:11:16.327', '92a3929a-0987-4d88-b32d-55e344e971d5', 223, 594, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[Enigma][1] is a repository of public available datasets. Its free plan offers public data search, with 10k API calls per month. Not all public databases are listed, but the list is enough for common cases.

I used it for academic research and it saved me a lot of time.

---

Another interesting source of data is the [@unitedstates project][2], containing data and tools to collect them, about the United States (members of Congress, geographic shapes).


  [1]: http://enigma.io
  [2]: http://theunitedstates.io/', 43, '2014-05-30 21:05:52.147', 'd3f78d63-1ae1-4cb2-8138-47cc87ed443b', 222, 'Add another source', 595, '5');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-05-31 07:53:35.590', '735dd6b5-5aca-41ef-80b5-11734be9a0fc', 128, '1', 596, '34');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-05-31 07:53:35.590', '14a2110d-0337-45ea-828e-444e7c15bdfa', 130, '2', 597, '34');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The output of my word alignment file looks as such:

    I wish to say with regard to the initiative of the Portuguese Presidency that we support the spirit and the political intention behind it . In bezug auf die Initiative der portugiesischen Präsidentschaft möchte ich zum Ausdruck bringen , daß wir den Geist und die politische Absicht , die dahinter stehen , unterstützen . 0-0 5-1 5-2 2-3 8-4 7-5 11-6 12-7 1-8 0-9 9-10 3-11 10-12 13-13 13-14 14-15 16-16 17-17 18-18 16-19 20-20 21-21 19-22 19-23 22-24 22-25 23-26 15-27 24-28
    It may not be an ideal initiative in terms of its structure but we accept Mr President-in-Office , that it is rooted in idealism and for that reason we are inclined to support it . Von der Struktur her ist es vielleicht keine ideale Initiative , aber , Herr amtierender Ratspräsident , wir akzeptieren , daß sie auf Idealismus fußt , und sind deshalb geneigt , sie mitzutragen . 0-0 11-2 8-3 0-4 3-5 1-6 2-7 5-8 6-9 12-11 17-12 15-13 16-14 16-15 17-16 13-17 14-18 17-19 18-20 19-21 21-22 23-23 21-24 26-25 24-26 29-27 27-28 30-29 31-30 33-31 32-32 34-33

What should i do to get from the output above to get the phrase tables that are used by MOSES?
', 122, '2014-05-31 14:28:42.317', '1f331690-cb36-41f6-87d9-bc9ff26f567f', 224, 598, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to get phrase tables from word alignments? - Machine Translation', 122, '2014-05-31 14:28:42.317', '1f331690-cb36-41f6-87d9-bc9ff26f567f', 224, 599, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-trasnlation>', 122, '2014-05-31 14:28:42.317', '1f331690-cb36-41f6-87d9-bc9ff26f567f', 224, 600, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve found this link in Data Science Central with a list of free datasets: [Big data sets available for free][1]


  [1]: http://www.datasciencecentral.com/profiles/blogs/big-data-sets-available-for-free', 290, '2014-05-31 19:59:15.563', '77d3d243-a7a2-4c5d-b8d5-e89a4d3deab9', 226, 604, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can someone kindly tell me about the trade-offs involved when selected Storm or Hadoop for data processing? Aside from the obvious one that Hadoop is a batch processing system and Storm is a real-time processing system. I have worked a bit with Hadoop but I haven''t worked with Storm. After looking through a lot of presentations and articles, I still haven''t been able to find a satisfactory and comprehensive answer. ', 339, '2014-06-01 10:25:51.163', 'e04c1976-9113-442d-b2c2-1eaf8f9adae5', 227, 605, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Tradeoffs between Storm and Hadoop?', 339, '2014-06-01 10:25:51.163', 'e04c1976-9113-442d-b2c2-1eaf8f9adae5', 227, 606, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop>', 339, '2014-06-01 10:25:51.163', 'e04c1976-9113-442d-b2c2-1eaf8f9adae5', 227, 607, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Can someone kindly tell me about the trade-offs involved when choosing between Storm and Hadoop for data processing? Of course, aside from the obvious one, that Hadoop is a batch processing system, and Storm is a real-time processing system.

I have worked a bit with Hadoop, but I haven''t worked with Storm. After looking through a lot of presentations and articles, I still haven''t been able to find a satisfactory and comprehensive answer.', 84, '2014-06-01 11:45:40.060', '5e7b9b36-6d68-43db-8945-ea424609365a', 227, 'Improving formatting.', 608, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Tradeoffs between Storm and Hadoop', 84, '2014-06-01 11:45:40.060', '5e7b9b36-6d68-43db-8945-ea424609365a', 227, 'Improving formatting.', 609, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><efficiency><hadoop><distributed>', 84, '2014-06-01 11:45:40.060', '5e7b9b36-6d68-43db-8945-ea424609365a', 227, 'Improving formatting.', 610, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Going through the presentation and material of Summingbird by Twitter, one of the reasons that is mentioned for using Storm and Hadoop clusters together in Summingbird is that processing through Storm results in cascading of error. In order to avoid this cascading of error and accumulation of it, Hadoop cluster is used to batch process the data and discard the Storm results after the same data is processed by Hadoop.

What is the reasons for generation of this accumulation of error? and why is it not present in Hadoop? Since I have not worked with Storm, I do not know the reasons for it. Is it because Storm uses some approximate algorithm to process the data in order to process them in real time? or is the cause something else?', 339, '2014-06-01 12:51:25.040', '42eb4fc6-2a18-4596-bf43-9b3cee753664', 228, 611, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Cascaded Error in Apache Storm', 339, '2014-06-01 12:51:25.040', '42eb4fc6-2a18-4596-bf43-9b3cee753664', 228, 612, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop>', 339, '2014-06-01 12:51:25.040', '42eb4fc6-2a18-4596-bf43-9b3cee753664', 228, 613, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is kind of like asking about the tradeoffs between frying pan and your drawer of silverware. They are not two things you compare, really. You might use them together as part of a larger project.

Hadoop itself is not one thing, but a name for a federation of services, like HDFS, Hive, HBase, MapReduce, etc. Storm is something you use with some of these services, like HDFS or HBase. It is a stream-processing framework. There are others within the extended Hadoop ecosystem, like Spark Streaming.

When would you choose a stream-processing framework? when you need to react to new data in near-real-time. If you need this kind of tool, you deploy this kind of tool, too.', 21, '2014-06-01 19:48:41.693', '424ff205-e32a-4d32-8bbd-2981c2ae1aa8', 229, 614, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Can someone kindly tell me about the trade-offs involved when choosing between Storm and MapReduce in Hadoop Cluster for data processing? Of course, aside from the obvious one, that Hadoop (processing via MapReduce in a Hadoop Cluster) is a batch processing system, and Storm is a real-time processing system.

I have worked a bit with Hadoop Eco System, but I haven''t worked with Storm. After looking through a lot of presentations and articles, I still haven''t been able to find a satisfactory and comprehensive answer.', 339, '2014-06-01 21:52:00.540', 'b2164607-f475-473b-9edc-d0cb8b58c698', 227, 'Improved the question to reduce ambiguity ', 615, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Tradeoffs between Storm and Hadoop (MapReduce)', 339, '2014-06-01 21:52:00.540', 'b2164607-f475-473b-9edc-d0cb8b58c698', 227, 'Improved the question to reduce ambiguity ', 616, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Can someone kindly tell me about the trade-offs involved when choosing between Storm and MapReduce in Hadoop Cluster for data processing? Of course, aside from the obvious one, that Hadoop (processing via MapReduce in a Hadoop Cluster) is a batch processing system, and Storm is a real-time processing system.

I have worked a bit with Hadoop Eco System, but I haven''t worked with Storm. After looking through a lot of presentations and articles, I still haven''t been able to find a satisfactory and comprehensive answer.

Note: The term tradeoff here is not meant to compare to similar things. It is meant to represent the consequences of getting results real-time that are absent from a batch processing system. ', 339, '2014-06-01 21:59:02.347', 'cddc982f-015b-4ec8-b11c-eea07693bd82', 227, 'Improved the question to reduce ambiguity', 617, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In general, you don''t want to use XML tags to tag documents in this way because tags may overlap.

[UIMA](http://uima.apache.org/), [GATE](http://gate.ac.uk/) and similar NLP frameworks denote the tags separate from the text.  Each tag, such as `Person`, `ACME`, `John` etc. is stored as the position that the tag begins and the position that it ends.  So, for the tag `ACME`, it would be stored as starting a position 11 and ending at position 17.

', 178, '2014-06-02 15:03:35.940', 'f445fb8a-553e-4211-917f-847579d43dc7', 230, 618, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to test the accuracy of a methodology. I ran it ~400 times and I got a different classification for each run. I also have the ground truth, i.e. the real classification to test against.

For each classification I computed a confusion matrix. Now I want to aggregate these results in order to get the overall confusion matrix. How can I achieve it?

May I sum all confusion matrices in order to obtain the overall one?

Thanks', 133, '2014-06-05 09:00:27.950', '31363516-cd4e-4936-a095-bc9ff2fa67e9', 231, 620, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to get an aggregate confusion matrix from n different classifications', 133, '2014-06-05 09:00:27.950', '31363516-cd4e-4936-a095-bc9ff2fa67e9', 231, 621, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><confusion-matrix><accuracy>', 133, '2014-06-05 09:00:27.950', '31363516-cd4e-4936-a095-bc9ff2fa67e9', 231, 622, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I want to test the accuracy of a methodology. I ran it ~400 times, and I got a different classification for each run. I also have the ground truth, i.e., the real classification to test against.

For each classification I computed a confusion matrix. Now I want to aggregate these results in order to get the overall confusion matrix. How can I achieve it?

May I sum all confusion matrices in order to obtain the overall one?', 84, '2014-06-05 15:21:40.640', '7a4551d8-3620-4b74-8d8d-1f70f95629fb', 231, 'Minor corrections.', 623, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are a few ways to achieve your "master confusion matrix".

 1. Sum all the confusion matrices together:  Like you suggested, summing this results in a confusion matrix.  The problem with this is you can not interpret totals.

 2. Average the entries.  This method is the same as number one, but you divide each entry by the number of trials (~400 in your case).  This would be my preferred method because then you can actually translate each category to a (mean) +- (an error measurement) and actually see which categories are the most volatile or stable.  Careful with interpreting this ''error measurement'' though.

 3.  Report a problem specific measurement of the confusion numbers.  For example, if your numbers have outliers, medians would preferred over means.

There are other statistics that are possible to report as well.  You can redo the method to keep track of individual classifications.  Then we can say other important stats like ''% of classifications that stay the same and are accurate'', etc...', 375, '2014-06-06 14:55:18.867', '7cdf03ac-cdf8-4a8c-b6a7-76f601367c67', 232, 624, '2');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (122, '2014-06-07 20:37:03.950', '4312f0e4-ac22-46dc-8451-4a7425918fbf', 224, '3', 626, '33');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To complete Damien''s answer, an example of dimensionality reduction in NLP is a [topic model](http://en.wikipedia.org/wiki/Topic_model), where you represent the document by a vector indicating the weights of its constituent topics.', 381, '2014-06-08 07:03:43.710', 'c7a83cbb-54a4-4d70-a66a-1bd10f30c3f6', 233, 627, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As Yann LeCun [mentioned][1], a number of PhD programs in data science will be popping up in the next few years.

[NYU][2] already have one, where Prof.LeCun is at right now.

A statistics or cs PhD in machine learning is probably more rigorous than a data science one.  Is data science PhD for the less mathy people like myself?

Are these cash cow programs?

There is a huge industry demand for big data, but what is the academic value of these programs, as you probably can''t be a professor or publish any paper.


  [1]: http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/chisdw1
  [2]: http://datascience.nyu.edu/academics/programs/', 386, '2014-06-09 04:43:03.497', '0d5dd006-8e5c-436b-804c-591312bbd968', 234, 628, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data science Ph.D. program, what do you think?', 386, '2014-06-09 04:43:03.497', '0d5dd006-8e5c-436b-804c-591312bbd968', 234, 629, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<knowledge-base>', 386, '2014-06-09 04:43:03.497', '0d5dd006-8e5c-436b-804c-591312bbd968', 234, 630, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data visualization is an important subfield in data science and python programmers would need to have available toolkits for them.

**Is there a python API to tablaeu?**

**Are there any python based data visualization toolkit?**', 122, '2014-06-09 08:34:29.337', 'c0d5a609-abdc-44a6-8f14-61cd2cd8421c', 235, 631, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are there any python based data visualization toolkit?', 122, '2014-06-09 08:34:29.337', 'c0d5a609-abdc-44a6-8f14-61cd2cd8421c', 235, 632, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><visualization>', 122, '2014-06-09 08:34:29.337', 'c0d5a609-abdc-44a6-8f14-61cd2cd8421c', 235, 633, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('No-one knows since no-one''s completed one of these PhD programs yet! However, I would look at the syllabus and the teachers to base my decision. It all depends on what you want to do; industry or academia?', 381, '2014-06-09 18:02:00.613', 'bfab3b37-d0ba-4e83-98fe-239cb27bc27a', 236, 634, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is a Tablaeu API and you can use Python to use it, but maybe not in the sense that you think. There is a Data Extract API that you could use to import your data into Python and do your visualizations there, so I do not know if this is going to answer your question entirely.

As in the first comment you can use Matplotlib from [Matplotlib website][1], or you could install Canopy from Enthought which has it available, there is also Pandas, which you could also use for data analysis and some visualizations. There is also a package called `ggplot` which is used in `R` alot, but is also made for Python, which you can find here [ggplot for python][2].

The Tableau data extract API and some information about it can be found [at this link][3]. There are a few web sources that I found concerning it using duckduckgo [at this link][4].
Here are some samples:

[Link 1][5]

[Link 2][6]

[Link 3][7]

As far as an API like matplotlib, I cannot say for certain that one exists. Hopefully this gives some sort of reference to help answer your question.

Also to help avoid closure flags and downvotes you should try and show some of what you have tried to do or find, this makes for a better question and helps to illicit responses.


  [1]: http://www.matplotlib.org
  [2]: https://pypi.python.org/pypi/ggplot
  [3]: http://www.tableausoftware.com/new-features/data-engine-api-0
  [4]: https://duckduckgo.com/?q=tableau%20PYTHON%20API&kp=1&kd=-1
  [5]: https://www.interworks.com/blogs/bbickell/2012/12/06/introducing-python-tableau-data-extract-api-csv-extract-example
  [6]: http://ryrobes.com/python/building-tableau-data-extract-files-with-python-in-tableau-8-sample-usage/
  [7]: http://nbviewer.ipython.org/github/Btibert3/tableau-r/blob/master/Python-R-Tableau-Predictive-Modeling.ipynb', 59, '2014-06-09 19:52:41.847', '87f549e9-5d23-4b2b-9f42-caf1587338e5', 237, 635, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think this question assumes a false premise. As a student at NYU, I only know of a Masters in Data Science. You linked to a page that confirms this.

It''s hard to gauge the benefit of a program that doesn''t exist yet.', 395, '2014-06-09 21:36:44.297', '2632501a-15ad-443d-ada3-52b3a04ae396', 238, 637, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It seems to me that the premise of a PhD is to expand knowledge in some little slice of the world. Since a "data scientist" is by nature is somewhat of a jack-of-all-trades it does seem a little odd to me. A masters program seems much more appropriate.

What do you hope to gain from a PhD? If the rigor scares (or bores) you, then what about a more applied area? Signal processing, robotics, applied physics, operations research, etc.', 403, '2014-06-09 21:51:53.793', '3f7678f2-b982-4e3e-97da-c1f35521b938', 241, 640, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**MapReduce**: A fault tolerant distributed computational framework. MapReduce allows you to operate over huge amounts of data- with a lot of work put in to prevent failure due to hardware. MapReduce is a poor choice for computing results on the fly because it is slow. (A typical MapReduce job takes on the order of minutes or hours, not microseconds)

A MapReduce job takes a file (or some data store) as an input and writes a file of results. If you want these results available to an application, it is your responsibility to put this data in a place that is accessible. This is likely slow, and there will be a lag between the values you can display, and the values that represent your system in its current state.

An important distinction to make when considering using MapReduce in building realtime systems is that of training your model, and applying your model. If you think your model parameters do not change quickly, you can fit them with MapReduce, and then have a mechanism for accessing these pre-fit parameters when you want to apply your model.

**Storm**: A real-time, streaming computational system. Storm is online framework, meaning, in this sense, a service that interacts with a running application. In contrast to MapReduce, it receives small pieces of data (not a whole file) as they are processed in your application. You define a DAG of operations to perform on the data. A common and simple use case for Storm is tracking counters, and using that information to populate a real-time dashboard.

Storm doesn''t have anything (necessarily) to do with persisting your data. Here, streaming is another way to say keeping the information you care about and throwing the rest away. In reality, you probably have a persistence layer in your application that has already recorded the data, and so this a good and justified separation of concerns.


**If you want to know more...**
If you would like to learn more about real-time systems that that fit parameters with MR and apply the models a different way [here are slides for a talk I gave on building real-time recommendation engines on HBase.][1]

An excellent paper that marries real-time counting and persistence in an interesting way is [Google News Personalization: Scalable Online Collaborative Filtering][2] Another interesting marriage of MR and Storm is [SummingBird][3].

Summingbird allows you to define data analysis operations that can be applied via Storm or MR.


  [1]: http://www.slideshare.net/cloudera/hbasecon-2013-24063525
  [2]: http://dl.acm.org/citation.cfm?id=1242610', 406, '2014-06-09 21:57:30.240', '556ceec3-79a0-4e9e-8690-c1409604c283', 242, 641, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**MapReduce**: A fault tolerant distributed computational framework. MapReduce allows you to operate over huge amounts of data- with a lot of work put in to prevent failure due to hardware. MapReduce is a poor choice for computing results on the fly because it is slow. (A typical MapReduce job takes on the order of minutes or hours, not microseconds)

A MapReduce job takes a file (or some data store) as an input and writes a file of results. If you want these results available to an application, it is your responsibility to put this data in a place that is accessible. This is likely slow, and there will be a lag between the values you can display, and the values that represent your system in its current state.

An important distinction to make when considering using MapReduce in building realtime systems is that of training your model, and applying your model. If you think your model parameters do not change quickly, you can fit them with MapReduce, and then have a mechanism for accessing these pre-fit parameters when you want to apply your model.

**Storm**: A real-time, streaming computational system. Storm is online framework, meaning, in this sense, a service that interacts with a running application. In contrast to MapReduce, it receives small pieces of data (not a whole file) as they are processed in your application. You define a DAG of operations to perform on the data. A common and simple use case for Storm is tracking counters, and using that information to populate a real-time dashboard.

Storm doesn''t have anything (necessarily) to do with persisting your data. Here, streaming is another way to say keeping the information you care about and throwing the rest away. In reality, you probably have a persistence layer in your application that has already recorded the data, and so this a good and justified separation of concerns.


**If you want to know more...**
If you would like to learn more about real-time systems that that fit parameters with MR and apply the models a different way [here are slides for a talk I gave on building real-time recommendation engines on HBase.][1]

An excellent paper that marries real-time counting and persistence in an interesting way is [Google News Personalization: Scalable Online Collaborative Filtering][2]

Another interesting marriage of MR and Storm is SummingBird. Summingbird allows you to define data analysis operations that can be applied via Storm or MR.


  [1]: http://www.slideshare.net/cloudera/hbasecon-2013-24063525
  [2]: http://dl.acm.org/citation.cfm?id=1242610', 406, '2014-06-09 22:03:08.983', '03079e95-d516-46dc-85b7-3c9dd8abc03d', 242, 'deleted 5 characters in body', 642, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A variety of methods are available to the user.   The support documentation gives walkthroughs and tips for when one or another model is most appropriate.

[This page][1] shows the following learning methods:

- "AssociationModel"
- "ClusteringModel"
- "GeneralRegressionModel"
- "MiningModel"
- "NaiveBayesModel"
- "NeuralNetwork"
- "RegressionModel"
- "RuleSetModel"
- "SequenceModel"
- "SupportVectorMachineModel"
- "TextModel"
- "TimeSeriesModel"
- "TreeModel"

  [1]: https://developers.google.com/prediction/docs/pmml-schema', 432, '2014-06-10 01:36:27.520', '1361689a-63bc-4d24-9ba1-3bb27e256825', 243, 643, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Coming from a programmers perspective, frameworks rarely target performance as the highest priority.  If your library is going to be widely leveraged the things people are likely to value most are ease of use, flexibility, and reliability.

Performance is generally valued in secondary competitive libraries.  "X library is better because it''s faster."  Even then very frequently those libraries will trade off the most optimal solution for one that can be widely leveraged.

By using any framework you are inherently taking a risk that a faster solution exists.  I might go so far as to say that a faster solution almost always exists.

Writing something yourself is not a guarantee of performance, but if you know what you are doing and have a fairly limited set of requirements it can help.

An example might be JSON parsing.  There are a hundred libraries out there for a variety of languages that will turn JSON into a referable object and vice versa.  I know of one implementation that does it all in CPU registers.  It''s measurably faster than all other parsers, but it is also very limited and that limitation will vary based on what CPU you are working with.

Is the task of building a high-performant environment specific JSON parser a good idea?  I would leverage a respected library 99 times out of 100.  In that one separate instance a few extra CPU cycles multiplied by a million iterations would make the development time worth it.

', 434, '2014-06-10 01:40:23.263', '0032c28c-8054-43f8-abd1-2c77b4eaa42c', 244, 644, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A variety of methods are available to the user.   The support documentation gives walkthroughs and tips for when one or another model is most appropriate.

[This page][1] shows the following learning methods:

- "AssociationModel"
- "ClusteringModel"
- "GeneralRegressionModel"
- "MiningModel"
- "NaiveBayesModel"
- "NeuralNetwork"
- "RegressionModel"
- "RuleSetModel"
- "SequenceModel"
- "SupportVectorMachineModel"
- "TextModel"
- "TimeSeriesModel"
- "TreeModel"

EDIT: I don''t see any specific information about the algorithms, though.  For example, does the tree model use information gain or gini index for splits?

  [1]: https://developers.google.com/prediction/docs/pmml-schema
', 432, '2014-06-10 01:43:47.883', 'a50d0c26-43b6-4369-80e3-3d6696cc11e1', 243, 'added 160 characters in body', 645, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Computer Science is itself a multi-disciplinary field which has varying requirements among universities.  For example, Stockholm University does not require any math above algebra for its CS programs (some courses may have higher requirements, but not often).

I am not sure what you mean by a machine learning program being more rigorous.  They are just two different programs.  Data Science would likely take a broader view and focus on application and management (business courses are maybe on offer?).  The research could be rigorous in its own right, but it definitely won''t be tailored to someone who wants to optimize new algorithms or solve the low-level problems of machine learning.

I don''t see the Ph.D program listed yet in the link you provided.  Will you please follow up here if you get more specific information?', 432, '2014-06-10 01:54:40.647', 'afb53716-3c7d-4a8c-9f4f-463669179517', 245, 646, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There will definitely be a translation task at the end if you prototype using just mongo.

When you run a MapReduce task on mongodb, it has the data source and structure built in.  When you eventually convert to hadoop, your data structures might not look the same.  You could leverage the mongodb-hadoop connector to access mongo data directly from within hadoop, but that won''t be quite as straightforward as you might think.  The time to figure out how exactly to do the conversion most optimally will be easier to justify once you have a prototype in place, IMO.

While you will need to translate mapreduce functions, the basic pseudocode should apply well to both systems.  You won''t find anything that can be done in MongoDB that can''t be done using Java or that is significantly more complex to do with Java.', 434, '2014-06-10 02:42:02.050', '5a681aba-8089-45d6-8074-74cc90ab57e9', 246, 647, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Bokeh][1] is an excellent data visualization library for python.
[NodeBox][2] is another that comes to mind.



  [1]: http://bokeh.pydata.org/
  [2]: http://www.cityinabottle.org/nodebox/', 434, '2014-06-10 02:50:51.153', 'cc53064d-d5be-43ca-9a15-4d7816a6658e', 247, 648, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A cash cow program?  No.  PhD programs are never cash cows.

I don''t know why you couldn''t be a professor with a PhD in data science.  Rarely does a professor of a given course have to have a specific degree in order to teach it.

As far as publishing goes, there are any number of related journals that would accept papers from somebody on topics that would be covered by the topic of Data Science.

When I went to college, MIS, Computer Engineering, and Computer Science were new subjects.  Most of the people in my graduating class for Computer Science couldn''t program anything significant at graduation.  Within a few years, CS programs around the country matured significantly.

When you are part of a new program, sometimes it''s possible to help define what it is that''s required for graduation.  Being a part of that puts you in rare company for that field.

As far as mathematical rigor is concerned, I would expect Data Science to leverage a heavy dose of mathematically based material.  I wouldn''t expect anything particularly new - statistics, calculus, etc. should have been covered in undergrad.  Masters and PhD programs should be more about applying that knowledge and not so much about learning it.', 434, '2014-06-10 03:21:56.473', '79dda3e4-aa17-4e6b-b996-2c9c526acb37', 248, 649, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Twitter uses Storm for real-time processing of data.  Problems can happen with real-time data.  Systems might go down.  Data might be inadvertently processed twice.  Network connections can be lost.  A lot can happen in a real-time system.

They use hadoop to reliably process historical data.  I don''t know specifics, but for instance, getting solid information from aggregated logs is probably more reliable than attaching to the stream.

If they simply relied on Storm for everything - Storm would have problems due to the nature of providing real-time information at scale.  If they relied on hadoop for everything, there''s a good deal of latency involved.  Combining the two with Summingbird is the next logical step.', 434, '2014-06-10 03:42:51.637', '246fc0a8-fce9-4286-9c70-dd1e3d255d99', 249, 650, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Google does not publish the models they use, but they specifically do not support models from the PMML specification.

If you look closely at the documentation on [this page][1], you will notice that the model selection within the schema is greyed out indicating that it is an unsupported feature of the schema.

The [documentation does spell out][2] that by default it will use a regression model for training data that has numeric answers, and an unspecified categorization model for training data that results in text based answers.

The Google Prediction API also supports hosted models (although only a few demo models are currently available), and models specified with a PMML transform.  The documentation does contain an [example of a model defined by a PMML transform][3].  (There is also a note on that page stating that PMML ...Model elements are not supported).

The PMML standard that google partially supports is [version 4.0.1][4].


  [1]: https://developers.google.com/prediction/docs/pmml-schema
  [2]: https://developers.google.com/prediction/docs/developer-guide#whatisprediction
  [3]: https://developers.google.com/prediction/docs/pmml-schema
  [4]: http://www.dmg.org//pmml-v4-0-1.html', 434, '2014-06-10 04:47:13.040', 'aefd1807-10c5-47a0-91ed-20fca6655527', 250, 651, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Having done the rewriting game over and over myself (and still doing it), my immediate reaction was *adaptability*.

While frameworks and libraries have a huge arsenal of (possibly intertwinable) routines for standard tasks, their framework property often (always?) disallows shortcuts.  In fact, most frameworks have some sort of core infrastructure around which a core layer of basic functionality is implemented.  More specific functionality makes use of the basic layer and is placed in a second layer around the core.

Now by shortcuts I mean going straight from a second layer routine to another second layer routine without using the core.  Typical example (from my domain) would be timestamps: You have a timestamped data source of some kind.  Thus far the job is simply to read the data off the wire and pass it to the core so your other code can feast on it.

Now your industry changes the default timestamp format for a very good reason (in my case they went from unix time to GPS time).  Unless your framework is industry-specific it is very unlikely that they''re willing to change the core representation of time, so you end up using a framework that *almost* does what you want.  Every time you access your data you have to convert it to industry-time-format first, and every time you want it modified you have to convert it back to whatever the core deems appropriate.  There is no way that you can hand over data straight from the source to a sink without double conversion.

This is where your hand-crafted frameworks will shine, it''s just a minor change and you''re back modelling the real world whereas all other (non-industry-specific) frameworks will now have a performance disadvantage.

Over time, the discrepancy between the real world and the model will add up.  With an off-the-shelf framework you''d soon be facing questions like: How can I represent `this` in `that` or how do make routine `X` accept/produce `Y`.

So far this wasn''t about C/C++.  But if, for some reason, you can''t change the framework, i.e. you do have to put up with double conversion of data to go from one end to another, then you''d typically employ something that minimises the additional overhead.  In my case, a TAI->UTC or UTC->TAI converter is best left to raw C (or an FPGA).  There is no elegance possible, no profound smart data structure that makes the problem trivial.  It''s just a boring switch statement, and why not use a language whose compilers are good at optimising exactly that?
', 451, '2014-06-10 05:57:13.897', '460d7015-658d-4f93-8c0c-b71c86c6be65', 251, 652, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There seem to be at least 2 ways to connect to HBase from external application, with language other then Java (i.e. Python):

 1. HBase Thrift API
 2. HBase Stargate (REST API)

Does anyone know which one should be used in which circumstances?
I.e. what are their main differences, and pros/cons?', 88, '2014-06-10 06:19:46.510', '643061a8-f39a-483a-ae8d-ec9e8a616fff', 252, 653, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('HBase connector - Thrift or REST', 88, '2014-06-10 06:19:46.510', '643061a8-f39a-483a-ae8d-ec9e8a616fff', 252, 654, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop>', 88, '2014-06-10 06:19:46.510', '643061a8-f39a-483a-ae8d-ec9e8a616fff', 252, 655, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('An aspiring data scientist here. I don''t know anything about Hadoop, but as I have been reading about Data Science and Big Data, I see a lot of talk about Hadoop. Is it absolutely necessary to learn Hadoop to be a Data Scientist? ', 456, '2014-06-10 06:20:20.817', '2bf8b6b8-0228-46fc-8d17-8438827d209d', 253, 656, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Do I need to learn Hadoop to be a Data Scientist?', 456, '2014-06-10 06:20:20.817', '2bf8b6b8-0228-46fc-8d17-8438827d209d', 253, 657, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop>', 456, '2014-06-10 06:20:20.817', '2bf8b6b8-0228-46fc-8d17-8438827d209d', 253, 658, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Actually there are more than one question to answer here:

 1. How to work on schemaless/loose/missing data
 2. How to label a person (from what I understand unsupervised) and create an identifier
 3. How to train your system so that it can tell you which attributes you should use in order to identify the person

As Rubens mentioned, you can use **decision tree** methods, specifically <a href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">Random Forests</a> for calculating the most important attributes based on information gain if you have already found a way to identify how to label a person.

However, if you do not have any label information maybe you can use some expert view for preliminary attribute selection. After that you make **unsupervised classification** in order to retrieve your labels. Lastly, you can select the most important fields using **Random Forest** or other methods like **Bayesian Belief Networks**.

In order to achieve all that, you also need complete data set. If your data set is loose you have to manually or heuristically find a way to couple attributes indicating same thing with different names. What is more, you can use *imputation* techniques such as <a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation Maximization</a> method and complete your data set. Or you can also work with Bayesian Networks and can leave missing fields as they are.', 463, '2014-06-10 07:10:45.057', '92b4a6a2-d8fc-4856-976d-c6f37039051f', 254, 659, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You should learn Hadoop if you want to be work as data scientist, but maybe before starting with Hadoop you should read something about ETL or Big Data... this book could be a good starting point: http://www.amazon.com/Big-Data-Principles-practices-scalable/dp/1617290343

Hope it helps and good luck!', 466, '2014-06-10 07:28:49.127', '0ab22aaa-bcd9-4d73-bc74-286d285dee92', 255, 660, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Different people use different tools for different things.  Terms like Data Science are generic for a reason.  A data scientist could spend an entire career without having to learn a particular tool like hadoop.  Hadoop is widely used, but it is not the only platform that is capable of managing and manipulating data, even large scale data.

I would say that a data scientist should be familiar with concepts like MapReduce, distributed systems, distributed file systems, and the like, but I wouldn''t judge someone for not knowing about such things.

It''s a big field.  There is a sea of knowledge and most people are capable of learning and being an expert in a single drop.  The key to being a scientist is having the desire to learn and the motivation to know that which you don''t already know.

', 434, '2014-06-10 07:30:53.510', '02941d4a-b366-4c3e-b82c-36445aae27b2', 256, 661, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes, you should learn a platform that is capable of dissecting your problem as a data parallel problem. Hadoop is one. For your simple needs (design patterns like counting, aggregation, filtering etc.) you need Hadoop and for more complex Machine Learning stuff like doing some Bayesian, SVM you need Mahout which in turn needs Hadoop (Now Apache Spark) to solve your problem using a data-parallel approach.

So Hadoop is a good platform to learn and really important for your batch processing needs. Not only Hadoop but you also need to know Spark (Mahout runs it''s algorithms utilizing Spark) & Twitter Storm (for your real time analytics needs). This list will continue and evolve so if you are good with the building blocks (Distributed Computing, Data-Parallel Problems and so on) and know how one such platform (say Hadoop) operates you will fairly quickly be up to speed on others.

', 11, '2014-06-10 07:42:57.470', '47a6a8bf-1c66-4261-b8a3-bab29aa6808c', 257, 662, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-mining><clustering><octave><k-means><categorical-data>', 97, '2014-06-10 07:53:48.253', '3073a77e-c4c3-4037-b200-abbcc996fa19', 22, 'Added 1 more relevant tag', 664, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Thrift is generally faster because the data being exchanged is smaller.  Stargate offers a web service which is an integration method that is widely supported, which is a concern when you are working with commercial products with limited integration possibilities.

In a closed environment where everything is controlled, I would prefer Thrift.  If I''m exposing data to external teams or systems I would prefer Stargate.', 434, '2014-06-10 07:56:45.587', 'dd65905f-a7fe-4907-87a7-88424248449b', 258, 665, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to point to [The Open Data Census][1]. It is an initiative of the Open Knowledge Foundation based on contributions from open data advocates and experts around the world.

The value of Open data Census is open, community driven, and systematic effort to collect and update the database of open datasets globally on country and, in some cases, [like U.S., on city level][2].

Also, it presents an opportunity to compare different countries and cities on in selected areas of interest.


  [1]: http://national.census.okfn.org/
  [2]: http://us-city.census.okfn.org/', 454, '2014-06-10 08:04:20.400', '2aacbbed-77d7-4625-841a-9c4743c8ec95', 259, 666, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Different people use different tools for different things.  Terms like Data Science are generic for a reason.  A data scientist could spend an entire career without having to learn a particular tool like hadoop.  Hadoop is widely used, but it is not the only platform that is capable of managing and manipulating data, even large scale data.

I would say that a data scientist should be familiar with concepts like MapReduce, distributed systems, distributed file systems, and the like, but I wouldn''t judge someone for not knowing about such things.

It''s a big field.  There is a sea of knowledge and most people are capable of learning and being an expert in a single drop.  The key to being a scientist is having the desire to learn and the motivation to know that which you don''t already know.

As an example:  I could hand the right person a hundred structured CSV files containing information about classroom performance in one particular class over a decade.  A data scientist would be able to spend a year gleaning insights from the data without ever needing to spread computation across multiple machines.  You could apply machine learning algorithms, analyze it using visualizations, combine it with external data about the region, ethnic makeup, changes to environment over time, political information, weather patterns, etc.  All of that would be "data science" in my opinion.  It might take something like hadoop to test and apply anything you learned to data comprising an entire country of students rather than just a classroom, but that final step doesn''t necessarily make someone a data scientist.  And not taking that final step doesn''t necessarily disqualify someone from being a data scientist.

', 434, '2014-06-10 08:21:19.197', '8f4d5aee-bd08-4e7e-b0f7-d29ec2c3ea6b', 256, 'added an example', 667, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The algorithm that is used in this case is called [one-vs-all classifier][1] or multiclass classifier.

In your case you have to take one class, e. g. number 1 , mark it as positive and combine the rest seven classes in one negative class. The neural network will output the probability of this case being class number 1 vs the rest of the classes.

Afterwords, you have to assign as positive another class, e.g. number 2, assign all other classes as one big negative class and get the predicted probability from the network again.

After repeating this procedure for all eight classes, assign each case to the the class that had the maximum probability from all the classes outputted from the neural network.


  [1]: https://class.coursera.org/ml-003/lecture/38', 454, '2014-06-10 08:38:27.093', 'ddb2f077-0758-4861-a209-d05e3caacafe', 260, 668, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You could take a look at CN2 rule learner in Orange http://orange.biolab.si/docs/latest/widgets/rst/classify/cn2/
', 480, '2014-06-10 09:21:14.013', '36e24cab-25ec-48c0-8e50-b5ccbb8f438c', 261, 669, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the main benefits from storing data in HDF? And what are the main data science tasks where HDF is really suitable and useful?', 97, '2014-06-10 09:26:06.593', 'e14f5001-4e2e-4c4a-a094-140e20f364f1', 262, 670, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Benefits from using Hierarchical Data Format', 97, '2014-06-10 09:26:06.593', 'e14f5001-4e2e-4c4a-a094-140e20f364f1', 262, 671, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-formats><hierarchical-data-format>', 97, '2014-06-10 09:26:06.593', 'e14f5001-4e2e-4c4a-a094-140e20f364f1', 262, 672, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Unfortunately, parallelization is not yet implemented in pandas. You can join [this github issue](http://github.com/pydata/pandas/issues/5751) if you want to participate in the development of this feature.

I don''t know any "magic unicorn package" for this purposes, so the best thing will be write your own solution. But if you still don''t want to spend time on that and want to learn something new  you can try the two methods built into MongoDB (map reduce and agg framework).  See [mongodb_agg_framework](http://docs.mongodb.org/manual/core/aggregation/).', 478, '2014-06-10 09:41:34.697', 'd66d4a07-b297-4d1b-ad3e-746d2e52d2ba', 263, 673, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can also give the Expectation Maximization clustering algorithm a try.  It can work on categorical data and will give you a statistical likelihood of which categorical value (or values) a cluster is most likely to take on.', 490, '2014-06-10 10:48:58.457', '9bfc6315-6165-4905-bfbf-7952e652c277', 264, 674, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a variety of NFL datasets that I think might make a good side-project, but I haven''t done anything with them just yet.

Coming to this site made me think of machine learning algorithms and I wondering how good they might be at either predicting the outcome of football games or even the next play.

It seems to me that there would be some trends that could be identified - on 3rd down and 1, a team with a strong running back *theoretically should* have a tendency to run the ball in that situation.

Scoring might be more difficult to predict, but the winning team might be.

My question is whether these are good questions to throw at a machine learning algorithm.  It could be that a thousand people have tried it before, but the nature of sports makes it an unreliable topic.', 434, '2014-06-10 10:58:58.447', '9f2bb3a5-a3ef-4757-904a-9faa34955818', 265, 675, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can machine learning algorithms predict sports scores or plays?', 434, '2014-06-10 10:58:58.447', '9f2bb3a5-a3ef-4757-904a-9faa34955818', 265, 676, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 434, '2014-06-10 10:58:58.447', '9f2bb3a5-a3ef-4757-904a-9faa34955818', 265, 677, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Being new to machine-learning in general, I''d like to start playing around and see what the possibilities are.

I''m curious as to what applications you might recommend that would offer the fastest time from installation to producing a meaningful result.

Also, any recommendations for good getting-started materials on the subject of machine-learning in general would be appreciated.', 434, '2014-06-10 11:05:47.273', 'ae26a95e-89c6-404b-92b8-360c3f28a5ef', 266, 678, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are some easy to learn machine-learning applications?', 434, '2014-06-10 11:05:47.273', 'ae26a95e-89c6-404b-92b8-360c3f28a5ef', 266, 679, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 434, '2014-06-10 11:05:47.273', 'ae26a95e-89c6-404b-92b8-360c3f28a5ef', 266, 680, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think [Weka][1] is a good starting point. You can do a bunch of stuff like supervised learning or clustering and easily compare a large set of algorithms na methodologies.

Weka''s manual is actually a book on machine learning and data mining that can be used as introductory material.

  [1]: http://www.cs.waikato.ac.nz/ml/weka/', 418, '2014-06-10 11:36:19.287', '554c6981-f1d6-4653-8df8-9d3e023240db', 268, 682, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Definitely they can.
I can target you to a **[nice paper][1]**. Once I used it for soccer league results prediction algorithm implementation, primarily aiming at having some value against bookmakers.

From paper''s abstract:
> a Bayesian dynamic generalized model to estimate the time dependent skills of all teams in a league, and to predict next weekend''s soccer matches.

Keywords:

> Dynamic Models, Generalized Linear Models, Graphical Models, Markov
> Chain Monte Carlo Methods, Prediction of Soccer Matches

  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.7448&rep=rep1&type=pdf', 97, '2014-06-10 11:37:28.293', 'b5d68d14-89a3-4759-a24c-760171f93746', 269, 683, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine learning and statistical techniques can improve the forecast, but nobody can predict the real result.

There was a kaggle competition a few month ago about [predicting the 2014 NCAA Tournament][1]. You can read the Competition Forum to get a better idea on what people did and what results did they achieve.


  [1]: https://www.kaggle.com/c/march-machine-learning-mania', 478, '2014-06-10 11:39:19.603', '08cac896-e8e9-41d8-b03f-241cb0623f8b', 270, 684, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It has been shown before that machine learning techniques can be applied for predicting sport results. Simple google search should give you a bunch of results.

However, it has also been showed (for NFL btw) that very complex predictive models, simple predictive models, questioning people, or crowd knowledge by utilising betting info, they all perform more or less the same. Source: "[Everything is obvious once you know the answer - How common sense Fails][1]", Chapter 7, by Duncan Watts.


  [1]: http://everythingisobvious.com/', 418, '2014-06-10 11:49:23.777', '39efabbe-79ea-4048-abb6-0af48316bcd8', 271, 685, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would recommend to start with some MOOC on machine learning. For example Andrew Ng''s [course][1] at coursera.

You should also take a look at [Orange][2] application. It has a graphical interface and probably it is easier to understand some ML techniques using it.


  [1]: https://www.coursera.org/course/ml
  [2]: http://orange.biolab.si/', 478, '2014-06-10 11:53:07.737', '38346f6e-e3c5-437c-970a-ba743ca90609', 272, 686, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can apply data science techniques to data on one machine so the answer to the question - is it absolutely necessary to learn Hadoop to be a data scientist is no.', 498, '2014-06-10 12:10:28.713', 'd3f7ccd7-4a9a-44b9-80bd-efc401b14fa5', 273, 687, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One benefit is wide support - C, Java, Perl, Python, and R all have HDF5 bindings.

Another benefit is speed.  I haven''t ever seen it benchmarked, but HDF is supposed to be faster than SQL databases.

I understand that it is very good when used with both large sets of scientific data and time series data - network monitoring, usage tracking, etc.

I don''t believe there is a size limitation for HDF files (although OS limits would still apply.

', 434, '2014-06-10 12:57:04.307', 'f2c4f682-6daa-4bfd-8887-c8a17f76138f', 274, 688, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For time series data in particular, [Quandl](http://www.quandl.com/) is an excellent resource -- an easily browsable directory of (mostly) clean time series.

One of their coolest features is [open-data stock prices](http://blog.quandl.com/blog/quandl-open-data/) -- i.e. financial data that can be edited wiki-style, and isn''t encumbered by licensing.', 508, '2014-06-10 13:17:48.433', '8572d413-8e41-494b-8f72-6a7fe86c01ae', 275, 689, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Not all government data is listed on data.gov - [Sunlight Foundation][1] put together a [set of spreadsheets][2] back in February describing sets of available data.


  [1]: http://sunlightfoundation.com/blog/2014/02/21/open-data-inventories-ready-for-human-consumption/
  [2]: https://drive.google.com/folderview?id=0B4QuErjcV2a0WXVDOURwbzh6S2s&usp=sharing', 434, '2014-06-10 13:38:31.207', 'd2daea04-a0e1-498b-8796-478c42bf2a59', 276, 691, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To be honest, I think that doing some projects will teach you much more than doing a full course. One reason is that doing a project is more motivating and open-ended than doing assignments.

A course, if you have the time AND motivation (real motivation), is better than doing a project. The other commentators have made good platform recommendations on tech.

I think, from a fun project standpoint, you should ask a question and get a computer to learn to answer it.

Some good classic questions that have good examples are:

 - Neural Networks for recognizing hand written digits
 - Spam email classification using logistic regression
 - Classification of objects using Gaussian Mixture models
 - Some use of linear regression, perhaps forecasting of grocery prices given neighborhoods

These projects have the math done, code done, and can be found with Google readily.

Other cool subjects can be done by you!

Lastly, I research robotics, so for me the most FUN applications are behavioral.
Examples can include (if you can play with an arduino)

Create a application, that uses logistic regression perhaps, that learns when to turn the fan off and on given the inner temperature, and the status of the light in the room.

Create an application that teaches a robot to move an actuator, perhaps a wheel, based on sensor input (perhaps a button press), using Gaussian Mixture Models (learning from demonstration).

Anyway, those are pretty advanced. The point I''m making is that if you pick a project that you (really really) like, and spend a few week on it, you will learn a massive amount, and understand so much more than you will get doing a few assignments.', 518, '2014-06-10 14:25:41.903', '92a9bbc5-a008-4264-9688-54aad5737932', 277, 692, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Assuming you''re familiar with programming I would recommend looking at [scikit-learn][1]. It has especially nice help pages that can serve as mini-tutorials/a quick tour through machine learning. Pick an area you find interesting and work through the examples.


  [1]: http://scikit-learn.org/stable/', 524, '2014-06-10 14:30:33.667', '665f8d63-d86e-434b-b846-017d28c58cc1', 278, 693, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is also another resource provided by The Guardian, the British Daily on their website. The datasets published by the Guardian Datablog are all hosted. Datasets related to Football Premier League Clubs'' accounts, Inflation and GDP details of UK, Grammy awards data etc.
The datasets are available at

 - http://www.theguardian.com/news/datablog/interactive/2013/jan/14/all-our-datasets-index', 514, '2014-06-10 14:57:47.810', '2f94890c-ef51-4c44-94fd-aebc193fd8da', 279, 694, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You can apply data science techniques to data on one machine so the answer to the question as the OP phrased it, is no.', 498, '2014-06-10 15:04:01.177', '09123e26-7fe2-4fa6-8ee6-b5caf0a9d2af', 273, 'precised grammar', 695, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am developing a system that is intended to capture the "context" of user activity within an application; it is a framework that web applications can use to tag user activity based on requests made to the system.  It is hoped that this data can then power ML features such as context aware information retrieval.

I''m having trouble deciding on what features to select in addition to these user tags - the URL being requested, approximate time spent with any given resource, estimating the current "activity" within the system.

I am interested to know if there are good examples of this kind of technology or any prior research on the subject - a cursory search of the ACM DL revealed some related papers but nothing really spot-on.', 531, '2014-06-10 15:08:54.073', '44af33ad-263a-4cdd-be41-6fb9a85efa83', 280, 696, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Feature selection for tracking user activity within an application', 531, '2014-06-10 15:08:54.073', '44af33ad-263a-4cdd-be41-6fb9a85efa83', 280, 697, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<feature-selection>', 531, '2014-06-10 15:08:54.073', '44af33ad-263a-4cdd-be41-6fb9a85efa83', 280, 698, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes. Why not?!
With so much of data being recorded in each sport in each game, smart use of data could lead us in obtaining important insights regarding player performance.

Some examples:

 - **Baseball**: In the movie Moneyball (which is an adaptation of the MoneyBall book), Brad Pitt plays a character who analyses player statistics to come up with a team that performs tremendously well! It was a depiction of the real life story of Oakland Athletics baseball team. For more info, http://www.theatlantic.com/entertainment/archive/2013/09/forget-2002-this-years-oakland-as-are-the-real-em-moneyball-em-team/279927/
 - **Cricket**: SAP Labs has come up with an auction analytics tool that has given insights about impact players to buy in the 2014 Indian Premier League auction for the Kolkata Knight Riders team, which eventually went on to win the 2014 IPL **Championship**. For more info, http://scn.sap.com/community/hana-in-memory/blog/2014/06/10/sap-hana-academy-cricket-demo--how-sap-hana-powered-the-kolkata-knight-riders-to-ipl-championship

So, yes, statistical analysis of the player records can give us insights about **which players are more likely to perform but not which players will perform**. So, machine learning, a close cousin of statistical analysis will be proving to be a game changer.', 514, '2014-06-10 16:25:24.223', 'f343fd8f-17fb-4799-9b28-ad10edbc7f7a', 282, 705, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Well, this may not answer the question thoroughly, but since you''re dealing with information retrieval, it may be of some use. [This page](http://moz.com/search-ranking-factors) mantains a set of features and associated *correlations* with page-ranking methods of search engines. As a disclaimer from the webpage itself:

> Note that these factors are not "proof" of what search engines use to rank websites, but simply show the characteristics of web pages that tend to rank higher.

The list pointed may give you some insights on which features would be nice to select. For example, considering the second most correlated feature, # of google +1''s, it may be possible to add some probability of a user making use of such service if he/she accesses many pages with high # of google +1 (infer "user context"). Thus, you could try to "guess" some other relations that may shed light on interesting features for your tracking app.', 84, '2014-06-10 17:06:54.950', 'e498aa53-308c-425d-9354-f0c77833a8f2', 284, 708, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are a lot of good questions about Football (and sports, in general) that would be awesome to throw to an algorithm and see what comes out. The tricky part is to know *what* to throw to the algorithm.

A team with a good RB could just pass on 3rd-and-short just because the opponents would probably expect run, for instance. So, in order to actually produce some worthy results, I''d break the problem in smaller pieces and analyse them statistically while throwing them to the machines!

There are a few (good) websites that try to do the same, you should check''em out and use whatever they found to help you out:

* [Football Outsiders](http://www.footballoutsiders.com/)
* [Advanced Football Analytics](http://www.advancedfootballanalytics.com/)

And if you truly want to explore Sports Data Analysis, you should definitely check the [Sloan Sports Conference](http://www.sloansportsconference.com/) videos. There''s a lot of them spread on Youtube.
', 553, '2014-06-10 17:15:52.953', '7780b9ba-7668-41a6-817c-d110a704ea87', 285, 709, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It depends on your employer. Many stipulate that you know it, especially if the job involves "big data", while others will let you learn on the job or not care. Take a look at the job boards and see for yourself!', 381, '2014-06-10 17:27:31.080', '995076c7-c53c-4812-bce6-63f2a5404526', 286, 710, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I do not know a standard answer to this, but I thought about it some times ago and I have some ideas to share.

When you have one confusion matrix, you have more or less a picture of how you classification model confuse (mis-classify) classes. When you repeat classification tests you will end up having multiple confusion matrices. The question is how to get a meaningful aggregate confusion matrix. The answer depends on what is the meaning of meaningful (pun intended). I think there is not a single version of meaningful.

One way is to follow the rough idea of multiple testing. In general, you test something multiple times in order to get more accurate results. As a general principle one can reason that averaging on the results of the multiple tests reduces the variance of the estimates, so as a consequence, it increases the precision of the estimates. You can proceed in this way, of course, by summing position by position and then dividing by the number of tests. You can go further and instead of estimating only a value for each cell of the confusion matrix, you can also compute some confidence intervals, t-values and so on. This is OK from my point of view. But it tell only one side of the story.

The other side of the story which might be investigated is how stable are the results for the same instances. To exemplify that I will take an extreme example. Suppose you have a classification model for 3 classes. Suppose that these classes are in the same proportion. If your model is able to predict one class perfectly and the other 2 classes with random like performance, you will end up having 0.33 + 0.166 + 0.166 = 0.66 misclassification ratio. This might seem good, but even if you take a look on a single confusion matrix you will not know that your performance on the last 2 classes varies wildly. Multiple tests can help. But averaging the confusion matrices would reveal this? My belief is not. The averaging will give the same result more or less, and doing multiple tests will only decrease the variance of the estimation. However it says nothing about the wild instability of prediction.

So another way to do compose the confusion matrices would better involve a prediction density for each instance. One can build this density by counting for each instance, the number of times it was predicted a given class. After normalization, you will have for each instance a prediction density rather a single prediction label. You can see that a single prediction label is similar with a degenerated density where you have probability of 1 for the predicted class and 0 for the other classes for each separate instance. Now having this densities one can build a confusion matrix by adding the probabilities from each instance and predicted class to the corresponding cell of the aggregated confusion matrix.

One can argue that this would give similar results like the previous method. However I think that this might be the case, the second method is less affected by how the samples from the tests are drawn, and thus more stable and closer to the reality.

Also the second method might be altered in order to obtain a third method, where one can assign as prediction the label with highest density from the prediction of a given instance.

I do not implemented those things but I plan to study further because I believe might worth spending some time. ', 108, '2014-06-10 17:32:19.120', '6b26c8be-1a42-452d-af5e-f5c21c0b56ca', 287, 711, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The goal determines the features, so I would initially take as many as possible, then use cross validation to select the optimal subset.

My educated guess is that a Markov model would work. If you discretize the action space (e.g., select this menu item, press that button, etc.), you can predict the next action based on the past ones. It''s a _sequence_ or [structured prediction](http://en.wikipedia.org/wiki/Structured_prediction) problem.', 381, '2014-06-10 17:36:11.580', 'e040466e-33cd-4913-93cd-fdcbbb8e5c6d', 288, 712, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yann LeCun mentioned in his [AMA](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/) that he considers having a PhD very important in order to get a job at a top company.

I have a masters in statistics and my undergrad was in economics(math intensive though), but I am now looking into ML PhD programs. Most programs say there are no absolutely necessary CS courses; however I tend to think most accepted students have at least a very strong CS background. I am currently working as a data scientist/statistician but my company will pay for courses. Should I take some intro software engineering courses at my local University to make myself a stronger candidate? What other advice you have for someone applying to PhD programs from outside the CS field?  ', 560, '2014-06-10 17:56:34.847', 'bb8f16f3-de1d-419c-bfc6-9e4f361f10a8', 289, 713, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Qualifications for PhD Programs', 560, '2014-06-10 17:56:34.847', 'bb8f16f3-de1d-419c-bfc6-9e4f361f10a8', 289, 714, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<education>', 560, '2014-06-10 17:56:34.847', 'bb8f16f3-de1d-419c-bfc6-9e4f361f10a8', 289, 715, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I found the pluralsight course [Introduction to machine learning encog][1] a great resource so start with. It uses the [Encog library][2] to quickly explore different ml techniques.


  [1]: http://pluralsight.com/training/courses/TableOfContents?courseName=introduction-to-machine-learning-encog&highlight=abhishek-kumar_introduction-to-machine-learning-encog-m2-applications!abhishek-kumar_introduction-to-machine-learning-encog-m3-tasks!abhishek-kumar_introduction-to-machine-learning-encog-m1-intro*1#introduction-to-machine-learning-encog-m2-applications
  [2]: http://www.heatonresearch.com/encog', 571, '2014-06-10 18:22:32.610', 'b5c99728-676c-4c64-9a1b-83d073406798', 290, 716, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I were you I would take a MOOC or two (e.g., [Algorithms, Part I](https://www.coursera.org/course/algs4partI), [Algorithms, Part II](https://www.coursera.org/course/algs4partII), [Functional Programming Principles in Scala](https://www.coursera.org/course/progfun)), a good book on data structures and algorithms, then just code as much as possible. You could implement some statistics or ML algorithms, for example; that would be good practice for you and useful to the community.

For a PhD program, however, I would also make sure I were familiar with the type of maths they use. If you want to see what it''s like at the deep end, browse the papers at the [JMLR](http://jmlr.org/papers/). That will let you calibrate yourself in regards to theory; can you sort of follow the maths?

Oh, and you don''t need a PhD to work at top companies, unless you want to join research departments like his. But then you''ll spend more time doing development, and you''ll need good coding skills...', 381, '2014-06-10 18:55:39.010', '850631b4-f385-476e-8df3-d9e3a178d25e', 291, 717, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Yann LeCun mentioned in his [AMA](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/) that he considers having a PhD very important in order to get a job at a top company.

I have a masters in statistics and my undergrad was in economics(math intensive though), but I am now looking into ML PhD programs. Most programs say there are no absolutely necessary CS courses; however I tend to think most accepted students have at least a very strong CS background. I am currently working as a data scientist/statistician but my company will pay for courses. Should I take some intro software engineering courses at my local University to make myself a stronger candidate? What other advice you have for someone applying to PhD programs from outside the CS field?

edit: I have taken a few MOOCs (Machine Learning, Recommender Systems, NLP) and code R/python on a daily basis. I have a lot of coding experience with statistical languages and implement ML algorithms daily. I am more concerned with things that I can put on applications.', 560, '2014-06-10 19:02:52.720', 'c89593a0-9d56-4a48-aae4-7e2bfe8aa873', 289, 'added 275 characters in body', 718, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Yann LeCun mentioned in his [AMA](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/) that he considers having a PhD very important in order to get a job at a top company.

I have a masters in statistics and my undergrad was in economics(math intensive), but I am now looking into ML PhD programs. Most programs say there are no absolutely necessary CS courses; however I tend to think most accepted students have at least a very strong CS background. I am currently working as a data scientist/statistician but my company will pay for courses. Should I take some intro software engineering courses at my local University to make myself a stronger candidate? What other advice you have for someone applying to PhD programs from outside the CS field?

edit: I have taken a few MOOCs (Machine Learning, Recommender Systems, NLP) and code R/python on a daily basis. I have a lot of coding experience with statistical languages and implement ML algorithms daily. I am more concerned with things that I can put on applications.', 560, '2014-06-10 19:25:37.803', '21105974-4460-491e-8b66-c6e5acf12fed', 289, 'deleted 7 characters in body', 719, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Your time would probably be better spent on Kaggle than in a PhD program. When you read the stories by winners ([Kaggle blog][1]) you''ll see that it takes a large amount of practice and the winners are not just experts of one single method.

On the other hand, being active and having a plan in a PhD program can get you connections that you otherwise would probably not get.

I guess the real question is for you - what are the reasons for wanting a job at a top company?


  [1]: http://blog.kaggle.com/', 587, '2014-06-10 19:43:11.860', '4e414ea2-a09e-4091-9ac5-923dc94bc0b6', 292, 720, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Perhaps a good way to paraphrase the question is, what are the advantages compared to alternative formats?

The main alternatives are, I think: a database, text files, or another packed/binary format.

The database options to consider are probably a columnar store or NoSQL, or for small self-contained datasets SQLite.  The main advantage of the database is the ability to work with data much larger than memory, to have random or indexed access, and to add/append/modify data quickly.  The main *dis*advantage is that it is much slower than HDF, for problems in which the entire dataset needs to be read in and processed.  Another disadvantage is that, with the exception of embedded-style databases like SQLite, a database is a system (requiring admnistration, setup, maintenance, etc) rather than a simple self-contained data store.

The text file format options are XML/JSON/CSV.  They are cross-platform/language/toolkit, and are a good archival format due to the ability to be self-describing (or obvious :).  If uncompressed, they are huge (10x-100x HDF), but if compressed, they can be fairly space-efficient (compressed XML is about the same as HDF).  The main disadvantage here is again speed: parsing text is much, much slower than HDF.

The other binary formats (npy/npz numpy files, blz blaze files, protocol buffers, Avro, ...) have very similar properties to HDF, except they are less widely supported (may be limited to just one platform: numpy) and may have specific other limitations.  They typically do not offer a compelling advantage.

HDF is a good complement to databases, it may make sense to run a query to produce a roughly memory-sized dataset and then cache it in HDF if the same data would be used more than once.  If you have a dataset which is fixed, and usually processed as a whole, storing it as a collection of appropriately sized HDF files is not a bad option.  If you have a dataset which is updated often, staging some of it as HDF files periodically might still be helpful.

To summarize, HDF is a good format for data which is read (or written) typically as a whole; it is the lingua franca or common/preferred interchange format for many applications due to wide support and compatibility, decent as an archival format, and very fast.

P.S. To give this some practical context, my most recent experience comparing HDF to alternatives, a certain small (much less than memory-sized) dataset took 2 seconds to read as HDF (and most of this is probably overhead from Pandas); ~1 minute to read from JSON; and 1 *hour* to write to database.  Certainly the database write could be sped up, but you''d better have a good DBA!  This is how it works out of the box.', 26, '2014-06-10 20:28:54.613', '309378bd-a8b6-48d9-8007-719b38618893', 293, 725, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As a former Hadoop engineer, it is not needed but it helps. Hadoop is just one system - the most common system, based on Java, and a ecosystem of products, which apply a particular technique "Map/Reduce" to obtain results in a timely manner. Hadoop is not used at Google, though I assure you they use big data analytics. Google uses their own systems, developed in C++. In fact, Hadoop was created as a result of Google publishing their Map/Reduce and BigTable (HBase in Hadoop) white papers.

Data scientists will interface with hadoop engineers, though at smaller places you may be required to wear both hats. If you are strictly a data scientist, then whatever you use for your analytics, R, Excel, Tableau, etc, will operate only on a small subset, then will need to be converted to run against the full data set involving hadoop. ', 602, '2014-06-10 20:40:25.623', '51b0d4da-63b6-4b04-928c-815967371671', 294, 726, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am glad you also found Yann LeCun''s AMA page, it''s very useful.

Here are my opinions
Q: Should I take some intro software engineering courses at my local University to make myself a stronger candidate?
A: No, you need to take more math courses.  It''s not the applied stuff that''s hard, it''s the theory stuff.  I don''t know what your school offers.  Take theoretical math courses, along with some computer science courses.

Q:What other advice you have for someone applying to PhD programs from outside the CS field?
A:  How closely related are you looking for.  Without a specific question, it''s hard to give a specific answer.
', 386, '2014-06-10 20:43:28.533', '90d3f93e-a274-4e8a-9018-ca165c09fb5d', 295, 727, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('HDP is an extension of LDA, designed to address the case where the number of mixture components (the number of "topics" in document-modeling terms) is not known a priori.  So that''s the reason why there''s a difference.

Using LDA for document modeling, one treats each "topic" as a distribution of words in some known vocabulary.  For each document a mixture of topics is drawn from a Dirichlet distribution, and then each word in the document is an independent draw from that mixture (that is, selecting a topic and then using it to generate a word).

For HDP (applied to document modeling), one also uses a Dirichlet process to capture the uncertainty in the number of topics.  So a common base distribution is selected which represents the countably-infinite set of possible topics for the corpus, and then the finite distribution of topics for each document is sampled from this base distribution.

As far as pros and cons, HDP has the advantage that the maximum number of topics can be unbounded and learned from the data rather than specified in advance.  I suppose though it is more complicated to implement, and unnecessary in the case where a bounded number of topics is acceptable.', 14, '2014-06-10 21:50:51.347', '0647bbf6-5143-4ef8-b46d-cce29b8f4712', 296, 728, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As in @damienfrancois answer feature selection is about selecting a subset of features. So in NLP it would be selecting a set of specific words (the typical in NLP is that each word represents a feature with value equal to the frequency of the word or some other weight based on TF/IDF or similar).

Dimensionality reduction is the introduction of new feature space where the original features are represented. The new space is of lower dimension that the original space. In case of text an example would be the [hashing trick][1] where a piece of text is reduced to a vector of few bits (say 16 or 32) or bytes. The amazing thing is that the geometry of the space in reserved (given enough bits), so relative distances between documents remain the same as in the original space, so you can deploy standard machine learning techniques without having to deal with unbound (and huge number of) dimensions found in text.


  [1]: http://en.wikipedia.org/wiki/Feature_hashing', 418, '2014-06-10 22:26:53.623', 'bc66cabf-290c-4c7a-b4cd-70bf460b95aa', 297, 729, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You already have a Masters in Statistics, which is great! In general, I''d suggest to people to take as much statistics as they can, especially Bayesian Data Analysis.

Depending on what you want to do with your PhD, you would benefit from foundational courses in the discipline(s) in your application area.  You already have Economics but if you want to do Data Science on social behavior, then courses in Sociology would be valuable.  If you want to work in fraud prevention, then a courses in banking and financial transactions would be good.  If you want to work in information security, then taking a few security courses would be good.

There are people who argue that it''s not valuable for Data Scientists to spend time on courses in sociology or other disciplines.  But consider the recent case of the Google Flu Trends project.  In [this article][1] their methods were strongly criticized for making avoidable mistakes.  The critics call it "Big Data hubris".

There''s another reason for building strength in social science disciplines: personal competitive advantage.  With the rush of academic degree programs, certificate programs, and MOOCs, there is a mad rush of students into the Data Science field.  Most will come out with capabilities for core Machine Learning methods and tools.  PhD graduates will have more depth and more theoretical knowledge, but they are all competing for the same sorts of jobs, delivering the same sorts of value.  With this flood of graduates, I expect that they won''t be able to command premium salaries.

But if you can differentiate yourself with a combination of formal education and practical experience in a particular domain and application area, then you should be able to set yourself apart from the crowd.

(Context: I''m in a PhD program in Computational Social Science, which has a heavy focus on modeling, evolutionary computation, and social science disciplines, and less emphasis on ML and other empirical data analysis topics).


  [1]: http://www.uvm.edu/~cdanfort/csc-reading-group/lazer-flu-science-2014.pdf', 609, '2014-06-10 22:29:52.873', '49498d35-0e47-44c2-9677-acd16836cd81', 298, 730, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Topological Data Analysis is a method explicitly designed for the setting you describe. Rather than a global distance metric, it relies only on a local metric of proximity or neighborhood. See: [Topology and data][1] and [Extracting insights from the shape of complex data using topology][2]. You can find additional resources at the website for Ayasdi.


  [1]: http://www.ams.org/bull/2009-46-02/S0273-0979-09-01249-X/S0273-0979-09-01249-X.pdf
  [2]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3566620/', 609, '2014-06-10 23:20:16.670', '4349ec34-4361-4ab9-8831-f0aeaa79fc9e', 300, 734, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**MapReduce**: A fault tolerant distributed computational framework. MapReduce allows you to operate over huge amounts of data- with a lot of work put in to prevent failure due to hardware. MapReduce is a poor choice for computing results on the fly because it is slow. (A typical MapReduce job takes on the order of minutes or hours, not microseconds)

A MapReduce job takes a file (or some data store) as an input and writes a file of results. If you want these results available to an application, it is your responsibility to put this data in a place that is accessible. This is likely slow, and there will be a lag between the values you can display, and the values that represent your system in its current state.

An important distinction to make when considering using MapReduce in building realtime systems is that of training your model, and applying your model. If you think your model parameters do not change quickly, you can fit them with MapReduce, and then have a mechanism for accessing these pre-fit parameters when you want to apply your model.

**Storm**: A real-time, streaming computational system. Storm is online framework, meaning, in this sense, a service that interacts with a running application. In contrast to MapReduce, it receives small pieces of data (not a whole file) as they are processed in your application. You define a DAG of operations to perform on the data. A common and simple use case for Storm is tracking counters, and using that information to populate a real-time dashboard.

Storm doesn''t have anything (necessarily) to do with persisting your data. Here, streaming is another way to say keeping the information you care about and throwing the rest away. In reality, you probably have a persistence layer in your application that has already recorded the data, and so this a good and justified separation of concerns.


**If you want to know more...**
If you would like to learn more about real-time systems that that fit parameters with MR and apply the models a different way [here are slides for a talk I gave on building real-time recommendation engines on HBase.][1]

An excellent paper that marries real-time counting and persistence in an interesting way is [Google News Personalization: Scalable Online Collaborative Filtering][2]

Another interesting marriage of MR and Storm is [SummingBird][3]. Summingbird allows you to define data analysis operations that can be applied via Storm or MR.


  [1]: http://www.slideshare.net/cloudera/hbasecon-2013-24063525
  [2]: http://dl.acm.org/citation.cfm?id=1242610
  [3]: https://github.com/twitter/summingbird/wiki', 406, '2014-06-10 23:33:28.443', '5b599d5d-e8e7-425c-a9fa-0598d9f56e37', 242, 'Gained enough reputation to add a 3rd link.', 735, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One reason that data cleaning is rarely fully automated is that there is so much judgment required to define what "clean" means given your particular problem, methods, and goals.

It may be as simple as imputing values for any missing data, or it might be as complex as diagnosing data entry errors or data transformation errors from previous automated processes (e.g. coding, censoring, transforming).  In these last two cases, the data *looks good* by outward appearance but it''s really erroneous.  Such diagnosis often requires manual analysis and inspection, and also out-of-band information such as information about the data sources and methods they used.

Also, some data analysis methods work better when erroneous or missing data is left blank (or N/A) rather than imputed or given a default value.  This is true when there is explicit representations of uncertainty and ignorance, such as Dempster-Shafer Belief functions.

Finally, it''s useful to have specific diagnostics and metrics for the cleaning process.  Are missing or erroneous values randomly distributed or are they concentrated in any way that might affect the outcome of the analysis.  It''s useful to test the effects of alternative cleaning strategies or algorithms to see if they affect the final results.

Given these concerns, I''m very suspicious of any method or process that treats data cleaning in a superficial, cavalier or full-automated fashion.  There are many devils hiding in those details and it pays to give them serious attention.', 609, '2014-06-11 00:32:54.887', '66513a55-3165-482c-a9fe-77d64c5f1e8a', 301, 736, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve seen a few similar systems over the years.  I remember a company called ClickTrax which if I''m not mistaken got bought by Google and some of their features are now part of Google Analytics.

Their purpose was marketing, but the same concept can be applied to user experience analytics.  The beauty of their system was that what was tracked was defined by the webmaster - in your case the application developer.

I can imagine as an application developer I would want to be able to see statistical data on two things - task accomplishment, and general feature usage.

As an example of task accomplishment, I might have 3 ways to print a page - Ctrl+P, File->Print, and a toolbar button.  I would want to be able to compare usage to see if the screenspace utilized by the toolbar button was actually worth it.

As an example of general feature usage, I would want to define a set of features within my application and focus my development efforts on expanding the features used most by my end users.  Some features that take maybe 5 clicks and are popular, I might want to provide a hotkey for, or slim down the number of clicks to activate that feature.  There is also event timing.  Depending on the application, I might want to know the average amount of time spent on a particular feature.

Another thing I would want to look at are click streams.  How are people getting from point A to point B in my application?  What are the most popular point B''s?  What are the most popular starting points?  ', 434, '2014-06-11 00:44:20.517', '37ff0e7f-5a6f-4524-8ab5-9ce4c8b39dcc', 302, 737, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This isn''t my area of specialty and I''m not familiar with Moses, but I found this after some searching.

I think you are looking for GIZA++.  You''ll see GIZA++ listed in the "Training" section (left menu) on the Moses home page, as the second step. GIZA++ is briefly described in tutorial fashion [here][1]. Here are a few tutorial PowerPoint slides: http://www.tc.umn.edu/~bthomson/wordalignment/GIZA.ppt

  [1]: http://stackoverflow.com/questions/5752043/is-there-a-tutorial-about-giza', 609, '2014-06-11 01:09:06.100', '5575a0ee-513a-43d9-b83b-ac6c1e85dac5', 303, 738, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The goal determines the features, so I would initially take as many as possible, then use cross validation to select the optimal subset.

My educated guess is that a Markov model would work. If you discretize the action space (e.g., select this menu item, press that button, etc.), you can predict the next action based on the past ones. It''s a _sequence_ or [structured prediction](http://en.wikipedia.org/wiki/Structured_prediction) problem.

For commercial offerings, search **app analytics**.', 381, '2014-06-11 01:19:18.183', 'cc0f0b78-def0-4e73-94a9-73eed650b2e3', 288, 'added 55 characters in body', 739, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is plenty of hype surrounding Hadoop and it''s ego-system. However, in practice, where many data sets are in the tera-byte range, is it not more reasonable to use Amazon RedShift for querying large data sets, rather than spending time and effort building a Hadoop cluster? Also, how does Amazon Redshift compare with Hadoop with respect to setup complexity, cost, and performance?

 ', 534, '2014-06-11 04:24:04.183', '174562c9-20cd-4c05-bde3-eef18d377810', 305, 744, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Does Amazon RedShift replace Hadoop for ~1XTB data?', 534, '2014-06-11 04:24:04.183', '174562c9-20cd-4c05-bde3-eef18d377810', 305, 745, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop><map-reduce><aws>', 534, '2014-06-11 04:24:04.183', '174562c9-20cd-4c05-bde3-eef18d377810', 305, 746, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Personally, I don''t think it''s all that difficult to set up a hadoop cluster, but I know that it is sometimes painful when you are getting started.

HDFS size limitations well exceed a TB (or did you mean exabyte?).  If I''m not mistaken it scales to yottabytes or some other measurement that I don''t even know the word for.  Whatever it is, it''s really big.

Tools like Redshift have their place, but I always worry about vendor specific solutions.  My main concern is always "what do I do when I am dissatisfied with their service?" - I can go to google and shift my analysis work into their paradigm or I can go to hadoop and shift that same work into that system.  Either way, I''m going to have to learn something new and do a lot of work translating things.

That being said, it''s nice to be able to upload a dataset and get to work quickly - especially if what I''m doing has a short lifecycle.  Amazon has done a good job of answering the data security problem.

If you want to avoid hadoop, there will always be an alternative.  But it''s not all that difficult to work with once you get going with it.', 434, '2014-06-11 05:17:12.253', 'a726728d-5eab-404a-8537-0be194b3e65b', 306, 747, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have read lot of blogs\article on how different type of industries are using Big Data Analytic. But most of these article fails to mention

 1. What kinda data these companies used. What was the size of the data
 2. What kinda of tools technologies they used to process the data
 3. What was the problem they were facing and how the insight they got the data helped them to resolve the issue.
 4. How they selected the tool\technology to suit their need.
 5. What kinda pattern they identified from the data & what kind of patterns they were looking from the data.

I wonder if someone can provide me answer to all these questions or a link which at-least answer some of the the questions.

It would be great if someone share how finance industry is making use of Big Data Analytic.', 496, '2014-06-11 06:07:45.767', '26bdcc75-4c32-4978-a700-e2d04d4e7700', 307, 748, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Big data case study or use case example', 496, '2014-06-11 06:07:45.767', '26bdcc75-4c32-4978-a700-e2d04d4e7700', 307, 749, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop><data-mining><social-network-analysis><usecase>', 496, '2014-06-11 06:07:45.767', '26bdcc75-4c32-4978-a700-e2d04d4e7700', 307, 750, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('News outlets tend to use "Big Data" pretty loosely.  Vendors usually provide case studies surrounding their specific products.  There aren''t a lot out there for open source implementations, but they do get mentioned.  For instance, Apache isn''t going to spend a lot of time building a case study on hadoop, but vendors like Cloudera and Hortonworks probably will.

Here''s an [example case study from Cloudera][1] in the finance sector.

Quoting the study:

> One major global financial services conglomerate uses Cloudera and Datameer to help
identify rogue trading activity. Teams within the firms asset management group are
performing ad hoc analysis on daily feeds of price, position, and order information. Having
ad hoc analysis to all of the detailed data allows the group to detect anomalies across
certain asset classes and identify suspicious behavior. Users previously relied solely on
desktop spreadsheet tools. Now, with Datameer and Cloudera, users have a powerful
platform that allows them to sift through more data more quickly and avert potential
losses before they begin.

.

> A leading retail bank is using Cloudera and Datameer to validate data accuracy and quality
as required by the Dodd-Frank Act and other regulations. Integrating loan and branch data
as well as wealth management data, the banks data quality initiative is responsible for
ensuring that every record is accurate. The process includes subjectingthe data to over
50 data sanity and quality checks. The results of those checks are trended over time to
ensure that the tolerances for data corruption and data domains arent changing adversely
and that the risk profiles being reported to investors and regulatory agencies are prudent
and in compliance with regulatory requirements. The results are reported through a data
quality dashboard to the Chief Risk Officer and Chief Financial Officer, who are ultimately
responsible for ensuring the accuracy of regulatory compliance reporting as well as
earnings forecasts to investors


I didn''t see any other finance related studies at Cloudera, but I didn''t search very hard.  You can have a look at [their library][2] here.

Also, Hortonworks has a [case study on Trading Strategies][3] where they saw a 20% decrease in the time it took to develop a strategy by leveraging K-means, Hadoop, and R.

![Each color indicates a group of strategies with similar probability of a profit and loss][4]

![how the trading system was improved by using Hadoop (Hortonworks Data Platform), and the k-means algorithm][5]


  [1]: http://www.cloudera.com/content/cloudera/en/resources/library/casestudy/joint-success-story-major-retail-bank-case-study-datameer.html
  [2]: http://www.cloudera.com/content/cloudera/en/resources/library.html?q=bank
  [3]: http://hortonworks.com/blog/building-stock-trading-strategies-20-faster-with-hadoop/
  [4]: http://i.stack.imgur.com/qX3Qy.png
  [5]: http://i.stack.imgur.com/Sj8U9.png', 434, '2014-06-11 06:49:04.070', '4ab8f87e-161d-4c90-92d8-ad182ae6f251', 308, 751, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**tl;dr:** They markedly differ in many aspects and I can''t think Redshift will replace Hadoop anytime soon.

-Function
You can''t run anything other than SQL on Redshift. Perhaps most importantly, you can''t run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it''s more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.


-Performance Profile
Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I''m using the term `indexing` very loose here). Therefore, it''s great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.

-Cost Profile
Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon''s Elastic Map Reduce). For example, if you are doing OLAP, it''s very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.

Having said that, we''ve replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly because it was way easier to develop because of Redshift''s maturity, it''s ACID characteristics and quicker response time. It''s a great tool to have, but it won''t replace Hadoop.', 638, '2014-06-11 06:51:19.143', 'b7ac9fe9-8655-4e73-b609-bf0e679102aa', 309, 752, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('News outlets tend to use "Big Data" pretty loosely.  Vendors usually provide case studies surrounding their specific products.  There aren''t a lot out there for open source implementations, but they do get mentioned.  For instance, Apache isn''t going to spend a lot of time building a case study on hadoop, but vendors like Cloudera and Hortonworks probably will.

Here''s an [example case study from Cloudera][1] in the finance sector.

Quoting the study:

> One major global financial services conglomerate uses Cloudera and Datameer to help
identify rogue trading activity. Teams within the firms asset management group are
performing ad hoc analysis on daily feeds of price, position, and order information. Having
ad hoc analysis to all of the detailed data allows the group to detect anomalies across
certain asset classes and identify suspicious behavior. Users previously relied solely on
desktop spreadsheet tools. Now, with Datameer and Cloudera, users have a powerful
platform that allows them to sift through more data more quickly and avert potential
losses before they begin.

.

> A leading retail bank is using Cloudera and Datameer to validate data accuracy and quality
as required by the Dodd-Frank Act and other regulations. Integrating loan and branch data
as well as wealth management data, the banks data quality initiative is responsible for
ensuring that every record is accurate. The process includes subjectingthe data to over
50 data sanity and quality checks. The results of those checks are trended over time to
ensure that the tolerances for data corruption and data domains arent changing adversely
and that the risk profiles being reported to investors and regulatory agencies are prudent
and in compliance with regulatory requirements. The results are reported through a data
quality dashboard to the Chief Risk Officer and Chief Financial Officer, who are ultimately
responsible for ensuring the accuracy of regulatory compliance reporting as well as
earnings forecasts to investors


I didn''t see any other finance related studies at Cloudera, but I didn''t search very hard.  You can have a look at [their library][2] here.

Also, Hortonworks has a [case study on Trading Strategies][3] where they saw a 20% decrease in the time it took to develop a strategy by leveraging K-means, Hadoop, and R.

![Each color indicates a group of strategies with similar probability of a profit and loss][4]

![how the trading system was improved by using Hadoop (Hortonworks Data Platform), and the k-means algorithm][5]


These don''t answer all of your questions.  I''m pretty sure both of these studies covered most of them. I don''t see anything about tool selection specifically.  I imagine sales reps had a lot to do with getting the overall product in the door, but the data scientists themselves leveraged the tools they were most comfortable with.  I don''t have a lot of insight into that area in the big data space.

  [1]: http://www.cloudera.com/content/cloudera/en/resources/library/casestudy/joint-success-story-major-retail-bank-case-study-datameer.html
  [2]: http://www.cloudera.com/content/cloudera/en/resources/library.html?q=bank
  [3]: http://hortonworks.com/blog/building-stock-trading-strategies-20-faster-with-hadoop/
  [4]: http://i.stack.imgur.com/qX3Qy.png
  [5]: http://i.stack.imgur.com/Sj8U9.png', 434, '2014-06-11 06:54:42.593', 'ea24869f-e170-4b7c-bb8c-426e07b558c0', 308, 'added 347 characters in body', 753, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**tl;dr:** They markedly differ in many aspects and I can''t think Redshift will replace Hadoop anytime soon.

-Function
You can''t run anything other than SQL on Redshift. Perhaps most importantly, you can''t run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it''s more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.


-Performance Profile
Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I''m using the term `indexing` very loose here). Therefore, it''s great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.

-Cost Profile
Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon''s Elastic Map Reduce). For example, if you are doing OLAP, it''s very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.

Having said that, we''ve replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly because it was way easier to develop because of Redshift''s maturity, it''s ACID characteristics and quicker response time. It''s a great tool to have, but it won''t replace Hadoop.

**EDIT**:  As for setup complexity, I''d even say it''s easier with Hadoop if you use AWS''s EMR. Their tools are so mature that it''s ridiculously easy to have your Hadoop job running. Redshift isn''t that mature yet and you have to be careful with how you load/delete data, which can add some complexity to your ETL. ', 638, '2014-06-11 06:56:32.073', 'e87f0010-4f77-4b26-b340-e954c1b7cb5a', 309, 'added 320 characters in body', 754, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**tl;dr:** They markedly differ in many aspects and I can''t think Redshift will replace Hadoop.

-Function
You can''t run anything other than SQL on Redshift. Perhaps most importantly, you can''t run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it''s more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.


-Performance Profile
Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I''m using the term `indexing` very loose here). Therefore, it''s great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.

-Cost Profile
Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon''s Elastic Map Reduce). For example, if you are doing OLAP, it''s very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.

Having said that, we''ve replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly because it was way easier to develop because of Redshift''s maturity, it''s ACID characteristics and quicker response time. It''s a great tool to have, but it won''t replace Hadoop.

**EDIT**:  As for setup complexity, I''d even say it''s easier with Hadoop if you use AWS''s EMR. Their tools are so mature that it''s ridiculously easy to have your Hadoop job running. Redshift isn''t that mature yet and you have to be careful with how you load/delete data, which can add some complexity to your ETL. ', 638, '2014-06-11 07:50:57.727', 'b47db0fb-d593-4ec8-b9b2-f109cc13b515', 309, 'deleted 13 characters in body', 755, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**tl;dr:** They markedly differ in many aspects and I can''t think Redshift will replace Hadoop.

-Function
You can''t run anything other than SQL on Redshift. Perhaps most importantly, you can''t run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it''s more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.


-Performance Profile
Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I''m using the term `indexing` very loose here). Therefore, it''s great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.

-Cost Profile
Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon''s Elastic Map Reduce). For example, if you are doing OLAP, it''s very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.

Having said that, we''ve replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly because it was way easier to develop because of Redshift''s Query Engine''s maturity versus Hive''s, it''s ACID characteristics and quicker response time. It''s a great tool to have, but it won''t replace Hadoop.

**EDIT**:  As for setup complexity, I''d even say it''s easier with Hadoop if you use AWS''s EMR. Their tools are so mature that it''s ridiculously easy to have your Hadoop job running. Redshift''s operation tools aren''t that mature yet and you have to be careful with how you load/delete data, which can add some complexity to your ETL. ', 638, '2014-06-11 08:55:46.877', '9dfe6d7a-d375-41af-b920-b229b8b430ce', 309, 'added 48 characters in body', 756, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**tl;dr:** They markedly differ in many aspects and I can''t think Redshift will replace Hadoop.

-Function
You can''t run anything other than SQL on Redshift. Perhaps most importantly, you can''t run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it''s more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.


-Performance Profile
Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I''m using the term `indexing` very loose here). Therefore, it''s great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.

-Cost Profile
Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon''s Elastic Map Reduce). For example, if you are doing OLAP, it''s very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.

Having said that, we''ve replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly for the ease of development. Redshift''s Query Engine is based on PostgreSQL and is very mature, compared to Hive''s. Its ACID characteristics make it easier to reason about it, and the quicker response time allows more testing to be done. It''s a great tool to have, but it won''t replace Hadoop.

**EDIT**:  As for setup complexity, I''d even say it''s easier with Hadoop if you use AWS''s EMR. Their tools are so mature that it''s ridiculously easy to have your Hadoop job running. Tools and mechanisms surrounding Redshift''s operation aren''t that mature yet. For example, Redshift can''t handle trickle loading and thus you have to come up with something that turns that into a batched load, which can add some complexity to your ETL. ', 638, '2014-06-11 09:07:33.570', 'ccf9cd79-e725-4b16-b0b1-9a137f44d9cc', 309, 'added 48 characters in body', 757, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I do not know a standard answer to this, but I thought about it some times ago and I have some ideas to share.

When you have one confusion matrix, you have more or less a picture of how you classification model confuse (mis-classify) classes. When you repeat classification tests you will end up having multiple confusion matrices. The question is how to get a meaningful aggregate confusion matrix. The answer depends on what is the meaning of meaningful (pun intended). I think there is not a single version of meaningful.

One way is to follow the rough idea of multiple testing. In general, you test something multiple times in order to get more accurate results. As a general principle one can reason that averaging on the results of the multiple tests reduces the variance of the estimates, so as a consequence, it increases the precision of the estimates. You can proceed in this way, of course, by summing position by position and then dividing by the number of tests. You can go further and instead of estimating only a value for each cell of the confusion matrix, you can also compute some confidence intervals, t-values and so on. This is OK from my point of view. But it tell only one side of the story.

The other side of the story which might be investigated is how stable are the results for the same instances. To exemplify that I will take an extreme example. Suppose you have a classification model for 3 classes. Suppose that these classes are in the same proportion. If your model is able to predict one class perfectly and the other 2 classes with random like performance, you will end up having 0.33 + 0.166 + 0.166 = 0.66 misclassification ratio. This might seem good, but even if you take a look on a single confusion matrix you will not know that your performance on the last 2 classes varies wildly. Multiple tests can help. But averaging the confusion matrices would reveal this? My belief is not. The averaging will give the same result more or less, and doing multiple tests will only decrease the variance of the estimation. However it says nothing about the wild instability of prediction.

So another way to do compose the confusion matrices would better involve a prediction density for each instance. One can build this density by counting for each instance, the number of times it was predicted a given class. After normalization, you will have for each instance a prediction density rather a single prediction label. You can see that a single prediction label is similar with a degenerated density where you have probability of 1 for the predicted class and 0 for the other classes for each separate instance. Now having this densities one can build a confusion matrix by adding the probabilities from each instance and predicted class to the corresponding cell of the aggregated confusion matrix.

One can argue that this would give similar results like the previous method. However I think that this might be the case sometimes, often when the model has low variance, the second method is less affected by how the samples from the tests are drawn, and thus more stable and closer to the reality.

Also the second method might be altered in order to obtain a third method, where one can assign as prediction the label with highest density from the prediction of a given instance.

I do not implemented those things but I plan to study further because I believe might worth spending some time. ', 108, '2014-06-11 09:39:34.373', 'd145096a-0a12-4a57-a5a4-a5341767f585', 287, 'added 49 characters in body', 758, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m working on improving an existing supervised classifier, for classifying {protein} sequences as belonging to a specific class (Neuropeptide hormone precursors), or not.

There are about 1,150 known "positives", against a background of about 13 million protein sequences ("Unknown/poorly annotated background"), or about 100,000 reviewed, relevant proteins, annotated with a variety of properties (but very few annotated in an explicitly "negative" way).

My previous implementation looked at this as a binary classification problem:
Positive set = Proteins marked as Neuropeptides.
Negative set: Random sampling of 1,300 samples (total) from among the remaining proteins of a roughly similar length-wise distribution.

That worked, but I want to greatly improve the machines discriminatory abilities (Currently, it''s at about 83-86% in terms of accuracy, AUC, F1, measured by CV, on multiple randomly sampled negative sets).

My thoughts were to:
1) Make this a multiclass problem, choosing 2-3 different classes of protein that will definetly be negatives, by their properties/functional class, along with (maybe) another randomly sampled set.
 (Priority here would be negative sets that are similar in their characteristics/features to the positive set, while still having defining characteristics) .
2) One class learning - Would be nice, but as I understand it, it''s meant just for anomaly detection, and has poorer performance than discriminatory approaches.

*) I''ve heard of P-U learning, which sounds neat, but I''m a programming N00b, and I don''t know of any existing implementations for it. (In Python/sci-kit learn).

So, does approach 1 make sense in a theoretical POV? Is there a best way to make multiple negative sets? (I could also simply use a massive [50K] pick of the "negative" proteins, but they''re all very very different from each other, so I don''t know how well the classifier would handle them as one big , unbalanced mix).
Thanks!

', 555, '2014-06-11 10:11:59.397', '85cf4006-f529-4e37-a2c3-abc9e7f6b75e', 310, 759, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One-Class discriminatory classification with imbalanced, heterogenous Negative background?', 555, '2014-06-11 10:11:59.397', '85cf4006-f529-4e37-a2c3-abc9e7f6b75e', 310, 760, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><python><classification>', 555, '2014-06-11 10:11:59.397', '85cf4006-f529-4e37-a2c3-abc9e7f6b75e', 310, 761, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The way I would attack the problem, in general, is to leverage statistical analysis like Principal Component Analysis or Ordinary Least Squares to help determine what attributes within these protein sequences are best suited to classify proteins as Neuropeptide hormone precursors.

In order to do that, you''ll have to convert the protein sequences into numeric data, but I believe some work has already been done in that regard using formulas leveraged in Amino Acid PCA.

See these two links:
http://www.ncbi.nlm.nih.gov/pubmed/24496727

http://www.ncbi.nlm.nih.gov/pubmed/16615809

Once that work has been done, I would attempt to classify using the entire dataset and a reinforcement learning algorithm, like [Naive Bayes][1] while slimming down the data into that which PCA has identified as important.

The reason I would try to use Bayes is because it has proven to be one of the best methods for determining spam vs. regular email, which has a similarly skewed dataset.

Having said all of that...

Slimming down the number or type of negative classifications might skew your results a few points one way or the other, but I don''t think you''ll see the long term effectiveness change substantially until you do the leg work of determining how to best remove the fuzziness from your training data.  That will either require a field expert or statistical analysis.

I could be completely off base.  I am interested in seeing some other answers, but that is my 2 cents.

  [1]: http://findingscience.com/ankusa/hbase/hadoop/ruby/2010/12/02/naive-bayes-classification-in-ruby-using-hadoop-and-hbase.html', 434, '2014-06-11 11:24:19.963', '7ff7f460-4ee9-45ab-a10b-8eb238358666', 311, 762, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Therriault, really happy to hear you are using Vertica! Full disclosure, I am the chief data scientist there :) . The workflow you describe is exactly what I encounter quite frequently and I am a true believer in preprocessing those very large datasets in the database prior to any pyODBC and pandas work. I''d suggest creating a view or table via a file based query just to ensure reproducible work. Good Luck', 655, '2014-06-11 11:32:13.433', 'd0ef6d9b-404d-4298-bb93-1457a791076c', 312, 763, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the good books about the science and mathematics behind data science? It feels like so many "data science" books are programming tutorials and don''t touch things like data generating processes and statistical inference. I can already code, what I am weak on is the math/stats/theory behind what I am doing.

If I am ready to burn $1000 on books (so around 10 books... sigh), what should I buy?', 663, '2014-06-11 13:28:35.980', '14e0c23d-ab11-4957-a0cb-dd110bb9c9c7', 313, 764, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Books about the "Science" in Data Science?', 663, '2014-06-11 13:28:35.980', '14e0c23d-ab11-4957-a0cb-dd110bb9c9c7', 313, 765, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics>', 663, '2014-06-11 13:28:35.980', '14e0c23d-ab11-4957-a0cb-dd110bb9c9c7', 313, 766, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What are the good books about the science and mathematics behind data science? It feels like so many "data science" books are programming tutorials and don''t touch things like data generating processes and statistical inference. I can already code, what I am weak on is the math/stats/theory behind what I am doing.

If I am ready to burn $1000 on books (so around 10 books... sigh), what should I buy?

Examples: Agresti''s [Categorical Data Analysis][1], [Linear Mixed Models for Longitudinal Data][2], etc... etc...


  [1]: http://www.amazon.com/Categorical-Data-Analysis-Alan-Agresti/dp/0470463635
  [2]: http://www.amazon.com/dp/1441902996/', 663, '2014-06-11 13:35:33.213', '33532ab6-7ffc-43c4-bc5a-e0e84075292c', 313, 'added 249 characters in body', 767, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I could only recomend one to you, it would be: [The Elements of Statistical Learning and Prediction](http://www.amazon.com/The-Elements-Statistical-Learning-Prediction/dp/0387848576) by Hastie, Tibshirani and Friedman.  It provides the math/statistics behind a lot of commonly used techniques in data science.

For Bayesian Techniques, [Bayesian Data Analysis](http://www.amazon.com/Bayesian-Analysis-Edition-Chapman-Statistical/dp/1439840954/ref=pd_sim_b_6?ie=UTF8&refRID=09QA1N2WPFHA4CTXJ8G4) by Gelman, Carlin, Stern, Dunson, Vehtari and Rubin is excellent.

[Statistical Inference](http://www.amazon.com/Statistical-Inference-George-Casella/dp/0534243126) by Casella and Berger is a good graduate-level textbook on the theoretical foundation of statistics.  This book does require a pretty high level of comfort with math (probability theory is based on measure theory, which is not trivial to understand).

With respect to data generating processes, I don''t have a recommendation for a book.  What I can say is that a good understanding of the assumptions of the techniques used and ensuring that the data was collected or generated in a manner that does not violate those assumptions goes a long way towards a good analysis.', 178, '2014-06-11 13:49:08.970', '25626139-4af7-4b07-bb1b-e65aaa71f394', 314, 768, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What are the books about the science and mathematics behind data science? It feels like so many "data science" books are programming tutorials and don''t touch things like data generating processes and statistical inference. I can already code, what I am weak on is the math/stats/theory behind what I am doing.

If I am ready to burn $1000 on books (so around 10 books... sigh), what could I buy?

Examples: Agresti''s [Categorical Data Analysis][1], [Linear Mixed Models for Longitudinal Data][2], etc... etc...


  [1]: http://www.amazon.com/Categorical-Data-Analysis-Alan-Agresti/dp/0470463635
  [2]: http://www.amazon.com/dp/1441902996/', 663, '2014-06-11 14:33:57.470', '6f0aa961-96e5-4b92-8132-695afdbd8cc7', 313, 'deleted 5 characters in body', 769, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There is plenty of hype surrounding Hadoop and its eco-system.  However, in practice, where many data sets are in the terabyte range, is it not more reasonable to use [Amazon RedShift][1] for querying large data sets, rather than spending time and effort building a Hadoop cluster?

Also, how does Amazon Redshift compare with Hadoop with respect to setup complexity, cost, and performance?


  [1]: http://aws.amazon.com/redshift/', 434, '2014-06-11 15:02:46.890', 'd38f2adb-1576-4cbb-bc25-15ba6a1ef01d', 305, 'minor typos', 770, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-11 15:02:46.890', 'd38f2adb-1576-4cbb-bc25-15ba6a1ef01d', 305, 'Proposed by 434 approved by 534 edit id of 53', 771, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As Konstantin has pointed, R performs all its computation in the system''s memory i.e. RAM. Hence, RAM capacity is a very important constraint for computation intensive operations in R. Overcoming this constraint, data is being stored these days in HDFS systems, where data isn''t loaded onto memory and program is run instead, program goes to the data and performs the operations, thus overcoming the memory constraints.  RHadoop (https://github.com/RevolutionAnalytics/RHadoop/wiki) is the connector you are looking for.

Coming to the impact on algorithms which are computation intensive, Random Forests/Decision Trees/Ensemble methods on a considerable amount of data (minimum 50,000 observations in my experience) take up a lot of memory and are considerably slow. To speed up the process, parallelization is the way to go and parallelization is inherently available in Hadoop! That''s where, Hadoop is really efficient.

So, if you are going for ensemble methods which are compute intensive and are slow, you would want to try out on the HDFS system which gives a considerable performance improvement.', 514, '2014-06-11 16:25:34.747', '99138a94-bc13-450f-ae32-32e9c83c18f3', 316, 773, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There is also another resource provided by The Guardian, the British Daily on their website. The datasets published by the Guardian Datablog are all hosted. Datasets related to Football Premier League Clubs'' accounts, Inflation and GDP details of UK, Grammy awards data etc.
The datasets are available at

 - http://www.theguardian.com/news/datablog/interactive/2013/jan/14/all-our-datasets-index

Some more resources. Some of the datasets are in R format or R commads exist for directly importing data to R.

 - http://www.inside-r.org/howto/finding-data-internet', 514, '2014-06-11 16:30:06.930', '2bc9baae-200b-4e18-af31-c8798b44bb57', 279, 'Added new links', 774, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you already know R Studio, then the caret package is a good place to start. Here are some tutorials:

 1. https://class.coursera.org/predmachlearn-002
 2. http://caret.r-forge.r-project.org/index.html

With R and caret you can easily load and splice data sets, feature reduction, principal component analysis, and train and predict using various algorithms.', 680, '2014-06-11 16:56:59.660', '24204f27-ae56-4560-939c-214a0bc162a0', 317, 775, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you can reproduce the 6x3 grid of graphs from the banner of the http://scikit-learn.org/ page then you will have learnt some ML and some Python. You didn''t mention a language. Python is easy enough to learn very quickly, and scikit-learn has a wide range of algorithms implemented.

Then try on your own data!
', 471, '2014-06-11 17:24:53.610', '66d361ac-7784-4649-8b04-353012bed1d8', 318, 776, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<hadoop><r><recommendation>', 158, '2014-06-11 17:35:51.877', '3825b584-67cc-4d63-b107-6142f5d2a0e2', 59, 'edited tags', 777, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve built an artificial neural network in python using the scipy.optimize.minimize (Conjugate gradient) optimization function.

I''ve implemented gradient checking, double checked everything etc and I''m pretty certain it''s working correctly.

I''ve run it a few times and it reaches ''Optimization terminated successfully'' however when I increase the number of hidden layers, the cost of the hypothesis increases (everything else is kept the same) after it has successfully terminated.

Intuitively it feels as if the cost should decrease when the number of hidden layers is increased, as it is able to generate a more complex hypothesis which can fit the data better, however this appears not to be the case.

I''d be interested to understand what''s going on here, or if I''ve implemented neural net incorrectly?

Thanks in advance.', 691, '2014-06-11 18:22:36.267', 'b6aa265c-7e15-48c3-b908-14cfd7a0a4a1', 319, 778, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Number of layers vs cost in a Neural Network', 691, '2014-06-11 18:22:36.267', 'b6aa265c-7e15-48c3-b908-14cfd7a0a4a1', 319, 779, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><python><neuralnetwork>', 691, '2014-06-11 18:22:36.267', 'b6aa265c-7e15-48c3-b908-14cfd7a0a4a1', 319, 780, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve built an artificial neural network in python using the scipy.optimize.minimize (Conjugate gradient) optimization function.

I''ve implemented gradient checking, double checked everything etc and I''m pretty certain it''s working correctly.

I''ve run it a few times and it reaches ''Optimization terminated successfully'' however when I increase the number of hidden layers, the cost of the hypothesis increases (everything else is kept the same) after it has successfully terminated.

Intuitively it feels as if the cost should decrease when the number of hidden layers is increased, as it is able to generate a more complex hypothesis which can fit the data better, however this appears not to be the case.

I''d be interested to understand what''s going on here, or if I''ve implemented neural net incorrectly?', 84, '2014-06-11 20:06:18.687', '0cd3d509-d654-478f-a056-c66bc165e565', 319, 'Improving formatting.', 782, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-11 20:06:18.687', '0cd3d509-d654-478f-a056-c66bc165e565', 319, 'Proposed by 84 approved by 691 edit id of 54', 783, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are so many ways to go wrong with a neural net that it''s going to be difficult to debug. Also, to address your intuition, each additional hidden layer makes learning much harder. With that said, here are some possibilities:

 1. You have added weight decay. Adding more layers adds more weights which increases your regularization cost.
 2. The problem is simple enough that a model with a single hidden layer is sufficient. Adding more hidden layers makes it harder for the network to learn (harder optimization problem).
 3. The optimization method is not doing a great job (I prefer climin to scipy.optimize).
 4. You are using the sigmoid/tanh activation function. The sigmoid function causes the vanishing gradient problem which makes learning hard with more layers. Try using the ReLu function.

Training neural nets takes a lot of practice, luck, and patience. Good luck.


', 574, '2014-06-11 20:34:51.873', 'b49f19a1-8b74-4ab6-9676-fb6aff9daf34', 320, 784, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The setup is simple: binary classification using a simple decision tree, each node of the tree has a single threshold applied on a single feature. In general, building a ROC curve requires moving a decision threshold over different values and computing the effect of that change on the true positive rate and the false positives rate of predictions. What''s that decision threshold in the case of a simple fixed decision tree?
', 418, '2014-06-11 23:52:37.823', 'a73013b1-9653-4b66-a269-cc48dfed4056', 323, 789, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How can we calculate AUC for a simple decision tree?', 418, '2014-06-11 23:52:37.823', 'a73013b1-9653-4b66-a269-cc48dfed4056', 323, 790, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 418, '2014-06-11 23:52:37.823', 'a73013b1-9653-4b66-a269-cc48dfed4056', 323, 791, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to extract news about a company from online news by using RODBC package in R. And I want to use the extracted data for sentiment analysis. I want to accomplish this in such a way that the positive news is assigned a value of +1, the negative news is assigned a value of -1 and the neutral news is assigned a value of 0.', 714, '2014-06-12 03:11:00.033', '803f2281-2434-40ee-8154-9a5d02a111b7', 324, 793, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How can I extract news about a particular company from various websites using RODBC package in R? And perform sentiment analysis on the data?', 714, '2014-06-12 03:11:00.033', '803f2281-2434-40ee-8154-9a5d02a111b7', 324, 794, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><text-mining>', 714, '2014-06-12 03:11:00.033', '803f2281-2434-40ee-8154-9a5d02a111b7', 324, 795, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This isn''t a question with a simple answer, so all I can really do is point you in the right direction.

The RODBC package isn''t meant to extract data online, it''s meant to pull data from a database.  If you will be leveraging that package, it will be after you pull data down from the web.

Jeffrey Bean put together a [slideshow tutorial][1] for doing sentiment analysis with Twitter data a few years back.  He used the Twitter stream as well as some data pulled in from web scraping.  It''s a good starting point.

There''s also this site that discusses a few different approaches to this problem in detail, including Bean''s, the sentiment package, and ViralHeat (which is a commercial sentiment analysis service who''s data you can pull into R).  Sentiment has since been removed ([archived versions here][2]), but the [qdap package][3] is available and is designed for use in transcript analysis.


  [1]: http://jeffreybreen.wordpress.com/2011/07/04/twitter-text-mining-r-slides/
  [2]: http://cran.us.r-project.org/src/contrib/Archive/sentiment/
  [3]: http://cran.us.r-project.org/web/packages/qdap/index.html', 434, '2014-06-12 05:17:46.453', '80cefc65-2dd8-46b9-8794-196d30118dee', 325, 796, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I maybe asking a very basic Q. I am, just starting to develop ML application for academic purposes, but i am also interested in knowing what people are using both academia and Industries. A lot of places i saw people using python. what do you recommand? i am currently using R and train myself in it.

Thanks , Ido', 721, '2014-06-12 06:04:48.243', '4ba2ec40-91ea-44ff-be0c-64e818fee364', 326, 797, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Python Vs R Machine learning', 721, '2014-06-12 06:04:48.243', '4ba2ec40-91ea-44ff-be0c-64e818fee364', 326, 798, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><python>', 721, '2014-06-12 06:04:48.243', '4ba2ec40-91ea-44ff-be0c-64e818fee364', 326, 799, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is nothing like "python is better" or "R is much better than x".

The only fact I know is that in the industry allots of people stick to python because that is what they learned at the university. The python community is really active and have a few great frameworks for ML and data mining etc.

But to be honest, if you get a good c programmer he can do the same as people do in python or r, if you got a good java programmer he can also do (near to) everything in java.

So just stick with the language you are comfortable with.', 115, '2014-06-12 07:05:05.653', '954384bc-c69c-46d9-a08b-834a0268871d', 327, 800, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is no "better" language. I have tried both of them and I am comfortable with Python so I work with Python only. Though I am still learning stuff, but I haven''t encounter any roadblock with Python till now. The good thing about Python is community is too good and you can get a lot of help on the Internet easily. Other than that, I would say go with the language you like not the one people recommend. ', 456, '2014-06-12 08:30:49.757', '04f68a22-0c75-4a8e-bfd7-04d898c2974f', 328, 801, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can also checkout the seaborn package for statistical charts.', 729, '2014-06-12 09:57:39.890', 'e07216ec-c5d9-4c92-a833-f33010974ef2', 330, 803, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Some additional thoughts.

The programming language ''per se'' is only a tool. All languages were designed to make some type of constructs more easy to build than others. And the knowledge and mastery of a programming language is more important and effective than the features of that language compared to others.

As far as I can see there are two dimensions of this question. The first dimension is the ability to explore, build proof of concepts or models at a fast pace, eventually having at hand enough tools to study what is going on (like statistical tests, graphics, measurement tools, etc). This kind of activity is usually preferred by researchers and data scientists (I always wonder what that means, but I use this term for its loose definition). They tend to rely on well-known and verified instruments, which can be used for proofs or arguments.

The second dimension is the ability to extend, change, improve or even create tools, algorithms or models. In order to achieve that you need a proper programming language. Roughly all of them are the same. If you work for a company, than you depend a lot on the company''s infrastructure, internal culture and your choices diminish significantly. Also, when you want to implement an algorithm for production use, you have to trust the implementation. And implementing in another language which you do not master will not help you much.

I tend to favor for the first type of activity the R ecosystem. You have a great community, a huge set of tools, proofs that these tools works as expected. Also, you can consider Python, Octave (to name a few), which are reliable candidates.

For the second task, you have to think before at what you really want. If you want robust production ready tools, than C/C++, Java, C# are great candidates. I consider Python as a second citizen in this category, together with Scala and friends. I do not want to flame a war, it''s my opinion only. But after more than 17 years as a developer, I tend to prefer a strict contract and my knowledge, than the freedom to do whatever you might think of (like it happens with a lot of dynamic languages).

Personally, I want to learn as much as possible. I decided that I have to choose the hard way, which means to implement myself everything from scratch. I use R as a model and inspiration. It has great treasures in libraries and a lot of experience distilled. However, as a programming language R, for me at least is a nightmare. So I decided to use Java, and use no additional library. That is only because of my experience, and nothing else.

If you have time, the best thing you can do is to spend some time with all these things. In this way you will earn for yourself the best answer possible, fitted for you. Dijkstra said once that the tools influence the way you think, so it is advisable to know your tools before letting them to model how you think. You can read more about that in his famous paper called [The Humble Programmer][1]


  [1]: http://www.cs.utexas.edu/~EWD/transcriptions/EWD03xx/EWD340.html', 108, '2014-06-12 10:09:23.887', 'efcbd099-d14c-4417-9171-dbf19497173e', 331, 804, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In order to build the ROC curve and AUC (Area under curve) you have to have a binary classifier which provides you at classification time, the distribution (or at least a score), not the classification label. To give you an example, suppose you have a binary classification model, with classes c1 and c2. For a given instance, your classifier would have to return a score for c1 and another for c2. If this score is a probability-like (preferrable), than something like p(c1), p(c2) would work. In plain English is translated like "I (the model) classify this instance as c1 with probability p(c1), and as c2 with probability  p(c2)=1-p(c1)".

This applies for all type of classifiers, not only for decision trees. Having these scores you can than compute ROC or AUC by varying a threshold on p(c1) values, from the smallest to the greatest value.

Now, if you have an implementation of a decision tree and you want to change that implementation to return scores instead of labels you have to compute those values. The most used way for decision trees is to use the proportion of classes from the leaf nodes. So, for example you have built a decision tree and when you predict the class for an instance you arrive at a leaf node which have (stored from the learning phase) 10 instances of class c1 and 15 instances of class c2, you can use the ratios as the scores. So, in our example, you would return p(c1) = 10 / (10+15) = 0.4 probability of class c1 and p(c2) = 15/(10+15)=0.6 probability of being class c2.

For further reading on the ROC curves, the best and inspiring source of information I found to be the Tom Fawcett paper called [An Introduction to ROC Analysis][1], it''s solid gold on this topic.


  [1]: https://www.google.ie/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0CCsQFjAA&url=https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf&ei=DYCZU96QGPOV7AaAx4CgBA&usg=AFQjCNECLoecin6ieT-0ymLQ--FoMjkMZw&sig2=3bGE2y1N3I2Re_YWn0EDFA', 108, '2014-06-12 10:27:14.480', 'e8d0e839-9363-400c-a214-2cc47752ef79', 332, 805, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve now seen two data science certification programs - the [John Hopkins one available at Coursera][1] and the [Cloudera one][2].

I''m sure there are others out there.

The John Hopkins set of classes is focused on R as a toolset, but covers a range of topics:

* R Programming
* cleaning and obtaining data
* Data Analysis
* Reproducible Research
* Statistical Inference
* Regression Models
* Machine Learning
* Developing Data Products
* And what looks to be a Project based completion task similar to Cloudera''s Data Science Challenge


The Cloudera program looks thin on the surface, but looks to answer the two important questions - "Do you know the tools", "Can you apply the tools in the real world".  Their program consists of:

* Introduction to Data Science
* Data Science Essentials Exam
* Data Science Challenge (a real world data science project scenario)


I am not looking for a recommendation on a program or a quality comparison.

I am curious about other certifications out there, the topics they cover, and how seriously DS certifications are viewed at this point by the community.


  [1]: https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage
  [2]: http://cloudera.com/content/cloudera/en/training/certification/ccp-ds.html', 434, '2014-06-12 10:52:03.410', '681b6f02-4fb9-442f-9e11-7591a53ecda9', 334, 807, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What do you think of Data Science certifications?', 434, '2014-06-12 10:52:03.410', '681b6f02-4fb9-442f-9e11-7591a53ecda9', 334, 808, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<education>', 434, '2014-06-12 10:52:03.410', '681b6f02-4fb9-442f-9e11-7591a53ecda9', 334, 809, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The certification programs you mentioned are really entry level courses. Personally, I think these certificates show only person''s persistence and they can be only useful to those who is applying for internships, not the real data science jobs.', 478, '2014-06-12 11:11:35.600', 'ef899587-672d-4263-830d-80b60022a69f', 335, 810, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Some additional thoughts.

The programming language ''per se'' is only a tool. All languages were designed to make some type of constructs more easy to build than others. And the knowledge and mastery of a programming language is more important and effective than the features of that language compared to others.

As far as I can see there are two dimensions of this question. The first dimension is the ability to explore, build proof of concepts or models at a fast pace, eventually having at hand enough tools to study what is going on (like statistical tests, graphics, measurement tools, etc). This kind of activity is usually preferred by researchers and data scientists (I always wonder what that means, but I use this term for its loose definition). They tend to rely on well-known and verified instruments, which can be used for proofs or arguments.

The second dimension is the ability to extend, change, improve or even create tools, algorithms or models. In order to achieve that you need a proper programming language. Roughly all of them are the same. If you work for a company, than you depend a lot on the company''s infrastructure, internal culture and your choices diminish significantly. Also, when you want to implement an algorithm for production use, you have to trust the implementation. And implementing in another language which you do not master will not help you much.

I tend to favor for the first type of activity the R ecosystem. You have a great community, a huge set of tools, proofs that these tools works as expected. Also, you can consider Python, Octave (to name a few), which are reliable candidates.

For the second task, you have to think before at what you really want. If you want robust production ready tools, then C/C++, Java, C# are great candidates. I consider Python as a second citizen in this category, together with Scala and friends. I do not want to start a flame war, it''s my opinion only. But after more than 17 years as a developer, I tend to prefer a strict contract and my knowledge, than the freedom to do whatever you might think of (like it happens with a lot of dynamic languages).

Personally, I want to learn as much as possible. I decided that I have to choose the hard way, which means to implement myself everything from scratch. I use R as a model and inspiration. It has great treasures in libraries and a lot of experience distilled. However, as a programming language R, for me at least is a nightmare. So I decided to use Java, and use no additional library. That is only because of my experience, and nothing else.

If you have time, the best thing you can do is to spend some time with all these things. In this way you will earn for yourself the best answer possible, fitted for you. Dijkstra said once that the tools influence the way you think, so it is advisable to know your tools before letting them to model how you think. You can read more about that in his famous paper called [The Humble Programmer][1]


  [1]: http://www.cs.utexas.edu/~EWD/transcriptions/EWD03xx/EWD340.html', 733, '2014-06-12 11:16:51.183', '14535633-b53d-47f5-afd0-5ecb4e4c0e5e', 331, 'corrected spelling ', 811, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-12 11:16:51.183', '14535633-b53d-47f5-afd0-5ecb4e4c0e5e', 331, 'Proposed by 733 approved by 108 edit id of 55', 812, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There isn''t a silver bullet language that can be used to solve each and every data related problem. The language choice depends on the context of the problem, size of data and if you are working at a workplace you have to stick to what they use.

Personally I use R more often than Python due to its visualization libraries and interactive style. But if I need more performance or structured code I definitely use Python since it has some of the best libraries as SciKit-Learn, numpy, scipy etc. I use both R and Python in my projects interchangeably.

So if you are starting on data science work I suggest you to learn both and it''s not difficult since Python also provides a similar interface to R with [Pandas][1].

If you have to deal with much larger datasets, you can''t escape eco-systems built with Java(Hadoop, Pig, Hbase etc).


  [1]: http://pandas.pydata.org/', 733, '2014-06-12 11:30:20.943', '414f4c6e-75ec-4dd5-b0a8-e5c296e96eb1', 336, 813, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would add to what others have said till now. There is no single answer that one language is better than other.

Having said that, R has a better community for data exploration and learning. It has extensive visualization capabilities. Python, on the other hand, has become better at data handling since introduction of pandas. Learning and development time is very less in Python, as compared to R (R being a low level language).

I think it ultimately boils down to the eco-system you are in and personal preferences. For more details, you can look at this comparison [here][1].


  [1]: http://www.analyticsvidhya.com/blog/2014/03/sas-vs-vs-python-tool-learn/', 735, '2014-06-12 11:54:59.140', 'b454d51c-11e7-4935-9ba8-559614e26c8d', 337, 814, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I did the first 2 courses and I''m planning to do all the others too.  If you don''t know R, it''s a really good program. There are assignments and quizzes every week. Many people find some courses very difficult. You are going to have hard time if you don''t have any programming experience (even if they say it''s not required).

Just remember.. it''s not because you can drive a car that you are a F1 pilot ;) ', 737, '2014-06-12 12:13:26.940', '163da822-0c73-4694-9167-11ff57f6f41f', 338, 815, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Some real important differences to consider when you are choosing R or Python over one another:

 - **Machine Learning** has 2 phases. Model Building and Prediction phase. Typically, model building is performed as a batch process and **predictions are done realtime**. The model building process is a compute intensive process while the prediction happens in a jiffy. Therefore, performance of an algorithm in Python or R doesn''t really affect the turn-around time of the user. Python 1, R 1.
 - **Production:** The real difference between Python and R comes in being production ready. Python, as such is a full fledged programming language and many organisations use it in their production systems. R is a statistical programming software favoured by many academia and due to the rise in data science and availability of libraries and being open source, the industry has started using R. Many of these organisations have their production systems either in Java, C++, C#, Python etc. So, ideally they would like to have the **prediction system** in the same language to reduce the latency and maintenance issues.
Python 2, R 1.
 - **Libraries:** Both the languages have enormous and reliable libraries. R has over 5000 libraries catering to many domains while Python has some incredible packages like **Pandas, NumPy, SciPy, Scikit Learn, Matplotlib**. Python 3, R 2.
 - **Development:** Both the language are interpreted languages. Many say that python has a good learning curve, it''s almost like reading english (to put it on a lighter note) but R has a reputation of having a steeper learning curve. Also, both of them have good IDEs (Spyder etc for Python and RStudio for R). Python 4, R 2.
 - **Speed:** R software initially had problems with large computations (say, like nxn matrix multiplications). But, this issue is addressed with the introduction of R by Revolution Analytics. They have re-written computation intensive operations in C which is blazingly fast. Python being a high level language is relatively slow. Python 4, R 3.
 - **Visualizations:** In data science, we frequently tend to plot data to showcase patterns to users. Therefore, visualisations become an important criteria in choosing a software and R completely kills Python in this regard. Thanks to Hadley Wickham for an incredible ggplot2 package. R wins hands down. Python 4, R 4.
 - **Dealing with Big Data:** One of the constraints of R is it stores the data in system memory (RAM). So, RAM capacity becomes a constraint when you are handling Big Data. Python does well, but I would say, as both R and Python have HDFS connectors, leveraging Hadoop infrastructure would give substantial performance improvement. So, Python 5, R 5.

So, both the languages are equally good. Therefore, depending upon your domain and the place you work, you have got to smartly choose the right language. Technology world usually prefers single language. Business users (marketing analytics, retail analytics) usually go with statistical programming language R since they frequently do quick prototyping and build visualisations which is faster do in R.', 514, '2014-06-12 12:59:00.663', 'dbd0ef4c-dd67-435e-8edb-22938d7529a0', 339, 817, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''s plenty. If you''ve ever used ggplot2 in R and want to do that in python:

https://pypi.python.org/pypi/ggplot/0.5.9

If you want to use a similar visualisation grammar (Vega) and go via D3 then:

https://github.com/wrobstory/vincent

Or if you want the full-on 3d shizzle:

http://docs.enthought.com/mayavi/mayavi/

', 471, '2014-06-12 13:38:10.877', '9c1aca28-f253-4f56-ae10-cc5f60e1a06b', 340, 818, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You have to first make it clear what do you mean by "learn Hadoop". If you mean using Hadoop, such as learning to program in MapReduce, then most probably it is a good idea. But fundamental knowledge (database, machine learning, statistics) may play a bigger role as time goes on.', 743, '2014-06-12 13:42:05.383', '1c7c326b-de34-4483-b8fd-285555534dbf', 341, 819, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Increasing the number of hidden layers for a standard neural network actually won''t improve results in a majority of cases. Changing the _size_ of the hidden layer will.

This fact has actually was noted historically and is the motivation behind the field of deep learning. Deep learning is effectively clever ways of training multilayer neural networks by, for example, isolating subsets of features when training different layers.

Good introductory video on this topic on [YouTube](https://www.youtube.com/watch?v=vXMpKYRhpmI&index=52&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)', 754, '2014-06-12 15:22:16.247', '063f10f5-3726-47a7-9e98-77916a239149', 343, 822, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Data visualization is an important sub-field in data science and python programmers would need to have available toolkits for them.

**Is there a Python API to Tableau?**

**Are there any Python-based data visualization toolkits?**', 471, '2014-06-12 16:32:27.490', 'a63fd4eb-f1ee-4b89-8b32-dffc84ffd9cd', 235, 'fixed spelling ''Tableau''', 824, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-12 16:32:27.490', 'a63fd4eb-f1ee-4b89-8b32-dffc84ffd9cd', 235, 'Proposed by 471 approved by 50 edit id of 56', 825, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Debugging Neural Networks', 381, '2014-06-12 16:43:59.513', 'df3f1c34-4a0f-45a3-9920-06b8ba9bc728', 319, 'edited title to match body', 827, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-12 16:43:59.513', 'df3f1c34-4a0f-45a3-9920-06b8ba9bc728', 319, 'Proposed by 381 approved by 50 edit id of 57', 828, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Introductory:

 - [Machine Learning: The Art and Science of Algorithms that Make Sense of Data (Flach)][1]
 - [Learning From Data (Abu-Mostafa et al.)][2]
 - [Introduction to Statistical Learning (James et al.)][3]

Digging deeper:

 - [Elements of Statistical Learning (Hastie et al.)][4]
 - [Pattern Recognition and Machine Learning (Bishop)][5]

Some special interest examples:

 - [Convex Optimization (Boyd)][6]
 - [Bayesian Reasoning and Machine Learning (Barber)][7]
 - [Probabilistic Graphical Models (Koller)][8]
 - [Neural Networks for Pattern Recognition (Bishop)][9]

Broader reference works on machine learning (not really what you asked for, but for completeness):

 - [Machine Learning: A Probabilistic Perspective (Murphy)][10]
 - [Artificial Intelligence: A Modern Approach (Russell & Norvig)][11]

Bonus paper:

 - [Statistical Modeling: The Two Cultures (Breiman)][12]


  [1]: http://www.amazon.com/Machine-Learning-Science-Algorithms-Sense/dp/1107422221/
  [2]: http://www.amazon.com/Learning-From-Data-Yaser-Abu-Mostafa/dp/1600490069/
  [3]: http://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370/
  [4]: http://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/
  [5]: http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/
  [6]: http://www.amazon.com/Convex-Optimization-Stephen-Boyd/dp/0521833787/
  [7]: http://www.cs.ucl.ac.uk/staff/d.barber/brml/
  [8]: http://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193/
  [9]: http://www.amazon.com/Networks-Pattern-Recognition-Advanced-Econometrics/dp/0198538642/
  [10]: http://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/
  [11]: http://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/
  [12]: http://projecteuclid.org/euclid.ss/1009213726', 554, '2014-06-12 16:52:46.557', 'c2b54984-0ce4-41fb-85f9-6dd65e9f891e', 345, 831, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":50,"DisplayName":"Robert Cartaino"}]}', 50, '2014-06-12 16:56:27.573', '0654da56-fea3-4cd9-915f-accdba1415e6', 203, 'via Vote', 832, '12');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":50,"DisplayName":"Robert Cartaino"}]}', 50, '2014-06-12 16:57:44.967', '9f85bcf8-226b-4993-bc17-9684e829d5e6', 203, 833, '13');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In addition to the courses and tutorials posted, I would suggest something a bit more ''hands on'': [Kaggle][1] has some introductory competitions that might pique your interest (most people start with the Titanic competition). And there''s a large variety of subjects to explore and compete in when you want to get more experience.


  [1]: http://www.kaggle.com/competitions', 554, '2014-06-12 17:03:15.733', '3dc0a14d-adb0-4b5b-8942-fc632b5abeb7', 346, 835, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As mentioned in above answers grasp the basics of ML by following MOOCs by Prof.Andrew Ng and [''Learning From Data''][1] by Prof. Yaser Abu-Mostafa.

R is the [clear winner][2] as the most used tool in Kaggle competitions. (Don''t forget to check the resources on Kaggle wiki and forums)

Learn basic R and Python. Coursera ''Data Science'' track has an [introductory R course][3]. Almost all the algorithms can be found in Python and R libraries. Feel free to use the algorithms you learned in few kaggle competitions. As a starting point compare the performance of several algorithms on Titanic dataset and Digit recognizer dataset on [kaggle][4].

And do continue practising on various datasets!


  [1]: http://work.caltech.edu/telecourse.html
  [2]: http://www.kaggle.com/wiki/Software
  [3]: https://www.coursera.org/course/rprog
  [4]: http://www.kaggle.com/', 733, '2014-06-12 17:58:21.467', 'affa3c12-4b7a-4bd9-bd67-60b29f10735c', 347, 836, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Increasing the number of hidden layers for a standard neural network actually won''t improve results in a majority of cases. Changing the _size_ of the hidden layer will.

This fact (that the number of hidden layers does very little) has actually was noted historically and is the motivation behind the field of deep learning. Deep learning is effectively clever ways of training multilayer neural networks by, for example, isolating subsets of features when training different layers.

Good introductory video on this topic on [YouTube](https://www.youtube.com/watch?v=vXMpKYRhpmI&index=52&list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)', 754, '2014-06-12 18:08:07.507', '84ed43ef-2706-4da1-b151-99a773978697', 343, 'added 52 characters in body', 837, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Not sure about the cloud era one, but one of my friends joined the John Hopkins one and in his words it''s "brilliant to get you started". It has also been recommended by a lot of people. I am planning to join it in few weeks. As far as seriousness is concerned, I don''t think these certifications are gonna help you land a job, but they sure will help you learn.  ', 456, '2014-06-12 18:33:21.540', 'd3ef804e-0a3b-474d-8624-8c597b3b6422', 348, 838, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I maybe asking a very basic question. I''m just starting to develop a ML application for academic purposes, but I''m also interested in knowing what people are using both in academia and industry. A lot of places I saw people using Python. What do you recommend? I''m currently using R and training myself in it.', 84, '2014-06-12 20:31:09.133', 'dac73363-fc90-418e-967e-752f70389528', 326, 'Fixed grammar, and improving formatting.', 842, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-12 20:31:09.133', 'dac73363-fc90-418e-967e-752f70389528', 326, 'Proposed by 84 approved by 50 edit id of 58', 843, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As a former analytics manager and a current lead data scientist, I am very leery of the need for data science certificates.  The term data scientist is pretty vague and the field of data science is in it''s infancy.  A certificates implies some sort of uniform standard which is just lacking in data science, it is still very much the wild west.

While a certificate is probably not going to hurt you, I think your time would be better spent developing the experience know when to use a certain approach, and depth of understanding to be able to explain that approach to a non-technical audience.', 780, '2014-06-12 20:51:59.930', '4c3a7f93-a9a3-4901-bdf5-d6855d99d839', 349, 844, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Could you give some examples of typical tasks that a data scientist does in his daily job and the must know minimum for each of the levels (like junior, senior, etc. if there are any)?
(if possible something like the following Programmer competency matrix http://www.starling-software.com/employment/programmer-competency-matrix.html)', 194, '2014-06-12 22:11:46.607', 'a91422f7-8ff8-48b7-81f4-df159d20cfdb', 350, 845, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Example tasks of a data scientist and the necessary knowledge', 194, '2014-06-12 22:11:46.607', 'a91422f7-8ff8-48b7-81f4-df159d20cfdb', 350, 846, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<knowledge-base>', 194, '2014-06-12 22:11:46.607', 'a91422f7-8ff8-48b7-81f4-df159d20cfdb', 350, 847, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Could you give some examples of typical tasks that a data scientist does in his daily job, and the must-know minimum for each of the levels (like junior, senior, etc. if there are any)? If possible, something like a [Programmer competency matrix](http://www.starling-software.com/employment/programmer-competency-matrix.html).', 84, '2014-06-12 22:42:11.590', 'bcc02fb0-0390-4dd6-b0c0-da493ee4221e', 350, 'Improving formatting.', 848, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-12 22:42:11.590', 'bcc02fb0-0390-4dd6-b0c0-da493ee4221e', 350, 'Proposed by 84 approved by 194 edit id of 60', 849, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are multiple certifications going on, but they have different focus area and style of teaching.

I prefer The Analytics Edge on eDX lot more over John Hopkins specialization, as it is more intensive and hands on. The expectation in John Hopkins specialization is to put in 3 - 4 hours a week vs. 11 - 12 hours a week on Analytics Edge.

From an industry perspective, I take these certifications as a sign of interest and not level of knowledge a person possesses. There are too many dropouts in these MOOCs. I value other experience (like participating in Kaggle competitions) lot more than undergoing XYZ certification on MOOC.', 735, '2014-06-13 03:50:24.610', '2a0a7cc8-7858-4a54-9615-d9fbcab4675f', 351, 850, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In some cases, [it may be impossible][1] to draw Euler diagrams with overlapping circles to represent all the overlapping subsets in the correct proportions. This type of data then requires using polygons or other figures to represent each set. When dealing with data that describes overlapping subsets, how can I figure out whether a simple Euler diagram is possible?




  [1]: http://www.ncbi.nlm.nih.gov/pubmed/20975147', 62, '2014-06-13 05:40:39.360', '820cc002-0290-4be8-9457-27766c0ebf1c', 352, 851, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How do I figure out if subsets can be plotted in a normal Euler diagram?', 62, '2014-06-13 05:40:39.360', '820cc002-0290-4be8-9457-27766c0ebf1c', 352, 852, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization>', 62, '2014-06-13 05:40:39.360', '820cc002-0290-4be8-9457-27766c0ebf1c', 352, 853, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('as I am very interested in programming and statistics, Data Science seems like a great career path to me - I like both fields and would like to combine them. Unfortunately, I have studied political science with a non-statistical sounding Master. I focused on statistics in this Master, visiting optional courses and writing a statistical thesis on a rather large dataset.

Since almost all job adds are requiring  a degree in informatics, physics or some other techy-field, I am wondering if there is a chance to become a data scientist or if I should drop that idea.

I am lacking knowledge in machine learning, sql and hadoop, while having a rather strong informatics and statistics background.

So can somebody tell me how feasible my goal of becoming a data scientist is? ', 791, '2014-06-13 07:28:37.763', 'efb1b3b5-eeb6-474f-b3af-db977fcc930b', 354, 855, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data Science as a Social Scientist?', 791, '2014-06-13 07:28:37.763', 'efb1b3b5-eeb6-474f-b3af-db977fcc930b', 354, 856, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics>', 791, '2014-06-13 07:28:37.763', 'efb1b3b5-eeb6-474f-b3af-db977fcc930b', 354, 857, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The downvotes are because of the topic, but I''ll attempt to answer your question as best I can since it''s here.

Data science is a term that is thrown around as loosely as Big Data.  Everyone has a rough idea of what they mean by the term, but when you look at the actual work tasks, a data scientist''s responsibilities will vary greatly from company to company.

Statistical analysis could encompass the entirety of the workload in one job, and not even be a consideration for another.

I wouldn''t chase after a job title per se.  If you are interested in the field, network (like you are doing now) and find a good fit.  If you are perusing job ads, just look for the ones that stress statistical and informatics backgrounds.  Hadoop and SQL are both easy to become familiar with given the time and motivation, but I would stick with the areas you are strongest in and go from there.', 434, '2014-06-13 10:08:14.087', 'db82f1f0-b62d-4ff7-842f-62503aae8af3', 355, 858, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I attack this problem frequently with inefficiency because it''s always pretty low on the priority list and my clients are resistant to change until things break.  I would like some input on how to speed things up.

I have multiple datasets of information in a SQL database.  The database is vendor-designed, so I have little control over the structure.  It''s a sql representation of a class-based structure.  It looks a little bit like this:

    Main-class table
     -sub-class table 1
     -sub-class table 2
      -sub-sub-class table
     ...
     -sub-class table n

Each table contains fields for each attribute of the class.  A join exists which contains all of the fields for each of the sub-classes which contains all of the fields in the class table and all of the fields in each parent class'' table, joined by a unique identifier.

There are hundreds of classes. which means thousands of views and tens of thousands of columns.

Beyond that, there are multiple datasets, indicated by a field value in the Main-class table.  There is the production dataset, visible to all end users, and there are several other datasets comprised of the most current version of the same data from various integration sources.

Daily, we run jobs that compare the production dataset to the live datasets and based on a set of rules we merge the data, purge the live datasets, then start all over again.  The rules are in place because we might trust one source of data more than another for a particular value of a particular class.

The jobs are essentially a series of SQL statements that go row-by-row through each dataset, and field by field within each row.  The common changes are limited to a handful of fields in each row, but since anything can change we compare each value.

There are 10s of millions of rows of data and in some environments the merge jobs can take longer than 24 hours.  We resolve that problem generally, by throwing more hardware at it, but this isn''t a hadoop environment currently so there''s a pretty finite limit to what can be done in that regard.

How would you go about scaling a solution to this problem such that there were no limitations?  And how would you go about accomplishing the most efficient data-merge?  (currently it is field by field comparisons... painfully slow).', 434, '2014-06-13 10:57:10.623', 'dabc70e8-a683-4526-8894-13186ea578e2', 356, 859, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to best accomplish high speed comparison of like data?', 434, '2014-06-13 10:57:10.623', 'dabc70e8-a683-4526-8894-13186ea578e2', 356, 860, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<scaling>', 434, '2014-06-13 10:57:10.623', 'dabc70e8-a683-4526-8894-13186ea578e2', 356, 861, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<scaling><sql>', 97, '2014-06-13 11:34:23.160', '5bbe468c-237e-46a8-b52b-a95d8d6576a9', 356, 'Adding relevant tags', 862, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-13 11:34:23.160', '5bbe468c-237e-46a8-b52b-a95d8d6576a9', 356, 'Proposed by 97 approved by 434 edit id of 61', 863, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve now seen two data science certification programs - the [John Hopkins one available at Coursera][1] and the [Cloudera one][2].

I''m sure there are others out there.

The John Hopkins set of classes is focused on R as a toolset, but covers a range of topics:

* R Programming
* cleaning and obtaining data
* Data Analysis
* Reproducible Research
* Statistical Inference
* Regression Models
* Machine Learning
* Developing Data Products
* And what looks to be a Project based completion task similar to Cloudera''s Data Science Challenge


The Cloudera program looks thin on the surface, but looks to answer the two important questions - "Do you know the tools", "Can you apply the tools in the real world".  Their program consists of:

* Introduction to Data Science
* Data Science Essentials Exam
* Data Science Challenge (a real world data science project scenario)


I am not looking for a recommendation on a program or a quality comparison.

I am curious about other certifications out there, the topics they cover, and how seriously DS certifications are viewed at this point by the community.

EDIT: These are all great answers.  I''m choosing the correct answer by votes.

  [1]: https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage
  [2]: http://cloudera.com/content/cloudera/en/training/certification/ccp-ds.html', 434, '2014-06-13 11:35:51.697', 'd40cb776-efca-408f-8b23-2e3719f2b50b', 334, 'added 79 characters in body', 864, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I lead data science teams for a major Internet company and I have screened hundreds of profiles and interviewed dozens for our teams around the world. Many candidates have passed the aforementioned courses and programs or bring similar credentials. Personally, I have also taken the courses, some are good, others are disappointing but none of them makes you a "data scientist".

In general, I agree with the others here. A certificate from Coursera or Cloudera just signalizes an interest but it does not move the needle.  There is a lot more to consider and you can have a bigger impact by providing a comprehensive repository of your work (github profile for example) and by networking with other data scientists. Anyone hiring for a data science profile will always prefer to see your previous work and coding style/abilities.
', 797, '2014-06-13 12:18:53.830', 'c5248924-dad9-4af5-bc60-477bf131f529', 357, 865, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Sounds interesting. Could the solution be to dump the data out, build a fast custom processing thingie to run it through and then import it back to the database? I''ve seen some blazing fast Java-based text processing tools for topic modeling that handle millions of lines of text per second.

If it''s an option then you can build a shell script to first dump the data in as good as format as possible. Then some pre-processing to separate the datasets, then real processing of comparison and changes. Lastly something that writes it back to a good format for input into database.

Definately not a one-afternoon project, but you could probably get it to work in a couple of weeks.', 587, '2014-06-13 12:30:45.290', 'fa53a23b-d88c-43d8-a526-fab45912f857', 358, 866, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Becoming a Data Scientist  Curriculum via Metromap][1] is a popular reference for this kind of question.


  [1]: http://nirvacana.com/thoughts/becoming-a-data-scientist/', 743, '2014-06-13 13:49:35.777', '2652b506-ceae-4d7d-8b44-c52d4da4d016', 359, 867, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('That''s because something called [bias-variance dilema][1]. The overfitted model means that we will have more complex decision boundary if we give more variance on model. The thing is, not only too simple models but also complex models are likely to have dis-classified result on unseen data. Consequently, over-fitted model is not good as under-fitted model. That''s why overfitting is bad and we need to fit the model somewhere in the middle.


  [1]: http://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma', 801, '2014-06-13 14:46:30.393', '9822bfa6-f037-4407-abf6-0b13da70825b', 360, 868, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As a former analytics manager and a current lead data scientist, I am very leery of the need for data science certificates.  The term data scientist is pretty vague and the field of data science is in it''s infancy.  A certificates implies some sort of uniform standard which is just lacking in data science, it is still very much the wild west.

While a certificate is probably not going to hurt you, I think your time would be better spent developing the experience to know when to use a certain approach, and depth of understanding to be able to explain that approach to a non-technical audience.', 780, '2014-06-13 15:42:46.987', '20f414a8-53e6-4433-92c1-02ce3bf28f2c', 349, 'added a word', 869, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<efficiency><scalability><sql>', 84, '2014-06-13 15:47:16.340', '58793edf-97ac-473d-868f-90995dbf8b8f', 356, 'Changing tags.', 870, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-13 15:47:16.340', '58793edf-97ac-473d-868f-90995dbf8b8f', 356, 'Proposed by 84 approved by 434 edit id of 62', 871, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Logic often states that by overfitting a model, it''s capacity to generalize is increased. That said, clearly at some point underfitting a model cause models to become worse regardless of the complexity of data.

How do you know when your model has struck the right balance and is not underfitting the data it seeks to model?

----------

**Note:** This is a followup to my question, "[Why Is Overfitting Bad?][1]"


  [1]: http://datascience.stackexchange.com/questions/61/why-is-overfitting-bad/', 158, '2014-06-13 16:44:29.323', '8b2f3983-9014-4e8b-a9ab-c44545765afd', 361, 872, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When is a Model Underfitted?', 158, '2014-06-13 16:44:29.323', '8b2f3983-9014-4e8b-a9ab-c44545765afd', 361, 873, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<efficiency><algorithms><parameter>', 158, '2014-06-13 16:44:29.323', '8b2f3983-9014-4e8b-a9ab-c44545765afd', 361, 874, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Logic often states that by underfitting a model, it''s capacity to generalize is increased. That said, clearly at some point underfitting a model cause models to become worse regardless of the complexity of data.

How do you know when your model has struck the right balance and is not underfitting the data it seeks to model?

----------

**Note:** This is a followup to my question, "[Why Is Overfitting Bad?][1]"


  [1]: http://datascience.stackexchange.com/questions/61/why-is-overfitting-bad/', 158, '2014-06-13 16:55:54.003', 'a98a4f4d-8543-44b7-82f6-3989d4986287', 361, 'added 1 character in body', 875, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Logic often states that by overfitting a model, it''s capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regardless of the complexity of data, and if so, why?


----------

**Related:** Followup to the question above, "[When is a Model Underfitted?][1]"


  [1]: http://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted', 158, '2014-06-13 16:58:23.247', '1ac30de9-45cf-465a-b29d-9e26c6cde852', 61, 'added 191 characters in body', 876, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can''t you create a hash for each classes, and then merge rows by rows, field by field only the classes where the hash changed ? It should be faster if most of the classes don''t change..

Or a hash of each rows or maybe columns.. depending on how the data normally change.. ', 737, '2014-06-13 17:14:26.260', '80daeffc-c924-47a3-8b66-7f2701575ba3', 362, 879, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Models are but abstractions of what is seen in real life. They are designed in order to abstract-away nitty-gritties of the real system in observation, while keeping sufficient information to support desired analysis.

If a model is overfit, it takes into account too many details about what is being observed, and small changes on such object may cause the model to lose precision. On the other hand, if a model is underfit, it evaluates so few attributes that noteworthy changes on the object may be ignored.

Note also that underfit may be seen as an *overfit*, depending on the dataset. If your input can be 99%-correctly-classified with a single attribute, you *overfit* the model to the data by simplifying the abstraction to a single characteristic. And, in this case, you''d be generalizing too much the 1% of the base into the 99%-class -- or also specifying the model so much that it can only *see* one class.

A reasonable way to say that a model is neither over nor underfit is by performing cross-validations. You split your dataset into *k* parts, and say, pick one of them to perform your analysis, while using the other *k - 1* parts to train your model. Considering that the input itself is not biased, you should be able to have as much variance of data to train and evaluate as you''d have while using the model in real life processing.', 84, '2014-06-13 17:14:57.517', '7e0b96cb-cf8d-44c5-815b-999ba6f6aea4', 363, 880, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The [Programmer Competency Matrix][1] is just a set of skills, which are more likely to occur when being a real programmer than other skills, they are not a checklist to being a programmer, or for that matter, required to be a programmer; most common way to know someone is a programmer is that they''re paid to be a programmer, which honestly has nothing to do with programming skills.

To be a data scientist, do data science.


  [1]: http://sijinjoseph.com/programmer-competency-matrix/', 158, '2014-06-13 17:36:21.937', '81dc7276-200b-45cc-8a67-f9b768cf2ebf', 364, 881, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('@OP: Choosing answers by votes is the WORST idea.

Your question becomes a popularity contest.  You should seek the right answer, I doubt you know what you are asking, know what you are looking for.

To the your question:
Q: how seriously DS certifications are viewed at this point by the community.

A: What is your goal from taking these courses?  For work, for school, for self-improvement, etc?  Coursera classes are very applied, you will not learn much theory, those are intentionally reserved for classroom setting.

Nonetheless, Coursera classes is very useful.  I''d say it is equivalent to one year of stat grad class, out of a two year program.

I am not sure of its industry recognition yet, because the problem of how did you actually take the course?  How much time did you spend?  It''s a lot easier to get A''s in these courses than a classroom paper-pencil exam.  So, there is be a huge quality variability from person to person.', 386, '2014-06-13 18:59:11.493', 'b4e017e2-e660-4ba2-9307-ef603c40132a', 365, 882, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To answer your question it is important to understand the frame of reference you are looking for, if you are looking for what philosophically you are trying to achieve in model fitting, check out Rubens answer he does a good job of explaining that context.

However, in practice your question is almost entirely defined by business objectives.

To give a concrete example, lets say that you are a loan officer, you issued loans that are $3,000 and when people pay you back you make $50.  Naturally you are trying to build a model that predicts how if a person defaults on their loan.  Lets keep this simple and say that the outcomes are either full payment, or default.

From a business perspective you can sum up a models performance with a contingency matrix:

![enter image description here][1]

When the model predicts someone is going to default, do they?  To determining the downsides of over and under fitting I find it helpful to think of it as an optimization problem, because in each cross section of predicted verses actual model performance there is either a cost or profit to be made:

![enter image description here][2]

In this example predicting a default that is a default means avoiding any risk, and predicted a non-default which doesn''t default will make $50 per loan issued.  Where things get dicey is when you are wrong, if you default when you predicted non-default you lose the entire loan principal and if you predict default when a customer actually would not have you suffer $50 of missed opportunity.  The numbers here are not important, just the approach.

With this framework we can now begin to understand the difficulties associated with over and under fitting.

Over fitting in this case would mean that your model works far better on you development/test data then it does in production.  Or to put it another way, your model in production will far underperform what you saw in development, this false confidence will probably cause you to take on far more risky loans then you otherwise would and leaves you very vulnerable to losing money.

On the other hand, under fitting in this context will leave you with a model that just does a poor job of matching reality.  While the results of this can be wildly unpredictable, (the opposite word you want to describe your predictive models), commonly what happens is standards are tightened up to compensate for this, leading to less overall customers leading to lost good customers.

Under fitting suffers a kind of opposite difficulty that over fitting does, which is under fitting gives you lower confidence.  Insidiously, the lack of predictability still leads you to take on unexpected risk, all of which is bad news.

In my experience the best way to avoid both of these situations is validating your model on data that is completely outside the scope of your training data, so you can have some confidence that you have a representative sample of what you will see "in the wild".

Additionally, it is always a good practice to revalidate your models periodically, to determine how quickly your model is degrading, and if it is still accomplishing your objectives.

Just to some things up, your model is under fitted when it does a poor job of predicting both the development and production data.

  [1]: http://i.stack.imgur.com/RgWr9.png
  [2]: http://i.stack.imgur.com/78kH2.png', 780, '2014-06-13 20:13:01.913', 'bf4f9970-7438-4d76-8f69-6dcb4d575c0f', 366, 883, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('@OP: Choosing answers by votes is the WORST idea.

Your question becomes a popularity contest.  You should seek the right answer, I doubt you know what you are asking, know what you are looking for.

To answer your question:
Q: how seriously DS certifications are viewed at this point by the community.

A: What is your goal from taking these courses?  For work, for school, for self-improvement, etc?  Coursera classes are very applied, you will not learn much theory, those are intentionally reserved for classroom setting.

Nonetheless, Coursera classes is very useful.  I''d say it is equivalent to one year of stat grad class, out of a two year program.

I am not sure of its industry recognition yet, because the problem of how did you actually take the course?  How much time did you spend?  It''s a lot easier to get A''s in these courses than a classroom paper-pencil exam.  So, there is be a huge quality variability from person to person.', 386, '2014-06-14 03:55:00.533', '12799921-e5a4-48e3-a3ba-01e903d8dcb1', 365, 'added 3 characters in body', 885, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you know R and it''s ggplot library, you could try ggplot for python:


I like it, because I do work in R and python, and both are virtually identical.

But if you are not familiar you have to deal with a very "unpythonic" syntax. But I think it''s an easy library overall.', 791, '2014-06-14 07:34:37.643', 'c718e853-3751-4740-9135-2ebc2b6dc84d', 368, 886, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have read lot of blogs\article on how different type of industries are using Big Data Analytic. But most of these article fails to mention

 1. What kinda data these companies used. What was the size of the data
 2. What kinda of tools technologies they used to process the data
 3. What was the problem they were facing and how the insight they got the data helped them to resolve the issue.
 4. How they selected the tool\technology to suit their need.
 5. What kinda pattern they identified from the data & what kind of patterns they were looking from the data.

I wonder if someone can provide me answer to all these questions or a link which at-least answer some of the the questions. I am looking for real world example.

It would be great if someone share how finance industry is making use of Big Data Analytic.', 496, '2014-06-14 18:04:53.527', 'f7c34903-832d-4f98-8360-41a0052be101', 307, 'added 38 characters in body', 889, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What kind of error measures do each give and how do I know when to use one over the other? If you could give an example of when to use each, that would be great as well! Thanks so much!', 838, '2014-06-14 18:53:32.243', '9ac75fbf-5efc-4efc-90f8-8992e644183c', 369, 890, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Difference between using RMSE and nDCG to evaluate Recommender Systems?', 838, '2014-06-14 18:53:32.243', '9ac75fbf-5efc-4efc-90f8-8992e644183c', 369, 891, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><recommendation>', 838, '2014-06-14 18:53:32.243', '9ac75fbf-5efc-4efc-90f8-8992e644183c', 369, 892, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''d like to use my MSc thesis as an opportunity to explore ''data science''. Frankly the term seems a little vague to me (or at least, I''ve heard so many people apply it to so many situations that its become diluted), but I expect it requires a) machine learning (rather than traditional statistics) b) a large enough dataset that you have to run analyses on clusters. Anyway, we don''t have any relevantly qualified professors at my college, so I''d like a dataset and problem that is accessible to a statistician, but could allow me to foray into this data science thing. Any suggestions? To keep this as narrow as possible, I''d ideally like links to open, well used datasets and example problems.

Thanks!

PS - engineering background, so I''m fairly comfy with math/programming', 839, '2014-06-14 19:54:53.193', 'cea24788-07ef-4c46-ba61-13215c474ae4', 370, 893, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data Science oriented dataset/research question for Statistics MSc thesis', 839, '2014-06-14 19:54:53.193', 'cea24788-07ef-4c46-ba61-13215c474ae4', 370, 894, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics><education><knowledge-base><definitions>', 839, '2014-06-14 19:54:53.193', 'cea24788-07ef-4c46-ba61-13215c474ae4', 370, 895, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m curious about natural language querying.  Stanford has what looks to be a strong set of [software for processing natural language][1].  I''ve also seen the [Apache OpenNLP library][2], and the [General Architecture for Text Engineering][3].

There are an incredible amount of uses for natural language processing and that makes the documentation of these projects difficult to quickly absorb.

Can you simplify things for me a bit and at a high level outline the tasks necessary for performing a basic translation of simple questions into SQL?

The first rectangle on my flow chart is a bit of a mystery.

![enter image description here][4]

For example, I might want to know:

How many books were sold last month?

And I''d want that translated into

    Select count(*) from sales where item_type=''book'' and
      sales_date >= ''5/1/2014'' and sales_date <= ''5/31/2014''

  [1]: http://nlp.stanford.edu/software/index.shtml
  [2]: http://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html
  [3]: http://gate.ac.uk/science.html
  [4]: http://i.stack.imgur.com/wJPx9.png', 434, '2014-06-14 20:32:06.143', 'd86224c3-81f3-4b92-bfa3-4cc4d5abfb03', 371, 896, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to process natural language queries?', 434, '2014-06-14 20:32:06.143', 'd86224c3-81f3-4b92-bfa3-4cc4d5abfb03', 371, 897, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp>', 434, '2014-06-14 20:32:06.143', 'd86224c3-81f3-4b92-bfa3-4cc4d5abfb03', 371, 898, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m curious about natural language querying.  Stanford has what looks to be a strong set of [software for processing natural language][1].  I''ve also seen the [Apache OpenNLP library][2], and the [General Architecture for Text Engineering][3].

There are an incredible amount of uses for natural language processing and that makes the documentation of these projects difficult to quickly absorb.

Can you simplify things for me a bit and at a high level outline the tasks necessary for performing a basic translation of simple questions into SQL?

The first rectangle on my flow chart is a bit of a mystery.

![enter image description here][4]

For example, I might want to know:

    How many books were sold last month?

And I''d want that translated into

    Select count(*)
      from sales
      where
       item_type=''book'' and
       sales_date >= ''5/1/2014'' and
       sales_date <= ''5/31/2014''

  [1]: http://nlp.stanford.edu/software/index.shtml
  [2]: http://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html
  [3]: http://gate.ac.uk/science.html
  [4]: http://i.stack.imgur.com/wJPx9.png', 434, '2014-06-14 20:39:25.657', 'a226f314-d88b-48da-8c29-26f5476fee0d', 371, 'added 39 characters in body', 899, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Just head to kaggle.com; it''ll keep you busy for a long time. For open data there''s the [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/). In fact, there''s a whole [Stackexchange site](http://opendata.stackexchange.com/) devoted to this; look there.', 381, '2014-06-14 20:52:00.873', '47cb9752-ed35-4c77-b5ad-484a5aff71ac', 372, 900, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The [Sunlight Foundation][1] is an organization that is focused on opening up and encouraging non-partisan analysis of government data.

There is a ton of analysis out there in the wild that can be used for comparison, and a wide variety of topics.

They provide [tools][2] and [apis][3] for accessing data, and have helped push to make data available in places like [data.gov][4].

One interesting project is [Influence Explorer][5].  You can get [source data here][6] as well as access to real time data.

You might also want to take a look at one of our more popular questions:

[Publicly available datasets][7].


  [1]: http://sunlightfoundation.com/
  [2]: http://sunlightfoundation.com/tools/
  [3]: http://sunlightfoundation.com/api/
  [4]: http://www.data.gov/
  [5]: http://influenceexplorer.com/
  [6]: http://data.influenceexplorer.com/
  [7]: http://datascience.stackexchange.com/questions/155/publicly-available-datasets', 434, '2014-06-14 21:13:20.110', '290dda9d-c8be-419c-98db-53cbd6ce6bbf', 373, 901, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Natural language querying poses very many intricacies which can be very difficult to generalize.  From a high level, I would start with trying to think of things in terms of nouns and verbs.

So for the sentence: How many books were sold last month?

You would start by breaking the sentence down with a parser which will return a tree format similar to this:

![enter image description here][1]

You can see that there is a subject books, a compound verbal phrase indicating the past action of sell, and then a noun phrase where you have the time focus of a month.

We can further break down the subject for modifiers: "how many" for books, and "last" for month.

Once you have broken the sentence down you need to map those elements to sql language e.g.: how many => count, books => book, sold => sales, month => sales_date (interval), and so on.

Finally, once you have the elements of the language you just need to come up with a set of rules for how different entities interact with each other, which leaves you with:

Select count(*)
  from sales
  where
   item_type=''book'' and
   sales_date >= ''5/1/2014'' and
   sales_date <= ''5/31/2014''

This is at a high level how I would begin, while almost every step I have mentioned is non-trivial and really the rabbit hole can be endless, this should give you many of the dots to connect.



  [1]: http://i.stack.imgur.com/ogoiY.png', 780, '2014-06-14 21:39:15.833', '8553f00a-bb3a-4711-95ca-923d35691bb4', 374, 902, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The majority of people use S3. However, Google Drive seems a promising alternative solution for storing large amounts of data. Are there specific reasons why one is better than the other?', 418, '2014-06-14 23:52:10.490', 'fb390798-6841-4d9b-acce-551187037c7d', 375, 903, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Amazon S3 vs Google Drive', 418, '2014-06-14 23:52:10.490', 'fb390798-6841-4d9b-acce-551187037c7d', 375, 904, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 418, '2014-06-14 23:52:10.490', 'fb390798-6841-4d9b-acce-551187037c7d', 375, 905, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m developing a distributed algorithm, and to improve efficiency, it relies both on the number of disks (one per machine), and on an efficient load balance strategy. With more disks, we''re able to reduce the time spent with I/O; and with an efficient load balance policy, we can distribute tasks without much data replication overhead.

There are many studies on the literature that deal with the same problem, and each of them runs different experiments to evaluate their proposal. Some experiments are specific of the strategy presented, and some others, like weak scaling (scalability) and strong scaling (speedup), are common to all of the works.

My question is, since the experiments are usually executed over entirely different infrastructures (disks, processors, # machines, network), how can one compare the results obtained and point out efficiency gains? For example, I may get 100% of speedup in my application running on 10 machines with Infiniband connection, whereas I could get the same or even worse results if my connection was Ethernet.', 84, '2014-06-15 00:00:51.657', '70322382-dfd3-4fbe-84ae-c1a88bf70a7d', 376, 906, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to compare experiments run over different infrastructures', 84, '2014-06-15 00:00:51.657', '70322382-dfd3-4fbe-84ae-c1a88bf70a7d', 376, 907, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><efficiency><performance><scalability><distributed>', 84, '2014-06-15 00:00:51.657', '70322382-dfd3-4fbe-84ae-c1a88bf70a7d', 376, 908, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m developing a distributed algorithm, and to improve efficiency, it relies both on the number of disks (one per machine), and on an efficient load balance strategy. With more disks, we''re able to reduce the time spent with I/O; and with an efficient load balance policy, we can distribute tasks without much data replication overhead.

There are many studies on the literature that deal with the same problem, and each of them runs different experiments to evaluate their proposal. Some experiments are specific of the strategy presented, and some others, like weak scaling (scalability) and strong scaling (speedup), are common to all of the works.

The problem is the experiments are usually executed over entirely different infrastructures (disks, processors, # machines, network), and depending on what is being evaluated, it may raise *false/unfair* comparisons. For example, I may get 100% of speedup in my application running on 10 machines with Infiniband connection, whereas I could get the same or even worse results if my connection was Ethernet.

So, how can one honestly compare different experiments to point out efficiency gains? ', 84, '2014-06-15 00:05:58.457', 'b1fe9999-16c1-4fb9-90de-4ff1b36e29b0', 376, 'added 91 characters in body', 909, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From our perspective on here, the big benefit of S3 is the ease of accessing the data from within EC2.

Google Drive is directly accessible from the Google Cloud platform.

There are a host of other differences that might matter depending on your usage requirements, but that''s the one that would matter most around here.

The only other difference I can think of that would matter to the DS community is that when you are sharing something, you have no control of the address of a given file on google drive.', 434, '2014-06-15 00:11:54.950', '3ac4d239-c39d-4afc-bed7-c004df9a0959', 377, 910, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Financial Services is a big user of Big Data, and innovator too.  One example is mortgage bond trading.  To answer your questions for it:

> What kinda data these companies used. What was the size of the data?

- Long histories of each mortgage issued for the past many years, and payments by month against them.  (Billions of rows)
- Long histories of credit histories.  (Billions of rows)
- Home price indices.  (Not as big)

> What kinda of tools technologies they used to process the data?

It varies.  Some use in-house solutions built on databases like Netezza or Teradata.  Others access the data via systems provided by the data providers.  (Corelogic, Experian, etc)  Some banks use columnal database technologies like KDB, or 1010data.

> What was the problem they were facing and how the insight they got the
> data helped them to resolve the issue.

The key issue is determining when mortgage bonds (mortgage backed-securities) will prepay or default.  This is especially important for bonds that lack the government guarantee.  By digging into payment histories, credit files, and understanding the current value of the house, it''s possible to predict the likelihood of a default.  Adding an interest rate model and prepayment model also helps predict the likelihood of a prepayment.

> How they selected the tool\technology to suit their need.

If the project is driven by internal IT, usually it''s based off of a large database vendor like Oracle, Teradata or Netezza.  If it''s driven by the quants, then they are more likely to go straight to the data vendor, or a 3rd party "All in" system.

> What kinda pattern they identified from the data & what kind of
> patterns they were looking from the data.

Linking the data gives great insights into who is likely to default on their loans, and prepay them.  When you aggregated the loans into bonds, it can be the difference between a bond issued at $100,000,000 being worth that amount, or as little as $20,000,000.', 842, '2014-06-15 01:25:48.563', '89d42e1b-0d40-4800-a642-d7b236580b71', 379, 914, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I suspect this will get closed since it is very narrow, but my 2 cents...

Data Science requires 3 skills:

- Math/Stats
- Programming
- Domain Knowledge

It can be very hard to show all three.  #1 and #2 can be signaled via degrees, but a hiring manager who may not have them doesn''t want to trust a liberal arts degree.  If you''re looking to get into Data Science, position yourself as a domain expert first.  Publish election predictions.  If you''re correct, cite them.  That will get you noticed.

If you''re Domain knowledge is A+ level, you don''t need A+ level programming skills, but learn programming enough so that you don''t need someone else to fetch data for you.', 842, '2014-06-15 01:29:15.240', '7c185fa8-e4a1-465e-b03d-55148e457c50', 380, 915, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('CAPM (Capital Asset Pricing Model) in Finance is a classic example of an underfit model.  It was built on the beautiful theory that "Investors only pay for risk they can''t diversify away" so expected excess returns are equal to correlation to market returns.

As a formula [0] Ra = Rf + B (Rm - Rf)
where Ra is the expected return of the asset, Rf is the risk free rate, Rm is the market rate of return, and Beta is the correlation to the Equity premium (Rm - Rf)

This is beautiful, elegant, and wrong.  Investors seem to require a higher rate of small stocks and value (defined by book to market, or dividend yield) stocks.

Fama and French [1] presented an update to the model, which adds additional Betas for Size and Value.

So how do you know in a general sense?  When the predictions you are making are wrong, and another variable with a logical explanation increases the prediction quality.  It''s easy to understand why someone might think small stocks are risky, independent of non-diversifiable risk.  It''s a good story, backed by the data.


[0]  http://www.investopedia.com/terms/c/capm.asp
[1]  http://en.wikipedia.org/wiki/Fama%E2%80%93French_three-factor_model', 842, '2014-06-15 01:36:51.693', 'd67fc9ab-aaf0-4b1f-8de4-0ddb6b47ba53', 381, 916, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve came across the following problem, that I recon is rather typical.

I have some large data, say, a few million rows. I run some non-trivial analysis on it, e.g. an SQL query consisting of several sub-queries. I get some result, stating, for example, that property X is increasing over time.

Now, there are two possible things that could lead to that:

 1. X is indeed increasing over time
 1. I have a bug in my analysis

How can I test that the first happened, rather than the second? A step-wise debugger, even if one exists, won''t help, since intermediate results can still consist of millions of lines.

The only thing I could think of was to somehow generate a small, synthetic data set with the property that I want to test and run the analysis on it as a unit test. Are there tools to do this? Particularly, but not limited to, SQL.', 846, '2014-06-15 12:26:50.060', '08fec671-6226-4508-96aa-3534c93f6e32', 382, 918, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to debug data analysis?', 846, '2014-06-15 12:26:50.060', '08fec671-6226-4508-96aa-3534c93f6e32', 382, 919, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><sql><experiments>', 846, '2014-06-15 12:26:50.060', '08fec671-6226-4508-96aa-3534c93f6e32', 382, 920, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a binary classification problem:

 - Approximately 1000 samples
 - 10 attributes, including binary, numeric and categorical

Which algorithm is the best choice for this type of problem?

By default I''m going to start with SVM, as it is considered the best for relatively clean and  not noisy data. ', 97, '2014-06-15 14:01:38.233', 'f5a1b211-4925-470a-9ede-42735652cf94', 384, 923, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Choose binary classification algorithm', 97, '2014-06-15 14:01:38.233', 'f5a1b211-4925-470a-9ede-42735652cf94', 384, 924, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><svm>', 97, '2014-06-15 14:01:38.233', 'f5a1b211-4925-470a-9ede-42735652cf94', 384, 925, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a binary classification problem:

 - Approximately 1000 samples in training set
 - 10 attributes, including binary, numeric and categorical

Which algorithm is the best choice for this type of problem?

By default I''m going to start with SVM, as it is considered the best for relatively clean and  not noisy data. ', 97, '2014-06-15 14:11:08.710', 'ea359b5b-dd65-4bb5-a036-0683ab6405ab', 384, 'Add tag, minor changes in text', 928, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<classification><svm><binary>', 97, '2014-06-15 14:11:08.710', 'ea359b5b-dd65-4bb5-a036-0683ab6405ab', 384, 'Add tag, minor changes in text', 929, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For low parameters, pretty limited sample size,  and a binary classifier logistic regression should be plenty powerful enough.  You can use a more advanced algorithm but it''s probably overkill. ', 780, '2014-06-15 14:23:19.793', 'db8edcfd-876b-403d-a703-153302ba19fc', 386, 931, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I once heard that filtering spam by using blacklists is not a good approach, since some user searching for entries in your dataset may be looking for particular information from the sources blocked. Also it''d become a burden to continuously validate the *current state* of each spammer blocked, checking if the site/domain still disseminate spam data.

Considering that any approach must be efficient and scalable, so as to support filtering on very large datasets, what are the strategies available to get rid of spam in a non-biased manner?', 84, '2014-06-15 15:11:29.970', 'f88175ab-9c83-46cc-9d41-a1553b7db775', 387, 932, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Filtering spam from retrieved data', 84, '2014-06-15 15:11:29.970', 'f88175ab-9c83-46cc-9d41-a1553b7db775', 387, 933, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><efficiency>', 84, '2014-06-15 15:11:29.970', 'f88175ab-9c83-46cc-9d41-a1553b7db775', 387, 934, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Spam filtering, especially in email, has been revolutionized by neural networks, here are a couple papers that provide good reading on the subject:

On Neural Networks And The Future Of Spam
A. C. Cosoi, M. S. Vlad, V. Sgarciu
http://ceai.srait.ro/index.php/ceai/article/viewFile/18/8

Intelligent Word-Based Spam Filter Detection Using
Multi-Neural Networks
Ann Nosseir, Khaled Nagati and Islam Taj-Eddin
http://www.ijcsi.org/papers/IJCSI-10-2-1-17-21.pdf

Spam Detection using Adaptive Neural Networks: Adaptive Resonance Theory
David Ndumiyana, Richard Gotora, and Tarisai Mupamombe
http://onlineresearchjournals.org/JPESR/pdf/2013/apr/Ndumiyana%20et%20al.pdf', 780, '2014-06-15 15:20:31.300', 'c8521f86-033c-4cb3-900e-ddf4926c118d', 388, 935, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I once heard that filtering spam by using blacklists is not a good approach, since some user searching for entries in your dataset may be looking for particular information from the sources blocked. Also it''d become a burden to continuously validate the *current state* of each spammer blocked, checking if the site/domain still disseminate spam data.

Considering that any approach must be efficient and scalable, so as to support filtering on very large datasets, what are the strategies available to get rid of spam in a non-biased manner?

**Edit**: if possible, any example of strategy, even if just the intuition behind it, would be very welcome along with the answer.', 84, '2014-06-15 15:23:01.007', '6261454c-4775-4e16-9c81-f1ce1b0ed4b2', 387, 'added 134 characters in body', 936, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a binary classification problem:

 - Approximately 1000 samples in training set
 - 10 attributes, including binary, numeric and categorical

Which algorithm is the best choice for this type of problem?

By default I''m going to start with SVM (preliminary having nominal attributes values converted to binary features), as it is considered the best for relatively clean and  not noisy data. ', 97, '2014-06-15 15:23:12.657', '7a561167-a9e3-471d-8381-84238951bb76', 384, 'More details', 937, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here is a suggestion:

 - Code your analysis in such a way that it can be run on sub-samples.
 - Code a complementary routine which can sample, either randomly, or by time, or by region, or ...  This may be domain-specific. This is where your knowledge enters.
 - Combine the two and see if the results are stable across subsamples.', 515, '2014-06-15 15:49:12.907', 'a44e941f-8cfd-4a96-b0fa-e5e09fb3ccd8', 389, 938, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('@OP: Choosing answers by votes is the WORST idea.

Your question becomes a popularity contest.  You should seek the right answer, I doubt you know what you are asking, know what you are looking for.

To answer your question:
Q: how seriously DS certifications are viewed at this point by the community.

A: What is your goal from taking these courses?  For work, for school, for self-improvement, etc?  Coursera classes are very applied, you will not learn much theory, they are intentionally reserved for classroom setting.

Nonetheless, Coursera classes are very useful.  I''d say it is equivalent to one year of stat grad class, out of a two year Master program.

I am not sure of its industry recognition yet, because the problem of how did you actually take the course?  How much time did you spend?  It''s a lot easier to get A''s in these courses than a classroom paper-pencil exam.  So, there is be a huge quality variation from person to person.', 386, '2014-06-15 15:56:06.780', '923303ba-3b1c-49bd-b3e0-01f7c16eb27b', 365, 'added 9 characters in body', 939, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When categorical variables are in the mix, I reach for Random Decision Forests, as it handles categorical variables directly without the 1-of-n encoding transformation. This loses less information.', 21, '2014-06-15 16:07:35.543', 'e61d4f45-b5e3-46df-af17-9e86e3ef3130', 390, 940, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is what I normally do - take up the most important variables (basis your business understanding and hypothesis - you can always revise it later), group by on these attributes to reduce the number of rows, which can then be imported into a Pivot. You should include the sum and count of the relevant metrics on each row.

Make sure that you don''t put any filters in the previous step. Once you have entire data at a summarized level, you can play around in Pivot tables and see what things are changing / increasing or decreasing.

If the data is too big to be summarized even on important parameters, you need to partition it in 3 - 4 subsets and then do this again.

Hope it helps.', 735, '2014-06-15 16:59:50.190', '5bb6eaeb-cb21-4990-87ad-07917213250f', 391, 941, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Spam filtering, especially in email, has been revolutionized by neural networks, here are a couple papers that provide good reading on the subject:

On Neural Networks And The Future Of Spam
A. C. Cosoi, M. S. Vlad, V. Sgarciu
http://ceai.srait.ro/index.php/ceai/article/viewFile/18/8

Intelligent Word-Based Spam Filter Detection Using
Multi-Neural Networks
Ann Nosseir, Khaled Nagati and Islam Taj-Eddin
http://www.ijcsi.org/papers/IJCSI-10-2-1-17-21.pdf

Spam Detection using Adaptive Neural Networks: Adaptive Resonance Theory
David Ndumiyana, Richard Gotora, and Tarisai Mupamombe
http://onlineresearchjournals.org/JPESR/pdf/2013/apr/Ndumiyana%20et%20al.pdf

EDIT:
The basic intuition behind using a neural network to help with spam filtering is by providing a weight to terms based on how often they are associated with spam.

Neural networks can be trained most quickly in a supervised -- you explicitly provide the classification of the sentence in the training set -- environment.  Without going into the nitty gritty the basic idea can be illustrated with these sentences:

Text = "How is the loss of the Viagra patent going to affect Pfizer", Spam = false
Text = "Cheap Viagra Buy Now", Spam = true
Text = "Online pharmacy Viagra Cialis Lipitor", Spam = true

For a two stage neural network, the first stage will calculate the likelihood of spam based off of if the word exists in the sentence.  So from our example:

viagra => 66%
buy => 100%
Pfizer => 0%
etc..

Then for the second stage the results in the first stage are used as variables in the second stage:

viagra & buy => 100%
Pfizer & viagra=> 0%

This basic idea is run for many of the permutations of the all the words in your training data.  The end results once trained is basically just an equation that based of the context of the words in the sentence can assign a probability of being spam.  Set spamminess threshold, and filter out any data higher then said threshold.', 780, '2014-06-15 21:22:10.500', '18f28bf6-d8df-4265-a31d-66f88d46209c', 388, 'Added some intuition', 946, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Linear SVM should be a good starting point. Take a look at [this][1] guide to choose the right estimator.


  [1]: http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html', 478, '2014-06-15 22:33:17.670', 'dbdec824-642c-41f7-b4e9-7c6bdfb06307', 393, 947, '2');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-06-15 22:35:42.893', '9e8e614c-28a3-4acd-96ac-87442438ab74', 224, '3', 948, '34');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Personally, we use S3 on top of GCE and really love it. Depending on how much data you''re dealing with, Google Drive just doesn''t quite match the 5 TB max that S3 gives you. Also, if you''re using python, `boto` does a pretty fantastic job of making most aws services pretty accessible regardless of what stack you''re dealing with. Even if you''re not using python, they''ve got a pretty straightforward API that generally is more accessible than Google Drive.

Instead of google drive, though google did recently release a cloud storage service, apart from drive, that lets you more closely integrate your storage with any gce instance you''ve got, https://cloud.google.com/products/cloud-storage/

They''ve got an API which seems to be pretty comparable to S3''s, but I can''t profess to having really played around with it much. Pricing-wise the two are identical, but I think that the large community and experience with aws in general still puts S3 squarely above both google''s cloud storage and google drive.', 548, '2014-06-16 04:21:36.340', 'ee2dc973-a8c5-4ea3-8e85-bc24ef7c18c5', 394, 949, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It''s hard to say without knowing a little more about your dataset, and how separable your dataset is based on your feature vector, but I would probably suggest using extreme random forest over standard random forests because of your relatively small sample set.

Extreme random forests are pretty similar to standard random forests with the one exception that instead of optimizing splits on trees, extreme random forest makes splits at random. Initially this would seem like a negative, but it generally means that you have significantly better generalization and speed, though the AUC on your training set is likely to be a little worse.

Logistic regression is also a pretty solid bet for these kinds of tasks, though with your relatively low dimensionality and small sample size I would be worried about overfitting. You might want to check out using K-Nearest Neighbors since it often performs very will with low dimensionalities, but it doesn''t usually handle categorical variables very well.

If I had to pick one without knowing more about the problem I would certainly place my bets on extreme random forest, as it''s very likely to give you good generalization on this kind of dataset, and it also handles a mix of numerical and categorical data better than most other methods.', 548, '2014-06-16 04:37:58.817', '60e9784f-2f61-41be-9dd2-ef822c1aebe7', 395, 951, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Alex made a number of good points, though I might have to push back a bit on his implication that DBSCAN is the best clustering algorithm to use here. Depending on your implementation, and whether or not you''re using accelerated indices (many implementations do not), your time and space complexity will both be `O(n2)`, which is far from ideal.

Personally, my go-to clustering algorithms are OpenOrd for winner-takes-all clustering and FLAME for fuzzy clustering. Both methods are indifferent to whether the metrics used are similarity or distance (FLAME in particular is nearly identical in both constructions). The implementation of OpenOrd in Gephi is `O(nlogn)` and is known to be more scalable than any of the other clustering algorithms present in the Gephi package.

FLAME on the other hand is great if you''re looking for a fuzzy clustering method. While the complexity of FLAME is a little harder to determine since it''s an iterative process, it has been shown to be sub-quadratic, and similar in run-speed to knn.', 548, '2014-06-16 04:51:47.847', '16326f5e-d2be-4ec3-aea1-f1c85d6a6ffe', 396, 952, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First you need to verify that your implementation of the algorithm is accurate. For that use a small sample of data and check whether the result is correct. At this stage the sample doesn''t need to be representative of the population.

Once the implementation is verified, you need to verify that there is a significant relationship among the variables that you try to predict. To do that define null hypothesis and try to reject the null hypothesis with a significant confidence level. ([hypothesis testing for linear regression][1])

There might be unit test frameworks for your SQL distribution. But using a programming language like R will be more easier to implement.


  [1]: http://stattrek.com/regression/slope-test.aspx', 733, '2014-06-16 06:34:53.683', '9dd5db07-7786-46d0-a160-927dc31db4af', 397, 953, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m currently in the very early stages of preparing a new research-project (still at the funding-application stage), and expect that data-analysis and especially visualisation tools will play a role in this project.

In view of this I face the following dilemma: Should I learn Python to be able to use its extensive scientific libraries (Pandas, Numpy, Scipy, ...), or should I just dive into similar packages of a language I''m already acquainted with (Racket, or to a lesser extent Scala)?

(Ideally I would learn Python in parallel with using statistical libraries in Racket, but I''m not sure I''ll have time for both)

I''m not looking for an answer to this dilemma, but rather for feedback on my different considerations:

My current position is as follows:

**In favour of Python:**

+ Extensively used libraries
+ Widely used (may be decisive in case of collaboration with others)
+ A lot of online material to start learning it
+ Conferences that are specifically dedicated to Scientific Computing with Python
+ Learning Python won''t be a waste of time anyway

**In favour of a language I already know:**

+ It''s a way to deepen my knowledge of one language rather than getting superficial knowledge of one more language (under the motto: you should at least know one language really well)
+ It is feasible. Both Racket and Scala have good mathematics and statistics libraries
+ I can start right away with learning what I need to know rather than first having to learn the basics

**Two concrete questions:**

1. What am I forgetting?
2. How big of a nuisance could the Python 2 vs 3 issue be?
', 872, '2014-06-16 07:32:29.137', '8cde3abb-ecd1-4ead-8b55-624be9da6eaa', 398, 954, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What to consider before learning a new language for data analysis', 872, '2014-06-16 07:32:29.137', '8cde3abb-ecd1-4ead-8b55-624be9da6eaa', 398, 955, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><visualization>', 872, '2014-06-16 07:32:29.137', '8cde3abb-ecd1-4ead-8b55-624be9da6eaa', 398, 956, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('According to me, all the factors, you have mentioned are superficial in nature. You have not considered the core of tool selection. In this case, there are 2 aspects, you mentioned:

1. Data analysis - What kind of analysis are you working on? There might be some analysis which are easier in some languages and more difficult in other.

2. Visualization - R provides similar community and learning material (as Python) and has the best visualizations compared to other languages here.

At this stage, you can be flexible with what language to learn, since you are starting from scratch.

Hope this helps.', 735, '2014-06-16 09:52:24.913', 'b838af25-8fab-465c-887a-2693b94b777c', 399, 957, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From my experience, the points to keep in mind when considering a data analysis platform are:

  1. Can it handle the size of the data that I need? If your data sets fit in memory, there''s usually no big trouble, although AFAIK Python is somewhat more memory-efficient than R. If you need to handle larger-than-memory data sets, the platform need to handle it conveniently. In this case, SQL would cover for basic statistics, Python + Apache Spark is another option.
  1. Does the platform covers all of my analysis needs? The greatest annoyance I''ve encountered in data mining projects is having to juggle between several tools, because tool A handles web connections well, tool B does the statistics and tool C renders nice pictures. You want your weapon-of-choice to cover as many aspects of your projects as possible. When considering this issue, Python is very comprehensive, but R has a lot of build-in statistical tests ready-to-use, if that''s what you need. ', 846, '2014-06-16 11:45:05.800', '016ab287-599f-454b-817c-3cff220ce9c3', 400, 958, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I like a multiple step strategy:

1. Write clean easy to understand code, as opposed to short-tricky code. I know statisticians like tricky code, but spotting problems in tricky code is dangerous.
( I am mentioning this because a supervisor of mine was fond of undocumented 500 lines python scrips - have fun debugging that mess and I have seen that pattern a lot, especially from people who are not from an IT background)

2. Break down your code in smaller functions, which can be tested and evaluated in smaller stes.

3. Look for connected elements, e.g. the number of cases with condition X is Y - so this query MUST return Y. Most often this is more complex, but doable.

4. When you are running your script the first time, test it with a small subsample and carefully check if everything is in order. While I like unit tests in IT, bugs in statistics scripts are often so pronounced that they are easily visible doing a carefully check. Or they are methodical errors, which are probably never caught by unit tests.

That should suffice to ensure a clean "one - off " job. But for a time series as you seem to have, I would add that you should check for values out of range, impossible combinations etc. For me, most scripts that have reached step 4 are probably bug free - and they will stay that way unless something changes. And most often, the data are changing - and that is something which should be checked for every run. Writing code for that can be time consuming and annoying, but it beats subtle errors due to data entry errors.', 791, '2014-06-16 13:21:58.777', '3cb91df8-cf88-404f-a372-e59e40c54277', 401, 960, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The [brat annotation tool][1] might be useful for you as per my comment.  I have tried many of them and this is the best I have found.  It has a nice user interface and can support a number of different types of annotations.  The annotations are stored in a separate .annot file which contain each annotation as well as its location within the original document.  A word of warning though, if you ultimately want to feed the annotations into a classifier like the Stanford NER tool then you will have to do some manipulation to get the data into a format that it will accept.


  [1]: http://brat.nlplab.org/', 387, '2014-06-16 13:25:48.453', 'ef42f839-f3bf-4e9d-b717-dee913c9d7fc', 402, 961, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('![enter image description here][1]I am trying to do Logistic Regression using SAS Enterprise Miner.
My Independent variables are
CPR/Inc (Categorical 1 to 7)
OD/Inc (Categorical 1 to 4)
Insurance (Binary 0 or 1)
Income Loss (Binary 0 or 1)
Living Arrangement (Categorical 1 to 7)
Employment Status (categorical 1 to 8)

My Dependent Variable is Default (Binary 0 or 1)

The following is the output from running Regression Model.

 Analysis of Maximum Likelihood Estimates

                                  Standard          Wald
Parameter       DF    Estimate       Error    Chi-Square    Pr > ChiSq    Exp(Est)

Intercept        1     -0.4148      0.0645         41.30        <.0001       0.660
CPR___Inc  1     1     -0.8022      0.1051         58.26        <.0001       0.448
CPR___Inc  2     1     -0.4380      0.0966         20.57        <.0001       0.645
CPR___Inc  3     1      0.3100      0.0871         12.68        0.0004       1.363
CPR___Inc  4     1    -0.00304      0.0898          0.00        0.9730       0.997
CPR___Inc  5     1      0.1331      0.0885          2.26        0.1324       1.142
CPR___Inc  6     1      0.1694      0.0881          3.70        0.0546       1.185
Emp_Status 1     1     -0.2289      0.1006          5.18        0.0229       0.795
Emp_Status 2     1      0.4061      0.0940         18.66        <.0001       1.501
Emp_Status 3     1     -0.2119      0.1004          4.46        0.0347       0.809
Emp_Status 4     1      0.1100      0.0963          1.30        0.2534       1.116
Emp_Status 5     1     -0.2280      0.1007          5.12        0.0236       0.796
Emp_Status 6     1      0.3761      0.0943         15.91        <.0001       1.457
Emp_Status 7     1     -0.3337      0.1026         10.59        0.0011       0.716
Inc_Loss   0     1     -0.1996      0.0449         19.76        <.0001       0.819
Insurance  0     1      0.1256      0.0559          5.05        0.0246       1.134
Liv_Arran  1     1     -0.1128      0.0916          1.52        0.2178       0.893
Liv_Arran  2     1      0.2576      0.0880          8.57        0.0034       1.294
Liv_Arran  3     1      0.0235      0.0904          0.07        0.7950       1.024
Liv_Arran  4     1      0.0953      0.0887          1.16        0.2825       1.100
Liv_Arran  5     1     -0.0493      0.0907          0.29        0.5871       0.952
Liv_Arran  6     1     -0.3732      0.0966         14.93        0.0001       0.689
OD___Inc   1     1     -0.2136      0.0557         14.72        0.0001       0.808
OD___Inc   2     1     -0.0279      0.0792          0.12        0.7248       0.973
OD___Inc   3     1     -0.0249      0.0793          0.10        0.7534       0.975


Now I used this Model to Score a new set of data. An example row of my new data is
CPR - 7
OD - 4
Living Arrangement - 4
Employment Status - 4
Insurance - 0
Income Loss - 1

For this sample row, the model predicted output (Probability of default = 1) as 0.7335
To check this manually, I added the estimates
Intercept + Emp Status 4 + Liv Arran 4 + Insurance 0
-0.4148   + 0.1100  +   0.0953   +   0.1256    =   -0.0839

Odds ratio = Exponential(-0.0839) = 0.9195

Hence probability = 0.9195 / (1 + 0.9195)  =   0.4790

I am unable to understand why there is such a mismatch between the Model''s predicted probability and theoretical probability.

Any help would be much appreciated .
Thanks


  [1]: http://i.stack.imgur.com/4Ih6o.png', 880, '2014-06-16 13:30:01.320', 'a01e7889-e1b8-4b5b-95ec-3cc4d96974d3', 403, 962, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Help with Logistic Regression', 880, '2014-06-16 13:30:01.320', 'a01e7889-e1b8-4b5b-95ec-3cc4d96974d3', 403, 963, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<categorical-data>', 880, '2014-06-16 13:30:01.320', 'a01e7889-e1b8-4b5b-95ec-3cc4d96974d3', 403, 964, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<classification><binary><svm><random-forest><logistic-regression>', 97, '2014-06-16 14:02:42.467', '4c1ae7a3-e860-4ddd-b3a3-841f4038c3ba', 384, 'Add tag', 965, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Personally I would advocate using something that is both not-specific to the NLP field, and something that is sufficiently general that it can still be used as a tool even when you''ve started moving beyond this level of metadata. I would especially pick a format that can be used regardless of development environment and one that can keep some basic structure if that becomes relevant (like tokenization)

It might seem strange, but I would honestly suggest `JSON`. It''s extremely well supported, supports a lot of structure, and is flexible enough that you shouldn''t have to move from it for not being powerful enough. For your example, something like this:

    {''text'': ''I saw the company''s manager last day.", {''Person'': {''name'': ''John''}, {''indices'': [0:1]}, etc...}

The one big advantage you''ve got over any NLP-specific formats here is that `JSON` can be parsed in any environment, and since you''ll probably have to edit your format anyway, JSON lends itself to very simple edits that give you a short distance to other formats.

You can also implicitly store tokenization information if you want:

    {"text": ["I", "saw", "the", "company''s", "manager", "last", "day."]}

', 548, '2014-06-16 14:35:20.980', '5eba5c03-3606-433b-93de-ffa39d8627dc', 404, 966, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Personally going to make a strong argument in favor of Python here. There are a large number of reasons for this, but I''m going to build on some of the points that other people have mentioned here:

 1. **Picking a single language:** It''s definitely possible to mix and match languages, picking `d3` for your visualization needs, `FORTRAN` for your fast matrix multiplies, and `python` for all of your networking and scripting. You can do this down the line, but keeping your stack as simple as possible is a good move, especially early on.
 2. **Picking something bigger than you:** You never want to be pushing up against the barriers of the language you want to use. This is a huge issue when it comes to languages like `Julia` and `FORTRAN`, which simply don''t offer the full functionality of languages like `python` or `R`.
 3. **Pick Community**: The one most difficult thing to find in any language is community. `Python` is the clear winner here. If you get stuck, you ask something on SO, and someone will answer in a matter of minutes, which is simply not the case for most other languages. If you''re learning something in a vacuum you will simply learn much slower.

In terms of the minus points, I might actually push back on them.

Deepening your knowledge of one language is a decent idea, but knowing *only* one language, without having practice generalizing that knowledge to other languages is a good way to shoot yourself in the foot. I have changed my entire favored development stack three time over as many years, moving from `MATLAB` to `Java` to `haskell` to `python`. Learning to transfer your knowledge to another language is far more valuable than just knowing one.

As far as feasibility, this is something you''re going to see again and again in any programming career. Turing completeness means you could technically do everything with `HTML4` and `CSS3`, but you want to pick the right tool for the job. If you see the ideal tool and decide to leave it by the roadside you''re going to find yourself slowed down wishing you had some of the tools you left behind.

A great example of that last point is trying to deploy `R` code. ''R''''s networking capabilities are hugely lacking compared to `python`, and if you want to deploy a service, or use slightly off-the-beaten path packages, the fact that `pip` has an order of magnitude more packages than `CRAN` is a huge help.', 548, '2014-06-16 15:00:04.577', '14ef9f5c-ebc6-4b24-a0fe-595bc33503d9', 405, 967, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I have a retail store and have a way to measure how many people enter my store every minute, and timestamp that data, how can I predict future foot traffic?

I have looked into machine learning algorithms, but I''m not sure which one to use.  In my test data, a year over year trend is more accurate compared to other things I''ve tried, like KNN(with what I think are sensible parameters and distance function).

It almost seems like this could be similar to financial modeling, where you deal with time series data.  Any ideas?

Thanks.', 886, '2014-06-16 15:49:55.673', '2da099ac-2471-4f38-b940-b257ee15937d', 406, 968, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How can I predict traffic based on previous time series data?', 886, '2014-06-16 15:49:55.673', '2da099ac-2471-4f38-b940-b257ee15937d', 406, 969, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><time-series>', 886, '2014-06-16 15:49:55.673', '2da099ac-2471-4f38-b940-b257ee15937d', 406, 970, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Personally I would advocate using something that is both not-specific to the NLP field, and something that is sufficiently general that it can still be used as a tool even when you''ve started moving beyond this level of metadata. I would especially pick a format that can be used regardless of development environment and one that can keep some basic structure if that becomes relevant (like tokenization)

It might seem strange, but I would honestly suggest `JSON`. It''s extremely well supported, supports a lot of structure, and is flexible enough that you shouldn''t have to move from it for not being powerful enough. For your example, something like this:

    {''text'': ''I saw the company''s manager last day.", {''Person'': [{''name'': ''John''}, {''indices'': [0:1]}, etc...]}

The one big advantage you''ve got over any NLP-specific formats here is that `JSON` can be parsed in any environment, and since you''ll probably have to edit your format anyway, JSON lends itself to very simple edits that give you a short distance to other formats.

You can also implicitly store tokenization information if you want:

    {"text": ["I", "saw", "the", "company''s", "manager", "last", "day."]}

', 548, '2014-06-16 16:04:45.320', '9d38a416-2b6b-4f73-8b90-0e7368a6d4cb', 404, 'added 2 characters in body', 971, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Other answers recommended a good set of books about the mathematics behind data science. But as you mentioned, its not just mathematics and activities like data collection and inference from data has their own rules and theories, even if not being as rigorous as mathematical backgrounds (yet).

For theses parts, I suggest the book [Beautiful Data: The Stories Behind Elegant Data Solutions][1] which contains twenty case-study like chapters written by people really engaged with real world data analysis problems. It does not contain any mathematics, but explores areas like collecting data, finding practical ways of using data in analyses, scaling and selecting the best solutions very well.

Another really interesting book is [Thinking with Data: How to Turn Information into Insights][2], which is not technical (=programming tutorial) either, but covers important topics on how to really use the data science power in decision making and real world problems.


  [1]: http://www.amazon.com/Beautiful-Data-Stories-Elegant-Solutions/dp/0596157118
  [2]: http://www.amazon.com/Thinking-Data-Turn-Information-Insights/dp/1449362931', 227, '2014-06-16 16:10:34.423', '1c7a3a0f-47b7-46bf-9f95-7e25624a46c5', 407, 972, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The problem with models like KNN is that they do not take into account seasonality (time-dependent variations in trend).  To take those into account, you should use Time Series analysis.

For count data, such as yours, you can use generalized linear auto-regressive moving average models (GLARMA).  Fortunately, there is an R package that implements them ([glarma](http://cran.r-project.org/web/packages/glarma/index.html)).

The [vignette](http://cran.r-project.org/web/packages/glarma/vignettes/glarma.pdf) is a good resource for the theory behind the tool.', 178, '2014-06-16 16:34:50.317', 'a3ca1e61-13f4-4d8d-b1ba-adefdd09b7d9', 408, 974, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think Christopher''s answers above are entirely sensible.  As an alternate approach (or perhaps just in addition to the advise he''s given), I might start by just visualizing the data a bit to try get a rough sense of what''s going on.

If you haven''t already done this, you might try adding a date''s month and day of week as features -- if you end up sticking with KNN, this will help the model pick up seasonality.

As a different way of taking this on, you might consider starting with a really, really basic model (like OLS).. these often go a long way in generating reasonable predictions.

Finally, the more we know about your data, the easier it will be for us to help generate suggestions -- What time frame are you observing?  What are the features you''re currently using?  etc.

Hope this helps --


 ', 889, '2014-06-16 17:26:17.697', 'ce2064b8-de4b-4021-b273-0b6f84594c08', 409, 975, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Personally I would advocate using something that is both not-specific to the NLP field, and something that is sufficiently general that it can still be used as a tool even when you''ve started moving beyond this level of metadata. I would especially pick a format that can be used regardless of development environment and one that can keep some basic structure if that becomes relevant (like tokenization)

It might seem strange, but I would honestly suggest `JSON`. It''s extremely well supported, supports a lot of structure, and is flexible enough that you shouldn''t have to move from it for not being powerful enough. For your example, something like this:

    {''text'': ''I saw the company''s manager last day.", {''Person'': [{''name'': ''John''}, {''indices'': [0:1]}, etc...]}

The one big advantage you''ve got over any NLP-specific formats here is that `JSON` can be parsed in any environment, and since you''ll probably have to edit your format anyway, JSON lends itself to very simple edits that give you a short distance to other formats.

You can also implicitly store tokenization information if you want:

    {"text": ["I", "saw", "the", "company''s", "manager", "last", "day."]}

EDIT: To clarify the mapping of metadata is pretty open, but here''s an example:

    {''body'': ''<some_text>'',
     ''metadata'':
      {''<entity>'':
        {''<attribute>'': ''<value>'',
         ''location'': [<start_index>, <end_index>]
        }
      }
    }

Hope that helps, let me know if you''ve got any more questions.', 548, '2014-06-16 17:35:41.207', 'd13c3c12-c88a-499e-a25a-30e6825a3f15', 404, 'added 2 characters in body', 976, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m currently working on implementing sgd for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate. Is the learning rate related to the shape of the error gradient, as it dictates the rate of descent? If so, how do you use this information to inform your decision about a value? If it''s not what sort of values should I choose, and how should I choose them. It seems like you would want small values to avoid overshooting, but how do you choose one such that you don''t get stuck in local minima or take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?

tl:dr How do I choose the learning rate for sgd?', 890, '2014-06-16 18:08:38.623', '5961e11a-d20e-4c7a-a1b0-f26ca4236a41', 410, 977, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Choosing a learning rate', 890, '2014-06-16 18:08:38.623', '5961e11a-d20e-4c7a-a1b0-f26ca4236a41', 410, 978, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><algorithms><neuralnetwork>', 890, '2014-06-16 18:08:38.623', '5961e11a-d20e-4c7a-a1b0-f26ca4236a41', 410, 979, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m currently working on implementing Stochastic Gradient Descent (SGD) for neural nets using backpropagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.

- Is the learning rate related to the shape of the error gradient, as it dictates the rate of descent?
- If so, how do you use this information to inform your decision about a value?
- If it''s not what sort of values should I choose, and how should I choose them?
- It seems like you would want small values to avoid overshooting, but how do you choose one such that you don''t get stuck in local minima or take to long to descend?
- Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?

In short: How do I choose the learning rate for SGD?', 322, '2014-06-16 18:47:55.867', '5ad02684-fef8-4da9-b245-704b151ffc02', 410, 'improved formatting', 980, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-16 18:47:55.867', '5ad02684-fef8-4da9-b245-704b151ffc02', 410, 'Proposed by 322 approved by 890 edit id of 67', 981, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It seems as though most languages have some number of scientific computing libraries available.

 - Python has `Scipy`
 - `Rust` has `SciRust`
 - `C++` has several including `ViennaCL` and `Armadillo`
 - `Java` has `Java Numerics` and `Colt` as well as several other

Not to mention languages like `R` and `Julia` designed explicitly for scientific computing.

 With so many options how do you choose the best language for a task? Additionally which languages will be the most performant? `Python` and `R` seem to have the most traction in the space, but logically a compiled language seems like it would be a better choice. Additionally compiled languages tend to have GPU acceleration, while interpreted languages like `R` and `Python` don''t. What should I take into account when choosing a language, and which languages provide the best balance of utility and performance? Also are there any languages with significant scientific computing resources that I''ve missed?', 890, '2014-06-16 19:14:38.553', '269c88c1-25d4-4d93-a9e9-942f96921aa1', 411, 982, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Best languages for scientific computing', 890, '2014-06-16 19:14:38.553', '269c88c1-25d4-4d93-a9e9-942f96921aa1', 411, 983, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<efficiency><statistics><tools><knowledge-base>', 890, '2014-06-16 19:14:38.553', '269c88c1-25d4-4d93-a9e9-942f96921aa1', 411, 984, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('It seems as though most languages have some number of scientific computing libraries available.

 - Python has `Scipy`
 - `Rust` has `SciRust`
 - `C++` has several including `ViennaCL` and `Armadillo`
 - `Java` has `Java Numerics` and `Colt` as well as several other

Not to mention languages like `R` and `Julia` designed explicitly for scientific computing.

 With so many options how do you choose the best language for a task? Additionally which languages will be the most performant? `Python` and `R` seem to have the most traction in the space, but logically a compiled language seems like it would be a better choice. And will anything ever outperform `Fortran`? Additionally compiled languages tend to have GPU acceleration, while interpreted languages like `R` and `Python` don''t. What should I take into account when choosing a language, and which languages provide the best balance of utility and performance? Also are there any languages with significant scientific computing resources that I''ve missed?', 890, '2014-06-16 19:22:00.133', '7cdb10f4-9778-4dff-a543-a3653571562c', 411, 'added 45 characters in body', 985, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What kind of error measures do RMSE and nDCG give while evaluating a recommender system, and how do I know when to use one over the other? If you could give an example of when to use each, that would be great as well!', 84, '2014-06-16 19:30:46.940', 'c4381bf7-5f8c-4a95-b03e-1fb7384d33f0', 369, 'Improving formatting.', 986, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Difference between using RMSE and nDCG to evaluate Recommender Systems', 84, '2014-06-16 19:30:46.940', 'c4381bf7-5f8c-4a95-b03e-1fb7384d33f0', 369, 'Improving formatting.', 987, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><recommendation><evaluation>', 84, '2014-06-16 19:30:46.940', 'c4381bf7-5f8c-4a95-b03e-1fb7384d33f0', 369, 'Improving formatting.', 988, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-16 19:30:46.940', 'c4381bf7-5f8c-4a95-b03e-1fb7384d33f0', 369, 'Proposed by 84 approved by 838 edit id of 63', 989, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn''t expose PII and subject my employer to liability.

For example, one task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as "Dave" and in another as "David," commercial entities can have many different abbreviations, and there are always some typos.

In the past, what we have done is to completely remove PII if we need to transfer any data to a third party. But this means that the third party has really no idea of the relationships between entities. We would prefer to be able to pass along some information about those *relationships*, without divulging identity.

For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) any PII. But the only method I am familiar with for doing this is ROT13, which hardly even counts as encryption; it''s like writing the names upside down and saying, "promise you won''t flip the paper over?"

Another **bad** solution would be to abbreviate everything. "Ellen Roberts" becomes "ER" and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person''s identity, and in other cases it''s too ambiguous; "Benjamin Othello Ames" and "Bank of America" will have the same initials, but their names are otherwise dissimilar. So it doesn''t do either of the things we want.

Are there accepted methods for obscuring data like this, or is it effectively impossible?', 322, '2014-06-16 19:48:31.797', 'e9508066-e052-4951-8166-705859cce859', 412, 990, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How can I transform names in a confidential data set to make it anonymous, but preserve some of the characteristics of the names?', 322, '2014-06-16 19:48:31.797', 'e9508066-e052-4951-8166-705859cce859', 412, 991, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<processing><data-cleaning>', 322, '2014-06-16 19:48:31.797', 'e9508066-e052-4951-8166-705859cce859', 412, 992, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Dimensionality reduction is choosing a basis within which you can represent most but not all of the variance within your data, thereby retaining the relevant information, while reducing the amount of information necessary to represent it. There are a variety of techniques for doing this including but not limited to `PCA`, `ICA`, and `Matrix Feature Factorization`. These will take existing data and reduce it to the most discriminative components.

Feature Selection is hand selecting features which are highly discriminative. This has a lot more to do with feature engineering than analysis, and requires significantly more work on the part of the data scientist. It requires an understanding of what aspects of your dataset are important in whatever predictions you''re making, and which aren''t. Generally feature selection is important if you want to obtain the best results, as it involves creating information that may not exist in your dataset.', 890, '2014-06-16 19:49:24.377', '8bc41458-802b-4d88-8cc7-8c2c99bcf8b3', 413, 993, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES (' - **Is the learning rate related to the shape of the error gradient, as
   it dictates the rate of descent?**
  - In plain SGD, no. A global learning rate is used which is indifferent to the error gradient. However the intuition you are getting at has inspired various modifications of the SGD update rule.

 - **If so, how do you use this information to inform your decision about a value?**

  - Adagrad is the most widely known of these and scales a global learning rate ** on each dimension based on l2 norm of the history of the error gradient *gt* on each dimension:

     ![enter image description here][1]


  - [Adadelta][2] is another such training algorithm which uses both a error gradient history like adagrad and the weight update history.

 - **If it''s not what sort of values should I choose, and how should I choose them?**

  - Setting learning rates for plain SGD in neural nets is usually a
   process of starting with a sane value such as 0.01 and then doing cross validation
   to find an optimal value. Typical values range over a few orders of
   magnitude from 0.001 up to 1.

 - **It seems like you would want small values to avoid overshooting, but
   how do you choose one such that you don''t get stuck in local minima
   or take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?**

  - Usually the value that''s best is near the highest stable learning
   rate and learning rate decay/annealing (either linear or
   exponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.

  [1]: http://i.stack.imgur.com/aP96K.png
  [2]: http://arxiv.org/pdf/1212.5701v1.pdf

', 548, '2014-06-16 19:53:09.957', 'dce0519c-4c4d-4d33-90ff-6784261a42ad', 414, 994, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn''t expose PII and subject my employer to liability.

For example, one task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as "Dave" and in another as "David," commercial entities can have many different abbreviations, and there are always some typos.

In the past, what we have done is to completely remove PII if we need to transfer any data to a third party. But this means that the third party has really no idea of the relationships between entities. We would prefer to be able to pass along some information about those *relationships*, without divulging identity.

For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) any PII. But the only method I am familiar with for doing this is ROT13, which hardly even counts as encryption; it''s like writing the names upside down and saying, "promise you won''t flip the paper over?"

Another **bad** solution would be to abbreviate everything. "Ellen Roberts" becomes "ER" and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person''s identity, and in other cases it''s too ambiguous; "Benjamin Othello Ames" and "Bank of America" will have the same initials, but their names are otherwise dissimilar. So it doesn''t do either of the things we want.

Are there accepted methods for obscuring data like this, or is it effectively impossible?

I have found [at least one paper](http://www.merl.com/publications/docs/TR2010-109.pdf) that looks relevant but it''s a bit over my head.', 322, '2014-06-16 19:54:07.673', '7de82803-3bfd-44a7-8ee8-c5fdedac4185', 412, 'added 140 characters in body', 995, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Dimensionality reduction is choosing a basis within which you can represent most but not all of the variance within your data, thereby retaining the relevant information, while reducing the amount of information necessary to represent it. There are a variety of techniques for doing this including but not limited to `PCA`, `ICA`, and `Matrix Feature Factorization`. These will take existing data and reduce it to the most discriminative components.These all allow you to represent most of the information in your dataset with fewer, more discriminative features.

Feature Selection is hand selecting features which are highly discriminative. This has a lot more to do with feature engineering than analysis, and requires significantly more work on the part of the data scientist. It requires an understanding of what aspects of your dataset are important in whatever predictions you''re making, and which aren''t. Generally feature selection is important if you want to obtain the best results, as it involves creating information that may not exist in your dataset.', 890, '2014-06-16 19:54:30.377', 'fdc9df93-7f8e-4f04-b62e-d9262f60abf8', 413, 'added 93 characters in body', 996, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is a pretty massive question, so this is not intended to be a full answer, but hopefully this can help to inform general practice around determining the best tool for the job when it comes to data science. Generally, I have a relatively short list of qualifications I look for when it comes to any tool in this space. In no particular order they are:

 - **Performance**: Basically boils down to how quickly the language does matrix multiplication, as that is more or less the most important task in data science.
 - **Scalability**: At least for me personally, this comes down to ease of building a distributed system. This is somewhere where languages like `Julia` really shine.
 - **Community**: With any language, you''re really looking for an active community that can help you when you get stuck using whichever tool you''re using. This is where `python` pulls very far ahead of most other languages.
 - **Flexibility**: Nothing is worse than being limited by the language that you use. It doesn''t happen very often, but trying to represent graph structures in `haskell` is a notorious pain, and `Julia` is filled with a lot of code architectures pains as a result of being such a young language.
 - **Ease of Use**: If you want to use something in a larger environment, you want to make sure that setup is a straightforward and it can be automated. Nothing is worse than having to set up a finnicky build on half a dozen machines.

There are a ton of articles out there about performance and scalability, but in general you''re going to be looking at a performance differential of maybe 5-10x between languages, which may or may not matter depending on your specific application. As far as GPU acceleration goes, `cudamat` is a really seamless way of getting it working with `python`, and the `cuda` library in general has made GPU acceleration far more accessible than it used to be.

The two primary metrics I use for both community and flexibility are to look at the language''s package manager, and the language questions on a site like SO. If there are a large number of high-quality questions and answers, it''s a good sign that the community is active. Number of packages and the general activity on those packages can also be a good proxy for this metric.

As far as ease of use goes, I am a firm believer that the only way to actually know is to actually set it up yourself. There''s a lot of superstition around a lot of Data Science tools, specifically things like databases and distributed computing architecture, but there''s no way to really know if something is easy or hard to setup up and deploy without just building it yourself.

', 548, '2014-06-16 20:10:12.167', 'b4ab88bb-5667-4ea9-8d07-041aace35449', 415, 997, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If feasible I would link related records (e.g., Dave, David, etc.) and replace them with a sequence number (1,2,3, etc.) or a [hash of the string](http://www.cse.yorku.ca/~oz/hash.html) that is used to represent all related records (e.g., David instead of Dave).

I assume that third parties need not have any idea what the real name is, otherwise you might as well give it to them.', 381, '2014-06-16 20:12:39.130', 'ef5af01c-fdf2-4ad9-bbf0-d265bb2e3779', 416, 998, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Blacklists aren''t  have value for a number of reasons:

1. They''re easy to set up and scale - it''s just a key/value store, and you can probably just re-use some of your caching logic for the most basic implementation.
2. Depending on the size and type of the spam attack, there will probably be some very specific terms or URLs being used. It''s much faster to throw that term into a blacklist than wait for your model to adapt.
3. You can remove items just as quickly as you added them.
4. Everybody understands how they work and any admin can use them.

The key to fighting spam is *monitoring*. Make sure you have some sort of interface showing which items are on your blacklist, how often they''ve been hit in the last 10 minutes / hour / day / month, and the ability to easily add and remove items.

You''ll want to combine a number of different spam detection models and tactics. Neural nets seem to be a good suggestion, and I''d recommend looking at *user behavior patterns* in addition to just content. Normal humans don''t do things like send batches of 1,000 emails every 30 seconds for 12 consecutive hours.', 414, '2014-06-16 20:18:50.123', 'f2830f26-15b1-46b9-96a4-49859414e0ab', 417, 999, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Dimensionality reduction is typically choosing a basis or mathematical representation within which you can describe most but not all of the variance within your data, thereby retaining the relevant information, while reducing the amount of information necessary to represent it. There are a variety of techniques for doing this including but not limited to `PCA`, `ICA`, and `Matrix Feature Factorization`. These will take existing data and reduce it to the most discriminative components.These all allow you to represent most of the information in your dataset with fewer, more discriminative features.

Feature Selection is hand selecting features which are highly discriminative. This has a lot more to do with feature engineering than analysis, and requires significantly more work on the part of the data scientist. It requires an understanding of what aspects of your dataset are important in whatever predictions you''re making, and which aren''t. Generally feature selection is important if you want to obtain the best results, as it involves creating information that may not exist in your dataset.', 890, '2014-06-16 21:03:21.820', '77ea72d3-d0fe-4130-91bb-dc780df97a16', 413, 'added 30 characters in body', 1000, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Dimensionality reduction is typically choosing a basis or mathematical representation within which you can describe most but not all of the variance within your data, thereby retaining the relevant information, while reducing the amount of information necessary to represent it. There are a variety of techniques for doing this including but not limited to `PCA`, `ICA`, and `Matrix Feature Factorization`. These will take existing data and reduce it to the most discriminative components.These all allow you to represent most of the information in your dataset with fewer, more discriminative features.

Feature Selection is hand selecting features which are highly discriminative. This has a lot more to do with feature engineering than analysis, and requires significantly more work on the part of the data scientist. It requires an understanding of what aspects of your dataset are important in whatever predictions you''re making, and which aren''t. Feature extraction usually involves generating new features which are composites of existing features. Both of these techniques fall into the category of feature engineering. Generally feature engineering is important if you want to obtain the best results, as it involves creating information that may not exist in your dataset, and increasing your signal to noise ratio.', 890, '2014-06-16 21:44:32.587', 'e23dc06d-cc0e-4f00-960f-259a82b5b9b4', 413, 'added 220 characters in body', 1001, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (' - **Is the learning rate related to the shape of the error gradient, as
   it dictates the rate of descent?**
  - In plain SGD, no. A global learning rate is used which is indifferent to the error gradient. However the intuition you are getting at has inspired various modifications of the SGD update rule.

 - **If so, how do you use this information to inform your decision about a value?**

  - Adagrad is the most widely known of these and scales a global learning rate ** on each dimension based on l2 norm of the history of the error gradient *gt* on each dimension:

     ![enter image description here][1]


  - [Adadelta][2] is another such training algorithm which uses both a error gradient history like adagrad and the weight update history and has the advantage of *not having a learning rate at all*.

 - **If it''s not what sort of values should I choose, and how should I choose them?**

  - Setting learning rates for plain SGD in neural nets is usually a
   process of starting with a sane value such as 0.01 and then doing cross validation
   to find an optimal value. Typical values range over a few orders of
   magnitude from 0.001 up to 1.

 - **It seems like you would want small values to avoid overshooting, but
   how do you choose one such that you don''t get stuck in local minima
   or take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?**

  - Usually the value that''s best is near the highest stable learning
   rate and learning rate decay/annealing (either linear or
   exponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.

  [1]: http://i.stack.imgur.com/aP96K.png
  [2]: http://arxiv.org/pdf/1212.5701v1.pdf

', 548, '2014-06-16 22:22:03.023', 'cd0d2747-3242-45c1-85b2-f1e087960750', 414, 'added 61 characters in body', 1002, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (' - **Is the learning rate related to the shape of the error gradient, as
   it dictates the rate of descent?**
  - In plain SGD, no. A global learning rate is used which is indifferent to the error gradient. However the intuition you are getting at has inspired various modifications of the SGD update rule.

 - **If so, how do you use this information to inform your decision about a value?**

  - Adagrad is the most widely known of these and scales a global learning rate ** on each dimension based on l2 norm of the history of the error gradient *gt* on each dimension:

     ![enter image description here][1]


  - [Adadelta][2] is another such training algorithm which uses both the error gradient history like adagrad and the weight update history and has the advantage of *not having to set a learning rate at all*.

 - **If it''s not what sort of values should I choose, and how should I choose them?**

  - Setting learning rates for plain SGD in neural nets is usually a
   process of starting with a sane value such as 0.01 and then doing cross validation
   to find an optimal value. Typical values range over a few orders of
   magnitude from 0.001 up to 1.

 - **It seems like you would want small values to avoid overshooting, but
   how do you choose one such that you don''t get stuck in local minima
   or take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?**

  - Usually the value that''s best is near the highest stable learning
   rate and learning rate decay/annealing (either linear or
   exponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.

  [1]: http://i.stack.imgur.com/aP96K.png
  [2]: http://arxiv.org/pdf/1212.5701v1.pdf

', 548, '2014-06-17 00:05:01.337', '163c158a-2f30-4d42-9473-cd8bb41fa32e', 414, 'added 9 characters in body', 1003, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to know what is the best way to classify a data set composed of mixed types of attributes, for example, textual and numerical. I know I can convert textual to boolean, but the vocabulary is diverse and data become too sparse. I also tried to classify the types of attributes separately and combine the results through meta-learning techniques, but it did not work well.', 900, '2014-06-17 00:16:24.287', '1b8fe372-c9f0-4af5-a3c4-71128ac009cb', 418, 1004, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Best way to classify datasets with mixed types of attributes', 900, '2014-06-17 00:16:24.287', '1b8fe372-c9f0-4af5-a3c4-71128ac009cb', 418, 1005, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification>', 900, '2014-06-17 00:16:24.287', '1b8fe372-c9f0-4af5-a3c4-71128ac009cb', 418, 1006, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First you need to decide what you want to do, then look for the right tool for that task.

A very general approach is to use R for first versions and to see if your approach is correct. It lacks a little in speed, but has very powerful commands and addon libraries, that you can try almost anything with it:
http://www.r-project.org/

The second idea is if you want to understand the algorithms behind the libraries, you might wanna take a look at the Numerical Recipies. They are available for different languages and free to use for learning. If you want to use them in commercial products, you need to ourchase a licence:
http://en.wikipedia.org/wiki/Numerical_Recipes

Most of the time performance will not be the issue but finding the right algorithms and parameters for them, so it is important to have a fast scripting language instead of a monster program that first needs to compile 10 mins before calculating two numbers and putting out the result.

And a big plus in using R is that it has built-in functions or libraries for almost any kind of diagram you might wanna need to visualize your data.

If you then have a working version, it is almost easy to port it to any other language you think is more performant.', 901, '2014-06-17 00:19:09.773', 'fd667c16-8bd0-42e7-91c7-32ec18d63d50', 419, 1007, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It is hard to answer this question without knowing more about the data.  That said, I would offer the following advice:

Most machine learning techniques can handle mixed-type data.  Tree based methods (such as AdaBoost and Random Forests) do well with this type of data.  The more important issue is actually the dimensionality, about which you are correct to be concerned.

I would suggest that you do something to reduce that dimensionality.  For example, look for the words or phrases that separate the data the best and discard the other words (note: tree based methods do this automatically).', 178, '2014-06-17 00:39:15.990', '43e49b6e-9a03-465c-a80f-0d9320e8fdae', 420, 1008, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (' - **Is the learning rate related to the shape of the error gradient, as
   it dictates the rate of descent?**
  - In plain SGD, no. A global learning rate is used which is indifferent to the error gradient. However the intuition you are getting at has inspired various modifications of the SGD update rule.

 - **If so, how do you use this information to inform your decision about a value?**

  - Adagrad is the most widely known of these and scales a global learning rate ** on each dimension based on l2 norm of the history of the error gradient *gt* on each dimension:

     ![enter image description here][1]


  - [Adadelta][2] is another such training algorithm which uses both the error gradient history like adagrad and the weight update history and has the advantage of *not having to set a learning rate at all*.

 - **If it''s not what sort of values should I choose, and how should I choose them?**

  - Setting learning rates for plain SGD in neural nets is usually a
   process of starting with a sane value such as 0.01 and then doing cross validation
   to find an optimal value. Typical values range over a few orders of
   magnitude from 0.0001 up to 1.

 - **It seems like you would want small values to avoid overshooting, but
   how do you choose one such that you don''t get stuck in local minima
   or take to long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?**

  - Usually the value that''s best is near the highest stable learning
   rate and learning rate decay/annealing (either linear or
   exponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.

  [1]: http://i.stack.imgur.com/aP96K.png
  [2]: http://arxiv.org/pdf/1212.5701v1.pdf

', 548, '2014-06-17 01:36:22.877', '7760ca4e-56e6-4e1b-aa1b-5423af4321b9', 414, 'added 1 character in body', 1010, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Does anyone know some good tutorials on online machine learning technics?
I.e. how it can be used in real-time environments, what are key differences compared to normal machine learning methods etc.', 88, '2014-06-17 04:31:34.067', '49470a49-6a18-4531-868c-93627b4195b8', 421, 1011, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Online machine learning tutorial', 88, '2014-06-17 04:31:34.067', '49470a49-6a18-4531-868c-93627b4195b8', 421, 1012, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 88, '2014-06-17 04:31:34.067', '49470a49-6a18-4531-868c-93627b4195b8', 421, 1013, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As an extension to our great list of [publicly available datasets](http://datascience.stackexchange.com/questions/155/publicly-available-datasets), I''d like to know if there is any list of publicly available social network datasets/crawling APIs. It would be very nice if alongside with a link to the dataset/API, characteristics of the data available were added. Such information should be, and is not limited to:

- the name of the social network;
- whether it allows for crawling its contents via an API or simply provides a given dataset;
- what kind of user information it provides (posts, profile, friendship network, ...).

Any suggestions and further characteristics to be added are very welcome.', 84, '2014-06-17 05:29:11.830', 'd56eb39a-b5fb-41a2-a494-4fc9f6d475af', 422, 1014, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Publicly available social network datasets/APIs', 84, '2014-06-17 05:29:11.830', 'd56eb39a-b5fb-41a2-a494-4fc9f6d475af', 422, 1015, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<databases><open-source><crawling>', 84, '2014-06-17 05:29:11.830', 'd56eb39a-b5fb-41a2-a494-4fc9f6d475af', 422, 1016, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As an extension to our great list of [publicly available datasets](http://datascience.stackexchange.com/questions/155/publicly-available-datasets), I''d like to know if there is any list of publicly available social network datasets/crawling APIs. It would be very nice if alongside with a link to the dataset/API, characteristics of the data available were added. Such information should be, and is not limited to:

- the name of the social network;
- what kind of user information it provides (posts, profile, friendship network, ...);
- whether it allows for crawling its contents via an API (and rate: 10/min, 1k/month, ...);
- whether it simply provides a snapshot of the whole dataset.

Any suggestions and further characteristics to be added are very welcome.', 84, '2014-06-17 05:34:40.857', '237f71ee-c091-429b-b849-8579fd8a2691', 422, 'added 62 characters in body', 1017, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m planning to run some experiments with very large datasets, and I''d like to distribute the computation. In fact, the main result of this experiment is to evaluate efficiency gains in comparison to previous proposals.

I have about ten machines available, each with 200GB of free space on hard disk. However, I would like to perform experiments on a greater number of nodes, to measure scalability more precisely.

Since I don''t have more computers, I thought about using a commodity cluster, but I''m not sure about the policies of usage, and I need to reliably measure execution times. Are there commodity services which would grant me that only my application would be running at a given time? Has anyone used such services yet?', 84, '2014-06-17 05:55:04.710', '275fee36-3971-4e60-bc06-5051918748ff', 423, 1018, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Measuring execution time on commodity computing', 84, '2014-06-17 05:55:04.710', '275fee36-3971-4e60-bc06-5051918748ff', 423, 1019, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><scalability><distributed><experiments>', 84, '2014-06-17 05:55:04.710', '275fee36-3971-4e60-bc06-5051918748ff', 423, 1020, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have recently seen this really cool feature (that was once) available in google sheets: you start by writing a few related keywords in consecutive cells, say: "blue", "green", "yellow", and it automatically generates the next keywords on the same theme (in this case, other colors).
See more examples in this video: http://youtu.be/dlslNhfrQmw.

I would like to reproduce this in my own program: I''m thinking of using Freebase, and it would work like this intuitively:

1. retrieve the list of given words in Freebase
2. find their "common denominator(s)" and construct a distance metric based on this
3. rank other concepts based on their "distance" to the original keywords
4. display the next closest concepts

As I''m not familiar with this area, my questions are:

* Is this correct, or is there a better way to do this ?
* What''s the best tool to execute each step of the process ?', 906, '2014-06-17 06:05:39.653', 'cd373c2d-4904-40c4-9212-09daa495a7c6', 424, 1021, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How it works: growing a list of related objects based on initial keywords?', 906, '2014-06-17 06:05:39.653', 'cd373c2d-4904-40c4-9212-09daa495a7c6', 424, 1022, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><text-mining><freebase>', 906, '2014-06-17 06:05:39.653', 'cd373c2d-4904-40c4-9212-09daa495a7c6', 424, 1023, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('An example from germany: Xing a site similar to linkedin but limited to german speaking countries.

Link to it''s developer central: https://dev.xing.com/overview

Provides access to: User profiles, Conversations between users (limited to the user itself), Job advertisings, Contacts and Contacts of Contacts, news from the network and some geolocation api.

Yes it has an api, but I did not find information about the rate. But it seems to me, that some information is limited to the consent of the user.
', 791, '2014-06-17 06:32:22.883', '8669ef53-7beb-476f-ae63-e06bb1170c60', 425, 1024, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are plenty on youtube and here''s a famous one by Andrew Ng from coursea: https://www.coursera.org/course/ml', 122, '2014-06-17 06:53:44.007', '87baac61-2a50-4807-8a24-b644d8052e29', 426, 1025, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are anyone using `Julia` ([http://julialang.org/][1]) for professional jobs?

Or use it instead of R , Matlab or Mathematica?

Is it a good language?

If you have to predict next 5-10 years: Do you think it grow up enough to became such a standard in datascience like R or similar?


  [1]: http://julialang.org/', 908, '2014-06-17 07:46:39.783', '0c99ef0e-7cda-4223-bbcb-6fbc84bef228', 427, 1026, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('does anyone use Julia programming language?', 908, '2014-06-17 07:46:39.783', '0c99ef0e-7cda-4223-bbcb-6fbc84bef228', 427, 1027, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<tools>', 908, '2014-06-17 07:46:39.783', '0c99ef0e-7cda-4223-bbcb-6fbc84bef228', 427, 1028, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to grow a list of related words based on initial keywords?', 906, '2014-06-17 08:46:10.590', '0570e571-34e0-4d60-bf15-aeea79df5f45', 424, 'edited title', 1029, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This critique is no longer justified:

While it is true that most of the standard and most respected R libraries were restricted to in-memory computations, there is a growing number of specialized libraries to deal with data that doesn''t fit into memory.
For instance, for random forests on large datasets, you have the library `bigrf`. More info here: http://cran.r-project.org/web/packages/bigrf/

Another area of growth is R''s connectedness to big data environments like hadoop, which opens another world of possibilities.', 906, '2014-06-17 09:33:37.230', '2bd84a8e-2b4e-4537-9cc7-67b0ecda0620', 428, 1030, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It''s not a social network per se, but Stackexchange publish their entire database dump periodically:

 - [Stackexchange data dump hosted on the archive.org][1]
 - [Post describing the database dump schema][2]

You can extract some social information by analyzing which users ask and answer to each other. One nice thing is that since posts are tagged, you can analyze sub-communities easily.


  [1]: https://archive.org/details/stackexchange
  [2]: http://meta.stackoverflow.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede', 846, '2014-06-17 09:56:49.180', '32e9693c-9c0b-4c94-b3fc-c419d94a807f', 429, 1031, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to understand how all the "big data" components play together in a real world use case, e.g. hadoop, monogodb/nosql, storm, kafka, ... I know that this is quite a wide range of tools used for different types, but I''d like to get to know more about their interaction in applications, e.g. thinking machine learning for an app, webapp, online shop: I have vistors/session, transaction data etc and store that; but if I want to make live on the fly recommendations, I can''t run a slow map/reduce jobs for that on some big database of logs I have.

Where can I learn more about the infrastructure aspects? I think I can use most of the tools on their own, but plugging them into each other seems to be an art of its own.
Are there any public examples/use cases etc available?
I understand that the individual pipelines strongly depend on the use case and the user, but just examples will probably be very useful to me.
Thanks a lot!', 913, '2014-06-17 10:37:22.987', '91b9254e-33c4-453b-98f5-9fd971c92e2a', 430, 1032, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Looking for example infrastructure stacks/workflows/pipelines', 913, '2014-06-17 10:37:22.987', '91b9254e-33c4-453b-98f5-9fd971c92e2a', 430, 1033, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata>', 913, '2014-06-17 10:37:22.987', '91b9254e-33c4-453b-98f5-9fd971c92e2a', 430, 1034, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A small collection of such links can be found at [here][1]. Many of them are social graphs.


  [1]: http://lgylym.github.io/big-graph/dataset.html', 743, '2014-06-17 12:37:17.150', '1bf752a2-cb7f-42dc-8fd0-231aa3e45af7', 431, 1035, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('1. There is a very nice library of online machine learning algorithms from a group at NTU, called [`LIBOL`][1]. This would be a very good place to start experimenting with the algorithms.
The [accompanying user guide][3], and [associated JMLR publication][2] are very nice introductions to the basic algorithms in this field.
2. Avrim Blum has an older and more technical [survey paper][4] on online learning algorithms.


  [1]: http://www.cais.ntu.edu.sg/~chhoi/libol/
  [2]: http://jmlr.org/papers/v15/hoi14a.html
  [3]: http://www.cais.ntu.edu.sg/~chhoi/libol/LIBOL_manual.pdf
  [4]: http://www.cs.cmu.edu/~avrim/Papers/survey.pdf', 241, '2014-06-17 12:41:58.900', '56264721-5cf8-40fc-b78c-9f7d618c94a1', 432, 1036, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One of the most detailed and clear explanations of setting up a complex analytics pipeline is from the folks over at [Twitch][1].
They give detailed motivations of each of the architecture choices for collection, transportation, coordination, processing, storage, and querying their data.
Compelling reading! Find it [here][2] and [here][3].


  [1]: http://www.twitch.tv/
  [2]: http://blog.twitch.tv/2014/04/twitch-data-analysis-part-1-the-twitch-statistics-pipeline/
  [3]: http://blog.twitch.tv/2014/04/twitch-data-analysis-part-2-architectural-decisions/', 241, '2014-06-17 13:31:30.503', '7de04bcd-5c14-42fa-bfdc-af14e142cc5c', 433, 1037, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m trying to understand how all the "big data" components play together in a real world use case, e.g. hadoop, monogodb/nosql, storm, kafka, ... I know that this is quite a wide range of tools used for different types, but I''d like to get to know more about their interaction in applications, e.g. thinking machine learning for an app, webapp, online shop.

I have vistors/session, transaction data etc and store that; but if I want to make recommendations on the fly, I can''t run slow map/reduce jobs for that on some big database of logs I have. Where can I learn more about the infrastructure aspects? I think I can use most of the tools on their own, but plugging them into each other seems to be an art of its own.

Are there any public examples/use cases etc available? I understand that the individual pipelines strongly depend on the use case and the user, but just examples will probably be very useful to me.', 84, '2014-06-17 13:37:47.400', 'c5f04580-b816-4bfc-8679-2b919342f4b0', 430, 'Improving formatting.', 1038, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><efficiency><scalability><distributed>', 84, '2014-06-17 13:37:47.400', 'c5f04580-b816-4bfc-8679-2b919342f4b0', 430, 'Improving formatting.', 1039, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-17 13:37:47.400', 'c5f04580-b816-4bfc-8679-2b919342f4b0', 430, 'Proposed by 84 approved by 913 edit id of 72', 1040, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is really no question here as you ask for pure conjectures but consider at least that

* this week has [Julia Con](http://juliacon.org/), the first Julia conference
* you could search GitHub and/or the registered Julia modules
', 515, '2014-06-17 13:44:43.807', 'b5545b8d-e2f8-4ab2-87a8-1a85726c970b', 434, 1041, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have an huge dataset from a relational database which I need to create a classification model for. Normally for this situation I would use ILP but due to special circumstances I can''t do that.

The other way to tackle this would be just to try to aggregate the values when I have a foreign relations however I have thousands of important and distinct rows for some nominal attributes (Ex: A patient with a relation to several distinct drug prescriptions) in which I just can''t do that without creating a new attributes for each distinct row of that nominal attribute and furthermore most of the new columns would have NULL values if I do that.

Is there any non-ILP algorithm that allows me to data mine relational databases without resort to technique like pivoting which would create thousands of new columns?', 917, '2014-06-17 13:46:06.367', '698ab671-65b3-4724-97e1-064bd097681e', 435, 1042, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Relational Data Mining without ILP', 917, '2014-06-17 13:46:06.367', '698ab671-65b3-4724-97e1-064bd097681e', 435, 1043, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><classification><relational-dbms>', 917, '2014-06-17 13:46:06.367', '698ab671-65b3-4724-97e1-064bd097681e', 435, 1044, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a huge dataset from a relational database which I need to create a classification model for. Normally for this situation I would use [Inductive Logic Programming](http://en.wikipedia.org/wiki/Inductive_logic_programming) (ILP), but due to special circumstances I can''t do that.

The other way to tackle this would be just to try to aggregate the values when I have a foreign relation. However, I have thousands of important and distinct rows for some nominal attributes (e.g.: A patient with a relation to several distinct drug prescriptions). So, I just can''t do that without creating a new attribute for each distinct row of that nominal attribute, and furthermore most of the new columns would have NULL values if I do that.

Is there any non-ILP algorithm that allows me to data mine relational databases without resorting to techniques like pivoting, which would create thousands of new columns?', 84, '2014-06-17 14:26:40.380', 'a52923f7-2b70-4ffe-85ac-f88a307ee9e2', 435, 'Improving formatting.', 1045, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-17 14:26:40.380', 'a52923f7-2b70-4ffe-85ac-f88a307ee9e2', 435, 'Proposed by 84 approved by 917 edit id of 73', 1046, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I personally have used `Julia` for a good number of professional projects, and while, as Dirk mentioned, this is purely conjecture, I can give some insights on where Julia really stands out. The question of whether or not these reasons will prove enough to have `Julia` succeed as a language is anyone''s guess.

 - **Distributed Systems**: Julia is the easiest language I''ve ever dealt with in terms of building distributed systems. This is becoming more and more relevant in computing, and will potentially become a deciding factor, but the question of whether or not `Julia`''a relative ease decides this is up for debate
 - **JIT Performance**: Julia''s JIT compiler is extremely fast, and while there is a lot of debate as to how accurate these benchmark numbers are, the [Julia Website][1] shows a series of relevant benchmarks
 - **Community**: This is an area where `Julia` just isn''t quite there. The community that is there is generally supportive, but not quite as knowledgable as the `R` or `python` communities, which is a definite minus.
 - **Extensibility**: This is another place where `Julia` is currently lacking, there is a large disconnect between the implies code patterns that `Julia` steers you toward and what it can actually support. The type system is currently overly bulky and difficult to use effectively.

Again, can''t say what this means for the future, but these are just a couple of relevant points when it comes to evaluating `Julia` in my opinion.




  [1]: http://julialang.org/', 548, '2014-06-17 14:55:47.103', '13eba611-8b71-451f-8dde-419f0d29dc9f', 436, 1047, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-cleaning><anonymization>', 381, '2014-06-17 15:19:24.040', '34827082-10b5-45a5-88b0-86fc1d521aed', 412, 'added some tags', 1048, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-17 15:19:24.040', '34827082-10b5-45a5-88b0-86fc1d521aed', 412, 'Proposed by 381 approved by 322 edit id of 68', 1049, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><education><beginner>', 97, '2014-06-17 16:17:09.043', '058b159f-e186-4b5e-996c-d62ffaaccd7e', 421, 'Adding more relevant tags', 1050, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-17 16:17:09.043', '058b159f-e186-4b5e-996c-d62ffaaccd7e', 421, 'Proposed by 97 approved by 50 edit id of 71', 1051, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-mining><definitions>', 322, '2014-06-17 16:17:20.473', '01941218-7f3d-413e-b72e-288f960f38a7', 14, 'see http://meta.datascience.stackexchange.com/a/18/322', 1052, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-17 16:17:20.473', '01941218-7f3d-413e-b72e-288f960f38a7', 14, 'Proposed by 322 approved by 50 edit id of 69', 1053, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<categorical-data><logistic-regression>', 97, '2014-06-17 16:17:36.433', '17bf7c97-819f-4f8b-8d36-438c85a013e2', 403, 'Adding tag.', 1054, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-17 16:17:36.433', '17bf7c97-819f-4f8b-8d36-438c85a013e2', 403, 'Proposed by 97 approved by 50 edit id of 66', 1055, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('If I have a retail store and have a way to measure how many people enter my store every minute, and timestamp that data, how can I predict future foot traffic?

I have looked into machine learning algorithms, but I''m not sure which one to use. In my test data, a year over year trend is more accurate compared to other things I''ve tried, like KNN (with what I think are sensible parameters and distance function).

It almost seems like this could be similar to financial modeling, where you deal with time series data. Any ideas?', 84, '2014-06-17 16:17:46.027', 'ae6a1ede-42c8-4104-9869-b14445c26049', 406, 'Improving formatting.', 1056, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-17 16:17:46.027', 'ae6a1ede-42c8-4104-9869-b14445c26049', 406, 'Proposed by 84 approved by 50 edit id of 75', 1057, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m currently using [General Algebraic Modeling System](http://en.wikipedia.org/wiki/General_Algebraic_Modeling_System) (GAMS), and more specifically CPLEX within GAMS, to solve a very large mixed integer programming problem. This allows me to parallelize the process over 4 cores (although I have more, CPLEX utilizes a maximum of 4 cores), and it finds an optimal solution in a relatively short amount of time.

Is there an open source mixed integer programming tool that I could use as an alternative to GAMS and CPLEX? It must be comparable in speed or faster for me to consider it. I have a preference for R based solutions, but I''m open to suggestions of all kinds, and other users may be interested in different solutions.', 84, '2014-06-17 16:18:02.437', '12ca9902-e0f3-4e22-88d2-9040eb9f017b', 189, 'Improving formatting.', 1058, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-17 16:18:02.437', '12ca9902-e0f3-4e22-88d2-9040eb9f017b', 189, 'Proposed by 84 approved by 50 edit id of 74', 1059, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''d like to use my MSc thesis as an opportunity to explore ''data science''. Frankly, the term seems a little vague to me (or at least, I''ve heard so many people apply it to so many situations that it''s become diluted), but I expect it to require:

1. machine learning (rather than traditional statistics);
2. a large enough dataset that you have to run analyses on clusters.

Anyway, we don''t have any relevantly qualified professors at my college, so I''d like a dataset and problem that is accessible to a statistician. But I also want it to allow me to foray into this data science thing.

Any suggestions? To keep this as narrow as possible, I''d ideally like links to open, well used datasets and example problems.

P.S.: I have an engineering background, so I''m fairly comfy with math/programming.', 84, '2014-06-17 16:18:05.737', '205c359b-e2a7-4b73-9ad6-d12622aed705', 370, 'Improving question.', 1060, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-17 16:18:05.737', '205c359b-e2a7-4b73-9ad6-d12622aed705', 370, 'Proposed by 84 approved by 50 edit id of 64', 1061, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":84,"DisplayName":"Rubens"},{"Id":50,"DisplayName":"Robert Cartaino"}]}', 50, '2014-06-17 16:26:05.453', '2abe6b2f-eb97-4e73-8fe6-987f710010a7', 427, '103', 1062, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Motivation
=

I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn''t expose PII and subject my employer to liability. Our usual approach here is to withhold data entirely, or in some cases to reduce its resolution; e.g., replacing an exact street address with the corresponding county or census tract.

This means that certain types of analysis and processing must be done in-house, even when a third party has resources and expertise more suited to the task. Since the source data is not disclosed, the way we go about this analysis and processing lacks transparency. As a result, any third party''s ability to perform QA/QC, adjust parameters or make refinements may be very limited.

Anonymizing Confidential Data
=

One task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as "Dave" and in another as "David," commercial entities can have many different abbreviations, and there are always some typos. I''ve developed scripts based on a number of criteria that determine when two records with non-identical names represent the same individual, and assign them a common ID.

At this point we can make the dataset anonymous by withholding the names and replacing them with this personal ID number. But this means the recipient has almost no information about e.g. the strength of the match. We would prefer to be able to pass along as much information as possible without divulging identity.

What Doesn''t Work
=

For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) PII. Perhaps we match strings in-house with edit distance <= 2, and the recipient wants to look at the implications of tightening that tolerance to edit distance <= 1.

But the only method I am familiar with that does this is ROT13, which hardly even counts as encryption; it''s like writing the names upside down and saying, "promise you won''t flip the paper over?"

Another **bad** solution would be to abbreviate everything. "Ellen Roberts" becomes "ER" and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person''s identity, and in other cases it''s too ambiguous; "Benjamin Othello Ames" and "Bank of America" will have the same initials, but their names are otherwise dissimilar. So it doesn''t do either of the things we want.

An inelegant alternative is to introduce additional fields to track certain attributes of the name, e.g.:

    +-----+----+-------------------+-----------+--------+
    | Row | ID | Name              | WordChars | Origin |
    +-----+----+-------------------+-----------+--------+
    | 1   | 17 | "AMELIA BEDELIA"  | (6, 7)    | Eng    |
    +-----+----+-------------------+-----------+--------+
    | 2   | 18 | "CHRISTOPH BAUER" | (9, 5)    | Ger    |
    +-----+----+-------------------+-----------+--------+
    | 3   | 18 | "C J BAUER"       | (1, 1, 5) | Ger    |
    +-----+----+-------------------+-----------+--------+
    | 4   | 19 | "FRANZ HELLER"    | (5, 6)    | Ger    |
    +-----+----+-------------------+-----------+--------+

I call this "inelegant" because it requires anticipating which qualities might be interesting and it''s relatively coarse. If the names are removed, there''s not much you can reasonably conclude about the strength of the match between rows 2 & 3, or about the distance between rows 2 & 4 (i.e., how close they are to matching).

Conclusion
=

The goal is to transform strings in such a way that as many useful qualities of the original string are preserves as possible while obscuring the original string. Decryption should be impossible, or so impractical as to be effectively impossible, no matter the size of the data set. In particular, a method that preserves the edit distance between arbitrary strings would be very useful.

I have found [at least one paper](http://www.merl.com/publications/docs/TR2010-109.pdf) that looks relevant but it''s a bit over my head.', 322, '2014-06-17 16:53:59.450', '167f2061-e2b5-4d09-bf1f-03a7aa3825b7', 412, 'added 2369 characters in body', 1063, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think that Bootstrap can be useful in my work, where we have a lot a variables that we don''t know the distribution of it. So, simulations could help.
What are good sources to learn about Bootstrap/other useful simulation methods?', 199, '2014-06-17 18:13:46.230', 'd35fe905-6e75-4066-b672-6840fa14ad74', 437, 1064, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are good sources to learn about Bootstrap?', 199, '2014-06-17 18:13:46.230', 'd35fe905-6e75-4066-b672-6840fa14ad74', 437, 1065, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><statistics><education>', 199, '2014-06-17 18:13:46.230', 'd35fe905-6e75-4066-b672-6840fa14ad74', 437, 1066, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('If feasible I would link related records (e.g., Dave, David, etc.) and replace them with a sequence number (1,2,3, etc.) or a [hash of the string](http://www.cse.yorku.ca/~oz/hash.html) that is used to represent all related records (e.g., David instead of Dave).

I assume that third parties need not have any idea what the real name is, otherwise you might as well give it to them.

edit: You need to define and justify what kind of operations the third party needs to be able to do. For example, what is wrong with using initials followed by a number (e.g., BOA-1, BOA-2, etc.) to disambiguate Bank of America from Benjamin Othello Ames? If that''s too revealing, you could bin some of the letters or names; e.g., [A-E] -> 1, [F-J] -> 2, etc. so BOA would 1OA, or ["Bank", "Barry", "Bruce", etc.] -> 1 so Bank of America is again 1OA. For more information see [k-anonymity](http://en.wikipedia.org/wiki/K-anonymity).', 381, '2014-06-17 18:16:29.693', '34437474-ce51-47df-89db-ab10fa3724d1', 416, 'added 537 characters in body', 1067, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Motivation
=

I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn''t expose PII and subject my employer to liability. Our usual approach here is to withhold data entirely, or in some cases to reduce its resolution; e.g., replacing an exact street address with the corresponding county or census tract.

This means that certain types of analysis and processing must be done in-house, even when a third party has resources and expertise more suited to the task. Since the source data is not disclosed, the way we go about this analysis and processing lacks transparency. As a result, any third party''s ability to perform QA/QC, adjust parameters or make refinements may be very limited.

Anonymizing Confidential Data
=

One task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as "Dave" and in another as "David," commercial entities can have many different abbreviations, and there are always some typos. I''ve developed scripts based on a number of criteria that determine when two records with non-identical names represent the same individual, and assign them a common ID.

At this point we can make the dataset anonymous by withholding the names and replacing them with this personal ID number. But this means the recipient has almost no information about e.g. the strength of the match. We would prefer to be able to pass along as much information as possible without divulging identity.

What Doesn''t Work
=

For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) PII. Perhaps we match strings in-house with edit distance <= 2, and the recipient wants to look at the implications of tightening that tolerance to edit distance <= 1.

But the only method I am familiar with that does this is ROT13, which hardly even counts as encryption; it''s like writing the names upside down and saying, "promise you won''t flip the paper over?"

Another **bad** solution would be to abbreviate everything. "Ellen Roberts" becomes "ER" and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person''s identity, and in other cases it''s too ambiguous; "Benjamin Othello Ames" and "Bank of America" will have the same initials, but their names are otherwise dissimilar. So it doesn''t do either of the things we want.

An inelegant alternative is to introduce additional fields to track certain attributes of the name, e.g.:

    +-----+----+-------------------+-----------+--------+
    | Row | ID | Name              | WordChars | Origin |
    +-----+----+-------------------+-----------+--------+
    | 1   | 17 | "AMELIA BEDELIA"  | (6, 7)    | Eng    |
    +-----+----+-------------------+-----------+--------+
    | 2   | 18 | "CHRISTOPH BAUER" | (9, 5)    | Ger    |
    +-----+----+-------------------+-----------+--------+
    | 3   | 18 | "C J BAUER"       | (1, 1, 5) | Ger    |
    +-----+----+-------------------+-----------+--------+
    | 4   | 19 | "FRANZ HELLER"    | (5, 6)    | Ger    |
    +-----+----+-------------------+-----------+--------+

I call this "inelegant" because it requires anticipating which qualities might be interesting and it''s relatively coarse. If the names are removed, there''s not much you can reasonably conclude about the strength of the match between rows 2 & 3, or about the distance between rows 2 & 4 (i.e., how close they are to matching).

Conclusion
=

The goal is to transform strings in such a way that as many useful qualities of the original string are preserved as possible while obscuring the original string. Decryption should be impossible, or so impractical as to be effectively impossible, no matter the size of the data set. In particular, a method that preserves the edit distance between arbitrary strings would be very useful.

I have found [at least one paper](http://www.merl.com/publications/docs/TR2010-109.pdf) that looks relevant but it''s a bit over my head.', 322, '2014-06-17 18:19:23.760', '180de4c0-26c0-4022-bf7a-fb6f1ef6d856', 412, 'edited body', 1071, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Halfway through reading your question, I realized Levenshtein Distance could be a nice solution to your problem.  Its good to see that you have a link to a paper on the topic, let me see if I can shed some light into what a Levenshtein solution would look like.

Levenshtein distance is used across many industries for entity resolution, what makes it useful is that it is a measure of the difference between two sequences.  In the case of string comparison it is just sequences characters.

This could help solve your problem by allowing you to provide one number that gives a measure of how similar the text of another field is.

Here is an example of a basic way of using Levenshtein with the data you gave:

![enter image description here][1]

This provides an ok solution, the distance of 8 provides some indication of a relationship, and it is very PII compliant.  However, it is still not super useful, let see what happens if we do some text magic to take only the first initial of the first name and the full last name dropping anything in the middle:

![enter image description here][2]

As you can see the Levenshtein distance of 0 is pretty indicative of a relationship.  Commonly data providers will combine a bunch of Levenshtein permutations of the first and last names with 1, 2, or all of the characters just to give some dimensionality as to how entities are related while still maintaining anonymity within the data.


  [1]: http://i.stack.imgur.com/vpaAH.png
  [2]: http://i.stack.imgur.com/JlPu9.png', 780, '2014-06-17 18:42:55.423', 'a4d101b1-f7c8-4b5a-820c-54ba76c0aa62', 439, 1072, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The [word2vec algorithm](https://code.google.com/p/word2vec/) may be a good way to retrieve more elements for a list of similar words. It is an unsupervised "deep learning" algorithm that has previously been demonstrated with Wikipedia-based training data (helper scripts are provided on the Google code page).

There are currently [C](https://code.google.com/p/word2vec/) and [Python](http://radimrehurek.com/gensim/models/word2vec.html) implementations. This [tutorial](http://radimrehurek.com/2014/02/word2vec-tutorial) by [Radim ehek](http://radimrehurek.com/), the author of the [Gensim topic modelling library](http://radimrehurek.com/gensim/), is an excellent place to start.

The ["single topic"](http://radimrehurek.com/2014/02/word2vec-tutorial#single) demonstration on the tutorial is a good example of retreiving similar words to a single term (try searching on ''red'' or ''yellow''). It should be possible to extend this technique to find the words that have the greatest overall similarity to a set of input words.', 922, '2014-06-17 18:59:14.947', 'b83b873e-2d85-43bc-9f0e-3d402270e61a', 440, 1073, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('If feasible I would link related records (e.g., Dave, David, etc.) and replace them with a sequence number (1,2,3, etc.) or a [hash of the string](http://www.cse.yorku.ca/~oz/hash.html) that is used to represent all related records (e.g., David instead of Dave).

I assume that third parties need not have any idea what the real name is, otherwise you might as well give it to them.

**edit**: You need to define and justify what kind of operations the third party needs to be able to do. For example, what is wrong with using initials followed by a number (e.g., BOA-1, BOA-2, etc.) to disambiguate Bank of America from Benjamin Othello Ames? If that''s too revealing, you could bin some of the letters or names; e.g., [A-E] -> 1, [F-J] -> 2, etc. so BOA would become 1OA, or ["Bank", "Barry", "Bruce", etc.] -> 1 so Bank of America is again 1OA.

For more information see [k-anonymity](http://en.wikipedia.org/wiki/K-anonymity).', 381, '2014-06-17 20:07:05.303', '585404fc-db48-44a2-81b2-d27c3d6799a4', 416, 'added 15 characters in body', 1075, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('With Hadoop 2.0 and YARN Hadoop is supposedly no longer tied only map-reduce solutions. With that advancement, what are the use cases for Apache Spark vs Hadoop considering both sit atop of HDFS? I''ve read through the introduction documentation for Spark, but I''m curious if anyone has encountered a problem that was more efficient and easier to solve with Spark compared to Hadoop.', 426, '2014-06-17 20:48:35.267', '22daf4cf-40da-4248-85ac-d76d546f503c', 441, 1076, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the use cases for Apache Spark vs Hadoop', 426, '2014-06-17 20:48:35.267', '22daf4cf-40da-4248-85ac-d76d546f503c', 441, 1077, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop><distributed><knowledge-base>', 426, '2014-06-17 20:48:35.267', '22daf4cf-40da-4248-85ac-d76d546f503c', 441, 1078, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Christopher''s answers seem very reasonable. In particular tree based methods do well with this sort of data because they branch on discriminative features. It''s a little hard to say without knowing your specific application, but in general if you think that some of your features might be significantly more discriminative than others, you could try some dimensionality reduction techniques to clean this up a bit.

Also if you use a dimensionality reduction technique you end up getting a slightly more robust format for your feature vector (they generally end up being straight numerical vectors instead of mixed data types), which might let you leverage different methods. You could also look into hand engineering features. With properly hand engineered features `Random Forest` will get you very close to state of the art on most tasks.', 548, '2014-06-17 21:17:31.210', '35f5aaba-6d3b-44e4-8b70-67fe080e807e', 442, 1079, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Motivation
=

I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn''t expose PII and subject my employer to liability. Our usual approach here is to withhold data entirely, or in some cases to reduce its resolution; e.g., replacing an exact street address with the corresponding county or census tract.

This means that certain types of analysis and processing must be done in-house, even when a third party has resources and expertise more suited to the task. Since the source data is not disclosed, the way we go about this analysis and processing lacks transparency. As a result, any third party''s ability to perform QA/QC, adjust parameters or make refinements may be very limited.

Anonymizing Confidential Data
=

One task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as "Dave" and in another as "David," commercial entities can have many different abbreviations, and there are always some typos. I''ve developed scripts based on a number of criteria that determine when two records with non-identical names represent the same individual, and assign them a common ID.

At this point we can make the dataset anonymous by withholding the names and replacing them with this personal ID number. But this means the recipient has almost no information about e.g. the strength of the match. We would prefer to be able to pass along as much information as possible without divulging identity.

What Doesn''t Work
=

For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) PII. Perhaps we match strings in-house with edit distance <= 2, and the recipient wants to look at the implications of tightening that tolerance to edit distance <= 1.

But the only method I am familiar with that does this is ROT13, which hardly even counts as encryption; it''s like writing the names upside down and saying, "promise you won''t flip the paper over?"

Another **bad** solution would be to abbreviate everything. "Ellen Roberts" becomes "ER" and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person''s identity, and in other cases it''s too ambiguous; "Benjamin Othello Ames" and "Bank of America" will have the same initials, but their names are otherwise dissimilar. So it doesn''t do either of the things we want.

An inelegant alternative is to introduce additional fields to track certain attributes of the name, e.g.:

    +-----+----+-------------------+-----------+--------+
    | Row | ID | Name              | WordChars | Origin |
    +-----+----+-------------------+-----------+--------+
    | 1   | 17 | "AMELIA BEDELIA"  | (6, 7)    | Eng    |
    +-----+----+-------------------+-----------+--------+
    | 2   | 18 | "CHRISTOPH BAUER" | (9, 5)    | Ger    |
    +-----+----+-------------------+-----------+--------+
    | 3   | 18 | "C J BAUER"       | (1, 1, 5) | Ger    |
    +-----+----+-------------------+-----------+--------+
    | 4   | 19 | "FRANZ HELLER"    | (5, 6)    | Ger    |
    +-----+----+-------------------+-----------+--------+

I call this "inelegant" because it requires anticipating which qualities might be interesting and it''s relatively coarse. If the names are removed, there''s not much you can reasonably conclude about the strength of the match between rows 2 & 3, or about the distance between rows 2 & 4 (i.e., how close they are to matching).

Conclusion
=

The goal is to transform strings in such a way that as many useful qualities of the original string are preserved as possible while obscuring the original string. Decryption should be impossible, or so impractical as to be effectively impossible, no matter the size of the data set. In particular, a method that preserves the edit distance between arbitrary strings would be very useful.

I''ve found a couple papers that might be relevant, but they''re a bit over my head:

- [Privacy Preserving String Comparisons Based on Levenshtein Distance](http://www.merl.com/publications/docs/TR2010-109.pdf)
- [An Empirical Comparison of Approaches to Approximate String
Matching in Private Record Linkage](https://www.uni-due.de/~hq0215/documents/2010/Bachteler_2010_An_Empirical_Comparison_Of_Approaches_To_Approximate_String_Matching_In_Private_Record_Linkage.pdf)', 322, '2014-06-17 22:23:03.030', '22aa434e-da71-497a-895c-fddf0200f083', 412, 'added 338 characters in body', 1081, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One option (depending on your dataset size) is to just provide edit distances (or other measures of similarity you''re using) as an additional dataset.

E.g.:

1. Generate a set of unique names in the dataset
2. For each name, calculate edit distance to each other name
3. Generate an ID or irreversable hash for each name
4. Replace names in the original dataset with this ID
5. Provide matrix of edit distances between ID numbers as new dataset

Though there''s still a lot that could be done to deanonimise the data from these even.

E.g. if "Tim" is known to be the most popular name for a boy, frequency counting of IDs that closely match the known percentage of Tims across the population might give that away.  From there you could then look for names with an edit distance of 1, and conclude that those IDs might refer to "Tom" or "Jim" (when combined with other info).', 474, '2014-06-17 22:28:36.070', 'a78ab732-7ca7-4783-b04f-ecbdfd7f99d4', 443, 1082, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A classic book is by B. Efron who created the technique:

 - Bradley Efron; Robert Tibshirani (1994). An Introduction to the Bootstrap. Chapman & Hall/CRC. ISBN 978-0-412-04231-7.', 418, '2014-06-17 22:29:36.720', 'd12a863a-e403-4ffa-9574-f6a7a704ba70', 444, 1083, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is your Masters in Computer Science?  Statistics?

Is ''data science'' going to be at the center of your thesis?  Or a side topic?

I''ll assume your in Statistics and that you want to focus your thesis on a ''data science'' problem.  If so, then I''m going to go against the grain and suggest that you *should not* start with a data set or an ML method.  Instead, you should seek an interesting research problem that''s poorly understood or where ML methods have not yet been proven successful, or where there are many competing ML methods but none seem better than others.

Consider this data source: [Stanford Large Network Dataset Collection](http://snap.stanford.edu/data/).  While you *could* pick one of these data sets, make up a problem statement, and then run some list of ML methods, that approach really doesn''t tell you very much about what *data science* is all about, and in my opinion doesn''t lead to a very good Masters thesis.

Instead, you might do this: look for all the research papers that use ML on some specific category -- e.g. Collaboration networks (a.k.a. co-authorship). As you read each paper, try to find out what they *were* able to accomplish with each ML method and what they weren''t able to address.  Especially look for their suggestions for "future research".

Maybe they all use the same method, but never tried competing ML methods.  Or maybe they don''t adequately validate their results, or maybe there data sets are small, or maybe their research questions and hypothesis were simplistic or limited.

Most important: try to find out where this line of research is going.  Why are they even bothering to do this?  What is significant about it?  Where and why are they encountering difficulties?', 609, '2014-06-17 23:30:45.897', '7abd7b77-0570-4b13-868b-3a8de7c9a86e', 445, 1084, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One of the references I mentioned in the OP led me to a potential solution that seems quite powerful, described in "Privacy-preserving record linkage using Bloom filters" ([doi:10.1186/1472-6947-9-41](http://www.biomedcentral.com/1472-6947/9/41)):

>A new protocol for privacy-preserving record linkage with encrypted identifiers allowing for errors in identifiers has been developed. The protocol is based on Bloom filters on q-grams of identifiers.

The article goes into detail about the method, which I will summarize here to the best of my ability.


A Bloom filter is a fixed-length series of bits storing the results of a fixed set of independent hash functions, each computed on the same input value. The output of each hash function should be an index value from among the possible indexes in the filter; i.e., if you have a 0-indexed series of 10 bits, hash functions should return values from 0 to 9.

The filter starts with each bit set to 0. After hashing the input value with each function from the set of hash functions, each bit corresponding to an index value returned by any hash function is set to 1. If the same index is returned by more than one hash function, the bit at that index is only set once. You could consider the Bloom filter to be a superposition of the set of hashes onto the fixed range of bits.

The protocol described in the above-linked article divides strings into n-grams, which are in this case sets of characters. As an example, `"hello"` might yield the following set of 2-grams:

    ["_h", "he", "el", "ll", "lo", "o_"]

Padding the front and back with spaces seems to be generally optional when constructing n-grams; the examples given in the paper that proposes this method use such padding.

Each n-gram can be hashed to produce a Bloom filter, and this set of Bloom filters can be superimposed on itself (bitwise OR operation) to produce the Bloom filter for the string.

If the filter contains many more bits than there are hash functions or n-grams, arbitrary strings are relatively unlikely to produce exactly the same filter. However, the more n-grams two strings have in common, the more bits their filters will ultimately share. You can then compare any two filters `A, B` by means of their Dice coefficient:

> D<sub>A, B</sub> = 2h / (a + b)

Where `h` is the number of bits that are set to 1 in both filters, `a` is the number of bits set to 1 in *only* filter A, and `b` is the number of bits set to 1 in *only* filter B. If the strings are exactly the same, the Dice coefficient will be 1; the more they differ, the closer the coefficient will be to `0`.

Because the hash functions are mapping an indeterminate number of unique inputs to a small number of possible bit indexes, different inputs may produce the same filter, so the coefficient indicates only a *probability* that the strings are the same or similar. The number of different hash functions and the number of bits in the filter are important parameters for determining the likelihood of false positives - pairs of inputs that are much less similar than the Dice coefficient produced by this method predicts.

I found [this tutorial](http://billmill.org/bloomfilter-tutorial/) to be very helpful for understanding the Bloom filter.

There is some flexibility in the implementation of this method; see also [this 2010 paper](https://www.uni-due.de/~hq0215/documents/2010/Bachteler_2010_An_Empirical_Comparison_Of_Approaches_To_Approximate_String_Matching_In_Private_Record_Linkage.pdf) (also linked at the end of the question) for some indications of how performant it is in relation to other methods, and with various parameters.', 322, '2014-06-17 23:38:20.133', 'b6610aea-81ab-43df-bace-58bc3d35eeba', 446, 1085, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a large number of samples which represent Manchester encoded bit streams encoded as audio signals. The frequency at which they are encoded is the primary frequency component when it is high, and there is a consistent amount of white noise in the background. I have manually decoded these streams, but I was wondering I could use some sort of machine learning technique to learn the encoding schemes. This would save a great deal of time manually recognizing these schemes. The difficulty is that different signals are encoded differently. Is it possible to build a model which can learn to decode more than one encoding scheme? How robust would such a model be, and what sort of techniques would I want to employ? `ICA` seems like it could be useful for isolating the frequency I care about, but how would I learn the encoding scheme?', 890, '2014-06-18 03:19:07.557', '14802b84-e425-45d8-a4f0-4bd8fbd43403', 447, 1088, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Learning signal encoding', 890, '2014-06-18 03:19:07.557', '14802b84-e425-45d8-a4f0-4bd8fbd43403', 447, 1089, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><scalability><algorithms><feature-selection>', 890, '2014-06-18 03:19:07.557', '14802b84-e425-45d8-a4f0-4bd8fbd43403', 447, 1090, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Not sure about the YARN, but I think that Spark makes a real difference compared to Hadoop (advertised as 100 times faster) if data can fit nicely in the memory of the computational nodes. Simply because it avoids hard disk access. If data doesn''t fit memory there''s still some gain because of buffering.
', 418, '2014-06-18 05:35:35.700', '50c0628e-c2cf-4455-9e8f-60b81d764f3a', 448, 1091, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Does anyone know some good tutorials on online machine learning technics?
I.e. how it can be used in real-time environments, what are key differences compared to normal machine learning methods etc.

UPD: Thank you everyone for answers, by "online" I mean methods which can be trained in a real-time mode, based on a new inputs one by one.', 88, '2014-06-18 07:17:45.110', '6a3fa3cf-46bc-4a4b-b3fa-21634474e71e', 421, 'detalization of question', 1093, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hadoop means HDFS, YARN, MapReduce, and a lot of other things. Do you mean Spark vs *MapReduce*? Because Spark runs on/with Hadoop, which is rather the point.

The primary reason to use Spark is for speed, and this comes from the fact that its execution can keep data in memory between stages rather than always persist back to HDFS after a Map or Reduce. This advantage is very pronounced for iterative computations, which have tens of stages each of which is touching the same data. This is where things might be "100x" faster. For simple, one-pass ETL-like jobs for which MapReduce was designed, it''s not in general faster.

Another reason to use Spark is its nicer high-level language compared to MapReduce. It provides a functional programming-like view that mimics Scala, which is far nicer than writing MapReduce code. (Although you have to either use Scala, or adopt the slightly-less-developed Java or Python APIs for Spark). [Crunch][1] and [Cascading][2] already provide a similar abstraction on top of MapReduce, but this is still an area where Spark is nice.

Finally Spark has as-yet-young but promising subprojects for ML, graph analysis, and streaming, which expose a similar, coherent API. With MapReduce, you would have to turn to several different other projects for this (Mahout, Giraph, Storm). It''s nice to have it in one package, albeit not yet ''baked''.

Why would you not use Spark? [paraphrasing][3] myself:

- Spark is primarily Scala, with ported Java APIs; MapReduce might be friendlier and more native for Java-based developers
- There is more MapReduce expertise out there now than Spark
- For the data-parallel, one-pass, ETL-like jobs MapReduce was designed for, MapReduce is lighter-weight compared to the Spark equivalent
- Spark is fairly mature, and so is YARN now, but Spark-on-YARN is still pretty new. The two may not be optimally integrated yet. For example until recently I don''t think Spark could ask YARN for allocations based on number of cores? That is: MapReduce might be easier to understand, manage and tune


  [1]: http://crunch.apache.org
  [2]: http://cascading.org
  [3]: https://www.quora.com/Apache-Spark/Assuming-you-have-a-system-with-both-Hadoop-and-Spark-installed-say-under-Yarn-is-there-any-reason-to-use-Hadoop-map-reduce-instead-of-the-equivalent-Spark-commands', 21, '2014-06-18 07:27:22.677', '91e4f92d-1e7b-49ca-8e6a-1e89a585ce64', 449, 1094, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('One option (depending on your dataset size) is to just provide edit distances (or other measures of similarity you''re using) as an additional dataset.

E.g.:

1. Generate a set of unique names in the dataset
2. For each name, calculate edit distance to each other name
3. Generate an ID or irreversable hash for each name
4. Replace names in the original dataset with this ID
5. Provide matrix of edit distances between ID numbers as new dataset

Though there''s still a lot that could be done to deanonymise the data from these even.

E.g. if "Tim" is known to be the most popular name for a boy, frequency counting of IDs that closely match the known percentage of Tims across the population might give that away.  From there you could then look for names with an edit distance of 1, and conclude that those IDs might refer to "Tom" or "Jim" (when combined with other info).', 474, '2014-06-18 08:08:02.163', '6001d4f4-f404-4916-a40d-49c4ef0259b2', 443, 'Fixed typo', 1095, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''d like to explore ''data science''. The term seems a little vague to me, but I expect it to require:

1. machine learning (rather than traditional statistics);
2. a large enough dataset that you have to run analyses on clusters.

What are some good datasets and problems, accessible to a statistician with some programming background, that I can use to explore the field of data science?

To keep this as narrow as possible, I''d ideally like links to open, well used datasets and example problems.', 322, '2014-06-18 13:53:25.307', '0fa46d35-a777-4e9b-b216-1361b63a19da', 370, 'reformulated the question to be more concise and less specific to the OP''s thesis', 1098, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-18 13:53:25.307', '0fa46d35-a777-4e9b-b216-1361b63a19da', 370, 'Proposed by 322 approved by 50 edit id of 76', 1099, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('![enter image description here][1]

I am trying to do Logistic Regression using SAS Enterprise Miner.
My Independent variables are

    CPR/Inc (Categorical 1 to 7)
    OD/Inc (Categorical 1 to 4)
    Insurance (Binary 0 or 1)
    Income Loss (Binary 0 or 1)
    Living Arrangement (Categorical 1 to 7)
    Employment Status (categorical 1 to 8)

My Dependent Variable is Default (Binary 0 or 1)

The following is the output from running Regression Model.

 Analysis of Maximum Likelihood Estimates


                                      Standard          Wald
    Parameter       DF    Estimate       Error    Chi-Square    Pr > ChiSq    Exp(Est)

    Intercept        1     -0.4148      0.0645         41.30        <.0001       0.660
    CPR___Inc  1     1     -0.8022      0.1051         58.26        <.0001       0.448
    CPR___Inc  2     1     -0.4380      0.0966         20.57        <.0001       0.645
    CPR___Inc  3     1      0.3100      0.0871         12.68        0.0004       1.363
    CPR___Inc  4     1    -0.00304      0.0898          0.00        0.9730       0.997
    CPR___Inc  5     1      0.1331      0.0885          2.26        0.1324       1.142
    CPR___Inc  6     1      0.1694      0.0881          3.70        0.0546       1.185
    Emp_Status 1     1     -0.2289      0.1006          5.18        0.0229       0.795
    Emp_Status 2     1      0.4061      0.0940         18.66        <.0001       1.501
    Emp_Status 3     1     -0.2119      0.1004          4.46        0.0347       0.809
    Emp_Status 4     1      0.1100      0.0963          1.30        0.2534       1.116
    Emp_Status 5     1     -0.2280      0.1007          5.12        0.0236       0.796
    Emp_Status 6     1      0.3761      0.0943         15.91        <.0001       1.457
    Emp_Status 7     1     -0.3337      0.1026         10.59        0.0011       0.716
    Inc_Loss   0     1     -0.1996      0.0449         19.76        <.0001       0.819
    Insurance  0     1      0.1256      0.0559          5.05        0.0246       1.134
    Liv_Arran  1     1     -0.1128      0.0916          1.52        0.2178       0.893
    Liv_Arran  2     1      0.2576      0.0880          8.57        0.0034       1.294
    Liv_Arran  3     1      0.0235      0.0904          0.07        0.7950       1.024
    Liv_Arran  4     1      0.0953      0.0887          1.16        0.2825       1.100
    Liv_Arran  5     1     -0.0493      0.0907          0.29        0.5871       0.952
    Liv_Arran  6     1     -0.3732      0.0966         14.93        0.0001       0.689
    OD___Inc   1     1     -0.2136      0.0557         14.72        0.0001       0.808
    OD___Inc   2     1     -0.0279      0.0792          0.12        0.7248       0.973
    OD___Inc   3     1     -0.0249      0.0793          0.10        0.7534       0.975

Now I used this Model to Score a new set of data. An example row of my new data is

    CPR - 7
    OD - 4
    Living Arrangement - 4
    Employment Status - 4
    Insurance - 0
    Income Loss - 1

For this sample row, the model predicted output (Probability of default = 1) as 0.7335
To check this manually, I added the estimates

    Intercept + Emp Status 4 + Liv Arran 4 + Insurance 0
    -0.4148   + 0.1100  +   0.0953   +   0.1256    =   -0.0839

Odds ratio = Exponential(-0.0839) = 0.9195

Hence probability = 0.9195 / (1 + 0.9195)  =   0.4790

I am unable to understand why there is such a mismatch between the Model''s predicted probability and theoretical probability.

Any help would be much appreciated .
Thanks


  [1]: http://i.stack.imgur.com/4Ih6o.png


', 368, '2014-06-18 13:53:29.630', 'bb95da8a-d2a9-498f-9aa4-c8714daab615', 403, 'cleaned up question', 1100, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Why is there such a mismatch between the Model''s predicted probability and theoretical probability in logistic regression?', 368, '2014-06-18 13:53:29.630', 'bb95da8a-d2a9-498f-9aa4-c8714daab615', 403, 'cleaned up question', 1101, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-18 13:53:29.630', 'bb95da8a-d2a9-498f-9aa4-c8714daab615', 403, 'Proposed by 368 approved by 50 edit id of 77', 1102, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m not quite sure, but maybe  locality-sensitive hashing is a good solution. It does hashing of input data (in your case - names), so original strings would be preserved. On the other side, the main idea of LSH is to maximize hashes likelihood for similar items. There are a lot of different LSH-implementations. I tried [Nilsimsa-hash][1] for comparing tweet texts, and it worked quite well. But I''m not sure, how well it will work in case of short strings (names) - this issue require testing. I tried your examples, and here is the result (name A, name B, "distance" - maximum is 120):

    1. AMELIA BEDELIA  - CHRISTOPH BAUER - 107
    2. AMELIA BEDELIA  - C J BAUER       - 82
    3. AMELIA BEDELIA  - FRANZ HELLER    - 91
    4. CHRISTOPH BAUER - C J BAUER       - 81
    5. CHRISTOPH BAUER - FRANZ HELLER    - 98
    6. C J BAUER       - FRANZ HELLER    - 83

As you see, CHRISTOPH BAUER and C J BAUER turned up to be the closest pair. But difference is not significant.
And just for example - hash representation of these names:

    AMELIA BEDELIA  6b208299602b5000c3005a048122a43a828020889042240005011c1880864502
    CHRISTOPH BAUER 22226448000ab10102e2860b52062487ff0000928e0822ee106028016cc01237
    C J BAUER       2282204100961060048050004400240006032400148000802000a80130402002
    FRANZ HELLER    58002002400880080b49172044020008030002442631e004009195020ad01158


  [1]: http://spdp.di.unimi.it/papers/pdcs04.pdf', 941, '2014-06-18 14:15:37.957', '6ad3e111-deae-402c-82c0-1279ec5fccd8', 450, 1103, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m developing a distributed application, and as it''s been designed, there''ll be a great load of communication during the processing. Since the communication is already as much *spread* along the entire process as possible, I''m wondering if there any standard solutions to improve the performance of the message passing layer of my application.

What changes/improvements could I apply to my code to reduce the time spent sending messages? For what it''s worth, I''m communicating up to 10GB between 9 computing nodes, and the framework I''m using is implemented with OpenMPI.', 84, '2014-06-18 14:36:07.980', '1bb27382-ad07-40db-be12-b98d6d1b4012', 451, 1104, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to speedup message passing between computing nodes', 84, '2014-06-18 14:36:07.980', '1bb27382-ad07-40db-be12-b98d6d1b4012', 451, 1105, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<efficiency><distributed><performance>', 84, '2014-06-18 14:36:07.980', '1bb27382-ad07-40db-be12-b98d6d1b4012', 451, 1106, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('One of the references I mentioned in the OP led me to a potential solution that seems quite powerful, described in "Privacy-preserving record linkage using Bloom filters" ([doi:10.1186/1472-6947-9-41](http://www.biomedcentral.com/1472-6947/9/41)):

>A new protocol for privacy-preserving record linkage with encrypted identifiers allowing for errors in identifiers has been developed. The protocol is based on Bloom filters on q-grams of identifiers.

The article goes into detail about the method, which I will summarize here to the best of my ability.


A Bloom filter is a fixed-length series of bits storing the results of a fixed set of independent hash functions, each computed on the same input value. The output of each hash function should be an index value from among the possible indexes in the filter; i.e., if you have a 0-indexed series of 10 bits, hash functions should return (or be mapped to) values from 0 to 9.

The filter starts with each bit set to 0. After hashing the input value with each function from the set of hash functions, each bit corresponding to an index value returned by any hash function is set to 1. If the same index is returned by more than one hash function, the bit at that index is only set once. You could consider the Bloom filter to be a superposition of the set of hashes onto the fixed range of bits.

The protocol described in the above-linked article divides strings into n-grams, which are in this case sets of characters. As an example, `"hello"` might yield the following set of 2-grams:

    ["_h", "he", "el", "ll", "lo", "o_"]

Padding the front and back with spaces seems to be generally optional when constructing n-grams; the examples given in the paper that proposes this method use such padding.

Each n-gram can be hashed to produce a Bloom filter, and this set of Bloom filters can be superimposed on itself (bitwise OR operation) to produce the Bloom filter for the string.

If the filter contains many more bits than there are hash functions or n-grams, arbitrary strings are relatively unlikely to produce exactly the same filter. However, the more n-grams two strings have in common, the more bits their filters will ultimately share. You can then compare any two filters `A, B` by means of their Dice coefficient:

> D<sub>A, B</sub> = 2h / (a + b)

Where `h` is the number of bits that are set to 1 in both filters, `a` is the number of bits set to 1 in *only* filter A, and `b` is the number of bits set to 1 in *only* filter B. If the strings are exactly the same, the Dice coefficient will be 1; the more they differ, the closer the coefficient will be to `0`.

Because the hash functions are mapping an indeterminate number of unique inputs to a small number of possible bit indexes, different inputs may produce the same filter, so the coefficient indicates only a *probability* that the strings are the same or similar. The number of different hash functions and the number of bits in the filter are important parameters for determining the likelihood of false positives - pairs of inputs that are much less similar than the Dice coefficient produced by this method predicts.

I found [this tutorial](http://billmill.org/bloomfilter-tutorial/) to be very helpful for understanding the Bloom filter.

There is some flexibility in the implementation of this method; see also [this 2010 paper](https://www.uni-due.de/~hq0215/documents/2010/Bachteler_2010_An_Empirical_Comparison_Of_Approaches_To_Approximate_String_Matching_In_Private_Record_Linkage.pdf) (also linked at the end of the question) for some indications of how performant it is in relation to other methods, and with various parameters.', 322, '2014-06-18 15:08:44.507', '2c9ab747-c896-441b-b4f4-57d9fd0212d7', 446, 'added 18 characters in body', 1107, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As @Christopher Lauden mentioned above, time-series analysis is most appropriate for this sort of thing. If, however, you wished to do a more traditional "machine learning approach", something that I have done in the past is to block up your data into overlapping windows of time as features, then use it to predict the next days (or weeks) traffic.

Your feature matrix would be something like:

    t1 | t2 | ... | tN
    t2 | t3 | ... | tN+1
    t3 | t4 | ... | tN+2
    ...
    tW | tW+1 | ... |tN+W

where `tI` is the traffic on day `I`. The feature you''ll be predicting is the traffic on the day after the last column. In essence, use a window of traffic to predict the next day''s traffic.

Any sort of ML model would work for this.

', 403, '2014-06-18 15:10:22.637', '59845e75-e644-4e41-a1e2-e9837d060038', 452, 1108, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I understand that compression methods may be split into two main sets: global and local. The first set works regardless of the data being processed, i.e., they do not rely on any characteristic of the data, and thus need not to perform any preprocessing over any part of the dataset (before the compression itself). On the other hand, local methods analyze the data, extracting informations that usually improve the compression rate.

While reading about some of these methods, I noticed that the unary method is not universal, which surprised me since I thought "globality" and "universality" referred to the same thing. The unary method does not rely on characteristics of the data to yield its encoding (i.e., it is a global method), and therefore it should be global/universal, shouldn''t it?

What is the difference between universal and global methods? Aren''t these classifications synonyms?', 84, '2014-06-18 15:27:23.313', 'dc2a4cd0-a52a-4655-8c11-2eaa066e5b8c', 453, 1109, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the difference between global and universal compression methods?', 84, '2014-06-18 15:27:23.313', 'dc2a4cd0-a52a-4655-8c11-2eaa066e5b8c', 453, 1110, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<algorithms>', 84, '2014-06-18 15:27:23.313', 'dc2a4cd0-a52a-4655-8c11-2eaa066e5b8c', 453, 1111, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a highly biased binary dataset - I have 1000x more examples of the negative class than the positive class. I would like to train a Tree Ensemble (like Extra Random Trees or a Random Forest) on this data but it''s difficult to create training datasets that contain enough examples of the positive class.

What would be the implications of doing a stratified sampling approach to normalize the number of positive and negative examples? In other words, is it a bad idea to, for instance, artificially inflate (by resampling) the number of positive class examples in the training set?', 403, '2014-06-18 15:48:19.497', '101e8039-3357-4a9c-a419-4abaf71bc559', 454, 1112, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the implications for training a Tree Ensemble with highly biased datasets?', 403, '2014-06-18 15:48:19.497', '101e8039-3357-4a9c-a419-4abaf71bc559', 454, 1113, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><feature-selection>', 403, '2014-06-18 15:48:19.497', '101e8039-3357-4a9c-a419-4abaf71bc559', 454, 1114, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I understand that compression methods may be split into two main sets: global and local. The first set works regardless of the data being processed, i.e., they do not rely on any characteristic of the data, and thus need not to perform any preprocessing over any part of the dataset (before the compression itself). On the other hand, local methods analyze the data, extracting informations that usually improve the compression rate.

While reading about some of these methods, I noticed that [the unary method is not universal](http://en.wikipedia.org/wiki/Universal_code_%28data_compression%29#Universal_and_non-universal_codes), which surprised me since I thought "globality" and "universality" referred to the same thing. The unary method does not rely on characteristics of the data to yield its encoding (i.e., it is a global method), and therefore it should be global/universal, shouldn''t it?

What is the difference between universal and global methods? Aren''t these classifications synonyms?', 84, '2014-06-18 16:07:53.890', '8ae7bdce-4cf0-444d-934f-0249674d96fa', 453, 'Improving question.', 1115, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I need a recommendation regarding datasets for text classification problem', 960, '2014-06-18 16:21:12.203', 'a5314e2e-a19e-481a-9630-457405b1a9e4', 455, 1116, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Question related to text classification', 960, '2014-06-18 16:21:12.203', 'a5314e2e-a19e-481a-9630-457405b1a9e4', 455, 1117, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><text-mining>', 960, '2014-06-18 16:21:12.203', 'a5314e2e-a19e-481a-9630-457405b1a9e4', 455, 1118, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you expect (or find) that nodes are requesting the same data more than once, perhaps you could benefit from a caching strategy? Especially where some data is used much more often than others, so you can target only the most frequently-used information.

If the data is mutable, you also need a way to confirm that it hasn''t changed since the last request that''s less expensive than repeating the request.

This is further complicated if each node has its own separate cache. Depending on the nature of your system and task(s), you could consider adding a node dedicated to serving information between the processing nodes, and building a single cache on that node.

For an example of when that *might* be a good idea, let''s suppose I retrieve some data from a remote data store over a low-bandwidth connection, and I have some task(s) requiring that data, which are distributed exclusively among local nodes. I definitely wouldn''t want each node requesting information separately over that low-bandwidth connection, which another node might have previously requested. Since my local I/O is much less expensive than my I/O over the low-bandwidth connection, I might add a node between the processing nodes and the remote source that acts as an intermediate server. This node would take requests from the processing nodes, communicate with the remote data store, and cache frequently-requested data to minimize the use of that low-bandwidth connection.

The core concepts here that *may* be applicable to your specific case are:

- Eliminate or reduce redundant I/O;
- Take advantage of trade-offs between memory use and computation time;
- Not all I/O is created equal.', 322, '2014-06-18 17:22:46.207', '5f6c7107-445a-467e-8900-5f4c8182ecbf', 456, 1119, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Firstly, I would generally agree with everything that AirThomas suggested. Caching things is generally good if you can, but I find it slightly brittle since that''s very dependent on exactly what your application is. Data compression is another very solid suggestion, but my impression on both of these is that the speedups you''re looking at are going to be relatively marginal. Maybe as high as 2-5x, but I would be very surprised if they were any faster than that.

Under the assumption that pure I/O (writing to/reading from memory) is *not* your limiting factor (if it is, you''re probably not going to get a lot faster), I would make a strong plug for [zeromq][1]. In the words of the creators:

> We took a normal TCP socket, injected it with a mix of radioactive
> isotopes stolen from a secret Soviet atomic research project,
> bombarded it with 1950-era cosmic rays, and put it into the hands of a
> drug-addled comic book author with a badly-disguised fetish for
> bulging muscles clad in spandex. Yes, ØMQ sockets are the world-saving
> superheroes of the networking world.

While that may be a little dramatic, `zeromq` sockets in my opinion are one of the most amazing pieces of software that the world of computer networks has put together in several years. I''m not sure what you''re using for your message-passing layer right now, but if you''re using something traditional like `rabbitmq`, you''re liable to see speedups of multiple orders of magnitude (personally noticed about 500x, but depends a lot of architecture)

Check out some basic benchmarks [here.][2]


  [1]: http://zeromq.org/
  [2]: http://blog.x-aeon.com/2013/04/10/a-quick-message-queue-benchmark-activemq-rabbitmq-hornetq-qpid-apollo/', 548, '2014-06-18 18:22:11.680', '40ac3eb9-194b-4013-baad-0e2b5f957fc2', 457, 1120, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[K-means](http://en.wikipedia.org/wiki/K-means_clustering) is a well known algorithm for clustering, but there is also an online variation of such algorithm (online K-means). What are the pros and cons of these approaches, and when should each be preferred?', 84, '2014-06-18 19:48:54.883', 'e10886bc-04b3-4fff-9cd9-1c26f3277669', 458, 1121, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('K-means vs. online K-means', 84, '2014-06-18 19:48:54.883', 'e10886bc-04b3-4fff-9cd9-1c26f3277669', 458, 1122, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<algorithms><clustering>', 84, '2014-06-18 19:48:54.883', 'e10886bc-04b3-4fff-9cd9-1c26f3277669', 458, 1123, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Online k-means (more commonly known as [sequential k-means](http://stackoverflow.com/questions/3698532/online-k-means-clustering)) and traditional k-means are very similar.  The difference is that online k-means allows you to update the model as new data is received.

Online k-means should be used when you expect the data to be received one by one (or maybe in chunks).  This allows you to update your model as you get more information about it.  The drawback of this method is that it is dependent on the order in which the data is received ([ref](http://www.cs.princeton.edu/courses/archive/fall08/cos436/Duda/C/sk_means.htm)).  ', 178, '2014-06-18 20:07:05.017', 'b08deea8-a528-4963-9c5d-995bcc82dbfc', 459, 1124, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A fast, easy an often effective way to approach this imbalance would be to randomly subsample the bigger class (which in your case is the negative class), run the classification N number of times with members from the two classes (one full and the other subsampled) and report the average metric values, the average being computed over N (say 1000) iterations.

A more methodical approach would be to execute the Mapping Convergence (MC) algorithm, which involves identifying a subset of strong negative samples with the help of a one-class classifier, such as OSVM or SVDD, and then iteratively execute binary classification on the set of strong negative and positive samples. More details of the MC algorithm can be found in this [paper][3].


  [3]: http://link.springer.com/article/10.1007/s10994-005-1122-7#page-1', 984, '2014-06-18 21:56:29.147', '2f99e130-afab-4a37-b33e-c0b2fd3c024d', 460, 1125, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Theres this side project Im working on where I need to structure a solution to the following problem.

 I have two groups of people (clients). Group A intends to buy and group B intends to sell a determined product X.
The product has a series of attributes x_i and my objective is to facilitate the transaction between A e B by matching their preferences. The main idea is to point out to each member of A a corresponding in B whos product better suits his needs, and vice versa.

Some complicating aspects of the problem:

1)The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design which is rare among the population and I cant predict. Cant previously list all the attributes;

2)Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design);

Any suggestion on how to approach this problem and solve it in an automated way?
The idea is to really think out of the box here so feel free to go wild on your suggestions.

I would also appreciate some references to other similar problems if possible.

Thanks!
', 986, '2014-06-18 22:10:58.497', 'c3117eab-4e00-40a7-a297-8f3992e5aa54', 461, 1126, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Preference Matching Algorithm', 986, '2014-06-18 22:10:58.497', 'c3117eab-4e00-40a7-a297-8f3992e5aa54', 461, 1127, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><text-mining>', 986, '2014-06-18 22:10:58.497', 'c3117eab-4e00-40a7-a297-8f3992e5aa54', 461, 1128, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would recommend training on more balanced subsets of your data. Training random forest on sets of randomly selected positive example with a similar number of negative samples. In particular if the discriminative features exhibit a lot of variance this will be fairly effective and avoid over-fitting. However in stratification it is important to find balance as over-fitting can become a problem regardless. I would suggest seeing how the model does with the whole data set then progressively increasing the ratio of positive to negative samples approaching an even ratio, and selecting for the one that maximizes your performance metric on some representative hold out data.

This paper seems fairly relevant http://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf it talks about a `weighted Random Forest` which more heavily penalizes misclassification of the minority class.  ', 548, '2014-06-18 22:27:06.503', '3f77fc36-6c73-474d-a731-9acb4318d596', 462, 1129, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My first suggestion would be to somehow map the non-quantifiable attributes to quantities with the help of suitable mapping functions. Otherwise, simply leave them out.

Secondly, I don''t think that you need to assume that the list of attributes is not finite. A standard and intuitive approach is to represent each attribute as an individual dimension in a vector space. Each product is then simply a point in this space. In that case, if you want to dynamically add more attributes you simply have to remap the product vectors into the new feature space (with additional dimensions).

With this representation, a seller is a point in the feature space with product attributes and a buyer is a point in the same feature space with the preference attributes. The task is then to find out the most similar buyer point for a given seller point.

If your dataset (i.e. the number of buyers/sellers) is not very large, you can solve this with a nearest neighbour approach implemented with the help of k-d trees.

For very large sized data, you can take an IR approach. Index the set of sellers (i.e. the product attributes) by treating each attribute as a separate term with the term-weight being set to the attribute value. A query in this case is a buyer which is also encoded in the term space as a query vector with appropriate term weights. The retrieval step would return you a list of top K most similar matches.', 984, '2014-06-18 22:45:25.677', '201ec198-7bea-4ee8-9aa3-73ff30f65711', 463, 1130, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Some standard datasets for text classification are the 20-News group, Reuters (with 8 and 52 classes) and WebKb. You can find all of them [here][1].


  [1]: http://web.ist.utl.pt/~acardoso/datasets/', 984, '2014-06-18 22:48:53.350', '7159e266-22a2-4d67-bf1b-78a4681c6d6d', 464, 1131, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('nDCG is used to evaluate a golden ranked list (typically human judged) against your output ranked list. The more is the correlation between the two ranked lists, i.e. the more similar are the ranks of the relevant items in the two lists, the closer is the value of nDCG to 1.

RMSE (Root Mean Squared Error) is typically used to evaluate regression problems where the output (a predicted scalar value) is compared with the true scalar value output for a given data point.

So, if you are simply recommending a score (such as recommending a movie rating), then use RMSE. Whereas, if you are recommending a list of items (such as a list of related movies), then use nDCG.  ', 984, '2014-06-18 22:58:35.260', 'ca1ef62b-3916-4660-b6b2-06da21f37c8e', 465, 1132, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I tried to detect outliers in the energy gas consumption of some dutch buildings, building a neural network model. I have very bad results, but I don''t find the reason.

I am not an expert so I would like to ask you what I can improve and what I''m doing wrong. This is the complete description: https://github.com/denadai2/Gas-consumption-outliers', 989, '2014-06-18 23:02:54.200', 'cf096d39-c830-4141-ad06-324410236298', 466, 1133, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Gas consumption outliers detection - Neural network project. Bad results', 989, '2014-06-18 23:02:54.200', 'cf096d39-c830-4141-ad06-324410236298', 466, 1134, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork>', 989, '2014-06-18 23:02:54.200', 'cf096d39-c830-4141-ad06-324410236298', 466, 1135, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m working on a project and need resources to get me up to speed.

The dataset is around 35000 observations on 30 or so variables.  About half the variables are categorical with some having many different possible values, i.e. if you split the categorical variables into dummy variables you would have a lot more than 30 variables.  But still probably on the order of a couple of hundred max.  (n>p).

The response we want to predict is ordinal with 5 levels (1,2,3,4,5).  Predictors are a mix of continuous and categorical, about half of each.  These are my thoughts/plans so far:
1.  Treat the response as continuous and run vanilla linear regression.
2.  Run nominal and ordinal logistic and probit regression
3.  Use MARS and/or another flavor of non-linear regression

I''m familiar with linear regression.  MARS is well enough described by Hastie and Tibshirani.  But I''m at a loss when it comes to ordinal logit/probit, especially with so many variables and a big data set.

The r package [glmnetcr][1] seems to be my best bet so far, but the documentation hardly suffices to get me where I need to be.

Where can I go to learn more?


  [1]: http://cran.r-project.org/web/packages/glmnetcr/index.html', 994, '2014-06-19 03:43:23.853', '3469df97-039b-41b5-a460-3ea50569d7fc', 468, 1137, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Learning ordinal regression in R?', 994, '2014-06-19 03:43:23.853', '3469df97-039b-41b5-a460-3ea50569d7fc', 468, 1138, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<logistic-regression>', 994, '2014-06-19 03:43:23.853', '3469df97-039b-41b5-a460-3ea50569d7fc', 468, 1139, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I recently saw a cool feature that [was once available](https://support.google.com/docs/answer/3543688?hl=en) in Google Sheets: you start by writing a few related keywords in consecutive cells, say: "blue", "green", "yellow", and it automatically generates similar keywords (in this case, other colors). See more examples in [this YouTube video](http://youtu.be/dlslNhfrQmw).

I would like to reproduce this in my own program. I''m thinking of using Freebase, and it would work like this intuitively:

1. Retrieve the list of given words in Freebase;
2. Find their "common denominator(s)" and construct a distance metric based on this;
3. Rank other concepts based on their "distance" to the original keywords;
4. Display the next closest concepts.

As I''m not familiar with this area, my questions are:

* Is there a better way to do this?
* What tools are available for each step?', 322, '2014-06-19 05:48:43.540', '574d4027-47be-4068-8c5f-fb0852027fee', 424, 'language, add reference', 1140, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-19 05:48:43.540', '574d4027-47be-4068-8c5f-fb0852027fee', 424, 'Proposed by 322 approved by 906 edit id of 78', 1141, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I suggest this tutorial on ordered logit: http://www.ats.ucla.edu/stat/r/dae/ologit.htm

It showcases the use of `polr` in the `MASS` package, and also explains the assumptions and how to interpret the results.', 906, '2014-06-19 05:52:57.527', 'a280cf42-c4b6-4c22-b636-2b3d43d75b2d', 469, 1142, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Just an idea - your data is highly seasonal: daily and weekly cycles are quite perceptible. So first of all, try to decompose your variables (gas and electricity consumption, temperature, and solar radiation). [Here is][1] a nice tutorial on time series decomposition for R.

After obtaining trend and seasonal components, the most interesting part begins. It''s just an assumption, but I think, gas and electricity consumption variables would be quite predictable by means of time series analysis (e.g., [ARIMA model][2]). From my point of view, the most exiting part here is to try to predict residuals after decomposition, using available data (temperature anomalies, solar radiation, wind speed). I suppose, these residuals would be outliers, you are looking for. Hope, you will find this useful.


  [1]: http://www.r-bloggers.com/time-series-decomposition/
  [2]: http://statsmodels.sourceforge.net/devel/examples/notebooks/generated/tsa_arma_0.html', 941, '2014-06-19 06:09:43.963', '73a3b3f6-e95b-414e-8d8d-42d89d001f53', 470, 1143, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One of the most widely used test collection for text categorization research (link below). I''ve used many times. Enjoy your exploration :)

http://www.daviddlewis.com/resources/testcollections/reuters21578/
or
http://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection', 944, '2014-06-19 07:22:38.987', '1bf91e3a-ac39-4898-b5c7-246de1b68f0f', 471, 1144, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I tried to detect outliers in the energy gas consumption of some dutch buildings, building a neural network model. I have very bad results, but I don''t find the reason.

I am not an expert so I would like to ask you what I can improve and what I''m doing wrong. This is the complete description: https://github.com/denadai2/Gas-consumption-outliers.

The neural network is a FeedFoward Network with Back Propagation. As described [here](http://nbviewer.ipython.org/github/denadai2/Gas-consumption-outliers/blob/master/3-%20Regression_NN.ipynb) I splitted the dataset in a "small" dataset of 41''000 rows, 9 features and I tried to add more features.

I trained the networks but the results have 14.14 RMSE, so it can''t predict so well the gas consumptions, consecutevely I can''t run a good outlier detection mechanism. I see that in some papers that even if they predict daily or hourly consumption in the electric power, they have errors like MSE = 0.01.

What can I improve? What am I doing wrong? Can you have a look of my description?', 989, '2014-06-19 08:55:10.640', '5a509716-0342-4848-b9fb-10bf1c9b5008', 466, 'added 696 characters in body', 1150, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The usual definition of regression (as far as I am aware) is _predicting a continuous output variable from a given set of input variables_.

Logistic regression is a binary classification algorithm, so it produces a categorical output.

Is it really a regression algorithm? If so, why?', 922, '2014-06-19 08:56:46.847', 'b87f711a-5d37-40b8-a9a0-c0238b58e1bd', 473, 1151, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is logistic regression a regression algorithm?', 922, '2014-06-19 08:56:46.847', 'b87f711a-5d37-40b8-a9a0-c0238b58e1bd', 473, 1152, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<algorithms><logistic-regression>', 922, '2014-06-19 08:56:46.847', 'b87f711a-5d37-40b8-a9a0-c0238b58e1bd', 473, 1153, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In network structure, what is the difference between **k-cliques** and **p-cliques**, can anyone give a brief explaination with examples? Thanks in advanced!', 957, '2014-06-19 09:42:28.160', '0ceee5d4-d4be-4a7a-bbdf-3b37355fba63', 474, 1154, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('network structure  k-cliques vs p-cliques', 957, '2014-06-19 09:42:28.160', '0ceee5d4-d4be-4a7a-bbdf-3b37355fba63', 474, 1155, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<social-network-analysis>', 957, '2014-06-19 09:42:28.160', '0ceee5d4-d4be-4a7a-bbdf-3b37355fba63', 474, 1156, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As you discuss the definition of regression is predicting a continuous variable. [Logistic regression][1] is a binary classifier.  Logistic regression is the application of a logit function, that turns (-inf,+inf) to [0,1], on the output of a usual regression approach. I think it is just for historical reasons that keeps that name.

Saying something like "I did some regression to classify images. In particular I used logistic regression." is wrong.


  [1]: http://en.wikipedia.org/wiki/Logistic_regression', 418, '2014-06-19 09:50:53.657', 'ba76a6e2-b69e-4723-b3e5-c14b2993a39a', 475, 1157, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As you discuss the definition of regression is predicting a continuous variable. [Logistic regression][1] is a binary classifier.  Logistic regression is the application of a logit function on the output of a usual regression approach. Logit function turns (-inf,+inf) to [0,1]. I think it is just for historical reasons that keeps that name.

Saying something like "I did some regression to classify images. In particular I used logistic regression." is wrong.


  [1]: http://en.wikipedia.org/wiki/Logistic_regression', 418, '2014-06-19 09:59:24.653', 'b562a709-afde-4f41-9f7a-8e8645aa119a', 475, 'added 9 characters in body', 1158, '5');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-06-19 10:30:43.993', '8e6903aa-1a4d-4fe9-9ea0-0864f03e149a', 477, 'from http://programmers.stackexchange.com/questions/245430/preference-matching-algorithm', 1160, '36');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-06-19 10:30:43.993', '6d157aa2-efb6-4ee7-b314-ffbf3fbb9fec', 478, 'from http://programmers.stackexchange.com/questions/245430/preference-matching-algorithm/245442#245442', 1161, '36');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('pipja', 'This can be a cross between machine learning and simple matching exercise.

I think X_i tend to be rather defined and finite, while A_i can be vague and not finite. From a pure algorithm perspective I would search for instances where X_i = A_i and store the results into a container of sort. The more hits for certain X''es where X_i_n = A_i_k the more points X scores. X''es are then presented to A in the order of points from best match to lowest match.

Onto the machine learning mechanism, as the algorithm serves a lot of As (by that mean thousands and thousands, even millions) patterns will start to develop and certain combination of A_i''s will be more prevalent, or in other words, worth more to other A_i''s for a certain category of A. Using these patterns, the weighting of points will be re-balanced for higher chance of hitting the correct offers.

Kind of like how a search engine works.', '2014-06-19 03:20:00.847', '671c143b-cc30-4630-aa60-210f1e7c36a1', 478, 1162, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''s this side project I''m working on where I need to structure a solution to the following problem.

 I have two groups of people (clients). Group "A" intends to buy and group "B" intends to sell a determined product "X".

The product has a series of attributes x_i and my objective is to facilitate the transaction between "A" e "B" by matching their preferences. The main idea is to point out to each member of "A" a corresponding in "B" whos product better suits his needs, and vice versa.

Some complicating aspects of the problem:

1. The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design which is rare among the population and I cant predict. Cant previously list all the attributes;

2. Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design).

Any suggestion on how to approach this problem and solve it in an automated way?
The idea is to really think out of the box here so feel free to "go wild" on your suggestions.

I would also appreciate some references to other similar problems if possible. ', 986, '2014-06-18 22:15:43.820', 'fa93f037-4c55-4680-aee3-c6e59827ec32', 477, 1163, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Preference Matching Algorithm', 986, '2014-06-18 22:15:43.820', 'fa93f037-4c55-4680-aee3-c6e59827ec32', 477, 1164, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<algorithms>', 986, '2014-06-18 22:15:43.820', 'fa93f037-4c55-4680-aee3-c6e59827ec32', 477, 1165, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One fairly powerful R package for regression with an ordinal categorical response is VGAM, on the CRAN. The vignette contains some examples of ordinal regression, but admittedly I have never tried it on such a large dataset, so I cannot estimate how long it may take. You may find some additional material about VGAM on the author''s [page][1]. Alternatively you could take a look at Laura Thompson''s [companion][2] to Agresti''s book "Categorical Data Analysis". Chapter 7 of Thompson''s book describes cumulative logit models, which are frequently used with ordinal responses.

Hope this helps!


  [1]: https://www.stat.auckland.ac.nz/~yee/VGAM/
  [2]: https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0CDUQFjAC&url=http://home.comcast.net/~lthompson221/Splusdiscrete2.pdf&ei=8bmiU4HSFcjA7Abk4YHgDg&usg=AFQjCNHuBp2_nRpaPOFgdkcQWJGuSO9V6A&sig2=hAy4d3mu9WCJZulqxCzraw
', 1004, '2014-06-19 10:35:37.190', '9bc0cfda-1405-49ca-804d-be124ea869c9', 480, 1167, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Is logistic regression actually a regression algorithm?', 922, '2014-06-19 10:55:38.920', '65a2b833-45f7-465a-9322-0fc1c214fd14', 473, 'edited title', 1168, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In network structure, what is the difference between **k-cliques** and **p-cliques**? Can anyone give a brief explaination with examples?', 84, '2014-06-19 11:40:12.107', '8753693b-33c0-4937-aa7b-8fe1c80fd139', 474, 'Improving formatting.', 1171, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Network structure: k-cliques vs. p-cliques', 84, '2014-06-19 11:40:12.107', '8753693b-33c0-4937-aa7b-8fe1c80fd139', 474, 'Improving formatting.', 1172, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<definitions><social-network-analysis><graphs>', 84, '2014-06-19 11:40:12.107', '8753693b-33c0-4937-aa7b-8fe1c80fd139', 474, 'Improving formatting.', 1173, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In graph theory a clique indicates a fully connected set of nodes: [as noted here][1], a p-clique simply indicates a clique comoprised of p nodes. A k-clique is an undirected graph and a number k, and the output is a clique of size k if one exists.

[Clique Problem][2]


  [1]: http://books.google.com/books?id=E3-OSVSPbU0C&pg=PA40&lpg=PA40&dq=%22graph%20theory%22,%20%22p-clique%22&source=bl&ots=smbhcK-9AC&sig=X0v_EbqSqB4WBbudgPbqo_j1pvk&hl=en&sa=X&ei=VNWiU8KaA9WxsQSQhYGIAg&ved=0CDEQ6AEwBA#v=onepage&q=%22graph%20theory%22,%20%22p-clique%22&f=false
  [2]: http://en.wikipedia.org/wiki/Clique_problem', 59, '2014-06-19 12:22:42.017', '6a8aa682-6c2a-4766-98eb-8df7f230bbbb', 481, 1176, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Which freely available datasets can I use to train a text classifier?', 922, '2014-06-19 12:41:13.937', 'ada4a3e9-319b-471c-a3b1-bef861fc674a', 455, 'Make the title an actual question; tidy up content', 1177, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Suggest text classifier training datasets', 922, '2014-06-19 12:41:13.937', 'ada4a3e9-319b-471c-a3b1-bef861fc674a', 455, 'Make the title an actual question; tidy up content', 1178, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-19 12:41:13.937', 'ada4a3e9-319b-471c-a3b1-bef861fc674a', 455, 'Proposed by 922 approved by 84, 960 edit id of 81', 1179, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Short Answer**

Yes, logistic regression is a regression algorithm and it does predict a continuous outcome: the probability of an event.  That we use it as a binary classifier is due to the interpretation of the outcome.

**Detail**

Logistic regression is a type of generalize linear regression model.

In an ordinary linear regression model, a continuous outcome, `y`, is modeled as the sum of the product of predictors and their effect:

    y = b_0 + b_1 * x_1 + b_2 * x_2 + ... b_n * x_n + e

where `e` is the error.

Generalized linear models do not model `y` directly.  Instead, they use transformations to expand the domain of `y` to all real numbers.  This transformation is called the link function.  For logistic regression the link function is the logit function (usually, see note below).

The logit function is defined as

    ln(y/(1 + y))

Thus the form of logistic regression is:

    ln(y/(1 + y)) = b_0 + b_1 * x_1 + b_2 * x_2 + ... b_n * x_n + e

where `y` is the probability of an event.

The fact that we use it as a binary classifier is due to the interpretation of the outcome.', 178, '2014-06-19 13:23:53.387', '2ac0a2a5-1cbf-47b0-9652-4a724d680c02', 482, 1181, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**Short Answer**

Yes, logistic regression is a regression algorithm and it does predict a continuous outcome: the probability of an event.  That we use it as a binary classifier is due to the interpretation of the outcome.

**Detail**

Logistic regression is a type of generalize linear regression model.

In an ordinary linear regression model, a continuous outcome, `y`, is modeled as the sum of the product of predictors and their effect:

    y = b_0 + b_1 * x_1 + b_2 * x_2 + ... b_n * x_n + e

where `e` is the error.

Generalized linear models do not model `y` directly.  Instead, they use transformations to expand the domain of `y` to all real numbers.  This transformation is called the link function.  For logistic regression the link function is the logit function (usually, see note below).

The logit function is defined as

    ln(y/(1 + y))

Thus the form of logistic regression is:

    ln(y/(1 + y)) = b_0 + b_1 * x_1 + b_2 * x_2 + ... b_n * x_n + e

where `y` is the probability of an event.

The fact that we use it as a binary classifier is due to the interpretation of the outcome.

Note: probit is another link function used for logistic regression but logit is the most widely used.', 178, '2014-06-19 13:47:55.017', 'a5a20204-17e2-48aa-bcb2-0bbdd28dae41', 482, 'Add forgotten note.', 1182, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You could try Neural Network. You can find 2 great explanations on how to apply NN on time series [here][1] and [here][2].

Note that it is best practice to :

 - Deseasonalize/detrend the input data (so that the NN will not learn the seasonality).
 - Rescale/Normalize the input data.

Because what you are looking for is a regression problem, the activation functions should be `linear` and not `sigmoid` or `tanh` and you aim to minimize the `sum-of-squares error` (as opposition to the maximization of the `negative log-likelihood` in a classification problem).


  [1]: http://stats.stackexchange.com/questions/10162/how-to-apply-neural-network-to-time-series-forecasting
  [2]: http://stackoverflow.com/questions/18670558/prediction-using-recurrent-neural-network-on-time-series-dataset', 968, '2014-06-19 14:15:07.433', '078d7be5-da61-4907-b824-224a8dc778f5', 483, 1183, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As @Christopher Lauden mentioned above, time-series analysis is most appropriate for this sort of thing. If, however, you wished to do a more traditional "machine learning approach", something that I have done in the past is to block up your data into overlapping windows of time as features, then use it to predict the next days (or weeks) traffic.

Your feature matrix would be something like:

    t1 | t2 | ... | tN
    t2 | t3 | ... | tN+1
    t3 | t4 | ... | tN+2
    ...
    tW | tW+1 | ... |tN+W

where `tI` is the traffic on day `I`. The feature you''ll be predicting is the traffic on the day after the last column. In essence, use a window of traffic to predict the next day''s traffic.

Any sort of ML model would work for this.

**Edit**

In response to the question, "can you elaborate on how you use this feature matrix":

The feature matrix has values indicating past traffic over a period of time (for instance, hourly traffic over 1 week), and we use this to predict traffic for some specified time period in the future. We take our historic data and build a feature matrix of historic traffic and label this with the traffic at some period in the future (e.g. 2 days after the window in the feature). Using some sort of regression machine learning model, we can take historic traffic data, and try and build a model that can predict how traffic moved in our historic data set. The presumption is that future traffic will resemble past traffic.

', 403, '2014-06-19 14:32:43.503', '586d3767-846b-45d4-a1db-e0b7a83d96b1', 452, 'clarified feature matrix.', 1184, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Logistic regression is regression, first and foremost. It becomes a classifier by adding a decision rule. I will give an example that goes backwards. That is, instead of taking data and fitting a model, I''m going to start with the model in order to show how this is truly a regression problem.

In logistic regression, we are modeling the log odds, or logit, that an event occurs, which is a continuous quantity. If the probability that event A occurs is P(A), the odds are:

P(A) / (1 - P(A))

The log odds, then, are:

log { (P(A) / (1 - P(A))) }

As in linear regression, we model this with a linear combination of coefficients and predictors:

logit = b0 + b1 * x1 + b2 * x2 ...

Imagine we are given a model of whether a person has gray hair. Our model uses age as the only predictor. Here, our event A = a person has gray hair:

log odds of gray hair = -10 + 0.25 * age

...Regression! Here is some Python code and a plot:

    %matplotlib inline
    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns

    x = np.linspace(0, 100, 100)

    def log_odds(x):
        return -10 + .25 * x

    plt.plot(x, log_odds(x))
    plt.xlabel("age")
    plt.ylabel("log odds of gray hair")

![plot of the log odds for our toy example][1]


Now, let''s make it a classifier. First, we need to transform the log odds to get out our probability P(A). We can use the sigmoid function:

P(A) = 1 / (1 + exp(-log odds))

Here''s the code:

    plt.plot(x, 1 / (1 + np.exp(-log_odds(x))))
    plt.xlabel("age")
    plt.ylabel("probability of gray hair")

![plot of the probability of gray hair for our toy example][2]

The last thing we need to make this a classifier is to add a decision rule. One very common rule is to classify a success whenever P(A) > 0.5. We will adopt that rule, which implies that our classifier will predict gray hair whenever a person is older than 40 and will predict non-gray hair whenever a person is under 40.

Logistic regression works great as a classifier in more realistic examples too, but before it can be a classifier, it must be a regression technique!

  [1]: http://i.stack.imgur.com/xR0OT.png
  [2]: http://i.stack.imgur.com/hSpCa.png', 1011, '2014-06-19 14:52:59.877', '43fd78ad-1954-498e-b8b9-6b94984e0eeb', 484, 1185, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There''s this side project I''m working on where I need to structure a solution to the following problem.

I have two groups of people (clients). Group `A` intends to buy and group `B` intends to sell a determined product `X`. The product has a series of attributes `x_i`, and my objective is to facilitate the transaction between `A` and `B` by matching their preferences. The main idea is to point out to each member of `A` a corresponding in `B` whose product better suits his needs, and vice versa.

Some complicating aspects of the problem:

1. The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design, which is rare among the population and I can''t predict. Can''t previously list all the attributes;

2. Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design);

Any suggestion on how to approach this problem and solve it in an automated way? The idea is to really think out of the box here, so feel free to "go wild" on your suggestions.

I would also appreciate some references to other similar problems if possible. ', 84, '2014-06-19 14:55:34.833', 'be7b2d6f-ce4d-4ef1-91b0-7c299d62af01', 461, 'Improving formatting.', 1186, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><text-mining><recommendation>', 84, '2014-06-19 14:55:34.833', 'be7b2d6f-ce4d-4ef1-91b0-7c299d62af01', 461, 'Improving formatting.', 1187, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-19 14:55:34.833', 'be7b2d6f-ce4d-4ef1-91b0-7c299d62af01', 461, 'Proposed by 84 approved by 50 edit id of 79', 1188, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a large number of samples which represent Manchester encoded bit streams as audio signals. The frequency at which they are encoded is the primary frequency component when it is high, and there is a consistent amount of white noise in the background.

I have manually decoded these streams, but I was wondering if I could use some sort of machine learning technique to learn the encoding schemes. This would save a great deal of time manually recognizing these schemes. The difficulty is that different signals are encoded differently.

Is it possible to build a model which can learn to decode more than one encoding scheme? How robust would such a model be, and what sort of techniques would I want to employ? [Independent Component Analysis](http://en.wikipedia.org/wiki/Independent_component_analysis) (ICA) seems like could be useful for isolating the frequency I care about, but how would I learn the encoding scheme?', 84, '2014-06-19 14:55:59.483', 'e4bb9b6a-b180-459f-8233-2f1321bca0c2', 447, 'Improving formatting.', 1189, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-19 14:55:59.483', 'e4bb9b6a-b180-459f-8233-2f1321bca0c2', 447, 'Proposed by 84 approved by 50 edit id of 80', 1190, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My answer would be no. I consider Data mining to be one of the miscellaneous fields in Data science.Data Mining is mostly considered on yielding questions rather than answering them.It is often termed as "detecting something new" when compared to Data science where the data scientist try to solve complex problems to be able to reach their end results.However both terms have many commonalities between them', 1015, '2014-06-19 16:07:29.907', 'bf59d5a0-b268-42e7-92d2-4f9cc8edeb92', 485, 1191, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('My answer would be no. I consider Data mining to be one of the miscellaneous fields in Data science. Data Mining is mostly considered on yielding questions rather than answering them. It is often termed as "detecting something new", when compared to Data science, where the data scientist try to solve complex problems to be able to reach their end results. However both terms have many commonalities between them.', 84, '2014-06-19 16:52:55.683', '261a7094-5b73-4603-be64-374dd16be225', 485, 'Improving formatting.', 1192, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A good list of publicly available social network datasets can be found on the Stanford Network Analysis Project website:

[SNAP datasets][1]

The site contains internet social network data (Facebook, Twitter, Google Plus), Citation networks for academic journals, co-purchasing networks from Amazon and several others kinds of networks. They have directed, undirected, and bipartite graphs and all datasets are snapshots that can be downloaded in compressed form.

  [1]: https://snap.stanford.edu/data/', 1011, '2014-06-19 18:01:36.120', '04fcc946-42ae-4b4e-93b7-32e9be24da70', 487, 1196, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I thought that generalized linear model (GLM) would be considered a statistical model, but a friend told me that some papers classify it as a machine learning technique. Which one is true (or more precise)? Any explanation would be appreciated.

P.S. I am obviously a beginner in this field.', 1021, '2014-06-19 18:02:24.650', '9a9d7e43-e618-4ce6-a410-2a9d79ae8db4', 488, 1197, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is GLM a statistical or machine learning model?', 1021, '2014-06-19 18:02:24.650', '9a9d7e43-e618-4ce6-a410-2a9d79ae8db4', 488, 1198, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><statistics>', 1021, '2014-06-19 18:02:24.650', '9a9d7e43-e618-4ce6-a410-2a9d79ae8db4', 488, 1199, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A GLM is absolutely a statistical model, but statistical models and machine learning techniques are not mutually exclusive. In general, statistics is more concerned with inferring parameters, whereas in machine learning, prediction is the ultimate goal.', 1011, '2014-06-19 18:05:51.070', '3eb2224f-342c-4dc1-9726-f07c9a7ae182', 489, 1200, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As suggested, «going wild». First of all, correct, if Im wrong:

* for exact product just a few features exist;
* there is no ultimate features list, and clients are able to add new features to their products;

If so, constructing full product-feature table could be computational expensive. And final data table would be extremely sparse.

So, the first step is narrowing customers (products) list for matching. Lets build bipartite graph, where sellers would be type-1 nodes, and buyers would be type-2 nodes. Reference to similar product feature creates edge between seller and buyer. Here is a sketch:

![graph][1]

Using graph, described above, for every exact sellers product you can select only buyers, who is interested in matching features (its possible to filter «at least one» common feature, full matching, or threshold level). But certainly, thats not enough. Next step is to compare feature values, described by seller and buyer. There are a lot of variants (e.g., kNN, or something else). But why not to try to solve this quest, using already existing graph? Lets add weights to edges:

* for continuous features (e.g., price):

  ![price_weight][2]

* for binary and non-quantifiable features - just logical biconditional:

  ![feature_weight][3]

The main idea here is to «scale» every feature to `[0; 1]` interval. Additionally, we can use feature coefficients to determine most important features. E.g., assuming price is twice more important than availability of some rare function:

![adj_w_1][4]

![adj_w_2][5]

Almost final step: simplifying graph structure and reducing variety of edges to one edge with weight equal to sum of previously calculated weights of different features. With reduced structure every pair of customers/products could have only one edge (no parallel edges). So, to find the best deal for exact seller you just need to select connected buyers with max weighted edges.

>! Future challenge: introduce cheap method for weighting edges on first step :)


  [1]: http://i.stack.imgur.com/q1JTN.png
  [2]: https://chart.googleapis.com/chart?cht=tx&chl=weight%20%3D%20%5Cfrac%7B%7Cprice_s_e_l_l%20-%20price_b_u_y%7C%7D%7B%5Cmax%28price%29%20-%20%5Cmin%28price%29%7D
  [3]: https://chart.googleapis.com/chart?cht=tx&chl=weight%20%3D%20feature_s_e_l_l%20%5Cleftrightarrow%20feature_b_u_y
  [4]: https://chart.googleapis.com/chart?cht=tx&chl=adj.price.weight%20%3D%202%20%5Ctimes%20price.weight
  [5]: https://chart.googleapis.com/chart?cht=tx&chl=adj.feature.weight%20%3D%201%20%5Ctimes%20feature.weight', 941, '2014-06-19 18:24:32.023', '95b2404a-77b5-46e2-939c-25af72d40fc1', 491, 1202, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m looking to use google''s word2vec implementation to build a named entity recognition system.  I''ve heard that recursive neural nets with back propagation through structure are well suited for named entity recognition tasks, but I''ve been unable to find a decent implementation or a decent tutorial for that type of model. Because I''m working with an atypical corpus, standard NER tools in NLTK and similar have performed very poorly, and it looks like I''ll have to train my own system.

In short, what resources are available for this kind of problem?  Is there a standard recursive neural net implementation available? ', 684, '2014-06-19 19:29:57.797', '2353374e-483d-4c9d-ab15-54daa423ef88', 492, 1204, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Word2Vec for Named Entity Recognition', 684, '2014-06-19 19:29:57.797', '2353374e-483d-4c9d-ab15-54daa423ef88', 492, 1205, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><python><nlp><neuralnetwork>', 684, '2014-06-19 19:29:57.797', '2353374e-483d-4c9d-ab15-54daa423ef88', 492, 1206, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Well firstly I would not even use things like Machine learning without having in depth knowledge.

Simplistic things I would do if I had this time series is:

1) Write sql queries to understand which of the times you have the busiest , avg and low foot traffic.

2) Then try to visualize the whole time series , and you could use basic pattern matching algorithms to pick up patterns.

This two things will help you understand what your data set is telling you.

Then with that in hand you will probably in a better state to use machine learning algorithms.

Also , i am currently working in building something on time series and using time series analysis will help you much more than machine learning.

For example there are pattern recognition that you can use that uses every day data to show patterns and ones which use up as much as 3 - 6 months of data to catch a pattern.

', 1029, '2014-06-19 19:33:12.067', '527a0458-8c01-4119-a171-8bd189c5ce77', 493, 1207, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here is the current format of my data set

    User-id | Threat_score
    aaa       45
    bbb       32
    ccc       20

The list contains the top 100 users with the highest threat scores. I generate such a list monthly. and have them stored in separate monthly files.

There are two things I would like to get from this data
<ul>1. Users who are consistently showing up in this list</ul>
<ul>2. Users who are consistently showing up in this list with high risk score </ul>
<ul>3. Users who have/reaching the high risk level very fast.</ul>

I am thinking a visual summary would be something nice. Like each month (somehow) decide the users whose past trend i want to plot and draw a graph with historic threat scores.

My Question(s): Are there any known visualization techniques that have dealt with similar requirements.
What ways should i be transforming my current data to achieve what i am looking for.

Any suggestions in the right direction are welcome!
Thanks', 1028, '2014-06-19 19:33:40.620', 'a8b5d6c6-de68-4f84-9ee3-d4b8cd919d3c', 494, 1208, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Techniques for trend extraction from dataset', 1028, '2014-06-19 19:33:40.620', 'a8b5d6c6-de68-4f84-9ee3-d4b8cd919d3c', 494, 1209, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization><graphs><dataset>', 1028, '2014-06-19 19:33:40.620', 'a8b5d6c6-de68-4f84-9ee3-d4b8cd919d3c', 494, 1210, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m looking to use google''s word2vec implementation to build a named entity recognition system.  I''ve heard that recursive neural nets with back propagation through structure are well suited for named entity recognition tasks, but I''ve been unable to find a decent implementation or a decent tutorial for that type of model. Because I''m working with an atypical corpus, standard NER tools in NLTK and similar have performed very poorly, and it looks like I''ll have to train my own system.

In short, what resources are available for this kind of problem?  Is there a standard recursive neural net implementation available?', 684, '2014-06-19 19:46:13.427', '2b97fd9b-3bb3-4f3b-8d55-348653d76fff', 492, 'deleted 1 character in body', 1211, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><python><nlp><neuralnetwork><recommendation>', 322, '2014-06-19 20:15:58.600', '50af793a-04b7-4379-820c-afb32af7ce9c', 492, 'add recommendation tag', 1212, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-19 20:15:58.600', '50af793a-04b7-4379-820c-afb32af7ce9c', 492, 'Proposed by 322 approved by 684 edit id of 84', 1213, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would add a third column called `month` and then concatenate each list. So if you have a top 100 list for 5 months you will create one big table with 500 entries:

    User-id | Threat_score | month
    aaa       45             1
    bbb       32             1
    ccc       20             1
    ...       ...            ...
    bbb       64             2
    ccc       29             2
    ...       ...            ...

Then, to answer your first question, you could simply count the occurrences of each user-id. For example, if user `bbb` is in your concatenated table five times, then you know that person made your list all five months.

To answer you second question, you could do a `group by` operation to compute some aggregate function of the users. A `group by` operation with an average function is a little crude and sensitive to outliers, but it would probably get you close to what you are looking for.

One possibility for the third question is to compute the difference in threat score between month `n-1` and month `n`. That is, for each month (not including the first month) you subtract the user''s previous threat score from the current threat score. You can make this a new column so your table would now look like:

    User-id | Threat_score | month | difference
    aaa       45             1       null
    bbb       32             1       null
    ccc       20             1       null
    ...       ...            ...     ...
    bbb       64             2       32
    ccc       29             2       9
    ...       ...            ...

With this table, you could again do a `group by` operation to find people who consistently have a higher threat score than the previous month or you could simply find people with a large difference between the current month and the previous month.

As you suggest, visualizing this data is a really good idea. If you care about these threat scores over time (which I think you do), I strongly recommend a simple line chart, with month on the x-axis and threat score on the y-axis. It''s not fancy, but it''s extremely easy to interpret and should give you useful information about the trends.

Most of this stuff (not the visualization) can be done in SQL and all of it can be done in R or Python (and many other languages). Good luck!', 1011, '2014-06-19 22:02:44.050', '4b3a788f-dedd-4e1b-9d3f-98a251a32224', 495, 1214, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Which freely available datasets can I use to train a text classifier?

We are trying to enhance our users engagement by recommending the most related content for him, so we thought If we classified our content based on a predefined bag of words we can recommend to him engaging content by getting his feedback on random number of posts already classified before.

We can use this info to recommend for him pulses labeled with those classes. But we found If we used a predefined bag of words not related to our content the feature vector will be full of zeros, also categories may be not relevant to our content. so for those reasons we tried another solution that will be clustering our content not classifying it.

Thanks :)', 960, '2014-06-19 23:37:28.490', '6d001ad2-75be-475b-9cc1-78d84ea4d05e', 455, 'added 662 characters in body', 1217, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Well, firstly, I would not even use things like Machine learning without having in depth knowledge. Simplistic things I would do if I had this time series is:

1. Write sql queries to understand which of the times you have the busiest, average and low foot traffic.
2. Then try to visualize the whole time series, and you could use basic pattern matching algorithms to pick up patterns.

This two things will help you understand what your data set is telling you. Then, with that in hand, you will probably be in a better state to use machine learning algorithms.

Also, I''m currently working in building something on time series, and using time series analysis will help you much more than machine learning. For example, there are pattern recognition algorithms that you can use that uses every day data to show patterns, and ones which use up to as much as 3 to 6 months of data to catch a pattern.', 84, '2014-06-20 00:34:48.087', '043f2bbb-1c42-4c36-adfa-aad5b7b3d470', 493, 'Improving answer.', 1222, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<open-source><dataset><crawling>', 84, '2014-06-20 01:04:04.270', 'c542f43c-3d05-43e6-b2d0-2dd854de6e81', 422, 'Correcting tag.', 1224, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Here is the current format of my data set

    User-id | Threat_score
    aaa       45
    bbb       32
    ccc       20

The list contains the top 100 users with the highest threat scores. I generate such a list monthly. and have them stored in separate monthly files.

There are three things I would like to get from this data
<ul>1. Users who are consistently showing up in this list</ul>
<ul>2. Users who are consistently showing up in this list with high risk score </ul>
<ul>3. Users who have/reaching the high risk level very fast.</ul>

I am thinking a visual summary would be something nice. Like each month (somehow) decide the users whose past trend i want to plot and draw a graph with historic threat scores.

My Question(s): Are there any known visualization techniques that have dealt with similar requirements.
What ways should i be transforming my current data to achieve what i am looking for.

Any suggestions in the right direction are welcome!
Thanks', 1011, '2014-06-20 01:22:44.263', 'd37b276d-8a19-4c87-9d33-fc755755d0b5', 494, 'Changed "two things" to "three things" since there are three items in the list', 1225, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-20 01:22:44.263', 'd37b276d-8a19-4c87-9d33-fc755755d0b5', 494, 'Proposed by 1011 approved by 84, 1028 edit id of 85', 1226, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In network structure, what is the difference between **k-cliques** and **p-cliques**, can anyone give a brief explaination with examples? Thanks in advanced!

============================
<br>EDIT:
I found an online [ppt][1] while I am googling, please take a look on **p.37** and **p.39**, can you comment on them?


  [1]: http://open.umich.edu/sites/default/files/SI508-F08-Week7-Lab6.ppt', 957, '2014-06-20 02:27:28.790', '4e44d84b-cbb4-4a8a-a9c5-025fc0fff690', 474, 'added 261 characters in body', 1229, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You have three questions to answer and 100 records per month to analyze.

Based on this size, I''d recommend doing analysis in a simple SQL database or a spreadsheet to start off with.  The first two questions are fairly easy to figure out.  The third is a little more difficult.

I''d definitely add a column for month and group all of that data together into a spreadsheet or database table given the questions you want to answer.

question 1. Users who are consistently showing up in this list

In excel, this answer should help you out:  http://superuser.com/questions/442653/ms-excel-how-count-occurence-of-item-in-a-list

For a SQL database:  http://stackoverflow.com/questions/2516546/select-count-duplicates

question 2. Users who are consistently showing up in this list with high risk score

This is just adding a little complexity to the above.  For SQL, you would further qualify your query based on a minimum risk score value.

In excel, a straight pivot isn''t going to work, you''ll have to copy the unique values in one column to another, then drag a CountIf function adjacent to each unique value, qualifying the countif function with a minimum risk score.

question 3. Users who have/reaching the high risk level very fast.

A fast rise in risk level could be defined as the difference between two months being larger than a given value.

For each user record you want to know the previous month''s threat value, or assume zero as the previous threat value.

If that difference is greater than your risk threshold, you want to include it in your report.  If not, they can be filtered from the list.

If I had to do this month after month, I would spend the two hours it might take to automate a report after the first couple of months.  I''d throw all the data in a SQL database and write a quick script in perl or java to iterate through the 100 records, do the calculation, and output the users who crossed the threshold.

If I needed it to look pretty, I''d use a reporting tool.  I''m not particularly partial to any of them.

If I needed to trend threshold values over time, I''d output the results for all people into a second table add records to that table each month.

If I just needed to do it once or twice, figuring out how to do it in excel by adding a new column using VLookUp and some basic math and a filter would probably be the fastest and easiest way to get it done.  I tend to avoid using excel for things I''ll need to use with consistency because there are limits that you run into when your data gets sizeable.', 434, '2014-06-20 03:18:07.653', '0c2cb9c2-18c2-469c-9e32-25aa0f3363e3', 496, 1230, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to find a formula, method, or model to use to analyze the likelihood that a specific event influenced some longitudinal data. I am having difficultly figuring out what to search for on Google.

Here is an example scenario:

Image you own a business that has an average of 100 walk-in customers every day. One day, you decide you want to increase the number of walk-in customers arriving at your store each day, so you pull a crazy stunt outside your store to get attention. Over the next week, you see on average 125 customers a day.

Over the next few months, you again decide that you want to get some more business, and perhaps sustain it a bit longer, so you try some other random things to get more customers in your store. Unfortunately, you are not the best marketer, and some of your tactics have little or no effect, and others even have a negative impact.

What methodology could I use to determine the probability that any one individual event positively or negatively impacted the number of walk-in customers? I am fully aware that correlation does not necessarily equal causation, but what methods could I use to determine the likely increase or decrease in your business''s daily walk in client''s following a specific event?

I am not interested in analyzing whether or not there is a correlation between your attempts to increase the number of walk-in customers, but rather whether or not any one single event, independent of all others, was impactful.

I realize that this example is rather contrived and simplistic, so I will also give you a brief description of the actual data that I am using:

I am attempting to determine the impact that a particular marketing agency has on their client''s website when they publish new content, perform social media campaigns, etc. For any one specific agency, they may have anywhere from 1 to 500 clients. Each client has websites ranging in size from 5 pages to well over 1 million. Over the course of the past 5 year, each agency has annotated all of their work for each client, including the type of work that was done, the number of webpages on a website that were influenced, the number of hours spent, etc.

Using the above data, which I have assembled into a data warehouse (placed into a bunch of star/snowflake schemas), I need to determine how likely it was that any one piece of work (any one event in time) had an impact on the traffic hitting any/all pages influenced by a specific piece of work. I have created models for 40 different types of content that are found on a website that describes the typical traffic pattern a page with said content type might experience from launch date until present. Normalized relative to the appropriate model, I need to determine the highest and lowest number of increased or decreased visitors a specific page received as the result of a specific piece of work.

While I have experience with basic data analysis (linear and multiple regression, correlation, etc), I am at a loss for how to approach solving this problem. Whereas in the past I have typically analyzed data with multiple measurements for a given axis (for example temperature vs thirst vs animal and determined the impact on thirst that increased temperate has across animals), I feel that above, I am attempting to analyze the impact of a single event at some point in time for a non-linear, but predictable (or at least model-able), longitudinal dataset. I am stumped :(

Any help, tips, pointers, recommendations, or directions would be extremely helpful and I would be eternally grateful!!!

Thank you', 1047, '2014-06-20 03:18:59.477', 'c96d893a-17a4-4ed5-9a38-afe08279c11c', 497, 1231, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What statistical model should I use to analyze the likelihood that a single event influenced longitudinal data', 1047, '2014-06-20 03:18:59.477', 'c96d893a-17a4-4ed5-9a38-afe08279c11c', 497, 1232, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><statistics>', 1047, '2014-06-20 03:18:59.477', 'c96d893a-17a4-4ed5-9a38-afe08279c11c', 497, 1233, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As suggested, **going wild**. First of all, correct me if Im wrong:

* Just a few features exist for each unique product;
* There is no ultimate features list, and clients are able to add new features to their products.

If so, constructing full product-feature table could be computational expensive. And final data table would be extremely sparse.

The first step is narrowing customers (products) list for matching. Lets build a bipartite graph, where sellers would be type-1 nodes, and buyers would be type-2 nodes. Create an edge between any seller and buyer every time they reference a similar product feature, as in the following sketch:

![graph][1]

Using the above graph, for every unique sellers product you can select only buyers who are interested in features that match the product (its possible to filter **at least one** common feature, match the full set of features, or set a threshold level). But certainly, thats not enough. The next step is to compare feature values, as described by the seller and buyer. There are a lot of variants (e.g., k-Nearest-Neighbors). But why not try to solve this question using the existing graph? Lets add weights to the edges:

* for continuous features (e.g., price):

  ![price_weight][2]

* for binary and non-quantifiable features - just logical biconditional:

  ![feature_weight][3]

The main idea here is to **scale** every feature to the interval `[0, 1]`. Additionally, we can use feature coefficients to determine most important features. E.g., assuming price is twice as important as availability of some rare function:

![adj_w_1][4]

![adj_w_2][5]

One of the final steps is simplifying the graph structure and reducing many edges to one edge with weight equal to the sum of the previously calculated weights of each feature. With such a reduced structure every pair of customers/products could have only one edge (no parallel edges). So, to find the best deal for exact seller you just need to select connected buyers with max weighted edges.

Future challenge: introduce a cheap method for weighting edges on first step :)


  [1]: http://i.stack.imgur.com/q1JTN.png
  [2]: https://chart.googleapis.com/chart?cht=tx&chl=weight%20%3D%20%5Cfrac%7B%7Cprice_%7Bsell%7D%20-%20price_%7Bbuy%7D%7C%7D%7B%5Cmax%28price%29%20-%20%5Cmin%28price%29%7D
  [3]: https://chart.googleapis.com/chart?cht=tx&chl=weight%20%3D%20feature_%7Bsell%7D%20%5Cleftrightarrow%20feature_%7Bbuy%7D
  [4]: https://chart.googleapis.com/chart?cht=tx&chl=adj.price.weight%20%3D%202%20%5Ctimes%20price.weight
  [5]: https://chart.googleapis.com/chart?cht=tx&chl=adj.feature.weight%20%3D%201%20%5Ctimes%20feature.weight', 322, '2014-06-20 03:28:55.463', 'fe4b4d6e-dc6d-44fc-9f55-076130b55a96', 491, 'use standard emphasis in place of double angle quotation characters; improve grammar; correct google chart subscripts', 1234, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-20 03:28:55.463', 'fe4b4d6e-dc6d-44fc-9f55-076130b55a96', 491, 'Proposed by 322 approved by 84, 941 edit id of 83', 1235, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Have you considered a frequency-based approach exploiting simple word co-occurence in corpora? At least, that''s what I''ve seen most folks use for this. I think it might be covered briefly in Manning and Schütze''s book, and I seem to remember something like this as a homework assignment back in grad school...

More background here: http://nlp.stanford.edu/IR-book/html/htmledition/automatic-thesaurus-generation-1.html

For this step:
> Rank other concepts based on their "distance" to the original keywords;

There are several semantic similarity metrics you could look into. Here''s a link to some slides I put together for a class project using a few of these similarity metrics in WordNet: http://www.eecis.udel.edu/~trnka/CISC889-11S/lectures/greenbacker-WordNet-Similarity.pdf', 819, '2014-06-20 04:09:04.230', '2fa6fc8a-092e-4cf2-b558-d6cb353ce0fc', 498, 1236, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Anecdotally, I''ve never been impressed with the output from hierarchical LDA. It just doesn''t seem to find an optimal level of granularity for choosing the number of topics. I''ve gotten much better results by running a few iterations of regular LDA, manually inspecting the topics it produced, deciding whether to increase or decrease the number of topics, and continue iterating until I get the granularity I''m looking for.

Remember: hierarchical LDA can''t read your mind... it doesn''t know what you actually intend to use the topic modeling for. Just like with k-means clustering, you should choose the k that makes the most sense for your use case.', 819, '2014-06-20 04:27:02.467', '7565825b-5e57-4554-a670-ffd4d894f8c7', 499, 1237, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In addition to Ben''s answer, the subtle distinction between statistical models and machine learning models is that, in statistical models, you explicitly decide the output equation structure prior to building the model. The model is built to compute the parameters/coefficients.

Take linear model or GLM for example,

    y = a1x1 + a2x2 + a3x3

Your independent variables are x1, x2, x3 and the coefficients to be determined are a1,a2,a3. You define your equation structure this way prior to building the model and compute a1,a2,a3. If you believe that y is somehow correlated to x2 in a non-linear way, you could try something like this.

    y = a1x1 + a2(x2)^2 + a3x3.

Thus, you put a restriction in terms of the output structure. Inherently statistical models are linear models unless you explicitly apply transformations like sigmoid or kernel to make them nonlinear (GLM and SVM).

In case of machine learning models, you rarely specify output structure and algorithms like decision trees are inherently non-linear and work efficiently.

Contrary to what Ben pointed out, machine learning models aren''t just about prediction, they do classification, regression etc which can be used to make predictions which are also done by various statistical models.', 514, '2014-06-20 06:49:25.237', '52f081df-669d-41ec-922a-b36a6aeddee9', 500, 1240, '2');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES (514, '2014-06-20 06:49:25.237', '1c40ee77-73d2-4659-966c-70583c77bdd3', 500, 1241, '16');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The best language depends on what you want to do. First remark: don''t limit yourself to one language. Learning a new language is always a good thing, but at some point you will need to choose. Facilities offered by the language itself are an obvious thing to keep into account *but* in my opinion the following are more important:

 - **available libraries**: do you have to implement everything from scratch or can you reuse existing stuff? Note that this these libraries need not be in whatever language you are considering, as long as you can interface easily. Working in a language without library access won''t help you get things done.
 - **number of experts**: if you want external developers or start working in a team, you have to consider how many people actually know the language. As an extreme example: if you decide to work in Brainfuck because you happen to like it, know that you will likely work alone. Many surveys exists that can help assess the popularity of languages, including the number of questions per language on SO.
 - **toolchain**: do you have access to *good* debuggers, profilers, documentation tools and (if you''re into that) IDEs?

I am aware that most of my points favor established languages. This is from a ''get-things-done'' perspective.

That said, I personally believe it is far better to become proficient in a low level language and a high level language:

 - low level: C++, C, Fortran, ... using which you can implement certain profiling hot spots *only if you need to* because developing in these languages is typically slower (though this is subject to debate). These languages remain king of the hill in terms of critical performance and are likely to stay on top for a long time.
 - high level: Python, R, Clojure, ... to ''glue'' stuff together and do non-performance critical stuff (preprocessing, data handling, ...). I find this to be important simply because it is much easier to do rapid development and prototyping in these languages.', 119, '2014-06-20 07:11:40.053', '1f804ba5-d361-448d-8b31-4b5906faaba5', 501, 1242, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The Dirichlet distribution is a multivariate distribution. We can denote the parameters of the Dirichlet as a vector of size K of the form ~ 1/B(a) * Product(x_i ^ (a_i-1)), where a is the vector of size K of the parameters, and sum of x_i = 1.

Now the LDA uses some constructs like:
- a document can have multiple topics (because of this multiplicity, we need the Dirichlet distribution); and there is a Dirichlet distribution which models this relation
- words can also belong to multiple topics, when you consider them outside of a document; so here we need another Dirichlet to model this

The previous two are distributions which you do not really see from data, this is why is called latent, or hidden.

Now, in Bayesian inference you use the Bayes rule to infer the posterior probability. For simplicity, let''s say you have data *x* and you have a model for this data governed by some parameters theta. In order to infer values for this parameters, in full Bayesian inference you will infer the posterior probability of these parameters using Bayes'' rule with *p(theta|x) = p(x|theta)p(theta|alpha)/p(x|alpha)*. In plain words is *posterior probability = likelihood x prior probability / marginal likelihood*. Note that here comes an *alpha*. This is your initial belief about this distribution, and is the parameter of the prior distribution. Usually this is chosen in such a way that will have a conjugate prior (so the distribution of the posterior is the same with the distribution of the prior) and often to encode some knowledge if you have one or to have maximum entropy if you know nothing.

The parameters of the prior are called *hyperparameters*. So, in LDA, both topic distributions, over documents and over words have also correspondent priors, which are denoted usually with alpha and beta, and because are the parameters of the prior distributions are called hyperparameters.

Now about choosing priors. If you plot some Dirichlet distributions you will note that if the individual parameters *alpha_k* have the same value, the pdf is symmetric in the simplex defined by the *x* values, which is the minimum or maximum for pdf is at the center.

If all the alpha_k have values lower than unit the maximum is found at corners

<img src="http://i.stack.imgur.com/5khZE.png" width="200" height="200">

or can if all values alpha_k are the same and greater than 1 the maximum will be found in center like

<img src="http://research.microsoft.com/en-us/um/people/cmbishop/prml/prmlfigs-png/Figure2.5c.png" width="200" height="200">

It is easy to see that if values for alpha_k are not equal the symmetry is broken and the maximum will be found near bigger values.

Additional, please note that values for priors parameters produce smooth pdfs of the distribution as the values of the parameters are near 1. So if you have great confidence that something is clearly distributed in a way you know, with a high degree of confidence, than values far from 1 in absolute value are to be used, if you do not have such kind of knowledge than values near 1 would be encode this lack of knowledge. It is easy to see why 1 plays such a role in Dirichlet distribution from the formula of the distribution itself.

Another way to understand this is to see that prior encode prior-knowledge. In the same time you might think that prior encode some prior seen data. This data was not saw by the algorithm itself, it was saw by you, you learned something, and you can model prior according to what you know (learned). So in the prior parameters (hyperparameters) you encode also how big this data set you apriori saw, because the sum of alpha_k can be that also as the size of this more or less imaginary data set. So the bigger the prior data set, the bigger is the confidence, the bigger the values of alpha_k you can choose, the sharper the surface near maximum value, which means also less doubts.

Hope it helped.

PS: It''s a hell to write something without LaTeX notation. I hope moderators/administrators will do something.', 108, '2014-06-20 07:14:16.953', 'e2f4e775-379c-42da-9f7b-11957df27af4', 202, 'edited body', 1243, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Regarding prediction, statistics and machine learning sciences started to solve mostly the same problem from different perspectives.

Basically statistics assumes that the data were produced by a given stochastic model. So, from a statistical perspective, a model is assumed and given various assumptions the errors are treated and the model parameters and other questions are inferred.

Machine learning comes from a computer science perspective. The models are algorithmic and usually very few assumptions are required regarding the data. We work with hypothesis space and learning bias. The best exposition of machine learning I found is contained in Tom Mitchell''s book called [Machine Learning][1].

For a more exhaustive and complete idea regarding the two cultures you can read the Leo Broiman paper called [Statistical Modeling: The Two Cultures][2]

However what must be added is that even if the two sciences started with different perspectives, both of them now now share a fair amount of common knowledge and techniques. Why, because the problems were the same, but the tools were different. So now machine learning is mostly treated from a statistical perspective (check the Hastie,Tibshirani, Friedman book [The Elements of Statistical Learning][3] from a machine learning point of view with a statistical treatement, and perhaps Kevin P. Murphy ''s book [Machine Learning: A probabilistic perspective][4], to name just a few of the best books available today).

Even the history of the development of this field show the benefits of this merge of perspectives. I will describe two events.

The first is the creation of CART trees, which was created by Breiman with a solid statistical background. At approximately the same time, Quinlan developed ID3,C45,See5, and so on, decision tree suite with a more computer science background. Now both this families of trees and the ensemble methods like bagging and forests become quite similar.

The second story is about boosting. Initially they were developed by Freund and Shapire when they discovered AdaBoost. The choices for designing AdaBoost were done mostly from a computational perspective. Even the authors did not understood well why it works. Only 5 years later Breiman (again!) described the adaboost model from a statistical perspective and gave an explanation for why that works. Since then, various eminent scientists, with both type of backgrounds, developed further those ideas leading to a Pleiads of boosting algorithms, like logistic boosting, gradient boosting, gentle boosting ans so on. It is hard now to think at boosting without a solid statistical background.

GLM is a statistical development. However new Bayesian treatments puts this algorithm also in machine learning playground. So I believe both claims could be right, since the interpretation and treatment of how it works could be different.

  [1]: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html
  [2]: http://www.google.ie/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CDIQFjAA&url=http://strimmerlab.org/courses/ss09/current-topics/download/breiman2001.pdf&ei=RQGkU_DgH8Le7AaRmID4DA&usg=AFQjCNGNnrlqadmBT2fZMT_NfoUQ1rEuow&sig2=g5qovUTsuuJUGGE64g0nrQ
  [3]: http://statweb.stanford.edu/~tibs/ElemStatLearn/
  [4]: http://mitpress.mit.edu/books/machine-learning-2', 108, '2014-06-20 10:01:03.363', '5acb87ff-6fd7-46b2-8f06-38e6d6fda114', 502, 1244, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><classification><clustering><text-mining>', 960, '2014-06-20 13:18:04.417', '29c7d025-3fe9-4f27-8e6c-ebb7eade8ef8', 455, 'edited tags', 1245, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m not an expert but I guess the main problem is to answer this question:

Has an/any event affected the number of hits on a certain day?

But I don''t know how to treat multiple events, so I would try to answer this question:

* Does event X affected the number of hits on a certain day?

Which can be answered using hypothesis testing with p-values (what scientist do to evaluate for instance if a medicine affects a disease or not).

By using p-values, you could determinate if the number of hits in a certain day were mere random and acceptable under normal circumstances or that they must correspond to a change in your model.

You can read more about p-values in [Open Intro to Statistics Book](http://www.openintro.org/stat/), I''ve actually learn about them from there.

Then, the other parts of the problem are how to identify your events and calculate the necessary parameters to answer your question (average/median, variance, etc.) and also how to keep that up-to-date and working.
  ', 1057, '2014-06-20 14:17:27.970', 'abb7299f-a7bb-4822-a5b1-2eb77c243892', 503, 1246, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m currently working with a dataset with a wide range of document lengths -- anywhere from a single word to a full page of text.  In addition, the grammatical structure and use of punctuation varies wildly from document to document.  The goal is to classify those documents into one of about 10-15 categories.  I''m currently using ridge regression and logistic regression for the task, and CV for the alpha values of ridge.  The feature vectors are tf-idf ngrams.

Recently I''ve noticed that longer documents are much less likely to be categorized. Why might this be the case, and how can one "normalize" for this kind of variation?  As a more general question, how does one typically deal with diverse data sets?  Should documents be grouped based off of metrics like document length, use of punctuation, grammatical rigor, etc. and then fed through different classifiers?  ', 684, '2014-06-20 14:58:09.320', '6df5a394-a844-4c32-a55e-d2a9ef83cb59', 504, 1247, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Dealing with diverse text data', 684, '2014-06-20 14:58:09.320', '6df5a394-a844-4c32-a55e-d2a9ef83cb59', 504, 1248, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><nlp>', 684, '2014-06-20 14:58:09.320', '6df5a394-a844-4c32-a55e-d2a9ef83cb59', 504, 1249, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For the record, I think this is the type of question that''s perfect for the data science Stack Exchange. I hope we get a bunch of real world examples of data problems and several perspectives on how best to solve them.

I would encourage you *not* to use p-values as they can be pretty misleading ([1][1], [2][2]). My approach hinges on you being able to summarize traffic on a given page before and after some intervention. What you care about is the difference in the *rate* before and after the intervention. That is, how does the number of hits per day change? Below, I explain a first stab approach with some simulated example data. I will then explain one potential pitfall (and what I would do about it).

First, let''s think about one page before and after an intervention. Pretend the intervention increases hits per day by roughly 15%:

    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns

    def simulate_data(true_diff=0):
        #First choose a number of days between [1, 1000] before the intervention
        num_before = np.random.randint(1, 1001)

        #Next choose a number of days between [1, 1000] after the intervention
        num_after = np.random.randint(1, 1001)

        #Next choose a rate for before the intervention. How many views per day on average?
        rate_before = np.random.randint(50, 151)

        #The intervention causes a `true_diff` increase on average (but is also random)
        rate_after = np.random.normal(1 + true_diff, .1) * rate_before

        #Simulate viewers per day:
        vpd_before = np.random.poisson(rate_before, size=num_before)
        vpd_after = np.random.poisson(rate_after, size=num_after)

        return vpd_before, vpd_after

    vpd_before, vpd_after = simulate_data(.15)

    plt.hist(vpd_before, histtype="step", bins=20, normed=True, lw=2)
    plt.hist(vpd_after, histtype="step", bins=20, normed=True, lw=2)
    plt.legend(("before", "after"))
    plt.title("Views per day before and after intervention")
    plt.xlabel("Views per day")
    plt.ylabel("Frequency")
    plt.show()

![Distribution of hits per day before and after the intervention][3]

We can clearly see that the intervention increased the number of hits per day, on average. But in order to quantify the difference in rates, we should use one company''s intervention for multiple pages. Since the underlying rate will be different for each page, we should compute the percent change in rate (again, the rate here is hits per day).

Now, let''s pretend we have data for `n = 100` pages, each of which received an intervention from the same company. To get the percent difference we take (mean(hits per day before) - mean(hits per day after)) / mean(hits per day before):

    n = 100

    pct_diff = np.zeros(n)

    for i in xrange(n):
        vpd_before, vpd_after = simulate_data(.15)
        # % difference. Note: this is the thing we want to infer
        pct_diff[i] = (vpd_after.mean() - vpd_before.mean()) / vpd_before.mean()

    plt.hist(pct_diff)
    plt.title("Distribution of percent change")
    plt.xlabel("Percent change")
    plt.ylabel("Frequency")
    plt.show()

![Distribution of percent change][4]

Now we have the distribution of our parameter of interest! We can query this result in different ways. For example, we might want to know the mode, or (approximation of) the most likely value for this percent change:

    def mode_continuous(x, num_bins=None):
        if num_bins is None:
            counts, bins = np.histogram(x)
        else:
            counts, bins = np.histogram(x, bins=num_bins)

        ndx = np.argmax(counts)
        return bins[ndx:(ndx+1)].mean()

    mode_continuous(pct_diff, 20)

When I ran this I got 0.126, which is not bad, considering our true percent change is 15. We can also see the number of positive changes, which approximates the probability that a given company''s intervention improves hits per day:

    (pct_diff > 0).mean()

Here, my result is 0.93, so we could say there''s a pretty good chance that this company is effective.

Finally, a potential pitfall: Each page probably has some underlying trend that you should probably account for. That is, even without the intervention, hits per day may increase. To account for this, I would estimate a simple linear regression where the outcome variable is hits per day and the independent variable is day (start at day=0 and simply increment for all the days in your sample). Then subtract the estimate, y_hat, from each number of hits per day to de-trend your data. Then you can do the above procedure and be confident that a positive percent difference is not due to the underlying trend. Of course, the trend may not be linear, so use discretion! Good luck!

  [1]: http://andrewgelman.com/2013/03/12/misunderstanding-the-p-value/
  [2]: http://occamstypewriter.org/boboh/2008/08/19/why_p_values_are_evil/
  [3]: http://i.stack.imgur.com/FJJqD.png
  [4]: http://i.stack.imgur.com/CAitf.png', 1011, '2014-06-20 16:09:41.637', '116e7792-19a1-4391-8120-1a7bc9b52f6b', 506, 1251, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Back in my data analyst days this type of problem was pretty typical.  Basically, everyone in marketing would come up with a crazy idea that the sold to higher ups as the single event that would boost KPI''s by 2000%.  The higher ups would approve them and then they would begin their "test".  Results would come back, and management would dump it on the data analysts to determine what worked and who did it.

The short answer is you cant really know if it wasn''t run as a random A/B style test on like time periods.  But I am very aware of how deficient that answer is, especially if the fact that a pure answer doesn''t exist is irrelevant to the urgency of future business decisions.  Here are some of the techniques I would use to salvage the analysis in this situation, bear in mind this is more of an art then a science.

**Handles**

A handle is something that exists in the data that you can hold onto.  From what you are telling me in your situation you have a lot of info on who the marketing agency is, when they tried a tactic, and to which site they applied it to.  These are your starting point and information like this going to be the corner stone of your analysis.

**Methodology**

The methodology is going to probably hold the strongest impact on which agencies are given credit for any and all gains so you are going to need to make sure that it is clearly outlines and all stake holders agree that it makes sense.  If you cant do that it is going to be difficult for people to trust your analysis.

An example of this are conversions. Say the marketing department purchases some leads and they arrive at our landing page, we would track them for 3 days, if they made a purchase within that time we would count them as having been converted.  Why 3 days, why not 5 or 1?  Thats not important as long as everyone agrees, you now have a definition you can build off of.

**Comparisons**

In an ideal would you would have a nice A/B test to prove a definitive relationship, I am going to assume that you are running short on those, still, you can learn something from a simple comparison of like data.  When companies are trying to determine the efficacy of radio advertising they will often run ads on offset months in the same market, or for several months in one market and compare that with the results in a separate but similar market.  Its doesn''t pass for science, but even with all that noise a strong results will almost always be noticeable.

I would combine these in your case to determine how long an event is given to register an effect.  Once you have the data from that time period run it against your modeled out traffic prediction, week over week growth, month over month etc. Which, can then allow a meaningful comparison between agencies, and across time periods.

**Pragmatism**

The aspiration is to be able to provide a deep understanding of cause and effect, but it is probably not realistic.  Because of how messy outside factors make your analysis, you are constantly going to run up against the question over and over again: Did this event raise volume/sales/click throughs, or would doing anything at all have had the same effect?  The best advise I can give for this is set very realistic goals for what you are looking to measure.  A good starting point is, within the methodology you have, which event had the largest impact.  Once you have those open your aperture from there.

**Summary**

Once you have reasoned out all of these aspects you can go about building a general solution which can then be automated.  The advantage to designing your solution in this manner is that the business logic is already built in.  This will make your results much more approachable and intuitive to non-technical business leaders.', 780, '2014-06-20 16:29:27.673', '842e553b-dcf9-40d4-91f9-24c4f8e87b0b', 507, 1252, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m not sure how you are applying a regression framework for document classification. The way I''d approach the problem is to apply a standard discriminative classification approach such as SVM.

In a discriminative classification approach the notion of similarity or inverse distance between data points (documents in this case) is pivotal. Fortunately for documents, there is a standard way of defining pairwise similarity. This is the standard [cosine similarity][1] measure which makes use of document length normalization to take different document lengths into account.

Thus, practically speaking, in cosine similarity you would work with relative term weights normalized by document lengths and hence document length diversity should not be a major issue in the similarity computation.

One also has to be careful when applying idf in term weights. If the number of documents is not significantly large the idf measure may be statistically imprecise thus adding noise to the term weights. It''s also a standard practice to ignore stopwords and punctuations.

  [1]: http://en.wikipedia.org/wiki/Cosine_similarity', 984, '2014-06-20 16:56:41.160', 'eda9e8e3-9e85-4f42-a5f8-e52c2ff5ae3d', 508, 1253, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Both feature selection and extraction are a part of spatial data mining.For example...if u have an agricultural land then selecting one particular area of that land would be feature selection.If u aim to find the affected plants in that area den u need to observe each plant based on a particular feature that is common in each plant so as to find the abnormalities...for this u would be considering feature extraction.In this example the original agricultural land corresponds to Dimensionality reduction.', 1015, '2014-06-20 17:30:39.097', '035af321-923b-4516-aabe-7a3dd464e264', 509, 1254, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A couple of words about social networks APIs. About a year ago I wrote a review of popular social networks APIs for researchers. Unfortunately, it is in Russian. Here is a summary:

**Twitter** (https://dev.twitter.com/docs/api/1.1)

* almost all data about tweets/texts and users is available;
* lack of sociodemographic data;
* great streaming API: useful for real time text processing;
* a lot of wrappers for programing languages;
* getting network structure (connections) is possible, but time-expensive (1 request per 1 minute).

**Facebook** (https://developers.facebook.com/docs/reference/api/)

* rate limits: about 1 request per second;
* well documented, sandbox present;
* FQL (SQL-like) and «regular Rest» Graph API;
* friendship data and sociodemographic features present;
* a lot of data is beyond *event horizon*: only friends'' and friends'' of friends data is more or less complete, almost nothing could be investigated about random user;
* some strange API bugs, and looks like nobody cares about it (e.g., some features available through FQL, but not through Graph API synonym).

**Instagram** (http://instagram.com/developer/)

* rate limits: 5000 requests per hour;
* real-time API (like Streaming API for Twitter, but with photos) - connection to it is a little bit tricky: callbacks are used;
* lack of sociodemographic data;
* photos, filters data available;
* unexpected imperfections (e.g., its possible to collect only 150 comments to post/photo).

**Foursquare** (https://developer.foursquare.com/overview/)

* rate limits: 5000 requests per hour;
* kingdom of geosocial data :)
* quite closed from researches because of privacy issues. To collect checkins data one need to build composite parser working with 4sq, bit.ly, and twitter APIs at once;
* again: lack of sociodemographic data.

**Google+** (https://developers.google.com/+/api/latest/)

* about 5 requests per second (try to verify);
* main methods: activities and people;
* like on Facebook, a lot of personal data for random user is hidden;
* lack of user connections data.

And out-of-competition: I reviewed social networks for Russian readers, and #1 network here is [**vk.com**][1]. Its translated to many languages, but popular only in Russia and other CIS countries. API docs link: http://vk.com/dev/. And from my point of view, its the best choice for homebrew social media research. At least, in Russia. Thats why:

* rate limits: 3 requests per second;
* public text and media data available;
* sociodemographic data available: for random user availability level is about 60-70%;
* connections between users are also available: almost all friendships data for random user is available;
* some special methods: e.g., there is a method to get online/offline status for exact user in realtime, and one could build schedule for his audience.

  [1]: http://en.wikipedia.org/wiki/VK_%28social_network%29', 941, '2014-06-20 17:34:51.633', '6309b724-f955-4c6c-8bf8-91ff9c6b9a08', 510, 1255, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('My answer would be no. I consider Data mining to be one of the miscellaneous fields in Data science. Data Mining is mostly considered on yielding questions rather than answering them. It is often termed as "detecting something new", when compared to Data science, where the data scientist try to solve complex problems to be able to reach their end results. However both terms have many commonalities between them.
For example..if u have an agricultural land where u aim to find the affected plants..Here spatial data mining plays a key role in doing this job.There are good chances that you may end  up with not only finding out the affected plants in the land but also the extent to which they are affected.......this is something that is not possible with data science.', 1015, '2014-06-20 17:36:05.023', '3c0ce509-8f9b-4ab0-932a-ca76f2478b3f', 485, 'added 359 characters in body', 1256, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I wonder which type of model cross-validation to choose: K-fold or random sub-sampling?

My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.

In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.

On the other hand I don''t like random sub-sampling feature: that some items won''t be ever selected for training, and some will be used more than once.', 97, '2014-06-20 17:57:46.363', 'd301b0cc-dbf6-403e-a745-1690dd130ee9', 511, 1257, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Cross-validation: K-fold vs Repeated random sub-sampling', 97, '2014-06-20 17:57:46.363', 'd301b0cc-dbf6-403e-a745-1690dd130ee9', 511, 1258, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<cross-validation><sampling><random-sub-sampling>', 97, '2014-06-20 17:57:46.363', 'd301b0cc-dbf6-403e-a745-1690dd130ee9', 511, 1259, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I wonder which type of model cross-validation to choose: K-fold or random sub-sampling?

My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.

In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.

On the other hand I don''t like random sub-sampling feature: that some items won''t be ever selected for training/validation, and some will be used more than once.', 97, '2014-06-20 18:04:53.227', 'af49ddbe-c13a-480a-8f99-60b1702ce7e5', 511, 'added 11 characters in body', 1260, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I wonder which type of model cross-validation to choose for classification problem: K-fold or random sub-sampling?

My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.

In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.

On the other hand I don''t like random sub-sampling feature: that some items won''t be ever selected for training/validation, and some will be used more than once.

Classification algorithms used: random forest & logistic regression.', 97, '2014-06-20 18:33:42.860', 'dc351b36-c664-40d1-b433-a0193ce86c9b', 511, 'added 72 characters in body', 1261, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here''s an approach I didn''t see mentioned: separate the process into two steps: the first step focused on encoding names so that alternative versions of the same name are encoded the same (or nearly the same), and the second step focused on making them anonymous.

For the first step, you could use one of the [Phonetic Algorithms (Soundex and variants)](https://en.wikipedia.org/wiki/Phonetic_encoding), applied to first name, last name, and initials in various orders.  (See [this article](http://www.stevemorse.org/phonetics/bmpm2.htm), also).  It''s in this step where you resolve similarities vs. differences in names to balance false positives from false negatives.

For the second step, you can pick any hashing or cryptographic method you like, without concern for how that method affects name matching.  This gives you freedom to use a method that has the best characteristics for both performance, robustness, and anonymity.', 609, '2014-06-20 18:56:52.110', '637bde0c-9019-46bf-bb19-e141973288a6', 512, 1262, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you have an adequate number of samples and want to use all the data, then k-fold cross-validation is the way to go. Having ~1,500 seems like a lot but whether it is adequate for k-fold cross-validation also depends on the dimensionality of the data (number of attributes and number of attribute values). For example, if each observation has 100 attributes, then 1,500 observations is low.

Another potential downside to k-fold cross-validation is the possibility of a single, extreme outlier skewing the results. For example, if you have one extreme outlier that can heavily bias your classifier, then in a 10-fold cross-validation, 9 of the 10 partitions will be affected (though for random forests, I don''t think you would have that problem).

Random subsampling (e.g., bootstrap sampling) is preferable when you are either undersampled or when you have the situation above, where you don''t want each observation to appear in k-1 folds.


', 964, '2014-06-20 19:15:52.093', '95f44e16-69bd-463a-9c4d-28cbd1e37990', 513, 1263, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**Edit: Warning, i leave my message but my answer seems wrong, please check out the comment below!**

I''m not an expert but I guess the main problem is to answer this question:

Has an/any event affected the number of hits on a certain day?

But I don''t know how to treat multiple events, so I would try to answer this question:

* Does event X affected the number of hits on a certain day?

Which can be answered using hypothesis testing with p-values (what scientist do to evaluate for instance if a medicine affects a disease or not).

By using p-values, you could determinate if the number of hits in a certain day were mere random and acceptable under normal circumstances or that they must correspond to a change in your model.

You can read more about p-values in [Open Intro to Statistics Book](http://www.openintro.org/stat/), I''ve actually learn about them from there.

Then, the other parts of the problem are how to identify your events and calculate the necessary parameters to answer your question (average/median, variance, etc.) and also how to keep that up-to-date and working.
  ', 1057, '2014-06-20 19:39:41.533', '3e8ff600-6fbd-4238-af10-803f113430f8', 503, 'added 104 characters in body', 1264, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I like Amir Ali Akbari''s suggestions, and I''ll add a few of my own, focusing on topics and skills that are not adequately covered in most machine learning and data analysis books that focus on math and/or programming.

Data Cleaning:

 - [Osborne 2012, *Best Practices in Data Cleaning*](http://www.amazon.com/Best-Practices-Data-Cleaning-Everything/dp/1412988012/)
 - [McCallom 2012, *Bad Data Handbook: Cleaning Up The Data So You Can Get Back To Work*](http://www.amazon.com/Bad-Data-Handbook-Cleaning-Back/dp/1449321887/)

Bayesian Data Analysis (alternative to Fisher-style Null Hypothesis Significance Testing):

 - [Kruschke 2011, *Doing Bayesian Data Analysis*](http://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0123814855/)

Inference in the face of uncertainty, incompleteness, contradictions, ambiguity, imprecision, ignorance, etc.:

 - [Schum & Starace, 2001, *The Evidential Foundations of Probabilistic Reasoning*](http://www.amazon.com/Evidential-Foundations-Probabilistic-Reasoning/dp/0810118211/)
 - [Bammer & Smithson 2008, *Uncertainty and Risk: Multidisciplinary Perspectives*](http://www.amazon.com/Uncertainty-Risk-Multidisciplinary-Perspectives-Earthscan/dp/1844074749/)
 - [Smithson 1989, *Ignorance and Uncertainty*](http://www.amazon.com/Ignorance-Uncertainty-Emerging-Paradigms-Cognitive/dp/0387969454/)
 - [CIA 2008, *A Tradecraft Primer: Structured Analytic Techniques for Improving Intelligence Analysis*](https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/Tradecraft%20Primer-apr09.pdf) (FREE! as PDF)
 - [Morgan & Winship 2007, *Counterfactuals and Causal Inference: Methods and Principles for Social Research*](http://www.amazon.com/Counterfactuals-Causal-Inference-Principles-Analytical/dp/0521671930/)

Experiments:

 - [Glennerster & Takavarasha 2013, *Running Randomized Evaluations: A Practical Guide*](http://www.amazon.com/Running-Randomized-Evaluations-Practical-Guide/dp/0691159270/)
 - [Dunning 2012, *Natural Experiments in the Social Sciences*](http://www.amazon.com/Natural-Experiments-Social-Sciences-Design-Based/dp/1107698006/)

Simulation:

 - [Epstein 2006, *Generative Social Science: Studies in Agent-Based Computational Modeling*](http://www.amazon.com/Generative-Social-Science-Agent-Based-Computational/dp/0691125473/)
 - [Nelson 2010, *Stochastic Modeling: Analysis and Simulation*](http://www.amazon.com/Stochastic-Modeling-Analysis-Simulation-Mathematics/dp/0486477703/)

Expert elicitation, probabilistic estimation:

 - [Hubbard 2014, *How to Measure Anything: Finding the Value of Intangibles in Business*](http://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/1118539273/)', 609, '2014-06-20 20:03:51.247', 'fcc488f3-6be9-4adc-8306-1ed6b1764309', 514, 1265, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I can''t add a comment yet, but just FYI ESL is [available for free online as a pdf][1]


  [1]: http://statweb.stanford.edu/~tibs/ElemStatLearn/', 426, '2014-06-20 20:44:39.280', 'b9e03ca9-8da1-4d86-be30-0e704cc76aaa', 515, 1266, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m  working on multiclass logistic regression model with a large number of features (numFeatures>100).  Using a Maximum Likelihood Estimation based cost function and gradient, the fmincg algorithm solves the problem quickly... however, I am also experimenting with a different cost function and do not have a gradient... is there a good way to speed up the calculation process? E.g. a different algorithm or fmincg setting?', 985, '2014-06-21 04:59:06.620', 'c03cd23d-966f-4934-b5da-752ab7b3e34f', 516, 1268, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('efficient solution of fmincg without providing gradient?', 985, '2014-06-21 04:59:06.620', 'c03cd23d-966f-4934-b5da-752ab7b3e34f', 516, 1269, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><data-mining>', 985, '2014-06-21 04:59:06.620', 'c03cd23d-966f-4934-b5da-752ab7b3e34f', 516, 1270, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m working on multiclass logistic regression model with a large number of features (numFeatures > 100). Using a Maximum Likelihood Estimation based on the cost function and gradient, the fmincg algorithm solves the problem quickly. However, I''m also experimenting with a different cost function and do not have a gradient.

Is there a good way to speed up the calculation process? E.g., is there a different algorithm or fmincg setting that I can use?', 84, '2014-06-21 06:38:58.260', '3bba4392-57dc-45ae-b609-29f955a9eae9', 516, 'Improving formatting.', 1271, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Efficient solution of fmincg without providing gradient?', 84, '2014-06-21 06:38:58.260', '3bba4392-57dc-45ae-b609-29f955a9eae9', 516, 'Improving formatting.', 1272, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you do not have a gradient available, but the problem is convex, you can use the [Nelder-Mead simplex method][1]. It is available in most optimization packages, for example in [scipy.optimize][2].


  [1]: http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method
  [2]: http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize', 119, '2014-06-21 07:02:10.687', 'b29fd315-6285-44dc-8887-a781a3a791a8', 517, 1273, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Please, could someone recommend a paper or blog post that describes the online k-means algorithm in a good way with code samples if there.

Thanks :)', 960, '2014-06-21 10:55:41.700', 'd9b42b23-4e5a-4363-8323-1e8aec1daa10', 518, 1274, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Online k-means explanation', 960, '2014-06-21 10:55:41.700', 'd9b42b23-4e5a-4363-8323-1e8aec1daa10', 518, 1275, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><clustering>', 960, '2014-06-21 10:55:41.700', 'd9b42b23-4e5a-4363-8323-1e8aec1daa10', 518, 1276, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The main problem here is that even before attempting to apply anomaly detection algorithms, you are not getting good enough predictions of gas consumption using neural networks.

If the main goal here is to reach the stage when anomaly detection algorithms could be used and you state that you have access to examples of successful application of linear regression for this problem, this approach could be more productive. One of the principles of successful machine learning application is that several different algorithms can be tried out before final selection based on results.

It you choose to tune your neural network performance, [learning curve][1] plotting the effect of change in different hyperparameters on the error rate can be used. The number of features, the order of the polynomial features, regularization parameter or number of layers in the network can be modified and selected by the best performance on cross validation set.


  [1]: http://stackoverflow.com/questions/4617365/what-is-a-learning-curve-in-machine-learning', 454, '2014-06-21 12:59:15.373', '775af418-0262-47d1-9063-15e8bb45006d', 519, 1277, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The main problem here is that even before attempting to apply anomaly detection algorithms, you are not getting good enough predictions of gas consumption using neural networks.

If the main goal here is to reach the stage when anomaly detection algorithms could be used and you state that you have access to examples of successful application of linear regression for this problem, this approach could be more productive. One of the principles of successful machine learning application is that several different algorithms can be tried out before final selection based on results.

It you choose to tune your neural network performance, [learning curve][1] plotting the effect of change in different hyperparameters on the error rate can be used. Hyperparameters that can be modified are:

 - number of features
 - order of the polynomial
 - regularization parameter
 - number of layers in the network

Best settings can be selected by the performance on cross validation set.


  [1]: http://stackoverflow.com/questions/4617365/what-is-a-learning-curve-in-machine-learning', 454, '2014-06-21 13:36:29.167', '322a0244-d0d4-4825-bca9-1cd14b15f392', 519, 'Improved formatting of the list.', 1278, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In your training notebook you present results for training with 20 epochs. Have you tried varying that parameter, to see if it affects your performance? This is an important parameter for back-propagation.

For estimating your model parameters, as user tomaskazemekas pointed out, plotting Learning Curves is a very good approach. In addition to that, you could also create a plot using a model parameter (e.g. training epochs or hidden layer size) vs. Training and Validation error. This will allow you to understand the bias/variance tradeoff, and help you pick a good value for your parameters. [Some info can be found here][1]. Naturally, it is a good idea to keep a small percentage of your data for a (third) Test set.

As a side note, it seems that increasing the number of neurons in your model show no significant improvement for your RMSE. This suggests that you could also try with a simpler model, i.e. with less neurons and see how your model behaves.

In fact, I would suggest (if you haven''t done so already) trying a simple model with few or no parameters first e.g. Linear Regression, and compare your results with the literature, just as a sanity check.


  [1]: http://www.astroml.org/sklearn_tutorial/practical.html#cross-validation-and-testing', 1085, '2014-06-21 19:13:47.757', 'e7099a30-e093-4b56-be81-de719d5a301a', 520, 1279, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I wonder which type of model cross-validation to choose for classification problem: K-fold or random sub-sampling (bootstrap sampling)?

My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.

In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.

On the other hand I don''t like random sub-sampling feature: that some items won''t be ever selected for training/validation, and some will be used more than once.

Classification algorithms used: random forest & logistic regression.', 97, '2014-06-22 10:03:50.983', 'bd9459b2-c9f1-4aad-b3aa-98a034412551', 511, 'Use more accurate term in header', 1281, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Cross-validation: K-fold vs Bootstrap sampling', 97, '2014-06-22 10:03:50.983', 'bd9459b2-c9f1-4aad-b3aa-98a034412551', 511, 'Use more accurate term in header', 1282, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<cross-validation><sampling>', 97, '2014-06-22 10:03:50.983', 'bd9459b2-c9f1-4aad-b3aa-98a034412551', 511, 'Use more accurate term in header', 1283, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Conjugate Gradient -- the cg in Function MINimization (nonlinear) Conjugate Gradiant -- requires you to have a gradient function (or approximation) since that is a critical part of the algorithm itself: it needs to find the steepest descent direction quickly.

``fminsearch`` implements Nelder-Mead, a nonlinear gradient-free method. Its convergence properties are not anywhere near as good.

What is your cost function? Are there approximations that are differentiable (pref. twice so you can use the very powerful quasi-Newton methods)?', 1061, '2014-06-22 12:02:52.773', 'a907c87a-f7c4-4655-92a6-b0bdba7c0a9c', 521, 1285, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have been able to optimize very strange functions with simulated annealing, and it does not require a gradient. Instead, it uses random numbers in a way very similar to Markov Chain Monte Carlo, which helps it avoid getting stuck in local optima. A decent explanation that gives the intuition behind it can be found in this lecture: [Simulated Annealing][1]. scipy 0.14 includes this algorithm in its optimization module: [scipy.optimize.anneal][2].


  [1]: http://iacs-courses.seas.harvard.edu/courses/am207/blog/lecture-13.html
  [2]: http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.anneal.html', 1011, '2014-06-22 13:18:38.760', '0ce70e5c-1052-4c77-a43a-cdb6fc1a634f', 522, 1286, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In your notebooks, I did not see your neural network model, can you point which library is using, how many layers you have and what type of neural network are you using?

In your notebooks, it seems you are using the noisy and outlier dataset to train the neural network, I think you should train the neural network on the dataset that you do not have any outliers so that you could see the observation distance from the prediction of the neural network to label the observation either outlier or not.



I wrote [couple](http://bugra.github.io/work/notes/2014-03-31/outlier-detection-in-time-series-signals-fft-median-filtering/) of [things](http://bugra.github.io/work/notes/2014-05-11/robust-regression-and-outlier-detection-via-gaussian-processes/) on outlier detection in time-series signals, your data is highly seasonal as sobach mentioned and you could use FFT(first link above) to get the overall trend in the signal. After you get the frequency component in the gas consumption, you could look at the high frequency components to get the outliers.

Also if you want to insist on using neural network for seasonal data, you may want to check recurrent neural networks out as they could incorporate the past observations better than a vanilla neural network, and supposedly may provide a better result for the data that you have. ', 1096, '2014-06-22 16:03:39.047', '46fc4ea9-843c-43a6-bd04-7c3d6bf98bf0', 523, 1287, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Please, could someone recommend a paper or blog post that describes the online k-means algorithm.

Thanks :)', 960, '2014-06-22 16:44:13.780', 'c01cdb68-d303-4a07-8229-e830556a9d66', 518, 'deleted 41 characters in body', 1288, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In order to understand the variety of ways machine learning can be integrated into production applications, I think it is useful to look at open source projects and papers/blog posts from companies describing their infrastructure.

The common theme that these systems have is the separation of model training from model application. In production systems, model application needs to be fast, on the order of 100s of ms, but there is more freedom in how frequently fitted model parameters (or equivalent) need to be updated.

People use a wide range of solutions for model training and deployment:

* Build a model in R/Python and export it as PMML
> * [AirBnB describes their model training][1] in R/Python and deployment of PMML models via OpenScoring.

* Build a model in MapReduce and access values in a custom system
> * [Conjecture is an open source project from Etsy][2] that allows for model training with [Scalding][3], an easier to use scala wrapper around MapReduce, and  deployment via Php.
>
> * [Kiji is an open source project from WibiData][4] that allows for real-time model scoring (application) as well as functioanlity for persisting user data and training models on that data via [Scalding.][5]

* Use an online system that allows for continuously updating model parameters.
> * [Google released a great paper about an online collaborative filtering][6] they implemented to deal with recommendations in Google News.


  [1]: http://nerds.airbnb.com/architecting-machine-learning-system-risk/
  [2]: http://codeascraft.com/2014/06/18/conjecture-scalable-machine-learning-in-hadoop-with-scalding/
  [3]: https://github.com/twitter/scalding
  [4]: http://www.kiji.org/
  [5]: https://github.com/twitter/scalding
  [6]: http://dl.acm.org/citation.cfm?id=1242610', 406, '2014-06-22 18:35:22.207', '29075854-a65f-46d1-8ee9-68405bc83a6e', 525, 1292, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Chapter 1 of Practical Data Science with R (http://www.manning.com/zumel/) has a great breakdown of the data science process, including team roles and how they relate to specific tasks. The book follows the models laid out in the chapter by referencing which stages/personnel this or that particular task would be performed by.  ', 1103, '2014-06-22 18:50:07.193', 'cc32ec69-a278-4c28-9a18-81edd5bf5a84', 526, 1293, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Chapter 1 of [Practical Data Science with R](http://www.manning.com/zumel/) has a great breakdown of the data science process, including team roles and how they relate to specific tasks. The book follows the models laid out in the chapter by referencing which stages/personnel this or that particular task would be performed by.', 84, '2014-06-22 19:31:48.937', 'ec3950fc-b66b-4646-bdef-7b1a553d5d2d', 526, 'Improving formatting.', 1294, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In order to understand the variety of ways machine learning can be integrated into production applications, I think it is useful to look at open source projects and papers/blog posts from companies describing their infrastructure.

The common theme that these systems have is the separation of model training from model application. In production systems, model application needs to be fast, on the order of 100s of ms, but there is more freedom in how frequently fitted model parameters (or equivalent) need to be updated.

People use a wide range of solutions for model training and deployment:

* Build a model, then export and deploy it with PMML
> * [AirBnB describes their model training][1] in R/Python and deployment of PMML models via OpenScoring.
>
> * [Pattern][2] is project related to [Cascading][3] that can consume PMML and deploy predictive models.

* Build a model in MapReduce and access values in a custom system
> * [Conjecture is an open source project from Etsy][4] that allows for model training with [Scalding][5], an easier to use scala wrapper around MapReduce, and  deployment via Php.
>
> * [Kiji is an open source project from WibiData][6] that allows for real-time model scoring (application) as well as functioanlity for persisting user data and training models on that data via [Scalding.][7]

* Use an online system that allows for continuously updating model parameters.
> * [Google released a great paper about an online collaborative filtering][8] they implemented to deal with recommendations in Google News.


  [1]: http://nerds.airbnb.com/architecting-machine-learning-system-risk/
  [2]: http://www.cascading.org/projects/pattern/
  [3]: http://www.cascading.org/
  [4]: http://codeascraft.com/2014/06/18/conjecture-scalable-machine-learning-in-hadoop-with-scalding/
  [5]: https://github.com/twitter/scalding
  [6]: http://www.kiji.org/
  [7]: https://github.com/twitter/scalding
  [8]: http://dl.acm.org/citation.cfm?id=1242610', 406, '2014-06-22 19:54:28.380', '5bea5e9f-44d6-4da2-88dc-84cef77d6bd5', 525, 'Added Pattern to the list of examples.', 1296, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a load of documents, which have a load of key value pairs in them. The key might not be unique so there might be multiple keys of the same type with different values.

I want to compare the similarity of the keys between 2 documents. More specifically the string similarity of these values. I am thinking of using something like the Smith-Waterman Algorithm to compare the similarity.

So I''ve drawn a picture of how I''m thinking about representing the data -

http://i.imgur.com/sNmGf98

The values in the cells are the result of the smith-waterman algorithm (or some other string similarity metric).

Image that this matrix represents a key type of "things" I then need to add the "things" similarity score into a vector of 0 or 1. Thats ok.

What I can''t figure out is how I determine if the matrix is similar or not similar - ideally I want to convert the matrix to an number between 0 and 1 and then I''ll just set a threshold to score it as either 0 or 1.

Any ideas how I can create a score of the matrix? Does anyone know any algorithms that do this type of thing (obviously things like how smith waterman works is kind of applicable)

Cheers

Dave', 1107, '2014-06-22 21:45:35.800', '4b4b0e39-4c50-49db-a49d-ac2f2766fc92', 527, 1297, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Score matrix string similarity', 1107, '2014-06-22 21:45:35.800', '4b4b0e39-4c50-49db-a49d-ac2f2766fc92', 527, 1298, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<algorithms><similarity>', 1107, '2014-06-22 21:45:35.800', '4b4b0e39-4c50-49db-a49d-ac2f2766fc92', 527, 1299, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a load of documents, which have a load of key value pairs in them. The key might not be unique so there might be multiple keys of the same type with different values.

I want to compare the similarity of the keys between 2 documents. More specifically the string similarity of these values. I am thinking of using something like the Smith-Waterman Algorithm to compare the similarity.

So I''ve drawn a picture of how I''m thinking about representing the data -

![enter image description here][1]

The values in the cells are the result of the smith-waterman algorithm (or some other string similarity metric).

Image that this matrix represents a key type of "things" I then need to add the "things" similarity score into a vector of 0 or 1. Thats ok.

What I can''t figure out is how I determine if the matrix is similar or not similar - ideally I want to convert the matrix to an number between 0 and 1 and then I''ll just set a threshold to score it as either 0 or 1.

Any ideas how I can create a score of the matrix? Does anyone know any algorithms that do this type of thing (obviously things like how smith waterman works is kind of applicable).


  [1]: http://i.stack.imgur.com/VC4em.png', 84, '2014-06-22 22:03:48.810', 'dcecb510-2af3-4ca3-b6ff-757789ce8a4d', 527, 'Improving formatting.', 1300, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a load of documents, which have a load of key value pairs in them. The key might not be unique so there might be multiple keys of the same type with different values.

I want to compare the similarity of the keys between 2 documents. More specifically the string similarity of these values. I am thinking of using something like the [Smith-Waterman Algorithm](http://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm) to compare the similarity.

So I''ve drawn a picture of how I''m thinking about representing the data -

![enter image description here][1]

The values in the cells are the result of the smith-waterman algorithm (or some other string similarity metric).

Image that this matrix represents a key type of "things" I then need to add the "things" similarity score into a vector of 0 or 1. Thats ok.

What I can''t figure out is how I determine if the matrix is similar or not similar - ideally I want to convert the matrix to an number between 0 and 1 and then I''ll just set a threshold to score it as either 0 or 1.

Any ideas how I can create a score of the matrix? Does anyone know any algorithms that do this type of thing (obviously things like how smith waterman works is kind of applicable).


  [1]: http://i.stack.imgur.com/VC4em.png', 84, '2014-06-22 22:44:19.880', 'a7b1eb05-82c2-4eb4-9650-a59cf2cb511f', 527, 'added 65 characters in body', 1301, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Airbnb][1] and [Etsy][2] both recently posted detailed information about their workflows.


  [1]: http://nerds.airbnb.com/architecting-machine-learning-system-risk/
  [2]: http://codeascraft.com/2014/06/18/conjecture-scalable-machine-learning-in-hadoop-with-scalding/', 159, '2014-06-23 03:44:57.643', '6106b197-7ea8-47f5-baec-631dfc09aac7', 529, 1305, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is a general recommendation that algorithms in ensemble learning combinations should be different in nature. Is there a classification table, a scale or some rules that allow to evaluate how far away are the algorithms from each other? What are the best combinations? ', 454, '2014-06-23 04:39:26.623', 'b6c2fd67-73c5-41a1-b318-0e4c3148d435', 530, 1306, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to select algorithms for ensemble methods?', 454, '2014-06-23 04:39:26.623', 'b6c2fd67-73c5-41a1-b318-0e4c3148d435', 530, 1307, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 454, '2014-06-23 04:39:26.623', 'b6c2fd67-73c5-41a1-b318-0e4c3148d435', 530, 1308, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a dataset with following specifications:

 - Training dataset with 193176 samples with 2821 positives
 - Test Dataset with 82887 samples with 673 positives
 - There are 10 features.

I want to perform a binary classification ( say, 0/1 ). The issue I am facing is that the data is very biased or rather sparse. After normalization and scaling the data along with some feature engineering and using a couple of different algorithms, these are the best results I could achieve:

    mean square error : 0.00804710026904
    Confusion matrix : [[82214   667]
                       [    0     6]]

i.e only 6 correct positive hits. This is using logistic regression. Here are the various things I tried with this:

 - Different algorithms like RandomForest, DecisionTree, SVM
 - Changing parameters value to call the function
 - Some intuition based feature engineering to include compounded features

Now, my questions are:

 1. What can I do to improve the number of positive hits ?
 2. How can one determine if there is an overfit in such a case ? ( I have tried plotting etc. )
 3. At what point could one conclude if maybe this is the best possible fit I could have? ( which seems sad considering only 6 hits out of 673 )
 4. Is there a way I could make the positive sample instances weigh more so the pattern recognition improves leading to more hits ?
 5. Which graphical plots could help detect outliers or some pattern intuition?

I am using the scikit-learn library with Python and all implementations are library functions.', 793, '2014-06-23 07:03:15.643', '62feffeb-5ca9-4187-ba80-c79dd44e682c', 531, 1309, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Binary classification model for sparse / biased data', 793, '2014-06-23 07:03:15.643', '62feffeb-5ca9-4187-ba80-c79dd44e682c', 531, 1310, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><python><classification><logistic-regression><topic-model>', 793, '2014-06-23 07:03:15.643', '62feffeb-5ca9-4187-ba80-c79dd44e682c', 531, 1311, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In general in an ensemble you try to combine the opinions of multiple classifiers. The idea is like asking a bunch of experts on the same thing. You get multiple opinions and you later have to combine their answers (e.g. by a voting scheme). For this trick to work you want the classifier to be different from each other, that is you don''t want to ask the same "expert" twice for the same thing.

In practice, the classifiers do not have to be different in the sense of a different algorithm. What you can do is train the same algorithm with different subset of the data or a different subset of futures (or both). If you use different training sets you end up with different models and different "independent" classifiers.

There is no gold rule on what''s work best in general. You have to try to see if there is any improvement for your specific problem. ', 418, '2014-06-23 08:03:50.327', '7e470ab0-5e5a-4c23-a0a0-15fa19869307', 532, 1312, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('1. Since you are doing binary classification, have you tried adjusting the classification threshold? Since your algorithm seems rather insensitive, I would try lowering it and check if there is an improvement.

2. You can always use [Learning Curves][1], or a plot of one model parameter vs. Training and Validation error to determine whether your model is overfitting. It seems it is under fitting in your case, but that''s just intuition.

3. Well, ultimately it depends on your dataset, and the different models you have tried. At this point, and without further testing, there can not be a definite answer.

4. Without claiming to be an expert on the topic, there are a number of different techniques you may follow (hint: [first link on google][2]), but in my opinion you should first make sure you choose your cost function carefully, so that it represents what you are actually looking for.

5. Not sure what you mean by pattern intuition, can you elaborate?

By the way, what were your results with the different algorithms you tried? Were they any different?

  [1]: http://stackoverflow.com/questions/4617365/what-is-a-learning-curve-in-machine-learning
  [2]: http://florianhartl.com/thoughts-on-machine-learning-dealing-with-skewed-classes.html', 1085, '2014-06-23 08:13:22.837', '11f26ba5-157b-4fd3-91b3-770dcc1ad8cf', 533, 1313, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In general in an ensemble you try to combine the opinions of multiple classifiers. The idea is like asking a bunch of experts on the same thing. You get multiple opinions and you later have to combine their answers (e.g. by a voting scheme). For this trick to work you want the classifier to be different from each other, that is you don''t want to ask the same "expert" twice for the same thing.

In practice, the classifiers do not have to be different in the sense of a different algorithm. What you can do is train the same algorithm with different subset of the data or a different subset of futures (or both). If you use different training sets you end up with different models and different "independent" classifiers.

There is no gold rule on what''s work best in general. You have to try to see if there is an improvement for your specific problem. ', 418, '2014-06-23 08:14:15.807', 'ddb8ad0f-c11c-4b26-a4f3-333c8e39f7a3', 532, 'deleted 1 character in body', 1314, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If your goal is to transform your matrix into a number (your similarity measure), you may want to use a [matrix norm][1].

For instance, using the [Frobenius norm][2] on your example would return 1.488086.


  [1]: http://en.wikipedia.org/wiki/Matrix_norm
  [2]: http://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm', 883, '2014-06-23 09:23:27.617', 'a47cb18f-58a5-41d6-b0e1-a1f123898233', 534, 1315, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As I understood, Document 1 and Document 2 may have different number of keys. And you wand to get final similarity evaluation between 0 and 1. If so, I would propose following algorithm:

1. Sum of max. vals is equal to 0.
2. Select maximum value from doc-doc matrix and add it to Sum of max. vals.
3. Remove row and column with maximum value from the matrix.
4. Repeat steps 2-3 until rows or columns are ended.
5. Denominate Sum of max. vals by average number of key words in two texts.

Final estimation would be equal to 1, if both documents have identical length, and every word from Doc 1 has equivalent in Doc 2.

You haven''t mentioned software, you are using, but here is **R** example of function, computing such similarity (it takes object of class matrix as input):

    eval.sim <- function(sim.matrix){
      similarity <- 0
      denominator <- sum(dim(sim.matrix)) / 2
      for(i in 1:(min(c(nrow(sim.matrix), ncol(sim.matrix))) - 1)){
        extract <- which(sim.matrix == max(sim.matrix), arr.ind=T)[1, ]
        similarity <- similarity + sim.matrix[extract[1], extract[2]]
        sim.matrix <- sim.matrix[-extract[1], -extract[2]]
      }
      similarity <- similarity + max(sm.copy)
      similarity <- similarity / denominator
    }', 941, '2014-06-23 09:42:46.450', '0e099b84-2007-48dc-a0a3-c6b51f8a6828', 535, 1316, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a dataset with following specifications:

 - Training dataset with 193176 samples with 2821 positives
 - Test Dataset with 82887 samples with 673 positives
 - There are 10 features.

I want to perform a binary classification ( say, 0/1 ). The issue I am facing is that the data is very biased or rather sparse. After normalization and scaling the data along with some feature engineering and using a couple of different algorithms, these are the best results I could achieve:

    mean square error : 0.00804710026904
    Confusion matrix : [[82214   667]
                       [    0     6]]

i.e only 6 correct positive hits. This is using logistic regression. Here are the various things I tried with this:

 - Different algorithms like RandomForest, DecisionTree, SVM
 - Changing parameters value to call the function
 - Some intuition based feature engineering to include compounded features

Now, my questions are:

 1. What can I do to improve the number of positive hits ?
 2. How can one determine if there is an overfit in such a case ? ( I have tried plotting etc. )
 3. At what point could one conclude if maybe this is the best possible fit I could have? ( which seems sad considering only 6 hits out of 673 )
 4. Is there a way I could make the positive sample instances weigh more so the pattern recognition improves leading to more hits ?
 5. Which graphical plots could help detect outliers or some intuition about which pattern would fit the best?

I am using the scikit-learn library with Python and all implementations are library functions.

**edit:**

Here are the results with a few other algorithms:

Random Forest Classifier(n_estimators=100)

    [[82211   667]
    [    3     6]]


Decision Trees:

    [[78611   635]
    [ 3603    38]]
', 793, '2014-06-23 10:38:31.273', '6cd8d47a-68cf-409f-88fb-b8881d6edf56', 531, 'added 66 characters in body', 1317, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('More than often data I am working with is not 100% clean. Even if it is reasonably clean, still there are there are pieces that need to be fixed.

When a fraction of data needs it, I write a script and incorporate it in data processing.

But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let''s focus on "small data", in CSV files or as SQL database.

The practical problems I encountered:

* Writing a general script trying solve all similar errors may give unintended consequences.
* Copying and modifying data may make a mess, as:
  * Generating it again will destroy all fixes.
  * When there are more errors of different kinds, it is easy to get to many copies of the same file and loose track of them.
* Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, _seems_ to be the best).

If there any good practice?', 289, '2014-06-23 11:03:09.767', '1ed3b8bb-d889-43f2-b954-c73bcb9ff032', 536, 1318, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Good practices for manual modifications of data', 289, '2014-06-23 11:03:09.767', '1ed3b8bb-d889-43f2-b954-c73bcb9ff032', 536, 1319, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-cleaning>', 289, '2014-06-23 11:03:09.767', '1ed3b8bb-d889-43f2-b954-c73bcb9ff032', 536, 1320, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('More than often data I am working with is not 100% clean. Even if it is reasonably clean, still there are pieces that need to be fixed.

When a fraction of data needs it, I write a script and incorporate it in data processing.

But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let''s focus on "small data", in CSV files or as SQL database.

The practical problems I encountered:

* Writing a general script trying solve all similar errors may give unintended consequences.
* Copying and modifying data may make a mess, as:
  * Generating it again will destroy all fixes.
  * When there are more errors of different kinds, it is easy to get to many copies of the same file and loose track of them.
* Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, _seems_ to be the best).

If there any good practice?', 1085, '2014-06-23 11:05:16.030', 'dd8d1e6e-6dcb-42b3-b05f-c19bfe581783', 536, 'fixed typo', 1321, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-23 11:05:16.030', 'dd8d1e6e-6dcb-42b3-b05f-c19bfe581783', 536, 'Proposed by 1085 approved by 289 edit id of 89', 1322, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('More than often data I am working with is not 100% clean. Even if it is reasonably clean, still there are pieces that need to be fixed.

When a fraction of data needs it, I write a script and incorporate it in data processing.

But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let''s focus on "small data", in CSV files or as SQL database.

The practical problems I encountered:

* Writing a general script trying solve all similar errors may give unintended consequences (e.g. matching cities that are different but happen to have similar names).
* Copying and modifying data may make a mess, as:
  * Generating it again will destroy all fixes.
  * When there are more errors of different kinds, it is easy to get to many copies of the same file and loose track of them.
* Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, _seems_ to be the best); and either we need to create more copies of data (as in the previous point) or run the script every time we load data.

If there any good practice?', 289, '2014-06-23 11:20:05.343', '8202ac86-2b21-45fb-b97e-2f294105440b', 536, 'added 194 characters in body', 1323, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In general in an ensemble you try to combine the opinions of multiple classifiers. The idea is like asking a bunch of experts on the same thing. You get multiple opinions and you later have to combine their answers (e.g. by a voting scheme). For this trick to work you want the classifiers to be different from each other, that is you don''t want to ask the same "expert" twice for the same thing.

In practice, the classifiers do not have to be different in the sense of a different algorithm. What you can do is train the same algorithm with different subset of the data or a different subset of features (or both). If you use different training sets you end up with different models and different "independent" classifiers.

There is no golden rule on what works best in general. You have to try to see if there is an improvement for your specific problem. ', 1085, '2014-06-23 11:21:51.677', 'b1ab6990-7925-42a9-9f37-c5c3e772c949', 532, 'grammar and typos', 1324, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-23 11:21:51.677', 'b1ab6990-7925-42a9-9f37-c5c3e772c949', 532, 'Proposed by 1085 approved by 418 edit id of 90', 1325, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data Science and Machine Learning include a lot of different topics and it´s hard to stay up-to-date about all the news about papers, researches or new tutorials and tools.

What sources do you use to get all the information?

I use mostly Reddit as my first source and the subreddits Machine Learning and R
[http://www.reddit.com/r/MachineLearning][1]

[http://www.reddit.com/r/rstats][2]

But also datatau.com and of course the great KDNuggets page


  [1]: http://www.reddit.com/r/MachineLearning
  [2]: http://www.reddit.com/r/rstats

  ', 1125, '2014-06-23 13:31:41.443', '5793bd79-ac07-4663-b98d-d6be47c959b7', 538, 1329, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are your favorite sources for news about Machine Learning and Data Science?', 1125, '2014-06-23 13:31:41.443', '5793bd79-ac07-4663-b98d-d6be47c959b7', 538, 1330, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 1125, '2014-06-23 13:31:41.443', '5793bd79-ac07-4663-b98d-d6be47c959b7', 538, 1331, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I like following certain tags on stats.stackexchange.com (a.k.a. CrossValidated), as well as the ```r``` tag on StackOverflow.

Arxiv.org is also pretty cool, it has several subsections devoted to stats and ML. ', 1127, '2014-06-23 13:36:18.273', '9c160522-39ce-4fb3-95bb-c446e0f4466f', 539, 1333, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Chapter 1 of Practical Data Science with R (http://www.manning.com/zumel/) has a great breakdown of the data science process, including team roles and how they relate to specific tasks. The book follows the models laid out in the chapter by referencing which stages/personnel this or that particular task would be performed by.  ', 1103, '2014-06-23 13:36:51.493', '4295ceea-6189-4d93-a2c8-655acd933980', 526, 'Rollback to [cc32ec69-a278-4c28-9a18-81edd5bf5a84]', 1334, '8');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the very first thing you do when you get your hands on a new data set (assuming it is cleaned and well structured)? Please share sample code snippets as I am sure this would be extremely helpful for both beginners and experienced. ', 1131, '2014-06-23 15:38:10.920', 'ba2d429a-4dea-4fe3-a53a-280aaf727686', 540, 1336, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First steps on a new cleaned dataset', 1131, '2014-06-23 15:38:10.920', 'ba2d429a-4dea-4fe3-a53a-280aaf727686', 540, 1337, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<knowledge-base><dataset>', 1131, '2014-06-23 15:38:10.920', 'ba2d429a-4dea-4fe3-a53a-280aaf727686', 540, 1338, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The #1 most important thing is to *explicitly document your process*.

Different people working on different data make different choices. Any scientists who claim that their work is entirely objective and rational are in denial about the nature of science; every single model we create is informed by our biases and perspectives. The key is that we each have different biases and perspectives, which is one of the huge strengths of peer review, and of science as a community effort.

You have already identified some trade-offs between an algorithmic solution and individual/case-by-case corrections. If we have training as scientists, we may be used to thinking that everything has to be applied as an overall rule, so we may bend over backwards to find correlations and fit curves to data that just won''t reasonably accept it. Applying an algorithm to the entire data set is not the only tool at your disposal.

But, if you use your individual judgment to make exceptions and make "manual" corrections in your data, be prepared to defend your judgment and argue your case. You can still take a methodical approach, perhaps by classifying your data in a systematic way, and choosing which subset of the data to apply your cleaning algorithm to based on that system of classification. And when you do this, you document it, you make your case, and you respect the judgment of your colleagues in the field.

On the other hand, why do all this extra work before you know it''s necessary? Plenty of "dirty" data sets will still produce useful results. Perhaps 0.5% of your data is "dirty" in a way that you know will have a bad influence on your end product. Well, any analysis is subject to error, and the more you obsess over that 0.5% the more you will start to think of it and treat it like it was 5%, or 25% - much more significant than it truly is. Try to apply your analysis while admitting you know there is some error, and only do extra work to clean the data once you can show for certain that your analysis fails or is not useful otherwise.', 322, '2014-06-23 15:48:10.033', 'f09894c2-6b46-4d01-b90c-5eb5e9d78cef', 541, 1339, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The #1 most important thing is to *explicitly document your process*.

Different people working on different data make different choices. Any scientists who claim that their work is entirely objective and rational are in denial about the nature of science; every single model we create is informed by our biases and perspectives. The key is that we each have different biases and perspectives, which is one reason that science is the pursuit of a society, not of individuals, and underscores the importance of peer review.

You have already identified some trade-offs between an algorithmic solution and individual/case-by-case corrections. If we have training as scientists, we may be used to thinking that everything has to be applied as an overall rule, so we may bend over backwards to find correlations and fit curves to data that just won''t reasonably accept it. Applying an algorithm to the entire data set can be powerful when it works, but it''s not the only tool at your disposal. Sometimes you need to use your judgment as the scientist.

But, if you use your individual judgment to make exceptions and to manually correct your data, be prepared to defend your judgment and argue your case. Be prepared to show that you considered all the data. If you''re going to manually correct 8 observations out of a set of 100,000, you need to do more than justify those 8 manual corrections - you also need to justify **not** correcting the other 99,992 observations.

You can still take a methodical approach, perhaps by classifying your data in a systematic way, and choosing which subset of the data to apply your cleaning algorithm to based on that system of classification. And when you do this, you document it, you make your case, and you respect the judgment of your colleagues in the field.

On the other hand, why do all this extra work before you know it''s necessary? Plenty of "dirty" data sets will still produce useful results. Perhaps 0.5% of your data is "dirty" in a way that you know will have a bad influence on your end product. Well, any analysis is subject to error, and the more you obsess over that 0.5% the more you will start to think of it and treat it like it was 5%, or 25% - much more significant than it truly is. Try to apply your analysis while admitting you know there is some error, and only do extra work to clean the data once you can show for certain that your analysis fails or is not useful otherwise.', 322, '2014-06-23 15:54:19.253', '84894177-7b1f-4646-9bd1-309b6ca64dac', 541, 'added 396 characters in body', 1340, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The #1 most important thing is to *explicitly document your process*.

Different people working on different data make different choices. Any scientists who claim that their work is entirely objective and rational are in denial about the nature of science; every single model we create is informed by our biases and perspectives. The key is that we each have different biases and perspectives, which is one reason that science is the pursuit of a society, not of individuals, and underscores the importance of peer review.

You have already identified some trade-offs between an algorithmic solution and individual/case-by-case corrections. If we have training as scientists, we may be used to thinking that everything has to be applied as an overall rule, so we may bend over backwards to find correlations and fit curves to data that just won''t reasonably accept it. Applying an algorithm to the entire data set can be powerful when it works, but it''s not the only tool at your disposal. Sometimes you need to use your judgment as the scientist.

But, if you use your individual judgment to make exceptions and to manually correct your data, be prepared to defend your judgment and argue your case. Be prepared to show that you considered all the data. If you''re going to manually correct 8 observations out of a set of 100,000, you need to do more than justify those 8 manual corrections - you also need to justify **not** correcting the other 99,992 observations.

You can still take a methodical approach, perhaps by classifying your data in a systematic way, and choosing which subset of the data to apply your cleaning algorithm to based on that system of classification. And when you do this, you document it, you make your case, and you respect the judgment of your colleagues in the field.

On the other hand, why do all this extra work before you know it''s necessary? Plenty of "dirty" data sets will still produce useful results. Perhaps 0.5% of your data is "dirty" in a way that you know will have a bad influence on your end product. Well, any analysis is subject to error, and the more you obsess over that 0.5% the more you will start to think of it and treat it like it was 5%, or 25% - much more significant than it truly is. Try to apply your analysis while admitting you know there is some error, and only do extra work to clean the data once you can show for certain that your analysis fails or is not useful otherwise.

For example...
-

Say you have a set of test results from a survey of wells, showing the concentrations of certain dissolved substances, hourly over the course of a year. And you observe in this set certain spikes, of short duration, and orders of magnitude higher than the surrounding data. If they are few, and obvious, and you know that the sensors used to produce the data set occasionally malfunction, then there''s no reason to apply an algorithmic solution to the entire data set. You have a choice between excluding some data or modifying it.

I recommend the exclusion route whenever possible, since you are making fewer assumptions when you don''t have to additionally choose a "correct" value. But if your analysis will absolutely fail with a discontinuity, there are many options. You could choose to replace "bad" values via linear interpolation. You could hold constant the previous value. If there is some great previous literature to apply, and you really have a strong case that the "bad" data is purely due to equipment malfunction, perhaps there''s an established model that you can apply to fill in those regions.

There are a great many approaches, which is why the most important thing is to document your process, explicitly and exhaustively. Arguing your case is important too, but not strictly necessary; the community is just more likely to ignore you if you don''t make a full effort to engage the review process.', 322, '2014-06-23 16:14:33.900', '295fb4d7-bbfd-460f-bd86-7fbd2f3d9bef', 541, 'added 396 characters in body', 1341, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('More than often data I am working with is not 100% clean. Even if it is reasonably clean, still there are pieces that need to be fixed.

When a fraction of data needs it, I write a script and incorporate it in data processing.

But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let''s focus on "small data", in CSV files or as SQL database.

The practical problems I encountered:

* Writing a general script trying solve all similar errors may give unintended consequences (e.g. matching cities that are different but happen to have similar names).
* Copying and modifying data may make a mess, as:
  * Generating it again will destroy all fixes.
  * When there are more errors of different kinds, it is easy to get to many copies of the same file and loose track of them.
* Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, _seems_ to be the best); and either we need to create more copies of data (as in the previous point) or run the script every time we load data.

If there any good practice?

EDIT:The question is on the workflow, not whether to use it or not.

(In my particular case I don''t want the end-user to see misspelled cities and, even worse, see two points of data, for the same city but with different spelling; the data is small, ~500 different cities, so manual corrections do make sense.)', 289, '2014-06-23 17:19:54.087', '4d127859-f20b-4c12-b270-a007b0338834', 536, 'added 316 characters in body', 1345, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Well, you mention in your question of the data being ''clean and well structured''. In practice, close to 70% of the time is spent in doing these two steps. Of course the first thing you do is to separate the training and test data. Considering the plethora of libraries and tools available irrespective of which technology/language you prefer to use, the next step would be to understand the data via graph plots and drawing useful intuitions specific to your target goal. This would then be followed by various other problem specific methods. As pointed out, the question is very broad and citing code snippets is simply not feasible.', 793, '2014-06-23 17:50:51.320', '21129bfb-9858-42c0-a3d4-38bb23a43a7b', 543, 1346, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You could also try:

 - [Quora][1]
 - [Machine Learning Daily][2]


  [1]: http://www.quora.com
  [2]: http://paper.li/MachineCoding/1370791453', 793, '2014-06-23 17:56:32.917', '9aec4919-be72-4062-ae54-ee5172bbff15', 544, 1347, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think this is a reasonable question. Here is what I do:

 1. Peak at the first few rows
 2. Visualize the distribution of the features I care about (histograms)
 3. Visualize the relationship between pairs of features (scatterplots)

I downloaded the abalone dataset from the UCI Machine Learning repository [here][1]. Let''s say I care about how height and diameter can be used to predict whole weight. For completeness, I''ve included the step of reading the data from file.

    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns

    data = pd.read_csv("abalone.data", header=False)
    data.columns = ["sex", "length", "diameter", "height",
                    "whole_weight", "shucked_weight",
                    "viscera_weight", "shell_weight", "rings"]

Now we can take a peak at the first few rows:

    data.head()

![Head of dataset][2]

Now, I know that the variables I care about are floating point values and they can be treated as continuous. I want to take a look to see how these three variables are distributed:

    fig = plt.figure(figsize=(20,5))
    plt.subplot(1, 3, 1)
    plt.hist(data[''diameter''], normed=True)
    plt.title("Diameter")
    plt.subplot(1, 3, 2)
    plt.hist(data[''height''], normed=True)
    plt.title("Height")
    plt.subplot(1, 3, 3)
    plt.hist(data[''whole_weight''], normed=True)
    plt.title("Whole Weight")
    plt.show()

![histograms][3]

Great! Now, I know that diameter and whole weight are skewed left and right (respectively). I also know that there are some outliers in terms of height (which is why matplotlib gives me extra room to the right of the distribution). Finally, I''d like to see if I can find any visual patterns between my predictors and outcome variable. I use a scatter plot for this:

    plt.figure(figsize=(15,5))
    plt.subplot(1, 2, 1)
    plt.plot(data[''diameter''], data[''whole_weight''], ''o'')
    plt.title("Diameter vs. Whole Weight")
    plt.ylabel("Whole Weight")
    plt.xlabel("Diameter")
    plt.subplot(1, 2, 2)
    plt.plot(data[''height''], data[''whole_weight''], ''o'')
    plt.title("Height vs. Whole Weight")
    plt.ylabel("Whole Weight")
    plt.xlabel("Height")
    plt.show()

![scatterplots][4]

Here, I see there is a non-linear relationship between diameter and whole weight and I''m going to have to deal with my height outliers. Now, I''m ready to do some analysis!

  [1]: https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data
  [2]: http://i.stack.imgur.com/ZEFH6.png
  [3]: http://i.stack.imgur.com/Rmq4n.png
  [4]: http://i.stack.imgur.com/FMmN1.png', 1011, '2014-06-23 18:38:34.260', '1e0ba174-c077-4a10-bb57-b10b011d55c3', 545, 1348, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Even if you are effectively modifying certain records by hand, as in the city name example you give, I would recommend doing it in code. The reason to strongly prefer code over hand-tweaking records is that the code makes a result reproducible. You want to make sure that you can always go from raw data to final result without any human intervention.

Here''s a quick example. Let''s say I have a list of city names in a pandas dataframe and I am certain they should all be "omaha" (you need to be absolutely certain, because changing values by hand is fraught with danger). But instead I have the following strings:

    pd.unique(data.city)
    array([''omaha'', ''omahd'', ''imaha'', ''omaka''], dtype=object)

You could make the change like this:

    data.city.values[data.city.values == ''omahd''] = ''omaha''
    data.city.values[data.city.values == ''imaha''] = ''omaha''
    data.city.values[data.city.values == ''omaka''] = ''omaha''

That code is ugly, but if you run it on the same raw dataset, you will *always* get the same result.', 1011, '2014-06-23 18:56:11.577', '0c7a707e-5877-476e-9953-d1da4ba1f6b9', 546, 1350, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I often am building a model (classification or regression) where I have some predictor variables that are sequences and I have been trying to find technique recommendations for summarizing them in the best way possible for inclusion as predictors in the model.

As a concrete example, say a model is being built to predict if a customer will leave the company in the next 90 days (anytime between t and t+90; thus a binary outcome). One of the predictors available is the level of the customers financial balance for periods t_0 to t-1. Maybe this represents monthly observations for the prior 12 months (i.e. 12 measurements).

I am looking for ways to construct features from this series. I use descriptives of each customers series such as the mean, high, low, std dev., fit a OLS regression to get the trend. Are their other methods of calculating features? Other measures of change or volatility? ', 1138, '2014-06-23 23:20:36.180', '5fd58f20-ce2d-4e2b-8275-cd58af9cbbdc', 548, 1352, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Feature Extraction Technique - Summarizing a Sequence of Data', 1138, '2014-06-23 23:20:36.180', '5fd58f20-ce2d-4e2b-8275-cd58af9cbbdc', 548, 1353, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><feature-selection><time-series>', 1138, '2014-06-23 23:20:36.180', '5fd58f20-ce2d-4e2b-8275-cd58af9cbbdc', 548, 1354, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What you''re trying to do here is reduce the dimensionality of your features. You can search for dimensionality reduction to get several options, but one very popular technique is principal components analysis (PCA). Principal components are not interpretable like the options you''ve mentioned, but they do a good job of summarizing all of the information.', 1011, '2014-06-24 00:54:13.110', '482ba8d2-292e-4ea0-9987-052d216e7bcb', 549, 1355, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Feature extraction is always a challenge and the less addressed topic in literature, since it''s widely application dependant.

Some ideas you can try:

- Raw data, measured day-by-day. That''s kind of obvious with some implications and extra preprocessing (normalisation) in order to make timelines of different length comparable.
- Higher moments: skewness, kurtosis, etc
- Derivative(s): speed of evolution
- Time span is not that large but maybe it is worth trying some time series analysis features like for example autocorrelation.
- Some customised features like breaking timeline in weeks and measure the quantities you already measure in each week separately. Then a non-linear classifier would be able to combine e.g first-week features with last-week features in order to get insight of evolution in time.
 ', 418, '2014-06-24 03:50:41.410', '82b3e8df-f62f-46c5-ae13-9fc41dc59845', 550, 1356, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('If feasible I would link related records (e.g., Dave, David, etc.) and replace them with a sequence number (1,2,3, etc.) or a [salted](http://en.wikipedia.org/wiki/Salt_%28cryptography%29) [hash of the string](http://www.cse.yorku.ca/~oz/hash.html) that is used to represent all related records (e.g., David instead of Dave).

I assume that third parties need not have any idea what the real name is, otherwise you might as well give it to them.

**edit**: You need to define and justify what kind of operations the third party needs to be able to do. For example, what is wrong with using initials followed by a number (e.g., BOA-1, BOA-2, etc.) to disambiguate Bank of America from Benjamin Othello Ames? If that''s too revealing, you could bin some of the letters or names; e.g., [A-E] -> 1, [F-J] -> 2, etc. so BOA would become 1OA, or ["Bank", "Barry", "Bruce", etc.] -> 1 so Bank of America is again 1OA.

For more information see [k-anonymity](http://en.wikipedia.org/wiki/K-anonymity).', 381, '2014-06-24 05:05:21.900', 'f81ea09e-e737-4be1-b578-93dd85771299', 416, 'added 63 characters in body', 1358, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('At first glance, you need to extract features from your time series (x - 12) - x. One possible approach is to compute summary metrics: average, dispersion, etc. But doing so, you will loose all time-series related information. But data, extracted from curve shape may be quite useful. I recommend you to look through [this][1] article, where authors propose algorithm for time series clustering. Hope, it will be useful. Additionally to such clustering you can add summary statistics to your feature list.


  [1]: http://cs.stanford.edu/people/jure/pubs/memeshapes-wsdm11.pdf', 941, '2014-06-24 07:03:35.567', 'c668f8f3-d684-4daf-a846-b803ab7fdba2', 552, 1360, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am a CS master student in data mining. My supervisor once told me that before I run any classifier or do anything with a dataset I must fully understand the data and make sure that the data is clean and correct.

My questions:

- What are the best practices to understand a dataset (high dimensional with numerical and nominal attributes)?

- Practices to make sure the dataset is clean?

- Practices to make sure the dataset doesn''t have wrong values or so?', 728, '2014-06-24 07:29:57.787', '96fa7305-513d-4281-89f6-1ff641fc908a', 554, 1364, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Datasets understanding best practices', 728, '2014-06-24 07:29:57.787', '96fa7305-513d-4281-89f6-1ff641fc908a', 554, 1365, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics><dataset>', 728, '2014-06-24 07:29:57.787', '96fa7305-513d-4281-89f6-1ff641fc908a', 554, 1366, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**1.  Do not modify the original data**

Having the original data source intact is important.  You may find that updates you make to the data are not valid.  You may also find a more efficient way to make updates and you will want to regression test those updates.

Always work with a copy of the data, and add columns/properties/metadata that includes any processed corrections.

Example, if your data is a .csv file that includes a city name that contains several misspellings:

    1. Copy the .csv to a new file
    2. Add a column for containing "adjusted_city_name"
    3. Copy data from the city_name column to the
       adjusted_city_name column and make corrections in that column.

**2. Document proposed changes**

Any changes you want to make to data should be documented so that they can be replicated moving forward.

Version control and timestamp the document every time you change it.  That will help in troubleshooting at a later date.

Be explicit.  Do not simply state "correct capitalization problems", state "ensure that the first letter of each city name begins with a capital letter and the remaining letters are lower-case."

Update the document with references to any automation routines that have been built to manage data cleansing.

**3. Decide on a standard data cleansing technology**

Whether you use perl, python, java, a particular utility, a manual process or something else is not the issue.  The issue is that in the future you want to hand the data cleansing process to somebody else.  If they have to know 12 different data cleansing technologies, delegating the cleansing procedure will be very difficult.

**4. Standardize the workflow**

There should be a standard way to handle new data.  Ideally, it will be as simple as dropping a file in a specific location and a predictable automated process cleanses it and hands off a cleansed set of data to the next processing step.

**5. Make as few changes as is absolutely necessary**

It''s always better to have a fault tolerant analysis than one that makes assumptions about the data.

**6. Avoid manual updates**

It''s always tempting, but people are error-prone and again it makes delegation difficult.

**Notes on manual processing**

To more completely address the original question as to whether there''s a "good" way to do manual processing, I would say no, there is not.  My answer is based on experience and is not one that I make arbitrarily.

I have had more than one project lose days of time due to a client insisting that a manual data cleansing process was just fine and could be handled internally.  You do not want your projects to be dependent on a single individual accomplishing a judgement based task of varying scale.

It''s much better to have that individual build and document a rule set based on what they would do than to have them manually cleanse data.  (And then automating that rule set)

If automation fails you in the end or is simply not possible, the ability to delegate that rule set to others without domain specific knowledge is vital.

In the end, routines to do something like correct city names can be applied to other data sets.', 434, '2014-06-24 07:35:16.930', 'a4547fec-0b62-4727-b885-f2e5751192d3', 555, 1367, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are basic things you can do with any set of data:

1. Validate values (String length tolerance, data type, formatting masks, required field presence, etc.)
2. Range correctness (Does this seemingly correct data fall within expected ranges of values)
3. Preliminary processing (If I attempt to analyze this data, can I perform the basics without running into errors)
4. Preliminary reporting (run a report against a data set and ensure that it passes a sanity test)
5. Defining null vs. empty vs. zero vs. False for any given column of data
6. Identifying data that is out of place (numeric values dramatically different than other values in a data set, string values that look like they might be misspelled, etc.)
7. Eliminating or correcting obviously errant data

Understanding data to identify errors is a whole different ball game, and it is very important.

For instance, you can have a rule that says a serial number must be present in a given data set and that serial number must be alphanumeric with a maximum string length of 255 and a minimum string length of 5.

Looking at the data, you may find one particular serial number value reads `"PLEASE ENTER SERIAL"`  It''s perfectly valid, but wrong.

That''s kind of an obvious one, but say you''re processing stock data and you had a price range for 1000 stocks that was under a dollar.  A lot of people would not know that a stock price so low is invalid on certain exchanges and perfectly valid on others.  You need knowledge about your data to understand if what you are seeing is problematic or not.

In the real world, you don''t always have the luxury of understanding your data intimately.

The way I avoid problems is by leveraging the people around me.  For small data sets, I can ask someone to review the data in it''s entirety.  For large ones, pulling a set of random samples and asking someone to do a sanity check on the data is more appropriate.

Further, questioning the source of the data and how well that data source can be trusted is imperative.  I often have multiple conflicting sources of data and we create rules to determine the "source of truth".  Sometimes one data set has great data in a given aspect, but other data sets are stronger in other areas.

Manually entered data is usually what I''m most skeptical about, but in some cases it is stronger than anything that can be acquired through automation.', 434, '2014-06-24 08:09:49.300', '17e7aa77-d4a4-4d42-8b63-87d97c1c99c5', 556, 1368, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a hobby project which I am contemplating committing to as a way of increasing my so far limited experience of machine learning. I have taken and completed the Coursera MOOC on the topic. My question is with regards to the feasibility of the project.

The task is the following:

Neighboring cats are from time to time visiting my garden, which I dislike since they tend to defecate on my lawn. I would like to have a warning system that alerts me when there''s a cat present so that I may go chase it off using my super soaker. For simplicity''s sake, say that I only care about a cat with black and white coloring.

I have setup a raspberry pi with camera module that can capture video and/or pictures of a part of the garden.

Sample image:

![Sample garden image][1]

My first idea was to train a classifier to identify cat or cat-like objects, but after realizing that I will be unable to obtain a large enough number of positive samples, I have abandoned that in favor of anomaly detection.

I estimate that if I captured a photo every second of the day, I would end up with maybe five photos containing cats (out of about 60,000 with sunlight) per day.

Is this feasible using anomaly detection? If so, what features would you suggest? My ideas so far would be to simply count the number of pixels with that has certain colors; do some kind of blob detection/image segmenting (which I do not know how do to, and would thus like to avoid) and perform the same color analysis on them.


  [1]: http://i.stack.imgur.com/cNqus.jpg', 1147, '2014-06-24 12:28:10.990', '4aec521f-d54c-482c-9433-026800cb81be', 559, 1373, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Detecting cats visually by means of anomaly detection', 1147, '2014-06-24 12:28:10.990', '4aec521f-d54c-482c-9433-026800cb81be', 559, 1374, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 1147, '2014-06-24 12:28:10.990', '4aec521f-d54c-482c-9433-026800cb81be', 559, 1375, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is an interesting and also quite ambitious project :)

I am not sure anomaly detection (at least in the sense described in the course you followed) would be a very fitting algorithm in this case.

I would consider a more viable approach to be what has been discussed at the end of that course where a Photo OCR workflow was demonstrated.

The approach would consist of segmenting your image in smaller "blocks", and going through them one-by-one using a supervised learning algorithm and try to classify each block according to whether it contains a cat or not. If one block contains a cat, the alarm goes off. As a bonus, you get the position of the cat as well, so that you may think of incorporating some "automatic" response as a future step to your project.

The benefit here is that you will not have to train your algorithm using a dataset specific to your garden (which, as you mention is difficult to create), but you can use images of cats taken off the net (e.g. perhaps you can search for "cat on grass" or something), and perhaps patches of photos from your (or other) gardens. Therefore you don;t have to spend your time collecting photos from your camera, and you avoid the risk of having a very small (comparable) sample of positives (i.e. cats).

Now, of course how easy it is to build an accurate cat detector is another topic..', 1085, '2014-06-24 12:55:26.457', '61ca7eb4-4629-4687-8e67-7b7dc4593d74', 560, 1376, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You could simplify your problem significantly by using a motion/change detection approach. For example, you could compare each image/frame with one from an early time (e.g., a minute earlier), then only consider pixels that have changed since the earlier time. You could then extract the rectangular region of change and use that as the basis for your classification or anomaly detection.

Taking this type of approach can significantly simplify your classifier and reduce your false target rate because you can ignore anything that is not roughly the size of a cat (e.g., a person or bird). You would then use the extracted change regions that were not filtered out to form the training set for your classifier (or anomaly detector).

Just be sure to get your false target rate sufficiently low before mounting a laser turret to your feline intrusion detection system.', 964, '2014-06-24 13:34:12.107', '8fc893c9-b02b-4272-9f54-0bc5c00ca29f', 561, 1377, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I often am building a model (classification or regression) where I have some predictor variables that are sequences and I have been trying to find technique recommendations for summarizing them in the best way possible for inclusion as predictors in the model.

As a concrete example, say a model is being built to predict if a customer will leave the company in the next 90 days (anytime between t and t+90; thus a binary outcome). One of the predictors available is the level of the customers financial balance for periods t_0 to t-1. Maybe this represents monthly observations for the prior 12 months (i.e. 12 measurements).

I am looking for ways to construct features from this series. I use descriptives of each customers series such as the mean, high, low, std dev., fit a OLS regression to get the trend. Are their other methods of calculating features? Other measures of change or volatility?

ADD:

As mentioned in a response below, I also considered (but forgot to add here) using Dynamic Time Warping (DTW) and then hierarchical clustering on the resulting distance matrix - creating some number of clusters and then using the cluster membership as a feature. Scoring test data would likely have to follow a process where the DTW was done on new cases and the cluster centroids - matching the new data series to their closest centroids... ', 1138, '2014-06-24 14:03:22.697', 'e3429e93-8490-4083-aaa4-e729af9f0709', 548, 'added 454 characters in body', 1378, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As a rule of thumb I always propose three different options:

 - Use a bagging learning technique, similar to that one followed by Random Forest. This technique allows the training of ''small'' classifiers which see a small portion of the whole data. Afterwards, a simple voting scheme (as in Random Forest) will lead you to a very interesting and robust classification.
 - Use any technique related to fusioning information or probabilistic fusion. This is a very suitable solution in order to combine different likelihoods from different classifiers.
 - My last suggestion is the use of fuzzy logic, a very adequate tool in order to combine information properly from a probabilistic (belonging) perspective.

The selection of specific methods or strategies will depend enormously on the data.', 1155, '2014-06-24 15:44:52.540', '683ec7fe-4fb3-4d0b-a968-3370ae6a1875', 562, 1379, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The strategy of motion/change detection is certainly adequate, but I would add an extra operation. I would detect those regions that are more likely to be changed, for instance, the ladder seems a place where humans can be (also cats) and grass where dogs, cats or humans can be.

I would capture a map with size of the object and trajectory and with this I would create a cluster with the aim of detecting an object (with specific size within the image in terms of pixels) that moves with a certain speed and trajectory.

You can achieve this by using R or I would suggest OpenCV in order to detect movement and follow different objects.', 1155, '2014-06-24 15:55:37.110', '34c5f9ec-1abe-473d-8599-47b7b6d0290b', 563, 1380, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve written a simple recommender which generates recommendations for users based on what they  have clicked. The recommender generates a data file of this format :

    userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)
    a,b,.2
    a,c,.3
    a,d,.4
    a,e,.1
    e,b,.3
    e,c,.5
    e,d,.8

I''ve looked at these graphs :
https://github.com/mbostock/d3/wiki/Gallery

But im not sure which to use or are there other''s that will better visualize user similarities based on this dataset?
', 237, '2014-06-24 18:59:30.560', '7571f267-5c5c-4975-b961-84378482a2dc', 564, 1381, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What visualization technique to best describe this dataset ?', 237, '2014-06-24 18:59:30.560', '7571f267-5c5c-4975-b961-84378482a2dc', 564, 1382, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization><javascript>', 237, '2014-06-24 18:59:30.560', '7571f267-5c5c-4975-b961-84378482a2dc', 564, 1383, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As I increase the number of trees in scikit learn''s GradientBoostingRegressor I get more negative predictions even though there are no negative values in my training or testing set. ', 1162, '2014-06-24 19:43:24.643', 'd1467c00-43c4-457f-b105-1c0a4dc4e27c', 565, 1384, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Gradient Boosting Regression- negative predictions despite no negative y values in training set', 1162, '2014-06-24 19:43:24.643', 'd1467c00-43c4-457f-b105-1c0a4dc4e27c', 565, 1385, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><python><algorithms>', 1162, '2014-06-24 19:43:24.643', 'd1467c00-43c4-457f-b105-1c0a4dc4e27c', 565, 1386, '3');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-06-24 19:59:24.757', 'e06b4913-3667-4f45-b2e4-dbd6d42eb34e', 566, 'from http://stats.stackexchange.com/questions/104589/named-entity-recognition-nltk-using-regular-expression', 1387, '36');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have been playing with NLTK toolkit. I come across this problem a lot and searched for solution online but nowhere I got a satisfying answer. So I am putting my query here.

Many times NER doesn''t tag consecutive NNPs as one NE. I think editing the NER to use RegexpTagger also can improve the NER.

Example:

Input: "Barack Obama is a great person."

Output:  Tree(''S'', [Tree(''PERSON'', [(''Barack'', ''NNP'')]), Tree(''ORGANIZATION'', [(''Obama'', ''NNP'')]), (''is'', ''VBZ''), (''a'', ''DT''), (''great'', ''JJ''), (''person'', ''NN''), (''.'', ''.'')])

where as

input: ''Former Vice President Dick Cheney told conservative radio host Laura Ingraham that he "was honored" to be compared to Darth Vader while in office.''

Output: Tree(''S'', [(''Former'', ''JJ''), (''Vice'', ''NNP''), (''President'', ''NNP''), Tree(''NE'', [(''Dick'', ''NNP''), (''Cheney'', ''NNP'')]), (''told'', ''VBD''), (''conservative'', ''JJ''), (''radio'', ''NN''), (''host'', ''NN''), Tree(''NE'', [(''Laura'', ''NNP''), (''Ingraham'', ''NNP'')]), (''that'', ''IN''), (''he'', ''PRP''), (''``'', ''``''), (''was'', ''VBD''), (''honored'', ''VBN''), ("''''", "''''"), (''to'', ''TO''), (''be'', ''VB''), (''compared'', ''VBN''), (''to'', ''TO''), Tree(''NE'', [(''Darth'', ''NNP''), (''Vader'', ''NNP'')]), (''while'', ''IN''), (''in'', ''IN''), (''office'', ''NN''), (''.'', ''.'')])

Here Vice/NNP, President/NNP, (Dick/NNP, Cheney/NNP) , is correctly extracted.

So I think if nltk.ne_chunk is used first and then if two consecutive trees are NNP there are high chances that both refers to one entity.

Any suggestion will be really appreciated. I am looking for flaws in my approach.

Thanks.', 'mousecoder', 1165, '2014-06-24 17:06:10.310', 'a61cfd73-c314-4450-b6e5-0bdedf475892', 566, 1388, '2');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Named Entity Recognition: NLTK using Regular Expression', 'mousecoder', 1165, '2014-06-24 17:06:10.310', 'a61cfd73-c314-4450-b6e5-0bdedf475892', 566, 1389, '1');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp>', 'mousecoder', 1165, '2014-06-24 17:06:10.310', 'a61cfd73-c314-4450-b6e5-0bdedf475892', 566, 1390, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve written a simple recommender which generates recommendations for users based on what they have clicked. The recommender generates a data file with the following format:

    userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)
    a,b,.2
    a,c,.3
    a,d,.4
    a,e,.1
    e,b,.3
    e,c,.5
    e,d,.8

I''ve looked at [some graphs](https://github.com/mbostock/d3/wiki/Gallery), but I''m not sure which one to use, or if are there other ones that will better display the user similarities from the dataset above. Any suggestions?
', 84, '2014-06-24 20:25:06.887', 'd130bfb8-0b1e-44e6-a344-7f87849d2933', 564, 'Improving formatting.', 1391, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What visualization technique to best describe a recommendation dataset?', 84, '2014-06-24 20:25:06.887', 'd130bfb8-0b1e-44e6-a344-7f87849d2933', 564, 'Improving formatting.', 1392, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think you''re looking for a [similarity matrix][1] (see bottom of the page). If you don''t have data on similarity between certain pairs, you can always leave them as grey or white. Also, this will only work for data sets small enough to actually make out what''s going on. I''d say 25 rows / columns maximum.

In a similarity matrix, x, and y coordinates correspond to the two things you''re comparing, while a [colormap][2] magnitude represents similarity


  [1]: http://cgi.mtc.sri.com/Cluster-Lab/
  [2]: http://en.wikipedia.org/wiki/False_color', 1163, '2014-06-24 20:28:33.693', '2064a06f-3962-4f6c-9b57-05bc07de03f9', 567, 1393, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve written a simple recommender which generates recommendations for users based on what they have clicked. The recommender generates a data file with the following format:

    userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)
    a,b,.2
    a,c,.3
    a,d,.4
    a,e,.1
    e,b,.3
    e,c,.5
    e,d,.8

I''ve looked at [some graphs](https://github.com/mbostock/d3/wiki/Gallery), but I''m not sure which one to use, or if are there other ones that will better display the user similarities from the dataset above. Any suggestions?

I''m aiming this visualization at business users who are not at all technical. I would just like to show them an easy to understand visual that details how similar some users and so convince these users that the recommendation system is useful.', 237, '2014-06-24 20:33:20.620', '6c495d45-4592-4119-880c-a82cf7f4b722', 564, 'added 191 characters in body', 1394, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I suggest the use of Hidden Markov Models, with two possible states: (1) high levels and (0) low levels.

This technique might be helpful to decode your signal. Probably you would need a specific HMM for each codification.

Concerning noise, you may use a Butterworth high pass filter to smooth your signal.', 1155, '2014-06-24 20:34:18.473', '4ad2500c-d4d9-4568-aa0f-643404e315fd', 568, 1395, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is it a bird? Is it a cat? We have black-and-white magpies here. so that would fail.

First thing would be to exclude all areas that are green, cats are seldom green.

Then compare the rest to a reference image to remove static things like stones and stairs.

Detecting objects of a minimum size should be possible, but for a classification the resolution is too low. Could be also your neighbor testing his new remote controlled drone.

With two cameras you could do a 3d mapping of the objects and eliminate flying objects.', 1164, '2014-06-24 20:35:44.713', 'abe520e8-189d-402a-b733-2335be433637', 569, 1396, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I think you''re looking for a [similarity matrix][1] (see bottom of the page). If you don''t have data on similarity between certain pairs, you can always leave them as grey or white. Also, this will only work for data sets small enough to actually make out what''s going on. I''d say 25 rows / columns maximum.

In a similarity matrix, x, and y coordinates correspond to the two things you''re comparing, while a [colormap][2] magnitude represents similarity

**EDIT**:
One thing you could do to replace the colormap is the insert, say, circles of different sizes according to the similarity metric. Or you could insert the numbers themselves, again, varying the size of the number as the magnitude of that number varies. Size usually works best is business visualizations.


  [1]: http://cgi.mtc.sri.com/Cluster-Lab/
  [2]: http://en.wikipedia.org/wiki/False_color', 1163, '2014-06-24 20:37:02.377', '53215b3a-caf0-4917-8a79-c9274630e26f', 567, 'added 318 characters in body', 1397, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Many times [Named Entity Recognition](http://en.wikipedia.org/wiki/Named-entity_recognition) (NER) doesn''t tag consecutive NNPs as one NE. I think editing the NER to use RegexpTagger also can improve the NER.

For example, consider the following input:

> "Barack Obama is a great person."

And the output:

    Tree(''S'', [Tree(''PERSON'', [(''Barack'', ''NNP'')]), Tree(''ORGANIZATION'', [(''Obama'', ''NNP'')]),
        (''is'', ''VBZ''), (''a'', ''DT''), (''great'', ''JJ''), (''person'', ''NN''), (''.'', ''.'')])

where as for the input:

> ''Former Vice President Dick Cheney told conservative radio host Laura Ingraham that he "was honored" to be compared to Darth Vader while in office.''

the output is:

    Tree(''S'', [(''Former'', ''JJ''), (''Vice'', ''NNP''), (''President'', ''NNP''),
        Tree(''NE'', [(''Dick'', ''NNP''), (''Cheney'', ''NNP'')]), (''told'', ''VBD''), (''conservative'', ''JJ''),
        (''radio'', ''NN''), (''host'', ''NN''), Tree(''NE'', [(''Laura'', ''NNP''), (''Ingraham'', ''NNP'')]),
        (''that'', ''IN''), (''he'', ''PRP''), (''``'', ''``''), (''was'', ''VBD''), (''honored'', ''VBN''),
        ("''''", "''''"), (''to'', ''TO''), (''be'', ''VB''), (''compared'', ''VBN''), (''to'', ''TO''),
        Tree(''NE'', [(''Darth'', ''NNP''), (''Vader'', ''NNP'')]), (''while'', ''IN''), (''in'', ''IN''),
        (''office'', ''NN''), (''.'', ''.'')])

Here `Vice/NNP, President/NNP, (Dick/NNP, Cheney/NNP)` is correctly extracted. So, I think if `nltk.ne_chunk` is used first, and then if two consecutive trees are NNP, there are higher chances that both refer to one entity.

I have been playing with NLTK toolkit, and I came across this problem a lot, but couldn''t find a satisfying answer. Any suggestion will be really appreciated. I''m looking for flaws in my approach.', 84, '2014-06-24 20:44:25.993', '938889b5-5cd5-429a-8aec-8d08adc33696', 566, 'Improving formatting.', 1398, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Is it a bird? Is it a cat? We have black-and-white cat-sized! magpies here. so that would fail.

First thing would be to exclude all areas that are green, cats are seldom green.

Then compare the rest to a reference image to remove static things like stones and stairs.

Detecting objects of a minimum size should be possible, but for a classification the resolution is too low. Could be also your neighbor testing his new remote controlled drone.

With two cameras you could do a 3d mapping of the objects and eliminate flying objects.', 1164, '2014-06-24 20:48:37.837', 'aca3e32f-2026-486f-b283-58b6d1d329b0', 569, 'added 11 characters in body', 1399, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve written a simple recommender which generates recommendations for users based on what they have clicked. The recommender generates a data file with the following format:

    userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)
    a,b,.2
    a,c,.3
    a,d,.4
    a,e,.1
    e,b,.3
    e,c,.5
    e,d,.8

I''ve looked at [some graphs](https://github.com/mbostock/d3/wiki/Gallery), but I''m not sure which one to use, or if are there other ones that will better display the user similarities from the dataset above. Any suggestions?

I''m aiming this visualization at business users who are not at all technical. I would just like to show them an easy to understand visual that details how similar some users are and so convince the business these users that the recommendation system is useful.', 237, '2014-06-24 21:25:13.257', '7a5b0990-980a-45db-a764-55a0238e1a89', 564, 'added 17 characters in body', 1400, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As I increase the number of trees in scikit learn''s GradientBoostingRegressor I get more negative predictions even though there are no negative values in my training or testing set. I have about 10 features most of them are binary. So some of the parameters that I was tuning were the number of trees/iterations, learning depth, and learning rate. The percentage of negative values seemed to max at ~ 2%. The learning depth of 1(stumps) seemed to have the largest % of negative values. This percentage also seemed to increase with more trees and a smaller learning rate. The dataset is from one of the kaggle playground competitions.', 1162, '2014-06-25 00:09:07.697', '2a32c7bd-a175-4251-b7e7-0c2a5a0cc89d', 565, 'added 451 characters in body', 1401, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Personally, I think Netflix got it right.  Break it down into a confidence rating from 1-5 and show your recommendations based on the number of yellow stars.

It doesn''t have to be stars, but those icon based graphs are very easy to interpret and get the point across clearly.

', 434, '2014-06-25 00:13:50.803', 'dbfde5f2-5dbe-4b39-b7c4-73d02c7df1ac', 570, 1402, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to rank some percentages. I have numerators and denominators for each ratio.
To give a concrete example:
ratio is total graduates/total students in the school.
But the issue is total number of students vary over a long range. (1000-20000)
Smaller schools seem to have higher percentage of students graduating but I want to standardize it and not let the size of the school affect the ranking.
Is there a way to do it?', 1174, '2014-06-25 02:33:22.673', 'be8d13b8-b338-4ca0-aeb2-08f10902eaa9', 571, 1403, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Standardize numbers for ranking ratios', 1174, '2014-06-25 02:33:22.673', 'be8d13b8-b338-4ca0-aeb2-08f10902eaa9', 571, 1404, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics>', 1174, '2014-06-25 02:33:22.673', 'be8d13b8-b338-4ca0-aeb2-08f10902eaa9', 571, 1405, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As I increase the number of trees in [scikit learn](http://scikit-learn.org/stable/)''s [`GradientBoostingRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), I get more negative predictions, even though there are no negative values in my training or testing set. I have about 10 features most of them are binary.

So, some of the parameters that I was tuning were:

- the number of trees/iterations;
- learning depth;
- and learning rate.

The percentage of negative values seemed to max at ~2%. The learning depth of 1(stumps) seemed to have the largest % of negative values. This percentage also seemed to increase with more trees and a smaller learning rate. The dataset is from one of the kaggle playground competitions.', 84, '2014-06-25 02:51:25.207', '94b160f8-2406-4fc8-823c-4c7e79a8dcdf', 565, 'Improving formatting.', 1406, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Gradient Boosting Regression: negative predictions despite no negative y values in training set', 84, '2014-06-25 02:51:25.207', '94b160f8-2406-4fc8-823c-4c7e79a8dcdf', 565, 'Improving formatting.', 1407, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><python><algorithms><scikit>', 84, '2014-06-25 02:51:25.207', '94b160f8-2406-4fc8-823c-4c7e79a8dcdf', 565, 'Improving formatting.', 1408, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is relatively simple to do mathematically.  First, fit a regression line to the scatter plot of "total graduates" (y) vs. "total students" (x).  You will probably see a downward sloping line if your assertion is correct (smaller schools graduate a higher %).

You can identify the slope and y-intercept for this line to convert it into an equation y = mx + b, and then do a little algebra to convert the equation  into normalized form: "y / x = m + b / x"

Then, with all the ratios in your data , you should *subtract* this RHS:

normalized ratio = (total grads / total students) - (m + b / total students)

If the result is postive, then the ratio is above normal for that size (i.e. above the regression line) and if it is negative it is below the regression line.  If you want all positive numbers, you can add a positive constant to move all results above zero.

----
This is how to do it mathematically, but I suggest that you consider whether it is wise, from a data analysis point of view, to normalize by school size.  This depends on the purpose of your analysis and specifically how this ratio is being analyzed in relation to other data.', 609, '2014-06-25 03:11:02.457', 'c68d64c4-0214-4209-957e-e812b2e0e73f', 572, 1409, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('OpenCV''s [background subtraction][1] will find objects moving about your harden. After that you could use a classifier or shape analysis to differentiate between cats, people, trees and etc.


  [1]: http://docs.opencv.org/trunk/doc/tutorials/video/background_subtraction/background_subtraction.html "background subtraction"', 1176, '2014-06-25 03:30:19.787', '553f3bbc-09b6-4740-aca0-5e9c1d330eaa', 573, 1411, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m trying to rank some percentages. I have numerators and denominators for each ratio. To give a concrete example, consider ratio as `total graduates / total students` in a school.

But the issue is that `total students` vary over a long range (1000-20000). Smaller schools seem to have higher percentage of students graduating, but I want to standardize it, and not let the size of the school affect the ranking. Is there a way to do it?', 84, '2014-06-25 03:59:46.827', '66676955-1260-4c3c-be0e-f28bd51d6c83', 571, 'Improving formatting.', 1412, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I suggest the use of Hidden Markov Models, with two possible states: (1) high levels and (0) low levels.

This technique might be helpful to decode your signal. Probably you would need a specific HMM for each codification.

If noise is an issue an FIR filter with a Blackman-Harris window function would allow you to isolate the frequency you''re concerned with. ', 890, '2014-06-25 06:58:43.670', '853361ff-a7e0-4374-b9cd-7d0a735c1494', 568, 'Wouldn''t want to high pass, high frequency noise could also be a concern, and a Butterworth is non-ideal. ', 1413, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-25 06:58:43.670', '853361ff-a7e0-4374-b9cd-7d0a735c1494', 568, 'Proposed by 890 approved by 1155 edit id of 91', 1414, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is a very interesting problem.

I faced a similar one by analyzing the pictures users upload to the social network. I did the following approach:

 - Rather than associating data to ages (15 y.o., 27 y.o., ...) what I did is to establish different groups of ages: Less than 18, from 18 to 30 and greater than 30 (this is due to the specific problem we were facing, but you can choose whatever intervals you want). This division helps a lot to solve the problem.
 - Afterwards, I created a hierarchical clustering (divisive or aggregative). Then I choose those branches where I had users with known ages (or group ages) and then for that branch I extended the same age to that group.

This approach is **semi-supervised learning** and I recommended it in case you only have some data labeled.

Please, notice that on a social network, people usually lie about the age (just for fun, or sometimes because they want to camuflate themselves on the social net).', 1155, '2014-06-25 07:22:33.170', '56c14e02-9a2d-4493-a7ec-6fc29ea5da25', 574, 1415, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve written a simple recommender which generates recommendations for users based on what they have clicked. The recommender generates a data file with the following format:

    userid,userid,simmilarity (between 0 and 1 - closer to 0 the more similar the users)
    a,b,.2
    a,c,.3
    a,d,.4
    a,e,.1
    e,b,.3
    e,c,.5
    e,d,.8

I''ve looked at [some graphs](https://github.com/mbostock/d3/wiki/Gallery), but I''m not sure which one to use, or if are there other ones that will better display the user similarities from the dataset above. Any suggestions?

I''m aiming this visualization at business users who are not at all technical. I would just like to show them an easy to understand visual that details how similar some users are and so convince the business that for these users the recommendation system is useful.

@Steve Kallestad do you mean something like this :

![enter image description here][1]


  [1]: http://i.stack.imgur.com/4zyQR.png', 237, '2014-06-25 07:56:29.733', 'f6db9830-e60d-4462-96a6-3f0fd83e46ed', 564, 'added 4 characters in body', 1416, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is a very good question and a common situation.

In my opinion there are three different factors that must be controlled:

 - Data: There exist already different benchmarks in order to evaluate algorithms and architectures. These data must be publicly available so that everybody can contrast their approaches.
 - Architecture: My suggestion is to test everything on the cloud, so that everyone can contrast their results and also there is no doubt the same machines and software is used.
 - Algorithms: If you have developed a distributed algorithm, it would be great to compare your algorithm on a specific data. In this case, algorithms must not be public.

So, answering your question, if you want to compare different experiments and state to what extent your distributed algorithm outperforms others, you should try to replicate as accurate as possible the same environment (data and architecture) where the experiments were carried out.

If this is not possible, my suggestion is that you test your algorithm with public data and cloud architecture so that **you become a referent** as you are facilitating the comparison of future algorithms.', 1155, '2014-06-25 09:19:25.560', '2b4f6344-353a-4266-a88d-5eab7749857a', 575, 1417, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You should try _arules_ package in R. It allows you to create not only the association rules but also to specify the length of each rule, the importance of each rule and also you can filter them, which is what you are looking for (try the rhs() command of this package).

', 1155, '2014-06-25 10:15:31.477', 'd22374a9-f0bf-4c32-9a2d-ee1c7cf2217a', 576, 1418, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Though it''s easy to say, it''s better to treat the environment that changes as variables, describe/estimate your algorithm''s performance base on these variables. And hopefully others will do the same. Of interest, [Experiments as Research Validation -- Have We Gone too Far?][1].


  [1]: http://infolab.stanford.edu/~ullman/pub/experiments.pdf', 743, '2014-06-25 10:59:56.737', '674adb8e-db34-4f30-8653-843a3fcbe0d5', 577, 1419, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have read some about it and I had the following blog in mind:

http://fellgernon.tumblr.com/post/46117939292/predicting-who-will-win-a-nfl-match-at-half-time#.UtehM7TWtQg

This blog deals with the prediction of a NFL match after the half time is already over. The prediction is 80% accurate with simple GLM model.

I do not know if that is suitable for soccer.', 1155, '2014-06-25 14:30:10.597', 'cc1a9275-37e2-4e5c-be4a-27df51c4ac3d', 580, 1426, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In my opinion, there are solutions to deal with categorical data in clustering. R comes with a specific distance for categorical data. This distance is called Gower (http://www.inside-r.org/packages/cran/StatMatch/docs/gower.dist) and it works pretty well.

', 1155, '2014-06-25 14:36:29.163', '1307f496-c73f-4a19-a19a-e0745ccbcb1d', 581, 1427, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Some research from [D. Nguyen et al.][1] try to predict twitter user''s age based on their tweets. Maybe you find them useful. They use logistic and linear regression.


  [1]: http://www.dongnguyen.nl/publications.html', 743, '2014-06-25 14:43:53.680', '861798d6-d2ea-4baf-9ebd-3d42a87683da', 582, 1428, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The original MacQueen k-means publication (the first to use the name "kmeans") is an online algorithm.

> MacQueen, J. B. (1967). "Some Methods for classification and Analysis of Multivariate Observations". Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability 1. University of California Press. pp. 281297

After assigning each point, the mean is incrementally updated.

As far as I can tell, it was also meant to be a single pass over the data only, although it can be trivially repeated multiple times to reassign points until convergence.

MacQueen usually takes fewer iterations than Lloyds to converge if your data is shuffled. On ordered data, it can have problems. On the downside, it requires more computation for each object, so each iteration takes slightly longer.

When you implement a parallel version of k-means, make sure to study the update formulas in MacQueens publication. They''re useful.', 924, '2014-06-25 15:42:19.287', '4b0106ec-e1c5-4a30-96a1-99ea52f35c5c', 583, 1429, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The original MacQueen k-means publication (the first to use the name "kmeans") is an online algorithm.

> MacQueen, J. B. (1967). "Some Methods for classification and Analysis of Multivariate Observations". Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability 1. University of California Press. pp. 281297

After assigning each point, the mean is incrementally updated using a simple weighted-average formula (old mean is weighted with n, the new observation is weighted with 1, if the mean had n observations before).

As far as I can tell, it was also meant to be a single pass over the data only, although it can be trivially repeated multiple times to reassign points until convergence.

MacQueen usually takes fewer iterations than Lloyds to converge if your data is shuffled (because it updates the mean faster!). On ordered data, it can have problems. On the downside, it requires more computation for each object, so each iteration takes slightly longer (additional math operations, obviously).', 924, '2014-06-25 15:46:21.267', '08da9b63-4cba-4ec1-97d2-ceb78f5384b9', 584, 1430, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('DBSCAN (see also: Generalized DBSCAN) does not require a distance.
All it needs is a *binary decision*. Commonly, one would use "distance < epsilon" but nothing says you cannot use "similarity > epsilon" instead. Triangle inequality etc. are not required.

Affinity propagation, as the name says, uses similarities.

Hierarchical clustering, except for maybe Ward linkage, does not make any assumption. In many implementations you can just use negative distances when you have similarities, and it will work just fine. Because all that is needed is min, max, and <.

Kernel k-means could work IF your similarity is a good kernel function. Think of it as computing k-means in a different vector space, where Euclidean distance corresponds to your similarity function. But then you need to know k.

PAM (K-medoids) should work. Assign each object to the most similary medoid, then choose the object with the highest average similarity as new medoid... no triangle inequality needed.

... and probably many many more. There are literally hundreds of clustering algorithms. **Most should work** IMHO. Very few seem to actually require metric properties. K-means has probably the strongest requirements: it minimizes *variance* (not distance, or similarity), and you must be able to compute means.', 924, '2014-06-25 15:53:30.723', '3fcab999-77da-45a5-9c8e-d9aa20b6a78a', 585, 1431, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As I increase the number of trees in [scikit learn](http://scikit-learn.org/stable/)''s [`GradientBoostingRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), I get more negative predictions, even though there are no negative values in my training or testing set. I have about 10 features most of them are binary.

So, some of the parameters that I was tuning were:

- the number of trees/iterations;
- learning depth;
- and learning rate.

The percentage of negative values seemed to max at ~2%. The learning depth of 1(stumps) seemed to have the largest % of negative values. This percentage also seemed to increase with more trees and a smaller learning rate. The dataset is from one of the kaggle playground competitions.

code is something like:

from sklearn.ensemble import GradientBoostingRegressor

X_train, X_test, y_train, y_test = train_test_split(X, y)

reg = GradientBoostingRegressor(n_estimators=8000, max_depth=1, loss = ''ls'', learning_rate = .01)

reg.fit(X_train, y_train)

ypred = reg.predict(X_test)
', 1162, '2014-06-25 15:56:42.597', 'd95d49fc-473c-4629-822a-2109cf89fb34', 565, 'added 299 characters in body', 1432, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Definitely they can.
I can target you to a **[nice paper][1]**. Once I used it for soccer league results prediction algorithm implementation, primarily aiming at having some value against bookmakers.

From paper''s abstract:
> a Bayesian dynamic generalized model to estimate the time dependent skills of all teams in a league, and to predict next weekend''s soccer matches.

Keywords:

> Dynamic Models, Generalized Linear Models, Graphical Models, Markov
> Chain Monte Carlo Methods, Prediction of Soccer Matches

Citation:

>Rue, Havard, and Oyvind Salvesen. "Prediction and retrospective analysis of soccer matches in a league." Journal of the Royal Statistical Society: Series D (The Statistician) 49.3 (2000): 399-418.

  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.7448&rep=rep1&type=pdf', 322, '2014-06-25 16:03:21.447', 'dab43e19-5685-4c06-80dc-1a29c84ff3c7', 269, 'added explicit citation in case the link rots', 1433, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-25 16:03:21.447', 'dab43e19-5685-4c06-80dc-1a29c84ff3c7', 269, 'Proposed by 322 approved by 97 edit id of 92', 1434, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As I increase the number of trees in [scikit learn](http://scikit-learn.org/stable/)''s [`GradientBoostingRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html), I get more negative predictions, even though there are no negative values in my training or testing set. I have about 10 features, most of which are binary.

Some of the parameters that I was tuning were:

- the number of trees/iterations;
- learning depth;
- and learning rate.

The percentage of negative values seemed to max at ~2%. The learning depth of 1(stumps) seemed to have the largest % of negative values. This percentage also seemed to increase with more trees and a smaller learning rate. The dataset is from one of the kaggle playground competitions.

My code is something like:

    from sklearn.ensemble import GradientBoostingRegressor

    X_train, X_test, y_train, y_test = train_test_split(X, y)

    reg = GradientBoostingRegressor(n_estimators=8000, max_depth=1, loss = ''ls'', learning_rate = .01)

    reg.fit(X_train, y_train)

    ypred = reg.predict(X_test)

', 322, '2014-06-25 16:46:30.323', 'a8508cbf-86e3-4ffc-b717-834f654c625e', 565, 'title, code format, add kaggle tag', 1435, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Why does Gradient Boosting regression predict negative values when there are no negative y-values in my training set?', 322, '2014-06-25 16:46:30.323', 'a8508cbf-86e3-4ffc-b717-834f654c625e', 565, 'title, code format, add kaggle tag', 1436, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><python><algorithms><scikit><kaggle>', 322, '2014-06-25 16:46:30.323', 'a8508cbf-86e3-4ffc-b717-834f654c625e', 565, 'title, code format, add kaggle tag', 1437, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-25 16:46:30.323', 'a8508cbf-86e3-4ffc-b717-834f654c625e', 565, 'Proposed by 322 approved by 1162 edit id of 93', 1438, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m new to the world of text mining and have been reading up on annotators at places like the <a href="http://uima.apache.org/">UIMA website</a>. I''m encountering many new terms like named entity recognition, tokenizer, lemmatizer, gazetteer, etc. Coming from a layman background, this is all very confusing so can anyone tell me or link to resources that can explain what the main categories of annotators are and what they do?', 1192, '2014-06-25 17:37:23.380', '113771fc-da23-4ef4-b8cc-fa03a308714b', 586, 1439, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the main types of NLP annotators?', 1192, '2014-06-25 17:37:23.380', '113771fc-da23-4ef4-b8cc-fa03a308714b', 586, 1440, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><text-mining>', 1192, '2014-06-25 17:37:23.380', '113771fc-da23-4ef4-b8cc-fa03a308714b', 586, 1441, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are plenty of sources which provide the historical stock data but they only provide the OHLC fields along with volume and adjusted close. Also a couple of sources I found provide market cap data sets but they''re restricted to US stocks. Yahoo Finance provides this data online but there''s no option to download it ( or none I am aware of ).



 - Where can I download this data for stocks belonging to various top stock exchanges across countries by using their ticker name ?
 - Is there some way to download it via Yahoo Finance or Google Finance ?

I need data for the last decade or so and hence need some script or API which would do this.
', 793, '2014-06-25 18:06:14.293', 'b4551880-520d-432d-b9d6-0ead54005d46', 587, 1442, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where can i download historical market capitalization and daily turnover data for stocks?', 793, '2014-06-25 18:06:14.293', 'b4551880-520d-432d-b9d6-0ead54005d46', 587, 1443, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining>', 793, '2014-06-25 18:06:14.293', 'b4551880-520d-432d-b9d6-0ead54005d46', 587, 1444, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Quant SE is better place for questions related to getting financial data:

 - [What data sources are available online][1]
 - http://quant.stackexchange.com/search?q=market+capitalization+data

  [1]: http://quant.stackexchange.com/questions/141/what-data-sources-are-available-online', 97, '2014-06-25 18:12:04.010', '160314c6-628a-4f16-a029-535a491eaa41', 588, 1445, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Where can I download historical market capitalization and daily turnover data for stocks?', 97, '2014-06-25 19:04:31.130', '3abf116f-8697-4822-8f08-3ffc4e725b6e', 587, 'Adding more relevant tag. This question is probably off-topic.', 1446, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-sources>', 97, '2014-06-25 19:04:31.130', '3abf116f-8697-4822-8f08-3ffc4e725b6e', 587, 'Adding more relevant tag. This question is probably off-topic.', 1447, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-25 19:04:31.130', '3abf116f-8697-4822-8f08-3ffc4e725b6e', 587, 'Proposed by 97 approved by 793 edit id of 94', 1448, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":434,"DisplayName":"Steve Kallestad"},{"Id":84,"DisplayName":"Rubens"},{"Id":84,"DisplayName":"Rubens"}]}', 84, '2014-06-25 20:26:56.000', '1ac4ac00-851a-4e16-92d1-d7d33c0caa8d', 538, '105', 1450, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":62,"DisplayName":"AsheeshR"}]}', 62, '2014-06-26 01:41:16.660', '93834e72-ebd1-424e-bfac-bab55448abba', 518, '102', 1452, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":84,"DisplayName":"Rubens"},{"Id":108,"DisplayName":"rapaio"},{"Id":62,"DisplayName":"AsheeshR"}]}', 84, '2014-06-26 01:43:32.487', '3cf478d1-34f0-4fcb-ba9f-27031905e25b', 324, '103', 1453, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<r><logistic-regression>', 906, '2014-06-26 01:50:53.147', 'b533514d-b9ea-4ab3-b3cb-fb0f715b82f7', 468, 'specific question about r -> r tag', 1458, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-26 01:50:53.147', 'b533514d-b9ea-4ab3-b3cb-fb0f715b82f7', 468, 'Proposed by 906 approved by 84, 62 edit id of 87', 1459, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('My data set is formatted like this:

    User-id | Threat_score
    aaa       45
    bbb       32
    ccc       20

The list contains the top 100 users with the highest threat scores. I generate such a list monthly and store each month''s list in its own file.

There are three things I would like to get from this data:
<ul>1. Users who are consistently showing up in this list</ul>
<ul>2. Users who are consistently showing up in this list with high threat scores</ul>
<ul>3. Users whose threat scores are increasing very quickly</ul>

I am thinking a visual summary would be nice; each month (somehow) decide which users I want to plot on a graph of historic threat scores.

Are there any known visualization techniques that deal with similar requirements?

How should I be transforming my current data to achieve what I am looking for?', 322, '2014-06-26 01:51:28.873', '3a21de8a-71b1-46eb-975c-164351f759cd', 494, 'clarify the type of dataset and add appropriate tag; grammar/language', 1462, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Techniques for trend extraction from unbalanced panel data', 322, '2014-06-26 01:51:28.873', '3a21de8a-71b1-46eb-975c-164351f759cd', 494, 'clarify the type of dataset and add appropriate tag; grammar/language', 1463, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<visualization><dataset><graphs>', 322, '2014-06-26 01:51:28.873', '3a21de8a-71b1-46eb-975c-164351f759cd', 494, 'clarify the type of dataset and add appropriate tag; grammar/language', 1464, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-26 01:51:28.873', '3a21de8a-71b1-46eb-975c-164351f759cd', 494, 'Proposed by 322 approved by 84, 62 edit id of 88', 1465, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":62,"DisplayName":"AsheeshR"}]}', 62, '2014-06-26 01:58:15.697', '028ad651-6d67-41d6-86f7-86ba51a6e24e', 411, '104', 1466, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":62,"DisplayName":"AsheeshR"}]}', 62, '2014-06-26 02:01:00.157', '1167023b-b46d-4ea1-be02-4bb0a8f88654', 313, '104', 1467, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"OriginalQuestionIds":[461],"Voters":[{"Id":84,"DisplayName":"Rubens"},{"Id":108,"DisplayName":"rapaio"},{"Id":434,"DisplayName":"Steve Kallestad"},{"Id":84,"DisplayName":"Rubens"}]}', 84, '2014-06-26 05:31:41.193', '4f867f7e-af98-4d29-be50-f0a8ec1ef20d', 477, '101', 1489, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you are totally unfamiliar with ordinal regression, I would try to read the Tabachnick / Fidell (http://www.pearsonhighered.com/educator/product/Using-Multivariate-Statistics-6E/0205849571.page)  chapter on the topic first - while not written for R, the book is very good at conveying the general logic and the "do''s" and "do nots".

As a question: What are your response catgeories exactly? If they are some sort of scale, like "good - bad" it would be ok to use a linear regression (market research does it all the time...), but if the items are more disjunct, an ordinal regression might be better.
I dimly remember that some books about structural equatiotion modelling mentioned that linear regression was superior for good scales than probit - bit I cannot recall the book at the moment, sorry!

The most serious problem might be the number of dummy variables - a couple of hundred dummy variables will make the analysis slow, hard to interpret and probably unstable - are there enough cases for each dummy / dummy-combination?', 791, '2014-06-26 06:51:12.243', '9f60fd7d-a1ec-41ce-9977-50ea05fb44c3', 591, 1491, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I usually take a two-step approach

1. compute univariate (variable by variable) summary statistics such as mean, range, variance, number of missing, cardinality, etc. for each variable and look for oddities (e.g. range not plausible given the meaning of the variable). Plot histograms for those odd variables.

2. split the data into manageable subsets (choose a meaningful variable and split the data according to it e.g. all positive examples, and all negative) and explore them visually (e.g. with [ggobi][1] ). Especially use tools like brushing and scatter plots to understand how variables are linked together.

And when you start building models, make sure to plot the residuals, looking for extreme errors that might be due to an outlier, or look at the confusion matrix and make sure it is balanced. Use k-fold cross validation to optimize your models and look at the variance of the training error for each fold, if one fold performs much worse than the others, it may contain outliers.

  [1]: http://www.ggobi.org/', 172, '2014-06-26 07:41:52.403', 'adff2f85-48f3-4234-b813-e641bf35804a', 592, 1492, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As many have answered, it is always best to avoid anything done manually as it is less reproducible/documentable. Your point about the overhead of writing a script vs. opening and fixing the file is valid, though.

Best practice is often to

- keep an untouched version of the data
- build a working copy of the data with errors fixed
- have a way to recreated a working copy from the original data

The last point can be done with a script. Then make sure to be as specific as needed to modify only the data you want to modify, and to write the script in such a way that adding a fix by modifying the script is as easy as modifying the data directly.

If your data lie in files, you can also use diffs/patches to store the original data along with the patches needed to produce the working data. To generate them, duplicate your working copy, perform the change, extract the diff/patch, save it, and delete the previous working copy.', 172, '2014-06-26 08:01:40.763', '667f5feb-d0c4-4dd5-9cda-ec06ea4cb5f6', 593, 1493, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When doing a Google image search, across the top it has figured out categories for the images of the topic being search for. I''m interested to know how this works and how it chooses and creates categories, unfortunately I can''t find much about it at all. Is anyone able to shed some light on algorithms they may be using to do this and what basis these categories are created from?

For example, if I search for "animals" I get the categories "cute", "baby", "wild", "farm", "zoo", "clipart". If I go into "wild" I then have subcategories "forest", "baby", "africa", "clipart", "rainforest", "domestic".', 1206, '2014-06-26 12:11:51.253', '7d12badc-79fc-46fb-9f2b-b6b46d4dfc0b', 594, 1507, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How does Google categorize results from it''s image search?', 1206, '2014-06-26 12:11:51.253', '7d12badc-79fc-46fb-9f2b-b6b46d4dfc0b', 594, 1508, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><google><search>', 1206, '2014-06-26 12:11:51.253', '7d12badc-79fc-46fb-9f2b-b6b46d4dfc0b', 594, 1509, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<br>
I am newbie with machine learning but I had an interesting problem.
I have a large sample of people and visited sites. Some people have indicated gender, age and other parameters. I want to restore these parameters to all people. Which way I look?<br>
I am familiar with neural networks, but it seems they don''t fit.', 1207, '2014-06-26 12:25:55.663', '0f5f8ff0-7a57-45e4-a833-eafed0160da3', 595, 1510, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How use neural networks with variable count of inputs... and really many inputs?', 1207, '2014-06-26 12:25:55.663', '0f5f8ff0-7a57-45e4-a833-eafed0160da3', 595, 1511, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><algorithms><neuralnetwork>', 1207, '2014-06-26 12:25:55.663', '0f5f8ff0-7a57-45e4-a833-eafed0160da3', 595, 1512, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As many have answered, it is always best to avoid anything done manually as it is less reproducible/documentable. Your point about the overhead of writing a script vs. opening and fixing the file is valid, though.

Best practice is often to

- keep an untouched version of the data
- build a working copy of the data with errors fixed
- have a way to re-create a working copy from the original data

The last point can be done with a script. Then make sure to be as specific as needed to modify only the data you want to modify, and to write the script in such a way that adding a fix by modifying the script is as easy as modifying the data directly.

If your data lie in files, you can also use diffs/patches to store the original data along with the patches needed to produce the working data. To generate them, duplicate your working copy, perform the change, extract the diff/patch, save it, and delete the previous working copy.', 172, '2014-06-26 12:39:40.443', 'e3e7f831-ea04-4c48-96aa-5a8b2103b729', 593, 'edited body', 1513, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<br>
I am newbie with machine learning but I had an interesting problem.
I have a large sample of people and visited sites. Some people have indicated gender, age and other parameters. I want to restore these parameters to all people. Which way I look?<br>
I am familiar with Neural Networks (supervised learning), but it seems they don''t fit.', 1155, '2014-06-26 13:15:16.667', '348222f1-6d83-423f-94a3-29cdf0429695', 595, 'Name of the methods must be in capital letters', 1514, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-26 13:15:16.667', '348222f1-6d83-423f-94a3-29cdf0429695', 595, 'Proposed by 1155 approved by 1207 edit id of 96', 1515, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<br>
I am newbie with machine learning but I had an interesting problem.
I have a large sample of people and visited sites. Some people have indicated gender, age and other parameters. I want to restore these parameters to all people. Which way I look? Which algorithm is suitable?<br>
I am familiar with Neural Networks (supervised learning), but it seems they don''t fit.', 1207, '2014-06-26 13:15:57.177', '6d52f718-1fc9-4ce3-950d-a7ab516fe198', 595, 'added 29 characters in body', 1516, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are several classic datasets for machine learning classification/regression tasks. The most popular are:

* [Iris Flower Data Set][1];
* [Titanic Data Set][2];
* [Motor Trend Cars][3];
* etc.

But does anyone know similar datasets for networks analysis / graph theory? More concrete - I''m looking for **Gold standard** datasets for comparing/evaluating/learning:

1. centrality measures;
2. network clustering algorithms.

I don''t need a huge list of publicly available networks/graphs, but a couple of actually must-know datasets.


  [1]: http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html
  [2]: http://www.kaggle.com/c/titanic-gettingStarted
  [3]: http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html', 941, '2014-06-26 13:32:18.050', 'ec561019-1864-480f-aa36-aa7c8693ed27', 596, 1517, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Network analysis classic datasets', 941, '2014-06-26 13:32:18.050', 'ec561019-1864-480f-aa36-aa7c8693ed27', 596, 1518, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><graphs>', 941, '2014-06-26 13:32:18.050', 'ec561019-1864-480f-aa36-aa7c8693ed27', 596, 1519, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am not working at Google, but I think it is some sort of recommendation system based on the words which millions of users searched before. So those people who search for "animals" often search for "wild animals" for example. Like in many online stores they recommend you to buy something in addition to the product you are looking for based on the previous purchases of other users.

There are many approaches how to build such recommendation system using machine learning, no one knows for sure what google uses.', 478, '2014-06-26 13:45:16.573', 'e203da03-fd7a-4f79-bd2d-0191a229679b', 597, 1520, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There exist many possibilities for populating empty gaps on data.

 - **Most repeated value**: Fill the gaps with the most common value.
 - **Create a distribution**: Make the histogram and drop values according to that distribution.
 - **Create a new label**: Since you do not have information, do not assume any value and create another label/category to indicate that value is empty.
 - **Create a classifier**: Make a relation among the variable with empty gaps and the rest of the data and create a simple classifier. With this, populate the rest of the data.

There exist many others, but these are the most common strategies. My suggestion is not to populate and to keep unknown what is unknown.', 1155, '2014-06-26 14:17:44.077', '9626dce1-0b12-45cd-b8f9-245f4e6e547f', 598, 1521, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The only thing I know about is benchmark data for Graph Databases, such as Neo4j.

You may find links similar to this one:
http://istc-bigdata.org/index.php/benchmarking-graph-databases/

where you can find data to test network analysis and graph theory.

Furthermore, you could play with the API of Twitter/Facebook to collect your own data. This is also a suggestion in case you do not find the data you are looking for.', 1155, '2014-06-26 14:21:10.513', '62a399b1-f4ce-4fd1-8527-df5eaefc7a67', 599, 1522, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am working on a political campaign where dozens of volunteers will be conducting door-knocking promotions over the next few weeks. Given a list with names, addresses and long/lat coordinates, what algorithms can be used to create an optimized walk list.', 1208, '2014-06-26 14:39:06.410', 'eb3901eb-55e4-4d4a-8039-7b020bd55bde', 600, 1523, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How do you create an optimized walk list given longitude and latitude coordinates?', 1208, '2014-06-26 14:39:06.410', 'eb3901eb-55e4-4d4a-8039-7b020bd55bde', 600, 1524, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<sql>', 1208, '2014-06-26 14:39:06.410', 'eb3901eb-55e4-4d4a-8039-7b020bd55bde', 600, 1525, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<dataset><data-sources>', 97, '2014-06-26 16:12:02.640', '35fe2905-e9e6-4bf5-841c-8d40a9fea929', 587, 'Adding more relevant tags.', 1529, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-26 16:12:02.640', '35fe2905-e9e6-4bf5-841c-8d40a9fea929', 587, 'Proposed by 97 approved by -1 edit id of 95', 1530, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<dataset>', 84, '2014-06-26 16:12:02.640', '0577251f-8109-4a6c-82ad-1dc10204477b', 587, 'Adding more relevant tags.', 1531, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I thought of expanding a bit on the answer by Stanpol. While recommendation system is one approach of suggesting related queries, one more standard information retrieval based approach is the query expansion technique.

Generally speaking, query expansion involves selecting additional terms from the top ranked documents retrieved in response to an initial query. Terms are typically selected by a combination of a term scoring function such as tf-idf and a co-occurrence based measure.

For example, in response to a query term "animal", a term selection function may choose the term "zoo", because

 - "zoo" may be a dominating term (high tf-idf) in the top (say 10) documents
 - "zoo" may occur frequently with the original query term "animal".', 984, '2014-06-26 16:19:46.480', 'eee0e8c9-5c8b-4024-bfb7-980f3ef1f497', 602, 1532, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('While doing a Google image search, the page displays some figured out categories for the images of the topic being searched for. I''m interested in learning how this works, and how it chooses and creates categories.

Unfortunately, I couldn''t find much about it at all. Is anyone able to shed some light on algorithms they may be using to do this, and what basis these categories are created from?

For example, if I search for "animals" I get the categories:

> "cute", "baby", "wild", "farm", "zoo", "clipart".

If I go into "wild", I then have subcategories:

> "forest", "baby", "africa", "clipart", "rainforest", "domestic".', 84, '2014-06-26 16:19:49.043', '87f6cb96-ca06-45bc-bd3b-660893ee316b', 594, 'Fixed grammar, and improving formatting.', 1533, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How does Google categorize results from its image search?', 84, '2014-06-26 16:19:49.043', '87f6cb96-ca06-45bc-bd3b-660893ee316b', 594, 'Fixed grammar, and improving formatting.', 1534, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m new to machine learning, but I have an interesting problem. I have a large sample of people and visited sites. Some people have indicated gender, age, and other parameters. Now I want to restore these parameters to each user.

Which way do I look for? Which algorithm is suitable to solve this problem? I''m familiar with Neural Networks (supervised learning), but it seems they don''t fit.', 84, '2014-06-26 16:25:31.680', '46d5c60e-c102-4d0e-a1cb-d8dca4fea812', 595, 'Fixed grammar, and improving formatting.', 1535, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to use neural networks with large and variable number of inputs?', 84, '2014-06-26 16:25:31.680', '46d5c60e-c102-4d0e-a1cb-d8dca4fea812', 595, 'Fixed grammar, and improving formatting.', 1536, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('People generally here something closely related to the [Travelling Salesman Problem][1] and think that it can''t be solved.

A good deal of work has been done on this topic and not all of it indicates that a solution is not available.  Depending on the parameters and the desired solution, you may be able to find something that will work.

You may want to give a look at the [OpenOpt][2] python library.

Another resource to look at would  be the [TSP Solver and Generator][3].

If you are using R, there is a [TSP package available][4].

Actually implementing a solution to your problem is a little too much to cover here, but this should provide a good starting point.  Within these packages and at the documentation within the links that I provided for you, you will find that there are a fairly wide variety of algorithmic strategies available.  You have a small geographic region and a small set of "salespeople", so the computational power needed to calculate a strategy within a reasonable time frame should be available on your desktop.

In practical terms, you don''t need to find the absolutely most optimal strategy.  You just need a very good one.  Pick a TSP package that looks the least overwhelming and give it a go.


  [1]: http://en.wikipedia.org/wiki/Travelling_salesman_problem
  [2]: http://openopt.org/TSP
  [3]: http://tspsg.info/
  [4]: http://tsp.r-forge.r-project.org/', 434, '2014-06-26 18:01:06.443', 'ab1c02b7-7670-4aa3-b6fe-d58b0bf34854', 603, 1542, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('People see something closely related to the [Travelling Salesman Problem][1] and think that it can''t be solved.

A good deal of work has been done on this topic and not all of it indicates that a solution is not available.  Depending on the parameters and the desired solution, you may be able to find something that will work.

You may want to give a look at the [OpenOpt][2] python library.

Another resource to look at would  be the [TSP Solver and Generator][3].

If you are using R, there is a [TSP package available][4].

Actually implementing a solution to your problem is a little too much to cover here, but this should provide a good starting point.  Within these packages and at the documentation within the links that I provided for you, you will find that there are a fairly wide variety of algorithmic strategies available.  You have a small geographic region and a small set of "salespeople", so the computational power needed to calculate a strategy within a reasonable time frame should be available on your desktop.

In practical terms, you don''t need to find the absolutely most optimal strategy.  You just need a very good one.  Pick a TSP package that looks the least overwhelming and give it a go.


  [1]: http://en.wikipedia.org/wiki/Travelling_salesman_problem
  [2]: http://openopt.org/TSP
  [3]: http://tspsg.info/
  [4]: http://tsp.r-forge.r-project.org/', 434, '2014-06-26 18:10:42.907', '6e571a15-6c64-4443-9125-0d439e2e28f5', 603, 'deleted 11 characters in body', 1543, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<algorithms>', 434, '2014-06-26 18:15:57.907', 'c4faca95-0b42-49d7-b3ca-1218549c89c3', 600, 'edited tags', 1544, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-26 18:15:57.907', 'c4faca95-0b42-49d7-b3ca-1218549c89c3', 600, 'Proposed by 434 approved by 84 edit id of 97', 1545, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Google isn''t going to give away their proprietary work, but we can speculate.

Here''s what I can gather from my limited usage:

1. The recommendations do not seem to be user, geography, or history specific.
2. There is never an empty recommendation (one that returns no results)
3. There is not always a recommendation (some searches just return images)
4. The recommendations are not always the same (consecutive searches sometimes return different recommendations)
5. Result ordering shifts regularly (search for a specific image and it won''t always be in the same place)
6. Very popular searches seem to be pre-calculated and more static than unpopular searches.
7. Recommendations are not always one additional word, and recommendations do not always include the base query.

It seems to me that they do this based on the history of the general end user population, they rotate recommendations when there are many popular ones, and they do some additional processing to determine that the result set is of a reasonable size.

I would postulate that it works as follows:

1. Use consecutive search strings from users (short-to-long-tail searches) as training data for a machine-learning algorithm.
2. Run searches that occur > N amount of times a week against that recommendation algorithm.
3. Validate and clean results.
4. Push them out to the general population in rotation/A-B testing.
5. Track click-throughs.
6. Refine results over time.', 434, '2014-06-26 19:16:31.470', 'abd43b02-d844-494f-8446-2516f7dbb600', 604, 1546, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m building a recommender system and using SVD as one of the preprocessing techniques.
However, I want to normalize all my preprocessed data between 0 and 1 because all of my similarity measures (cosine, pearson, euclidean) depend on that assumption.

After I take the SVD, is there a standard way to normalize the matrix between 0 and 1? Thanks!', 838, '2014-06-26 19:23:46.043', 'e83d2288-d1ae-4d21-bd74-2764b37ab706', 605, 1547, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to normalize results of Singular Value Decomposition (SVD) between 0 and 1?', 838, '2014-06-26 19:23:46.043', 'e83d2288-d1ae-4d21-bd74-2764b37ab706', 605, 1548, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><statistics><recommendation><feature-selection>', 838, '2014-06-26 19:23:46.043', 'e83d2288-d1ae-4d21-bd74-2764b37ab706', 605, 1549, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Folks here stated great steps, but I think there are great information at the following link [what I do when I get a new data set as told through tweets][1],
It sums up the steps the folks tweeted answering the great @hmason question "Data people: What is the very first thing you do when you get your hands on a new data set?"

Hope it will be useful.

  [1]: http://simplystatistics.org/2014/06/13/what-i-do-when-i-get-a-new-data-set-as-told-through-tweets/', 1220, '2014-06-26 20:49:38.470', '88912743-d7e7-40b2-b37e-54f61db6f1b5', 606, 1555, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What I''d like to start with this question is a compilation of the differences between these two "box/kits"... I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These two are full on VMs with alot of relevant software on them, but I haven''t been able to find any side-by-side comparison.

Here''s a start from my research, but if someone could tell me that one is objectively more rich-featured and useful to get started then that would help greatly:

Kit -> vm is on vagrant cloud (4 GB) and seems to be more "hip" with R, iPython notebook, and other useful command-line tools (html->txt, json->xml, etc). There is a book being released in August with detail.

Tool -> vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.


', 1223, '2014-06-26 22:17:52.570', '14df6955-68c1-4b02-8faa-7e8a922b7c9c', 607, 1556, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('www.datasciencetoolBOX.org vs www.datasciencetoolKIT.org', 1223, '2014-06-26 22:17:52.570', '14df6955-68c1-4b02-8faa-7e8a922b7c9c', 607, 1557, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<tools>', 1223, '2014-06-26 22:17:52.570', '14df6955-68c1-4b02-8faa-7e8a922b7c9c', 607, 1558, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What I''d like to start with this question is a compilation of the differences between these two "box/kits"... I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These VMs have alot of software on them, but I haven''t been able to find any side-by-side comparison.

Here''s a start from my research, but if someone could tell me that one is objectively more rich-featured and useful to get started then that would help greatly:

Kit -> vm is on vagrant cloud (4 GB) and seems to be more "hip" with R, iPython notebook, and other useful command-line tools (html->txt, json->xml, etc). There is a book being released in August with detail.

Tool -> vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.


', 1223, '2014-06-26 22:37:12.007', 'f52bcb3e-e3f7-461d-bfab-155b794236dd', 607, 'deleted 25 characters in body', 1567, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and MLP that I put together.

The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of `lambda * current_weight` is added as well before the combined delta is subtracted at the end of the batch, where `lambda` is the regularisation parameter.

My implementation of backpropagation uses per-item weight updates. *Does a smaller regularisation term per item work just as well?*

For instance `lambda * current_weight / N` where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.
', 836, '2014-06-26 22:58:32.380', '6d735bba-cc76-4c0b-81d1-2b13aad421df', 608, 1569, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Any differences in regularisation in MLP between batch and individual updates?', 836, '2014-06-26 22:58:32.380', '6d735bba-cc76-4c0b-81d1-2b13aad421df', 608, 1570, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork>', 836, '2014-06-26 22:58:32.380', '6d735bba-cc76-4c0b-81d1-2b13aad421df', 608, 1571, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I thought of expanding a bit on the answer by Stanpol. While recommendation system is one approach of suggesting related queries, one more standard information retrieval based approach is the query expansion technique.

Generally speaking, query expansion involves selecting additional terms from the top ranked documents retrieved in response to an initial query. Terms are typically selected by a combination of a term scoring function such as tf-idf and a co-occurrence based measure.

For example, in response to a query term "animal", a term selection function may choose the term "zoo", because

 - "zoo" may be a dominating term (high tf-idf) in the top (say 10) documents retrieved in response to the query "animal"
 - "zoo" may co-occur frequently (in close proximity) with the original query term "animal" in these documents', 984, '2014-06-26 23:01:36.187', '1c298bf2-8f71-4242-9bf9-6e6516c5e7ed', 602, 'added 86 characters in body', 1572, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These VMs have alot of software on them, but I haven''t been able to find any side-by-side comparison.

Here''s a start from my research, but if someone could tell me that one is objectively more rich-featured, with a larger community of support, and useful to get started then that would help greatly:

Kit -> vm is on vagrant cloud (4 GB) and seems to be more "hip" with R, iPython notebook, and other useful command-line tools (html->txt, json->xml, etc). There is a book being released in August with detail.

Tool -> vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.


', 1223, '2014-06-26 23:16:13.523', 'a43da90f-38a2-4a24-b102-af3878ad07f0', 607, 'deleted 73 characters in body', 1573, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Below is a very good note (page 12) on learning rate in Neural Nets (Back Prob) by Andrew Ng. You will find details relating to learning rate.

http://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf

For your 4th-point, you''re right that normally one has to choose a "balanced" learning rate, that should neither overshoot nor converge too slowly. One can plot the learning rate w.r.t. the descent of the cost function to diagnose/ fine tune. In practice, Andrew normally uses the L-BFGS algorithm (mentioned in page 12) to get the "good enough" learning rate.

', 1224, '2014-06-26 23:46:22.487', 'c6bcd4c5-2f4b-43ab-bcbc-c6c14400f965', 609, 1574, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These VMs have alot of software on them, but I haven''t been able to find any side-by-side comparison.

Here''s a start from my research, but if someone could tell me that one is objectively more rich-featured, with a larger community of support, and useful to get started then that would help greatly:

KIT -> vm is on vagrant cloud (4 GB) and seems to be more "hip" with R, iPython notebook, and other useful command-line tools (html->txt, json->xml, etc). There is a book being released in August with detail.

BOX -> vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.


', 1223, '2014-06-26 23:51:05.580', '5b1d654c-acdc-4c86-ad38-5c271a50a345', 607, 'deleted 1 character in body', 1575, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m building a recommender system and using SVD as one of the preprocessing techniques.
However, I want to normalize all my preprocessed data between 0 and 1 because all of my similarity measures (cosine, pearson, euclidean) depend on that assumption.

After I take the SVD (A = USV^T), is there a standard way to normalize the matrix ''A'' between 0 and 1? Thanks!

Edit: I want all of my similarity measurements to give results between 0 and 1 and my normalized euclidean distance in particular fails if the input matrix does not have values between 0 and 1.', 838, '2014-06-27 00:22:33.810', 'de0b32ff-5179-40f5-aed9-11dc2372acc5', 605, 'added 213 characters in body', 1576, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and [Multilayer perceptron](http://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) that I put together.

The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of `lambda * current_weight` is added as well before the combined delta is subtracted at the end of the batch, where `lambda` is the regularisation parameter.

My implementation of backpropagation uses per-item weight updates. *Does a smaller regularisation term per item work just as well?*

For instance `lambda * current_weight / N` where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.
', 84, '2014-06-27 02:30:51.597', 'ac63404f-3f9a-4fee-a307-3e2438906746', 608, 'Improving formatting.', 1577, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There are several classic datasets for machine learning classification/regression tasks. The most popular are:

* [Iris Flower Data Set][1];
* [Titanic Data Set][2];
* [Motor Trend Cars][3];
* etc.

But does anyone know similar datasets for networks analysis / graph theory? More concrete - I''m looking for **Gold standard** datasets for comparing/evaluating/learning:

1. centrality measures;
2. network clustering algorithms.

I don''t need a huge list of publicly available networks/graphs, but a couple of actually must-know datasets.

EDIT:

It''s quite difficult to provide exact features for "gold standard dataset", but here are some thoughts. I think, real classic dataset should satisfy these criteria:

* Multiple references in articles and textbooks;
* Inclusion in well-known network analysis software packages;
* Sufficient time of existence;
* Usage in a number of courses on graph analysis.

Concerning my field of interest, I also need labeled classes for vertices and/or precomputed (or predefined) "authority scores" (i.e. centrality estimates). After asking this question I continued searching, and here are some suitable examples:

* [Zachary''s Karate Club][4]: introduced in 1977, cited more than 1.5k times (according to Google Scholar), vertexes have attribute Faction (which can be used for clustering).
* [Erdos Collaboration Network][5]: unfortunately, I haven''t find this network in form of data-file, but it''s rather famous, and if someone will enrich network with  mathematicians'' specialisations data, it also could be used for testing clustering algorithms.


  [1]: http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html
  [2]: http://www.kaggle.com/c/titanic-gettingStarted
  [3]: http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html
  [4]: http://networkdata.ics.uci.edu/data.php?id=105
  [5]: http://www.orgnet.com/Erdos.html', 941, '2014-06-27 05:22:25.863', '36329d10-6030-4bfe-84af-9045da715162', 596, 'added features for "gold standard dataset"', 1578, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I had almost the same problem: ''restoring'' age, gender, location for social network users. But I used users'' ego-networks, not visited sites statistics. And I faced with two almost independent tasks:

1. ''Restoring'' or ''predicting'' data. You can use a bunch of different technics to complete this task, but my vote is for simplest ones (KISS, yes). E.g., in my case, for age prediction, mean of ego-network users'' ages gave satisfactory results (for about 70% of users error was less than +/-3 years, in my case it was enough). It''s just an idea, but you can try to use for age prediction weighted average, defining weight as similarity measure between visited sites sets of current user and others.
2. Evaluating prediction quality. Algorithm from task-1 will produce prediction almost in all cases. And second task is to determine, if prediction is reliable. E.g., in case of ego network and age prediction: can we trust in prediction, if a user has only one ''friend'' in his ego network? This task is more about machine-learning: it''s a binary classification problem. You need to compose features set, form training and test samples from your data with both right and wrong predictions. Creating appropriate classifier will help you to filter out unpredictable users. But you need to determine, what are your features set. I used a number of network metrics, and summary statistics on feature of interest distribution among ego-network.

This approach wouldn''t populate all the gaps, but only predictable ones.', 941, '2014-06-27 07:06:14.297', 'ab0fc4a0-1719-4a09-9eeb-159914351593', 611, 1582, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why not just determine the correct value of lambda for your case using validation? Since you are not doing online learning (and thus have a dataset), it should be easy. I''m not sure if lambda in per-item learning is inversely proportional to the size of the training set.

Rather, the value of lambda should be considered per update of the parameter vector; dividing by the number of samples would affect lambda for each update. In other words: for batch training you have one parameter update *per dataset pass*, while for online one update *per sample*.

I recently stumbled upon this [Crossvalidated Question][1], which seems quite similar to yours. There is a link to a paper about [a new SGD algorithm][2], with some relevant content. It might be useful to take a look (especially pages 1742-1743).

  [1]: http://stats.stackexchange.com/questions/64224/regularization-and-feature-scaling-in-online-learning
  [2]: http://leon.bottou.org/publications/pdf/jmlr-2009.pdf', 1085, '2014-06-27 07:19:11.420', '18bea3d8-9a3e-417d-b467-f8e1674b0d1d', 612, 1583, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and [Multilayer perceptron](http://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) that I put together.

Currently I tend to cross-validate and keep the network with best score on the validation set to avoid over-fitting. This works OK, but adding regularisation would benefit me in that correct choice of the regularisation algorithm and parameter would make my network converge on a non-overfit model more systematically.

The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of `lambda * current_weight` is added as well before the combined delta is subtracted at the end of the batch, where `lambda` is the regularisation parameter.

My implementation of backpropagation uses per-item weight updates. I am concerned that I cannot just copy the batch approach, although it looks OK intuitively to me. *Does a smaller regularisation term per item work just as well?*

For instance `lambda * current_weight / N` where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.
', 836, '2014-06-27 09:45:14.983', '30175be9-f2c1-4a92-acfb-1abeb2d1445f', 608, 'explain that I already cross-validate', 1584, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and [Multilayer perceptron](http://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) that I put together.

Currently to avoid over-fitting, I cross-validate and keep the network with best score so far on the validation set. This works OK, but adding regularisation would benefit me in that correct choice of the regularisation algorithm and parameter would make my network converge on a non-overfit model more systematically.

The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of `lambda * current_weight` is added as well before the combined delta is subtracted at the end of the batch, where `lambda` is the regularisation parameter.

My implementation of backpropagation uses per-item weight updates. I am concerned that I cannot just copy the batch approach, although it looks OK intuitively to me. *Does a smaller regularisation term per item work just as well?*

For instance `lambda * current_weight / N` where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.
', 836, '2014-06-27 09:52:53.277', 'a667bfd5-c115-4d5e-ad0d-261be6354b56', 608, 'edited body', 1585, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Regularization is relevant in per-item learning as well. I would suggest to start with a basic validation approach for finding out lambda, whether you are doing batch or per-item learning. This is the easiest and safest approach. Try manually with a number of different values. e.g. 0.001. 0.003, 0.01, 0.03, 0.1 etc. and see how your validation set behaves. Later on you may automate this process by introducing a linear or local search method.

As a side note, I believe the value of lambda should be considered in relation to the updates of the parameter vector, rather than the training set size. For batch training you have one parameter update *per dataset pass*, while for online one update *per sample* (regardless of the training set size).

I recently stumbled upon this [Crossvalidated Question][1], which seems quite similar to yours. There is a link to a paper about [a new SGD algorithm][2], with some relevant content. It might be useful to take a look (especially pages 1742-1743).

  [1]: http://stats.stackexchange.com/questions/64224/regularization-and-feature-scaling-in-online-learning
  [2]: http://leon.bottou.org/publications/pdf/jmlr-2009.pdf', 1085, '2014-06-27 09:54:54.977', '92c30f9a-1354-4e96-b081-51210dd8e374', 612, 'Updated according to comments and edited second paragraph.', 1586, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have A  Doubt regarding MapReduce. Actually I understood the Hadoop Mapreduce and its features.
But What i am confused is R Mapreduce..
one difference i know from book is R utilize maximum of RAM. so do perform parallel processing integrated  R with Hadoop.

My Doubt is
------------------

     1.  R can do All Stats, Math and Data Science related stuff. but why R mapreduce.
     2.  By  Using R Mapreduce Any New Task can achieve. If Yes Please specify.
     3.  We can Achieve the task with using R+Hadoop (directly) but what is the importance of Mapreduce in R  and How it is different from Normal Mapreduce.', 1235, '2014-06-27 12:03:53.357', 'de9443ce-65c6-431e-90b3-01d0e64ceef3', 613, 1588, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Difference Between Hadoop Mapreduce(Java) and RHadoop mapreduce', 1235, '2014-06-27 12:03:53.357', 'de9443ce-65c6-431e-90b3-01d0e64ceef3', 613, 1589, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><hadoop><map-reduce>', 1235, '2014-06-27 12:03:53.357', 'de9443ce-65c6-431e-90b3-01d0e64ceef3', 613, 1590, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To complement what **insys** said :

Regularization is used when computing the backpropagation for all the weights in your MLP.
Therefore, instead of computing the gradient in regard to all the input of the training set (`batch`) you only use some/one item(s) (`stochastic or semi-stochastic`).
You will end up limiting a result of the update in regard to one item instead of all which is also correct.

Also, if i remember correctly, Andrew NG used `L2-regularization`.
The `/N` in `lambda * current_weight / N` is not mandatory, it just helps rescaling the input. However if you choose not to use it, you will have (in most of the case) to select another value for `lambda`.

You can also use the [Grid-search algorithm][1] to choose the best value for `lambda` (the *hyperparameter* => the one you have to choose).


  [1]: http://en.wikipedia.org/wiki/Hyperparameter_optimization', 968, '2014-06-27 12:28:25.467', 'c72634fa-0f7a-4278-b315-33933bd00ba6', 614, 1591, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One standard reference written from social science perspective is [J Scott Long''s Limited Dependent Variables](http://www.amazon.com/Regression-Categorical-Dependent-Quantitative-Techniques/dp/0803973748) book. It goes much deeper than say Tabachnik suggested in [another answer](http://datascience.stackexchange.com/a/591/1237): Tabachnik is a cookbook at best, with little to no explanations of the "why", and it seems like you would benefit from figuring this out in more detail that can be found in Long''s book. Ordinal regression should be covered in most introductory econometrics courses (Wooldridge''s [Cross-Section and Panel Data](http://www.amazon.com/Econometric-Analysis-Cross-Section-Panel/dp/0262232588/) is a great graduate-level book), as well as quantitative social science courses (sociology, psychology), although I would imagine that the latter will loop back to Long''s book.

Given that your number of variables is wa-a-ay lower than the sample size, the R package you should be looking is probably [`ordinal`](http://cran.r-project.org/web/packages/ordinal/) rather than `glmnetcr`. [Another answer](http://datascience.stackexchange.com/a/469/1237) mentioned that you can find this functionality in a more mainstream `MASS` package.', 1237, '2014-06-27 13:22:22.577', 'a0401bf9-b052-44f5-81c8-ddfdcf093326', 615, 1592, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a matrix that is populated with discrete elements, and I need to cluster them (using R) into intact groups. So, for example, take this matrix:

>[A B B C A]
>[A A B A A]
>[A B B C C]
>[A A A A A]

There would be two separate clusters for A, two separate clusters for C, and one cluster for B.

The output I''m looking for would ideally assign a unique ID to each cluster, something like this:

>[1 2 2 3 4]
>[1 1 2 4 4]
>[1 2 2 5 5]
>[1 1 1 1 1]

Right now I wrote a code that does this recursively by just iteratively checking nearest neighbor, but it quickly overflows when the matrix gets large (i.e., 100x100).

Is there a built in function in R that can do this? I looked into raster and image processing, but no luck. I''m convinced it must be out there.

Thanks!', 1240, '2014-06-27 15:58:19.340', '2ab2703e-d5b7-4942-8daf-b9830d53467a', 616, 1593, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Identifying clusters or groups in a matrix', 1240, '2014-06-27 15:58:19.340', '2ab2703e-d5b7-4942-8daf-b9830d53467a', 616, 1594, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><clusters>', 1240, '2014-06-27 15:58:19.340', '2ab2703e-d5b7-4942-8daf-b9830d53467a', 616, 1595, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What do you think is distance measure in your case?

I assume there are three dimensions here:

 - `RowN` (row number)
 - `ColN` (column number)
 - `Value` (value: A, B or C)

Is `value` scaled? In other words, is `A < B < C`?

If yes, then

 - you can replace `{A, B, C}` with `{0, 1, 2}` (or may be `{10, 11, 12}`, if you want this difference be less important than RowN and ColN attributes)
 - normalize your data
 - use, for example, K-Means clustering algorithm (http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html) from `stats` R package

In that case the distance between two will be:

    Sqrt( (RowN1-RowN2)^2 + (ColN1-ColN2)^2 + (Value1-Value2)^2 )

If `value` is not scaled (regular categorical variable), use some [modifications of K-Means that work with categorical data][1].


  [1]: http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data

So in case of 100x100 matrix you have 10000 observations and three variables, which is pretty trivial sample size.', 97, '2014-06-27 16:40:17.393', '11b9385f-3a52-4681-be73-c0a34713e585', 617, 1596, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What do you think is distance measure in your case?

I assume there are three dimensions here:

 - `RowN` (row number)
 - `ColN` (column number)
 - `Value` (value: A, B or C)

That means data you get from `4x5` matrix looks like:

    Sample1 -> (1, 1, A)
    Sample2 -> (1, 2, B)
    ...
    Sample5 -> (1, 5, A)
    Sample6 -> (2, 1, A)
    ...
    Sample15 -> (3, 5, C)
    ...
    Sample20 -> (4, 5, A)

Is `value` scaled? In other words, is `A < B < C`?

If yes, then

 - you can replace `{A, B, C}` with `{0, 1, 2}` (or may be `{10, 11, 12}`, if you want this difference be less important than RowN and ColN attributes)
 - normalize your data
 - use, for example, K-Means clustering algorithm (http://stat.ethz.ch/R-manual/R-patched/library/stats/html/kmeans.html) from `stats` R package

In that case the distance between two will be:

    Sqrt( (RowN1-RowN2)^2 + (ColN1-ColN2)^2 + (Value1-Value2)^2 )

If `value` is not scaled (regular categorical variable), use some [modifications of K-Means that work with categorical data][1].


  [1]: http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data

So in case of 100x100 matrix you have 10000 observations and three variables, which is pretty trivial sample size.', 97, '2014-06-27 16:49:26.077', '74e8b163-0e19-4334-bddf-44bdedf8adf4', 617, 'Add more details to answer.', 1597, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a matrix that is populated with discrete elements, and I need to cluster them (using R) into intact groups. So, for example, take this matrix:

    [A B B C A]
    [A A B A A]
    [A B B C C]
    [A A A A A]

There would be two separate clusters for A, two separate clusters for C, and one cluster for B.

The output I''m looking for would ideally assign a unique ID to each cluster, something like this:

    [1 2 2 3 4]
    [1 1 2 4 4]
    [1 2 2 5 5]
    [1 1 1 1 1]

Right now I wrote a code that does this recursively by just iteratively checking nearest neighbor, but it quickly overflows when the matrix gets large (i.e., 100x100).

Is there a built in function in R that can do this? I looked into raster and image processing, but no luck. I''m convinced it must be out there.', 322, '2014-06-27 17:06:52.027', '314e6abb-697f-4148-a6e3-3988326ea02b', 616, 'code blocks instead of quotes for fixed width alignment; "clustering" is a better tag than "clusters"', 1598, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<r><clustering>', 322, '2014-06-27 17:06:52.027', '314e6abb-697f-4148-a6e3-3988326ea02b', 616, 'code blocks instead of quotes for fixed width alignment; "clustering" is a better tag than "clusters"', 1599, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-27 17:06:52.027', '314e6abb-697f-4148-a6e3-3988326ea02b', 616, 'Proposed by 322 approved by 84 edit id of 100', 1600, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Cross-validation: K-fold vs Repeated random sub-sampling', 97, '2014-06-27 17:36:39.530', '7405d122-2937-442e-90e6-95b797fe6ae4', 511, 'Get header back', 1601, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Although adesantos has already given a good answer, I would like to add a little background information.

The name for the problem you are looking at is "imputation". As adesantos already said, one of the possibilities is to fit a distribution. For example, you could fit a multivariate Gaussian to the data. You will get the mean only from the samples you know and you calculate the covariances only from the samples you know. You can then use standard MVG results to impute the missing data linearly.

This is probably the simplest probabilistic method of imputation and it is already quite involved. If you are a neural networks, a recently proposed method that can do so are deep latent gaussian models by Rezende et al. However, understand the method will require a lot of neural net knowledge, quite some variational Bayes knowledge about Markov chains.

Another method, which I have hear to work well is to train a generative stochastic network (Bengio et al). This is done by training a denoising auto encoder on the data you have (neglecting missing values in the reconstruction loss). Say you have a reconstruction function f and a input x. Then you will reconstruct it via x'' = f(x). You then reset the values of x'' with those you know from x. (I.e. you only keep the values that were missing before reconstruction.) If you do so many times, you are guaranteed to sample from the distribution given the values you know.

But in either case, these methods require quite some knowledge about statistics and neural nets.', 1193, '2014-06-27 19:18:11.433', 'aaac8eb5-39e8-4358-b4ed-83f1bc7de52e', 618, 1602, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[rhadoop][1] (the part you are interested in is now called [rmr2][2]) is simply a client API for MapReduce written in R. You invoke MapReduce using R package APIs, and send an R function to the workers, where it is executed by an R interpreter locally. But it is otherwise exactly the same MapReduce.

You can call anything you like in R this way, but no R functions are themselves parallelized to use MapReduce in this way. The point is simply that you can invoke M/R from R. I don''t think it somehow lets you do anything more magical than that.


  [1]: https://github.com/RevolutionAnalytics/RHadoop
  [2]: https://github.com/RevolutionAnalytics/rmr2', 21, '2014-06-27 19:37:44.847', '4f13ec19-758e-47d9-a8f8-fee61c3ba14b', 619, 1603, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I understand Hadoop MapReduce and its features but I am confused about R MapReduce.

One difference I have read is that R utilizes maximum RAM. So do perform parallel processing integrated R with Hadoop.

My doubt is:
-

1.  R can do all stats, math and data science related stuff, but why R MapReduce?
2.  Is there any new task I can achieve by using R MapReduce instead of Hadoop MapReduce? If yes, please specify.
3.  We can achieve the task by using R with Hadoop (directly) but what is the importance of MapReduce in R and how it is different from normal MapReduce?', 322, '2014-06-27 19:39:00.247', '5dd8aaad-b72d-48ec-a979-15e7336226b6', 613, 'took my best shot... can''t parse the third sentence', 1604, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-27 19:39:00.247', '5dd8aaad-b72d-48ec-a979-15e7336226b6', 613, 'Proposed by 322 approved by 21 edit id of 99', 1605, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Below is a very good note (page 12) on learning rate in Neural Nets (Back Propagation) by Andrew Ng. You will find details relating to learning rate.

http://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf

For your 4th point, you''re right that normally one has to choose a "balanced" learning rate, that should neither overshoot nor converge too slowly. One can plot the learning rate w.r.t. the descent of the cost function to diagnose/fine tune. In practice, Andrew normally uses the L-BFGS algorithm (mentioned in page 12) to get a "good enough" learning rate.

', 322, '2014-06-27 20:17:32.853', '636ec2b7-b9a6-4055-a37f-b2be7745c8f9', 609, 'prob -> propagation, some grammar', 1609, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-27 20:17:32.853', '636ec2b7-b9a6-4055-a37f-b2be7745c8f9', 609, 'Proposed by 322 approved by 84 edit id of 101', 1610, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes, it''s problematic.  If you oversample the minority, you risk overfitting.  If you undersample the majority, you risk missing aspects of the majority class.  Stratified sampling, btw, is the equivalent to assigning non-uniform misclassification costs.

Alternatives:

(1) Independently sampling several subsets from the majority class and making multiple classifiers by combining each subset with all the minority class data, as suggested in the answer from @Debasis and described in this [EasyEnsemble paper][1],

(2) [SMOTE (Synthetic Minority Oversampling Technique)][2] or [SMOTEBoost, (combining SMOTE with boosting)][3] to create synthetic instances of the minority class by making nearest neighbors in the feature space.  SMOTE is implemented in R in [the DMwR package][4].


  [1]: http://cse.seu.edu.cn/people/xyliu/publication/tsmcb09.pdf
  [2]: http://arxiv.org/pdf/1106.1813.pdf
  [3]: http://www3.nd.edu/~nchawla/papers/ECML03.pdf
  [4]: http://cran.r-project.org/web/packages/DMwR/index.html', 953, '2014-06-27 21:45:23.237', '9c092d89-12d8-49f7-821d-4b561a05ef28', 620, 1612, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There''s this side project I''m working on where I need to structure a solution to the following problem.

I have two groups of people (clients). Group `A` intends to buy and group `B` intends to sell a determined product `X`. The product has a series of attributes `x_i`, and my objective is to facilitate the transaction between `A` and `B` by matching their preferences. The main idea is to point out to each member of `A` a corresponding in `B` whose product better suits his needs, and vice versa.

Some complicating aspects of the problem:

1. The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design, which is rare among the population and I can''t predict. Can''t previously list all the attributes;

2. Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design);

Any suggestion on how to approach this problem and solve it in an automated way? The idea is to really think out of the box here, so feel free to "go wild" on your suggestions.

I would also appreciate some references to other similar problems if possible.

---

Great suggestions! Many similarities in to the way Im thinking of approaching the problem.

The main issue on mapping the attributes is that the level of detail to which the product should be described depends on each buyers. Lets take an example of a car. The product car has lots and lots of attributes that range from its performance, mechanical structure, price etc.

Suppose I just want a cheap car, or an electric car. Ok, thats easy to map because they represent main features of this product. But lets say, for instance, that I want a car with Dual-Clutch transmission or Xenon headlights. Well there might be many cars on the data base with this attributes but I wouldnt ask the seller to fill in this level of detail to their product prior to the information that there is someone looking them.  Such a procedure would require every seller fill a complex, very detailed, form just try to sell his car on the platform. Just wouldnt work.

But still, my challenge is to try to be as detailed as necessary in the search to make a good match. So the way Im thinking is mapping main aspects of the product, those that are probably relevant to everyone, to narrow down de group of potential sellers.
Next step would be a refined search. In order to avoid creating a too detailed form I could ask buyers and sellers to write a free text of their specification. And then use some word matching algorithm to find possible matches. Although I understand that this is not a proper solution to the problem because the seller cannot guess what the buyer needs.  But might get me close.

The weighting criteria suggested is great.  It allows me to quantify the level to which the seller matches the buyers needs.  The scaling part might be a problem though, because the importance of each attribute varies from client to client. Im thinking of using some kind of pattern recognition or just asking de buyer to input the level of importance of each attribute.

Im not a native English speaker so hope Im being able to express my ideas properly.
', 21, '2014-06-27 22:58:13.613', 'df7dc38b-7574-4a47-99ef-78f05d1338ee', 461, 'appended answer 505 as supplemental', 1616, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is a bunch of datasets made free by UC Irvine [to play with here](https://archive.ics.uci.edu/ml/datasets.html). Among those datasets, [there are a few dozen textual datasets](https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table) that might help you guys with your task.

Those are kind of generic datasets, so depending on your purpose they should not be used as the only data to train your models, or else your model -- while it might work -- will not produce quality results.', 553, '2014-06-28 01:07:27.263', '92b10a37-208a-412e-8df3-f6351c448268', 621, 1618, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m not sure if your question classifies as a clustering problem. In clustering you are trying to discover clusters of similar examples using unlabelled data. Here, it seems you wish to enumerate existing "clusters" of nearby nodes.

To be honest, I have no idea of such a function in R. But, as far as the algorithm is concerned, I believe what you are looking for is [Connected-Component Labeling][1]. Kind of a bucket fill, for matrices.

The wikipedia article is linked above. One of the algorithms presented there, termed as single-pass algorithm, is as follows:

    One-Pass(Image)
            [M, N]=size(Image);
            Connected = zeros(M,N);
            Mark = Value;
            Difference = Increment;
            Offsets = [-1; M; 1; -M];
            Index = [];
            No_of_Objects = 0;

       for i: 1:M :
           for j: 1:N:
                if(Image(i,j)==1)
                     No_of_Objects = No_of_Objects +1;
                     Index = [((j-1)*M + i)];
                     Connected(Index)=Mark;
                     while ~isempty(Index)
                          Image(Index)=0;
                          Neighbors = bsxfun(@plus, Index, Offsets'');
                          Neighbors = unique(Neighbors(:));
                          Index = Neighbors(find(Image(Neighbors)));
                          Connected(Index)=Mark;
                     end
                     Mark = Mark + Difference;
                end
          end
      end

I guess it''d be easy to roll your own using the above.

  [1]: http://en.wikipedia.org/wiki/Connected-component_labeling', 1085, '2014-06-28 14:34:17.487', 'e1162f38-43db-4df2-bbd9-03d282411671', 622, 1619, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m currently using several different classifiers on various entities extracted from text, and using precision/recall as a summary of how well each separate classifier performs across a given dataset.

I''m wondering if there''s a meaningful way of comparing the performance of these classifiers in a similar way, but which also takes into account the total numbers of each entity in the test data that''s being classified?

Currently, I''m using precision/recall as a measure of performance, so might have something like:

                        Precision Recall
    Person classifier   65%       40%
    Company classifier  98%       90%
    Cheese classifier   10%       50%
    Egg classifier      100%      100%

However, the dataset I''m running these on might contain 100k people, 5k companies, 500 cheeses, and 1 egg.

So is there a summary statistic I can add to the above table which also takes into account the total number of each item? Or is there some way of measuring the fact that e.g. 100% prec/rec on the Egg classifier might not be meaningful with only 1 data item?

Let''s say we had hundreds of such classifiers, I guess I''m looking for a good way to answer questions like "Which classifiers are underperforming? Which classifiers lack sufficient test data to tell whether they''re underperforming?".

', 474, '2014-06-28 14:57:18.177', '1f36761d-906f-40fa-9da2-acb698f82ec9', 623, 1620, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Measuring performance of different classifiers with different sample sizes', 474, '2014-06-28 14:57:18.177', '1f36761d-906f-40fa-9da2-acb698f82ec9', 623, 1621, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 474, '2014-06-28 14:57:18.177', '1f36761d-906f-40fa-9da2-acb698f82ec9', 623, 1622, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am brand new to the field of data science, want to break into it, and there are so many tools out there.  These VMs have alot of software on them, but I haven''t been able to find any side-by-side comparison.

Here''s a start from my research, but if someone could tell me that one is objectively more rich-featured, with a larger community of support, and useful to get started then that would help greatly:

datasciencetoolKIT.org -> vm is on vagrant cloud (4 GB) and seems to be more "hip" with R, iPython notebook, and other useful command-line tools (html->txt, json->xml, etc). There is a book being released in August with detail.

datasciencetoolBOX.org -> vm is a vagrant box (24 GB) downloadable from their website. There seems to be more features here, and more literature.


', 1223, '2014-06-28 20:49:26.073', 'ec352044-d2c2-4779-b84f-954508811d4a', 607, 'added 38 characters in body; edited title', 1623, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Do you need a virtual machine as an instrument for your data science practice?', 1223, '2014-06-28 20:49:26.073', 'ec352044-d2c2-4779-b84f-954508811d4a', 607, 'added 38 characters in body; edited title', 1624, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In most cases a practicing data scientist creates his own working environment on personal computed installing preferred software packages. Normally it is sufficient and efficient use of computing resources, because to run a virtual machine (VM) on your main machine you have to allocate a significant portion of RAM for it. The software will run noticeably slower on both the main and the virtual machine unless a lot of RAM.

Due to this impact on speed it is not common to use VMs as main working environment but they are a good solution in several cases when there is a need of additional working environment.

The VMs be considered when:

 1. There is a need to easily replicate a number of identical computing
    environments when teaching a course or doing a presentation on a
    conference.
 2. There is a need to save and recreate an exact environment for an experiment or a calculation.
 3. There is a need to run a different OS or to test a solution on a tool that runs on a different OS.
 4. One wants to try out a bundle of software tools before installing
    them on the main machine. E.g. there is an opportunity to instal an instance of CHS on a VM during an [Intro to Hadoop][1] course on Udacity.
 5. VMs are sometimes used for fast deployment in the cloud like AWS EC, Rackspace etc.

The VMs mentioned in the original question are made as easily installable data science software bundles. There are more than these two. This [blog post][2] by Jeroen Janssens gives a comparison of at least four:

1. Data Science Toolbox
2. Mining the Social Web
3. Data Science Toolkit
4. Data Science Box




  [1]: https://www.udacity.com/course/ud617
  [2]: http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html', 454, '2014-06-29 07:06:00.430', 'd252d701-2b9e-40b9-be98-5f1d07df7b1d', 624, 1627, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In most cases a practicing data scientist creates his own working environment on personal computed installing preferred software packages. Normally it is sufficient and efficient use of computing resources, because to run a virtual machine (VM) on your main machine you have to allocate a significant portion of RAM for it. The software will run noticeably slower on both the main and the virtual machine unless a lot of RAM.

Due to this impact on speed it is not common to use VMs as main working environment but they are a good solution in several cases when there is a need of additional working environment.

The VMs be considered when:

 1. There is a need to easily replicate a number of identical computing
    environments when teaching a course or doing a presentation on a
    conference.
 2. There is a need to save and recreate an exact environment for an experiment or a calculation.
 3. There is a need to run a different OS or to test a solution on a tool that runs on a different OS.
 4. One wants to try out a bundle of software tools before installing
    them on the main machine. E.g. there is an opportunity to instal an instance of Hadoop (CDH) on a VM during an [Intro to Hadoop][1] course on Udacity.
 5. VMs are sometimes used for fast deployment in the cloud like AWS EC, Rackspace etc.

The VMs mentioned in the original question are made as easily installable data science software bundles. There are more than these two. This [blog post][2] by Jeroen Janssens gives a comparison of at least four:

1. Data Science Toolbox
2. Mining the Social Web
3. Data Science Toolkit
4. Data Science Box




  [1]: https://www.udacity.com/course/ud617
  [2]: http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html', 454, '2014-06-29 07:20:51.053', 'a3fc09cb-4e31-431d-bc4d-63848aaaf2d9', 624, 'Appended Hadoop server name to CDH', 1628, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Maybe you can check here - http://snap.stanford.edu/data/

For each data set you will also see references of the works where they have been used', 816, '2014-06-29 12:48:08.840', 'a190b0eb-5442-4aa2-987b-28712fc8bb0c', 625, 1629, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In my opinion, it is difficult to compare the performance when there is such a big difference of size. On this link, (please check it out here in Wikipedia http://en.wikipedia.org/wiki/Effect_size), you may see different strategies.

The one I suggest is one related to the variance. For instance, consider the performance of the classifier (100%) and the person classifier (65%). The minimum error you commit with the former classifier is 100%. However, the minimum error you can commit with the latter classifier is 10e-5.

So one way to compare classifier is to have on mind this **Rule of Three** (http://en.wikipedia.org/wiki/Rule_of_three_(statistics) where you can compare the performance and its variability.

Other possibility is **F-measure** which is a combination of Precision and Recall and it is somehow independent to the effect size.', 1155, '2014-06-29 21:11:47.727', '779c614a-2d2f-410a-830d-bbac8ea98969', 626, 1630, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Roughly speaking, over-fitting typically occurs when the ratio

![enter image description here][1]

is too high.

Think of over-fitting as a situation where your model learn the training data by heart instead of learning the big pictures which prevent it from being able to generalized to the test data: this happens when the model is too complex with respect to the size of the training data, that is to say when the size of the training data is to small in comparison with the model complexity.

Examples:

- if your data is in two dimensions, you have 10000 points in the training set and the model is a line, you are likely to *under*-fit.
- if your data is in two dimensions, you have 10 points in the training set and the model is 100-degree polynomial, you are likely to *over*-fit.

![enter image description here][2]


From a theoretical standpoint, the amount of data you need to properly train your model is a crucial yet far-to-be-answered question in machine learning. One such approach to answer this question is the [VC dimension][3]. Another is the [bias-variance tradeoff](https://www.quora.com/Machine-Learning/What-is-an-intuitive-explanation-for-bias-variance-tradeoff/answer/Franck-Dernoncourt?share=1).

From an empirical standpoint, people typically plot the training error and the test error on the same plot and make sure that they don''t reduce the training error at the expense of the test error:

![enter image description here][4]

I would advise to watch [Coursera'' Machine Learning course](https://www.coursera.org/course/ml), section "10: Advice for applying Machine Learning".

(PS: please go [here](http://meta.datascience.stackexchange.com/q/6/843) to ask for TeX support on this SE.)

  [1]: http://i.stack.imgur.com/AVVpB.png
  [2]: http://i.stack.imgur.com/HH23h.png
  [3]: http://math.stackexchange.com/a/656167/24265
  [4]: http://i.stack.imgur.com/I7LiT.png', 843, '2014-06-29 22:44:02.560', 'fa6baaab-f6d1-4f18-bc04-af01d8975a3b', 627, 1631, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A model underfits when it is too simple with regards to the data it is trying to model.

One way to detect such situation is to use the [biasvariance approach](http://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma), which can represented like this:

![enter image description here][1]

Your model is underfitted when you have a high bias.


----------


To know whether you have a too high bias or a too high variance, you view the phenomenon in terms of training and test errors:

High bias: This learning curve shows high error on both the training and test sets, so the algorithm is suffering from high bias:

![enter image description here][2]

High variance: This learning curve shows a large gap between training and test set errors, so the algorithm is suffering from high variance.

![enter image description here][4]




If an algorithm is suffering from high variance:

- more data will probably help
- otherwise reduce the model complexity

If an algorithm is suffering from high bias:

- increase reduce the model complexity

I would advise to watch [Coursera'' Machine Learning course](https://www.coursera.org/course/ml), section "10: Advice for applying Machine Learning", from which I took the above graphs.

  [1]: http://i.stack.imgur.com/t0zit.png
  [2]: http://i.stack.imgur.com/KFjM4.png
  [3]: http://i.stack.imgur.com/Y9bpL.png
  [4]: http://i.stack.imgur.com/Ypj9y.png
  [5]: http://i.stack.imgur.com/xtDWb.png', 843, '2014-06-29 23:14:39.513', '887b5083-cf6e-46d8-b6f3-8433378626dd', 628, 1632, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('#Do you need a VM?

You need to keep in mind that a virtual machine is a software emulation of your own or another machine hardware configuration that can run an operating systems. In most basic terms, it acts as a layer interfacing between the virtual OS, and your own OS which then communicates with the lower level hardware to provide support to the virtual OS. What this means for you is:

#Cons

###Hardware Support

A drawback of virtual machine technology is that it supports only the hardware that both the virtual machine hypervisor and the guest operating system support. Even if the guest operating system supports the physical hardware, it sees only the virtual hardware presented by the virtual machine.
The second aspect of virtual machine hardware support is the hardware presented to the guest operating system. No matter the hardware in the host, the hardware presented to the guest environment is usually the same (with the exception of the CPU, which shows through). For example, VMware GSX Server presents an AMD PCnet32 Fast Ethernet card or an optimized VMware-proprietary network card, depending on which you choose. The network card in the host machine does not matter. VMware GSX Server performs the translation between the guest environment''s network card and the host environment''s network card. This is great for standardization, but it also means that host hardware that VMware does not understand will not be present in the guest environment.


###Performance Penalty

Virtual machine technology imposes a performance penalty from running an additional layer above the physical hardware but beneath the guest operating system. The performance penalty varies based on the virtualization software used and the guest software being run. This is significant.

#Pros

###Isolation

> One of the key reasons to employ virtualization is to isolate applications from each other. Running everything on one machine would be great if it all worked, but many times it results in undesirable interactions or even outright conflicts. The cause often is software problems or business requirements, such as the need for isolated security. Virtual machines allow you to isolate each application (or group of applications) in its own sandbox environment. The virtual machines can run on the same physical machine (simplifying IT hardware management), yet appear as independent machines to the software you are running. For all intents and purposesexcept performance, the virtual machines are independent machines. If one virtual machine goes down due to application or operating system error, the others continue running, providing services your business needs to function smoothly.

###Standardization

> Another key benefit virtual machines provide is standardization. The hardware that is presented to the guest operating system is uniform for the most part, usually with the CPU being the only component that is "pass-through" in the sense that the guest sees what is on the host. A standardized hardware platform reduces support costs and increases the share of IT resources that you can devote to accomplishing goals that give your business a competitive advantage. The host machines can be different (as indeed they often are when hardware is acquired at different times), but the virtual machines will appear to be the same across all of them.

###Ease of Testing

> Virtual machines let you test scenarios easily. Most virtual machine software today provides snapshot and rollback capabilities. This means you can stop a virtual machine, create a snapshot, perform more operations in the virtual machine, and then roll back again and again until you have finished your testing. This is very handy for software development, but it is also useful for system administration. Admins can snapshot a system and install some software or make some configuration changes that they suspect may destabilize the system. If the software installs or changes work, then the admin can commit the updates. If the updates damage or destroy the system, the admin can roll them back.
Virtual machines also facilitate scenario testing by enabling virtual networks. In VMware Workstation, for example, you can set up multiple virtual machines on a virtual network with configurable parameters, such as packet loss from congestion and latency. You can thus test timing-sensitive or load-sensitive applications to see how they perform under the stress of a simulated heavy workload.

###Mobility

> Virtual machines are easy to move between physical machines. Most of the virtual machine software on the market today stores a whole disk in the guest environment as a single file in the host environment. Snapshot and rollback capabilities are implemented by storing the change in state in a separate file in the host information. Having a single file represent an entire guest environment disk promotes the mobility of virtual machines. Transferring the virtual machine to another physical machine is as easy as moving the virtual disk file and some configuration files to the other physical machine. Deploying another copy of a virtual machine is the same as transferring a virtual machine, except that instead of moving the files, you copy them.


#Which VM should I use if I am starting out?

The Data Science Box or the Data Science Toolbox are your best bets if you just getting into data science. They have the basic software that you will need, with the primary difference being the virtual environment in which each of these can run. The DSB can run on AWS while the DST can run on Virtual Box (which is the most common tool used for VMs).

#Sources

- http://www.devx.com/vmspecialreport/Article/30383
- http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html
', 62, '2014-06-30 01:51:11.027', '30078f3c-8485-4b61-8e57-fd185f89fc39', 629, 1633, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here''s a crazy idea: talk to the volunteers who know the neighborhoods and who have done door-to-door work before.  Get their advice and ideas.  They will probably have insights that no algorithm will produce, and those modifications will be valuable to any computer-generated route list.  One example: Avoiding crossing heavily traveled streets with slow lights or no lights.  Another example: pairs of volunteers working on opposite sides of the same street will feel safer than a volunteer working that street alone.', 609, '2014-06-30 04:00:35.077', '7c3aba73-2309-4f8f-8319-c9b11fd214ef', 630, 1634, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As I understood, Document 1 and Document 2 may have different number of keys. And you wand to get final similarity evaluation between 0 and 1. If so, I would propose following algorithm:

1. Sum of max. vals is equal to 0.
2. Select maximum value from doc-doc matrix and add it to Sum of max. vals.
3. Remove row and column with maximum value from the matrix.
4. Repeat steps 2-3 until rows or columns are ended.
5. Denominate Sum of max. vals by average number of key words in two texts.

Final estimation would be equal to 1, if both documents have identical length, and every word from Doc 1 has equivalent in Doc 2.

You haven''t mentioned software, you are using, but here is **R** example of function, computing such similarity (it takes object of class matrix as input):

    eval.sim <- function(sim.matrix){
      similarity <- 0
      denominator <- sum(dim(sim.matrix)) / 2
      for(i in 1:(min(c(nrow(sim.matrix), ncol(sim.matrix))) - 1)){
        extract <- which(sim.matrix == max(sim.matrix), arr.ind=T)[1, ]
        similarity <- similarity + sim.matrix[extract[1], extract[2]]
        sim.matrix <- sim.matrix[-extract[1], -extract[2]]
      }
      similarity <- similarity + max(sm.copy)
      similarity <- similarity / denominator
    }

In python -

    def score_matrix(sim_matrix):

    similarity = 0
    denominator = sum(sim_matrix.shape) / 2
    max_value = 0

    for i in range(1, min(sim_matrix.shape) + 1):

    x, y = np.where(sim_matrix == np.max(sim_matrix))

    if i == 1:
   max_value = sim_matrix[(x,y)][0]

    if any(j <= 1 for j in sim_matrix.shape):
   break

    similarity = similarity + sim_matrix[(x,y)][0]
    sim_matrix = np.delete(sim_matrix,(x),axis=0)
    sim_matrix = np.delete(sim_matrix,(y),axis=1)

    similarity = similarity + max_value
    similarity = similarity / denominator

    return similarity', 1107, '2014-06-30 05:35:26.340', '60e32443-2106-4d9f-8059-39a789379fe9', 535, 'Added a python version, not sure it works as intended', 1639, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-06-30 05:35:26.340', '60e32443-2106-4d9f-8059-39a789379fe9', 535, 'Proposed by 1107 approved by -1 edit id of 102', 1640, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As I understood, Document 1 and Document 2 may have different number of keys. And you wand to get final similarity evaluation between 0 and 1. If so, I would propose following algorithm:

1. Sum of max. vals is equal to 0.
2. Select maximum value from doc-doc matrix and add it to Sum of max. vals.
3. Remove row and column with maximum value from the matrix.
4. Repeat steps 2-3 until rows or columns are ended.
5. Denominate Sum of max. vals by average number of key words in two texts.

Final estimation would be equal to 1, if both documents have identical length, and every word from Doc 1 has equivalent in Doc 2.

You haven''t mentioned software, you are using, but here is **R** example of function, computing such similarity (it takes object of class matrix as input):

    eval.sim <- function(sim.matrix){
      similarity <- 0
      denominator <- sum(dim(sim.matrix)) / 2
      for(i in 1:(min(c(nrow(sim.matrix), ncol(sim.matrix))) - 1)){
        extract <- which(sim.matrix == max(sim.matrix), arr.ind=T)[1, ]
        similarity <- similarity + sim.matrix[extract[1], extract[2]]
        sim.matrix <- sim.matrix[-extract[1], -extract[2]]
      }
      similarity <- similarity + max(sm.copy)
      similarity <- similarity / denominator
    }

In python -

    import numpy as np

    def score_matrix(sim_matrix):
        similarity = 0
        denominator = sum(sim_matrix.shape) / 2
        for i in range(min(sim_matrix.shape)):
            x, y = np.where(sim_matrix == np.max(sim_matrix))[0][0], np.where(sim_matrix == np.max(sim_matrix))[1][0]
            similarity += sim_matrix[x, y]
            sim_matrix = np.delete(sim_matrix,(x),axis=0)
            sim_matrix = np.delete(sim_matrix,(y),axis=1)
        return similarity / denominator
', 941, '2014-06-30 05:35:26.340', '4b6f5cc6-0d2a-4b57-b4bc-32c3ebe11fa5', 535, 'Added a python version, not sure it works as intended, simplified python version', 1641, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m working on a fraud detection system. In this field, new frauds appear regularly, so that new features have to be added to the model on ongoing basis.

I wonder what is the best way to handle it (from the development process perspective)? Just adding a new feature into the feature vector and re-training the classifier seems to be a naive approach, because too much time will be spent for re-learning of the old features.

I''m thinking along the way of training a classifier for each feature (or a couple of related features), and then combining the results of those classifiers with an overall classifier. Are there any drawbacks of this approach? How to choose the classification algorithm for the overall classifier?', 1271, '2014-06-30 09:43:01.940', '123d3a1f-5f76-42f5-b1f1-f1fd766523ae', 634, 1643, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Handling flexible feature set', 1271, '2014-06-30 09:43:01.940', '123d3a1f-5f76-42f5-b1f1-f1fd766523ae', 634, 1644, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata>', 1271, '2014-06-30 09:43:01.940', '123d3a1f-5f76-42f5-b1f1-f1fd766523ae', 634, 1645, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In an ideal world, you retain all of your historical data, and do indeed run a new model with the new feature extracted retroactively from historical data. I''d argue that the computing resource spent on this is quite useful actually. Is it really a problem?

Yes, it''s a widely accepted technique to build an ensemble of classifiers and combine their results. You can build a new model in parallel just on new features and average in its prediction. This should add value, but, you will never capture interaction between the new and old features this way, since they will never appear together in a classifier.
', 21, '2014-06-30 10:47:06.453', '46f5b0b4-5c9e-4725-9b38-8ec829eeda64', 635, 1646, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am an ml noob. I have a task at hand of predicting click probability given user information like city, state, os version, os family, device, browser family browser version, city, etc. I have been recommended to try logit since logit seems to be what MS and Google are using too. I have some questions regarding logistic regression like:

Click and non click is a very very unbalanced class and the simple glm predictions do not look good. How to make the data work through this?

All variables I have are categorical and things like device and city can be numerous. Also the frequency of occurrence of some devices or some cities can be very very low. So how to deal with what I can say is a very random variety of categorical variables?

One of the variables that we get is device id also. This is a very unique feature that can be translated to a user''s identity. How to make use of it in logit, or should it be used in a completely different model based on a user identity?', 1273, '2014-06-30 12:05:38.597', '3abc3f93-272c-416c-9896-30e7b5c35970', 636, 1647, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data preparation and machine algo for click prediction', 1273, '2014-06-30 12:05:38.597', '3abc3f93-272c-416c-9896-30e7b5c35970', 636, 1648, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><data-mining><dataset><data-cleaning>', 1273, '2014-06-30 12:05:38.597', '3abc3f93-272c-416c-9896-30e7b5c35970', 636, 1649, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here''s an idea that just popped out of the blue  what if you make use of [Random Subspace Sampling][1] (as in fact Sean Owen already suggested) to train a bunch of new classifiers every time a new feature appears (using a random feature subset, including the new set of features). You could train those models on a subset of samples as well to save some training time.

This way you can have new classifiers possibly taking on both new and old features, and at the same time keeping your old classifiers. You might even, perhaps using a cross validation technique to measure each classifier''s performance, be able to kill-off the worst performing ones after a while, to avoid a bloated model.



  [1]: http://en.wikipedia.org/wiki/Random_subspace_method', 1085, '2014-06-30 13:56:10.753', '0ffc2231-a1ed-48f6-9c2d-ce366b6a0bd5', 637, 1650, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m currently working on a project that would benefit from personalized predictions.  Given an input document, a set of output documents, and a history of user behavior, I''d like to predict which of the output documents are clicked.

In short, I''m wondering what the typical approach to this kind of personalization problem is.  Are models trained per user, or does a single global model take in summary statistics of past user behavior to help inform that decision?  Per user models won''t be accurate until the user has been active for a while, while most global models have to take in a fixed length feature vector (meaning we more or less have to compress a stream of past events into a smaller number of summary statistics).  ', 684, '2014-06-30 20:51:58.640', 'c8afdff9-81c5-4b02-b18d-0e2586eeb2fc', 640, 1653, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Large Scale Personalization - Per User vs Global Models', 684, '2014-06-30 20:51:58.640', 'c8afdff9-81c5-4b02-b18d-0e2586eeb2fc', 640, 1654, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 684, '2014-06-30 20:51:58.640', 'c8afdff9-81c5-4b02-b18d-0e2586eeb2fc', 640, 1655, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m currently searching for labeled datasets to use to train a model to extract named entities from informal text (think something similar to tweets). Because capitalization and grammar are often lacking in the documents in my dataset, I''m looking for out of domain data that''s a bit more "informal" than the news articles and journal entries that many of today''s state of the art named entity recognition systems are trained on.  Any recommendations?  So far I''ve only been able to locate 50k tokens from twitter published here: https://github.com/aritter/twitter_nlp/blob/master/data/annotated/ner.txt', 684, '2014-06-30 21:02:05.053', 'eac507ab-c47b-429c-ba38-53705d1c9ed9', 641, 1656, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Dataset for Named Entity Recognition on Informal Text', 684, '2014-06-30 21:02:05.053', 'eac507ab-c47b-429c-ba38-53705d1c9ed9', 641, 1657, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><nlp>', 684, '2014-06-30 21:02:05.053', 'eac507ab-c47b-429c-ba38-53705d1c9ed9', 641, 1658, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The answer to this question is going to vary pretty wildly depending on the size and nature of your data. At a high level, you could think of it as a special case of multilevel models; you have the option of estimating a model with complete pooling (i.e., a universal model that doesn''t distinguish between users), models with no pooling (a separate model for each user), and partially pooled models (a mixture of the two). You should really read Andrew Gelman on this topic if you''re interested.

You can also think of this as a learning-to-rank problem that either tries to produce point-wise estimates using a single function or instead tries to optimize on some list-wise loss function (e.g., NDCG).

As with most machine learning problems, it all depends on what kind of data you have, the quality of it, the sparseness of it, and what kinds of features you are able to extract from it. If you have reason to believe that each and every user is going to be pretty unique in their behavior, you might want to build a per-user model, but that''s going to be unwieldy fast -- and what do you do when you are faced with a new user?', 159, '2014-06-30 23:10:53.397', 'e91f1737-5d4b-43ff-84cf-027a8b26336d', 642, 1659, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I guess you say that you want to use 3-fold cross-validation because you know something about your data (that using k=10 would cause overfitting? I''m curious to your reasoning). I am not sure that you know this, if not then you can simply use a larger k.

If you still think that you cannot use standard k-fold cross-validation, then you could modify the algorithm a bit: say that you split the data into 30 folds and each time use 20 for training and 10 for evaluation (and then shift up one fold and use the first and the last 9 as evaluation and the rest as training). This means that you''re able to use all your data.

When I use k-fold cross-validation I usually run the process multiple times with a different randomisation to make sure that I have sufficient data, if you don''t you will see different performances depending on the randomisation. In such cases I would suggest sampling. The trick then is to do it often enough.', 127, '2014-07-01 08:04:30.127', '1b0d02a3-470b-4590-b904-06b9e16f99c6', 643, 1660, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For a recommendation system I''m using cosign similarity to compute similarities between items. However, for items with small amounts of data I''d like to bin them under a general "average" category (in the general not mathematical sense). To accomplish this I''m currently trying to create a synthetic observation to represent that middle of the road point.

So for example if these were my observations (rows are observations, cols are features):

    [[0, 0, 0, 1, 1, 1, 0, 1, 0],
     [1, 0, 1, 0, 0, 0, 1, 0, 0],
     [1, 1, 1, 1, 0, 1, 0, 1, 1],
     [0, 0, 1, 0, 0, 1, 0, 1, 0]]

A strategy where I''d simply take the actual average of all features across observations would generate a synthetic datapoint such as follows, which I''d then append to the matrix before doing the similarity calculation.

    [ 0.5 ,  0.25,  0.75,  0.5 ,  0.25,  0.75,  0.25,  0.75,  0.25]

While this might work well with certain similarity metrics (e.g. L1 distance) I''m sure there are much better ways for cosign similarity. Though, at the moment, I''m having trouble reasoning through way through angles between lines in high dimensional space.

Any ideas?', 754, '2014-07-01 13:44:23.290', '4df24e06-da1a-433e-bf14-43ade7e858ee', 644, 1661, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Create most "average" cosign similarity observation', 754, '2014-07-01 13:44:23.290', '4df24e06-da1a-433e-bf14-43ade7e858ee', 644, 1662, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation><similarity>', 754, '2014-07-01 13:44:23.290', '4df24e06-da1a-433e-bf14-43ade7e858ee', 644, 1663, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For a recommendation system I''m using cosine similarity to compute similarities between items. However, for items with small amounts of data I''d like to bin them under a general "average" category (in the general not mathematical sense). To accomplish this I''m currently trying to create a synthetic observation to represent that middle of the road point.

So for example if these were my observations (rows are observations, cols are features):

    [[0, 0, 0, 1, 1, 1, 0, 1, 0],
     [1, 0, 1, 0, 0, 0, 1, 0, 0],
     [1, 1, 1, 1, 0, 1, 0, 1, 1],
     [0, 0, 1, 0, 0, 1, 0, 1, 0]]

A strategy where I''d simply take the actual average of all features across observations would generate a synthetic datapoint such as follows, which I''d then append to the matrix before doing the similarity calculation.

    [ 0.5 ,  0.25,  0.75,  0.5 ,  0.25,  0.75,  0.25,  0.75,  0.25]

While this might work well with certain similarity metrics (e.g. L1 distance) I''m sure there are much better ways for cosine similarity. Though, at the moment, I''m having trouble reasoning through way through angles between lines in high dimensional space.

Any ideas?', 754, '2014-07-01 14:18:38.743', '36d186ae-bc83-4cf7-8d6f-20f30d81b275', 644, 'edited body; edited title', 1664, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Create most "average" cosine similarity observation', 754, '2014-07-01 14:18:38.743', '36d186ae-bc83-4cf7-8d6f-20f30d81b275', 644, 'edited body; edited title', 1665, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For a recommendation system I''m using cosine similarity to compute similarities between items. However, for items with small amounts of data I''d like to bin them under a general "average" category (in the general not mathematical sense). To accomplish this I''m currently trying to create a synthetic observation to represent that middle of the road point.

So for example if these were my observations (rows are observations, cols are features):

    [[0, 0, 0, 1, 1, 1, 0, 1, 0],
     [1, 0, 1, 0, 0, 0, 1, 0, 0],
     [1, 1, 1, 1, 0, 1, 0, 1, 1],
     [0, 0, 1, 0, 0, 1, 0, 1, 0]]

A strategy where I''d simply take the actual average of all features across observations would generate a synthetic datapoint such as follows, which I''d then append to the matrix before doing the similarity calculation.

    [ 0.5 ,  0.25,  0.75,  0.5 ,  0.25,  0.75,  0.25,  0.75,  0.25]

While this might work well with certain similarity metrics (e.g. L1 distance) I''m sure there are much better ways for cosine similarity. Though, at the moment, I''m having trouble reasoning my way through angles between lines in high dimensional space.

Any ideas?', 754, '2014-07-01 15:33:10.697', 'c570ee47-b873-4369-9692-3f987929be0c', 644, 'deleted 5 characters in body', 1666, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You need to look at the confidence interval of the statistic.  This helps measure how much uncertainty in the statistic, which is largely a function of sample size.', 178, '2014-07-01 17:01:12.263', 'e0afa250-206c-4168-99c4-e55405112a5b', 645, 1667, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to build a nonparametric density function for a fairly large dataset that can be evaluated efficently, and can be updated efficiently when new points are added. There will only ever be a maximum of 4 independent variables, but we can start off with 2. Lets use a gaussian kernel. Let the result be a probability density function, i.e. its volume will be 1.

In each evaluation, we can omit all points for which the evaluation point is outside a certain ellipsoid corresponding to the minimum gaussian value we care about. We can change this threshold for accuracy or performance, and the maximum number of points inside the threshold will depend on the chosen covariance matrix of the kernel. Then, we can evaluate the distribution approximately using the subset of points.

If we use a fixed kernel, then we can use the eigenvalues and eigenvectors we get from the covariance matrix to transform each point so that the threshold ellipsoid is a fixed circle. We can then shove all the transformed points into a spatial index, and efficiently find all points within the required radius of the evaluation point.

However, we would the kernel to be variable for two reasons. (1) to fit the data better, and (2) because adding or modifying points would require the fixed kernel to be updated, which would mean that the entire data set would need to be reindexed. With a variable kernel, we could make new/updated points only affect the closest points.

Specifically, is there a spatial index that can efficiently find ellipses surrounding a given point from a set of around 10 million ellipses of different shapes and sizes?

More generally though, does my approach look sound? I am open to answers like "give up and precalculate a grid of results". Answers much appreciated!', 1283, '2014-07-01 17:03:31.907', 'b8f4207e-b819-4fe9-b8b3-784a6e284e36', 646, 1668, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Spatial index for variable kernel nonparametric density', 1283, '2014-07-01 17:03:31.907', 'b8f4207e-b819-4fe9-b8b3-784a6e284e36', 646, 1669, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-indexing-techniques>', 1283, '2014-07-01 17:03:31.907', 'b8f4207e-b819-4fe9-b8b3-784a6e284e36', 646, 1670, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Singular Value Decomposition is a linear algebraic technique as a result of which the notion of normalization is hard to define. In principle, you can do this normalization by dividing each element A(i,j) of the matrix by the sum (or max) of the elements in that particular (ith) row, i.e. A(i,j) = A(i, j) / \sum_{k=1}^{n} A(i,k)

However, a more elegant way to achieve this would be to apply a probabilistic dimensionality reduction technique such as [PLSA][1] or [LDA][2]. These dimensionality reduction techniques ensure that you always end up with probability values strictly between 0 and 1.


  [1]: http://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis
  [2]: http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation', 984, '2014-07-01 18:48:13.757', '0c6ad330-0ae0-4d62-ab98-23fdbb0c93c4', 648, 1674, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You are doing the correct thing. Technically, this averaging leads to computing the [centroid][1] in the Euclidean space of a set of N points. The centroid works pretty well with cosine similarities (cosine of the angles between normalized vectors), e.g. [the Rocchio algorithm][2].


  [1]: http://en.wikipedia.org/wiki/Centroid
  [2]: http://en.wikipedia.org/wiki/Rocchio_algorithm', 984, '2014-07-01 23:11:35.627', 'a55c2eb9-62a7-4b05-abea-59e2a9359fcf', 649, 1675, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As @Debasis writes, what you are doing is correct. Do not worry about the relative values as the cosine similarity focusses on the angle, not the length of the vector.

Note that the averages that you compute actually are the proportion of observations that have that feature in your subset. If the subset is sufficiently large, you can interpret them as probabilities.

', 172, '2014-07-02 08:40:09.773', 'cf39b222-ad94-474f-91f6-6bdc38fca16b', 650, 1676, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('More often than not, data I am working with is not 100% clean. Even if it is reasonably clean, still there are portions that need to be fixed.

When a fraction of data needs it, I write a script and incorporate it in data processing.

But what to do if only a few entries needs to be fixed (e.g. misspelled city or zip code)? Let''s focus on "small data",  such as that in CSV files or a relational database.

The practical problems I encountered:

* Writing a general script trying solve all similar errors may give unintended consequences (e.g. matching cities that are different but happen to have similar names).
* Copying and modifying data may make a mess, as:
  * Generating it again will destroy all fixes.
  * When there are more errors of different kinds, too many copies of the same file result, and it is hard to keep track of them all.
* Writing a script to modify particular entries seems the best, but there is overhead in comparison to opening CSV and fixing it (but still, _seems_ to be the best); and either we need to create more copies of data (as in the previous point) or run the script every time we load data.

What are the best practices in such a case as this?

EDIT: The question is on the workflow, not whether to use it or not.

(In my particular case I don''t want the end-user to see misspelled cities and, even worse, see two points of data, for the same city but with different spelling; the data is small, ~500 different cities, so manual corrections do make sense.)', 24, '2014-07-02 08:48:41.517', '5a234d8b-c9c3-41af-9ea9-b6170ed14a12', 536, 'Just tightening things up a bit to improve readability', 1677, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-02 08:48:41.517', '5a234d8b-c9c3-41af-9ea9-b6170ed14a12', 536, 'Proposed by 24 approved by 289 edit id of 105', 1678, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The number of data in the class is sometimes referred to as the `support` of the classifier. It tells how much you can trust your result, like a p-value would allow you to trust or distrust some test.

One approach you can use is to compute several classifier performance measures, not only precision and recall, but also true positive rate, false positive rate, specificity, sensitivity, positive likelihood, negative likelihood, etc. and see whether they are consistent with one another. If one of the measure maxes out (100%) and the other do not, it is often, in my experience, indicative of something went wrong (e.g. poor support, trivial classifier, biased classifier, etc.). See [this][1] for a list of classifier performance measures.


  [1]: http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf', 172, '2014-07-02 08:50:55.980', '9ce5fb04-3080-4787-8461-5a274c742513', 651, 1679, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Simply, one common approach is to increase the complexity of the model, making it simple, and most probably underfitting at first, and increasing the complexity of the model  until early signs of overfitting are witnessed using a resampling technique such as cross validation, bootstrap, etc.

You increase the complexity either by adding parameters (number of hidden neurons for artificial neural networks, number of trees in a random forest) or by relaxing the regularization (often named lambda, or C for support vector machines) term in your model.', 172, '2014-07-02 08:58:25.413', '175deb96-e187-41a5-97a2-069500723adc', 652, 1680, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to find which classification methods, that do not use a training phase, are available.

The scenario is gene expression based classification, in which you have a matrix of gene expression of m genes (features) and n samples (observations).
A signature for each class is also provided (that is a list of the features to consider to define to which class belongs a sample).

An application (non-training) is the [Nearest Template Prediction][1] method. In this case it is computed the cosine distance between each sample and each signature (on the common set of features). Then each sample is assigned to the nearest class (the sample-class comparison resulting in a smaller distance). No already classified samples are needed in this case.

A different application (training) is the [kNN][2] method, in which we have a set of already labeled samples. Then, each new sample is labeled depending on how are labeled the k nearest samples.

Are there any other non-training methods?

Thanks

  [1]: http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0015543
  [2]: http://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm', 133, '2014-07-02 13:40:27.000', '83be9aa6-45f6-4580-9795-d27756f36aec', 653, 1681, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which non-training classification methods are available?', 133, '2014-07-02 13:40:27.000', '83be9aa6-45f6-4580-9795-d27756f36aec', 653, 1682, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 133, '2014-07-02 13:40:27.000', '83be9aa6-45f6-4580-9795-d27756f36aec', 653, 1683, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What you are asking about is [Instance-Based Learning](http://en.wikipedia.org/wiki/Instance-based_learning). k-Nearest Neighbors (kNN) appears to be the most popular of these methods and is applicable to a wide variety of problem domains. Another general type of instance-based learning is [Analogical Modeling](http://en.wikipedia.org/wiki/Analogical_modeling), which uses instances as exemplars for comparison with new data.

You referred to kNN as an application that uses training but that is not correct (the Wikipedia entry you linked is somewhat misleading in that regard). Yes, there are "training examples" (labeled instances) but the classifier doesn''t learn/train from these data. Rather, they are only used whenever you actually want to classify a new instance, which is why it is considered a "lazy" learner.

Note that the Nearest Template Prediction method you mention effectively is a form of kNN with `k=1` cosine distance as the distance measure.', 964, '2014-07-02 14:14:22.593', 'aee08f32-496d-4adc-91ef-9d4f34c35da2', 654, 1684, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to use ANN in order to automate trading forex, preferebly USD/EUR or USD/GBP. I know this is hard and may not be straightforward. I have already read some parers, did some experiments but no much luck still. I would like to get advice from EXPERTS to make this work.

Here is what I did so far:
1.I got tick by tick data for the month of july 2013. It has bid/ask/bid volume/ask volume.
2.Extracted all ticks for the time frame 12PM to 14PM for all days.
3.From this data, created a data set where each entry consists of n bid values in sequence.
4. Used that data to train an ANN with n-1 inputs and the output is the forecasted nth bid value.
5. The ANN had n-1 inputs neurons, (n-1)*2 + 1 hidden and 1 out put neuron. Input layer had linear TF, hidden had log TF and output had linear TF.
6. Trained the network with back propagation with n-125 first and then 10.

For both n, the MSE did not drop below .5 and stayed at that value during full training. Assumes that this could be due to the time series being totally random, so used the R package to find partial autocorrelation on the data set. (pacf). This gave non zero values for 2 and 3 lags only.

Question 1: What does this mean exactly?

Then  used hurst exponent to evaluate the randomness.
in R:
hurst(values) showed values above .9?
Question 2: It supposed to be nearly random and should have a value closer to .5?


Repeated the training of the ANN with n=3. The ANN was trained and was able to obtain a pretty low value for MSE. However, the calculated output from this ANN does not differ much from the n-1 th bid value. It looks like ANN just takes the last bid as the next bid! Tries different networks structures (All multilayer perceptions), different training params, etc, but results are same.

Question 3: Any suggestions to improve the accuracy? Any other training methods than back propogation?

', 1300, '2014-07-02 14:40:55.413', '7d33531f-48f8-4537-9e37-4ed85f39e383', 655, 1685, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Forex forecasting with neural networks', 1300, '2014-07-02 14:40:55.413', '7d33531f-48f8-4537-9e37-4ed85f39e383', 655, 1686, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork>', 1300, '2014-07-02 14:40:55.413', '7d33531f-48f8-4537-9e37-4ed85f39e383', 655, 1687, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What you are asking about is [Instance-Based Learning](http://en.wikipedia.org/wiki/Instance-based_learning). k-Nearest Neighbors (kNN) appears to be the most popular of these methods and is applicable to a wide variety of problem domains. Another general type of instance-based learning is [Analogical Modeling](http://en.wikipedia.org/wiki/Analogical_modeling), which uses instances as exemplars for comparison with new data.

You referred to kNN as an application that uses training but that is not correct (the Wikipedia entry you linked is somewhat misleading in that regard). Yes, there are "training examples" (labeled instances) but the classifier doesn''t learn/train from these data. Rather, they are only used whenever you actually want to classify a new instance, which is why it is considered a "lazy" learner.

Note that the Nearest Template Prediction method you mention effectively is a form of kNN with `k=1` and cosine distance as the distance measure.', 964, '2014-07-02 14:46:55.010', '7b2b43b6-98e0-45ad-bd14-bc5652ae0826', 654, 'added 4 characters in body', 1688, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Some suggestions

1. Remove items appearing too infrequently in the data. That will reduce the dimensionality by several orders of magnitude. If a feature occurs less than say 10 times, it''s likely that it''s not adding any predictive value, and it may lead to overfitting due to low frequency
2. Try a Linear SVM instead. They handle large dimensional data very well in terms of not overfitting. They also often have the option to assign relative weights to different classes, which may help address your unbalanced problem above. The sklearn svm (which simply wraps some other packages such as libsvm) has this option.
3. Don''t use the ID column. Producing a model per user will most probably lead to overfitting. Instead, feed in attributes that describe the user that allows the model to generalize over similar users. You could try fitting a separate model per user, but you need a lot of data per user to do this well.
4. It sounds like you really need to try some feature selection here, to reduce the dimensionality of the problem. But try 1 and 2 first, as they may give you good results sooner (although the end solution may still work better with some good feature selection). Sklearn again has a number of options for feature selection.
', 1301, '2014-07-02 15:52:44.470', 'bdde9df7-5e92-4b05-a292-876aa9a1e41b', 656, 1689, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Suppose I want to use CART as classification tree (I want a categorical response). I have the training set, and I split it using observation labels.

Now, to build the decision tree (classification tree) how are selected the features to decide which label apply to testing observations?

Supposing we are working on gene expression matrix, in which each element is a real number, is that done using features that are more distant between classes?
', 133, '2014-07-02 16:41:47.467', '28a76c3d-79e5-4f91-905b-fc45382e9d30', 657, 1690, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How are selected the features for a decision tree in CART?', 133, '2014-07-02 16:41:47.467', '28a76c3d-79e5-4f91-905b-fc45382e9d30', 657, 1691, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 133, '2014-07-02 16:41:47.467', '28a76c3d-79e5-4f91-905b-fc45382e9d30', 657, 1692, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A model underfits when it is too simple with regards to the data it is trying to model.

One way to detect such situation is to use the [biasvariance approach](http://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma), which can represented like this:

![enter image description here][1]

Your model is underfitted when you have a high bias.


----------


To know whether you have a too high bias or a too high variance, you view the phenomenon in terms of training and test errors:

High bias: This learning curve shows high error on both the training and test sets, so the algorithm is suffering from high bias:

![enter image description here][2]

High variance: This learning curve shows a large gap between training and test set errors, so the algorithm is suffering from high variance.

![enter image description here][4]




If an algorithm is suffering from high variance:

- more data will probably help
- otherwise reduce the model complexity

If an algorithm is suffering from high bias:

- increase the model complexity

I would advise to watch [Coursera'' Machine Learning course](https://www.coursera.org/course/ml), section "10: Advice for applying Machine Learning", from which I took the above graphs.

  [1]: http://i.stack.imgur.com/t0zit.png
  [2]: http://i.stack.imgur.com/KFjM4.png
  [3]: http://i.stack.imgur.com/Y9bpL.png
  [4]: http://i.stack.imgur.com/Ypj9y.png
  [5]: http://i.stack.imgur.com/xtDWb.png', 843, '2014-07-02 16:55:27.367', 'bf053754-ae18-4cac-bfb9-f1721e0c5335', 628, 'deleted 7 characters in body', 1693, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to use another type of data, not atomic data, as a feature for a prediction.
Suppose I have a Table with those Features:
- Column 1: Categorical - House
- Column 2: Numerical - 23.22
- Column 3: A Vector - [ 12, 22, 32 ]
- Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]
- Column 5: A List [ 122, Boolean ]
I would like to predict/classify ... Columns 2 ... for example....

I am making a Software to automatically respond questions... Any type...like "Where Foo was Born ?" ...

I first make a query to a search engine --->>> then I get some Text data as a Result.
So I do all the Parsing Staff... Tagging, Stemming, Parsing, Splitting...
My first approach was to make a table, each row with a line of text.. and a lot of Features...like ... First Word ... Tag of First Word.. Chunks, etc..
But with this approach I am missing the relationships between the Sentences.

I would like to know if there is an algorithm that look inside the Tree Structures... Vectors... and make the relations and extract whatever is relevant for predicting/classifying.
I rather know a library that does that then an algorithm that I have to implement...

Thank you very much !', 1306, '2014-07-02 20:54:53.767', '8a0d0ec8-456a-4d03-8628-10730ae66039', 658, 1694, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Prediction with not atomic features', 1306, '2014-07-02 20:54:53.767', '8a0d0ec8-456a-4d03-8628-10730ae66039', 658, 1695, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 1306, '2014-07-02 20:54:53.767', '8a0d0ec8-456a-4d03-8628-10730ae66039', 658, 1696, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would like to use ANN to automate trading currencies, preferably USD/EUR or USD/GBP. I know this is hard and may not be straightforward. I have already read some papers and done some experiments but without much luck. I would like to get advice from EXPERTS to make this work.

Here is what I did so far:

1. I got tick by tick data for the month of july 2013. It has bid/ask/bid volume/ask volume.
2. Extracted all ticks for the time frame 12PM to 14PM for all days.
3. From this data, created a data set where each entry consists of n bid values in sequence.
4. Used that data to train an ANN with n-1 inputs and the output is the forecasted nth bid value.
5. The ANN had n-1 inputs neurons, (n-1)*2 + 1 hidden and 1 output neuron. Input layer had linear TF, hidden had log TF and output had linear TF.
6. Trained the network with back propagation with n-125 first and then 10.

For both n, the MSE did not drop below 0.5 and stayed at that value during full training. Assuming that this could be due to the time series being totally random, I used the R package to find partial autocorrelation on the data set (pacf). This gave non zero values for 2 and 3 lags only.

Question 1: What does this mean exactly?

Then I used hurst exponent to evaluate the randomness. In R, hurst(values) showed values above 0.9.

Question 2: It is supposed to be nearly random. Should it have a value closer to 0.5?

I repeated the training of the ANN with n=3. The ANN was trained and was able to obtain a pretty low value for MSE. However, the calculated output from this ANN does not differ much from the (n-1)th bid value. It looks like ANN just takes the last bid as the next bid! I tried different network structures (all multilayer perceptions), different training parameters, etc, but results are same.

Question 3: How can I improve the accuracy? Are there any other training methods than backpropagation?', 322, '2014-07-02 22:18:40.597', '6ab92a34-0931-445d-94b7-ba705626aaa1', 655, 'formatting, grammar, spelling', 1697, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Foreign exchange market forecasting with neural networks', 322, '2014-07-02 22:18:40.597', '6ab92a34-0931-445d-94b7-ba705626aaa1', 655, 'formatting, grammar, spelling', 1698, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-02 22:18:40.597', '6ab92a34-0931-445d-94b7-ba705626aaa1', 655, 'Proposed by 322 approved by 1300 edit id of 106', 1699, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As Steve Kallestad has said, this is a TSP problem, and there are wonderful free solvers to find approximate solutions.

It may be too much work for what you are looking for, but you may try use one of those solvers in combination with the Google Maps API, to find real walking distances beetwen your coordinates:
https://developers.google.com/maps/documentation/directions/#DirectionsRequests

(I have never used this API, so I don''t know how easy or effective it would be)
', 1289, '2014-07-03 08:00:10.823', 'd0fc3a83-0ca9-4fcc-875f-a77d805275df', 659, 1700, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I m new to RHadoop and also to RMR...
I had an requirement to write a Mapreduce Job in R Mapreduce. I have Tried writing but While executing this it gives an Error.
Tring to read the file from hdfs
Error:
------------

    Error in mr(map = map, reduce = reduce, combine = combine, vectorized.reduce,  :
       hadoop streaming failed with error code 1


Code :
------


    Sys.setenv(HADOOP_HOME="/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop")
    Sys.setenv(HADOOP_CMD="/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/bin/hadoop")

    Sys.setenv(HADOOP_STREAMING="/opt/cloudera/parcels/CDH-4.7.0-1.cdh4.7.0.p0.40/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.7.0.jar")
    library(rmr2)
    library(rhdfs)
    hdfs.init()
    day_file = hdfs.file("/hdfs/bikes_LR/day.csv","r")
    day_read = hdfs.read(day_file)
    c = rawToChar(day_read)

    XtX =
      values(from.dfs(
        mapreduce(
          input = "/hdfs/bikes_LR/day.csv",
          map=
            function(.,Xi){
             yi =c[Xi[,1],]
             Xi = Xi[,-1]
             keyval(1,list(t(Xi)%*%Xi))
           },
      reduce = function(k,v )
      {
        vals =as.numeric(v)
        keyval(k,sum(vals))
      } ,
      combine = TRUE)))[[1]]

    XtY =
     values(from.dfs(
        mapreduce(
         input = "/hdfs/bikes_LR/day.csv",
         map=
           function(.,Xi){
             yi =c[Xi[,1],]
             Xi = Xi[,-1]
            keyval(1,list(t(Xi)%*%yi))
           },
         reduce = TRUE ,
         combine = TRUE)))[[1]]
    solve(XtX,XtY)



    Input:
    ------------

    instant,dteday,season,yr,mnth,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt
    1,2011-01-01,1,0,1,0,6,0,2,0.344167,0.363625,0.805833,0.160446,331,654,985
    2,2011-01-02,1,0,1,0,0,0,2,0.363478,0.353739,0.696087,0.248539,131,670,801
    3,2011-01-03,1,0,1,0,1,1,1,0.196364,0.189405,0.437273,0.248309,120,1229,1349
    4,2011-01-04,1,0,1,0,2,1,1,0.2,0.212122,0.590435,0.160296,108,1454,1562
    5,2011-01-05,1,0,1,0,3,1,1,0.226957,0.22927,0.436957,0.1869,82,1518,1600
    6,2011-01-06,1,0,1,0,4,1,1,0.204348,0.233209,0.518261,0.0895652,88,1518,1606
    7,2011-01-07,1,0,1,0,5,1,2,0.196522,0.208839,0.498696,0.168726,148,1362,1510
    8,2011-01-08,1,0,1,0,6,0,2,0.165,0.162254,0.535833,0.266804,68,891,959
    9,2011-01-09,1,0,1,0,0,0,1,0.138333,0.116175,0.434167,0.36195,54,768,822
    10,2011-01-10,1,0,1,0,1,1,1,0.150833,0.150888,0.482917,0.223267,41,1280,1321



     Please Suggest me any mistakes.
', 1235, '2014-07-03 10:49:50.993', '541c90b5-99c9-42af-8fac-925323a10df2', 660, 1702, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Linear Regression in R Mapreduce(RHadoop)', 1235, '2014-07-03 10:49:50.993', '541c90b5-99c9-42af-8fac-925323a10df2', 660, 1703, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><hadoop><map-reduce>', 1235, '2014-07-03 10:49:50.993', '541c90b5-99c9-42af-8fac-925323a10df2', 660, 1704, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I Download and Installed CDH 5 Package succesfully on a Single Linux Node in Pseudo-distributed Mode on my CentOS 6.5

Starting Hadoop and Verifying it is Working Properly as in the below link

http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Quick-Start/cdh5qs_prereq.html#../CDH5-Quick-Start/../CDH5-Requirements-and-Supported-Versions/../CDH5-Requirements-and-Supported-Versions/../CDH5-Requirements-and-Supported-Versions/../CDH5-Requirements-and-Supported-Versions/../CDH5-Quick-Start/../CDH5-Quick-Start/../CDH5-Quick-Start/../CDH5-Quick-Start/../CDH5-Quick-Start/cdh5qs_mrv1_pseudo.html

I succesfully finished the following steps

Step 1: Format the NameNode.

Step 2: Start HDFS

Step 3: Create the /tmp Directory

Step 4: Create the MapReduce system directories:

Step 5: Verify the HDFS File Structure

Step 6: Start MapReduce

while following command in step 7 I get the following error.

Step 7: Create User Directories

***$ sudo -u hdfs hadoop fs -mkdir -p /user/hadoopuser***

***mkdir: ''/user/hadoopuser'': No such file or directory***

(where hadoopuser is my linux login username)

If I create the directory manually as /user/hadoopuser in the filesystem, it is not accepting.

How to success the step 7:?

Please provide the sloution to procced the remaining installation.', 1314, '2014-07-03 14:18:14.387', '73689669-b92f-4765-8a8e-f38b4b931ade', 661, 1705, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Challenge in Cloudera Hadoop CDH5 Installation', 1314, '2014-07-03 14:18:14.387', '73689669-b92f-4765-8a8e-f38b4b931ade', 661, 1706, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop>', 1314, '2014-07-03 14:18:14.387', '73689669-b92f-4765-8a8e-f38b4b931ade', 661, 1707, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Note that I am doing everything in R.

The problem goes as follow:

Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don''t. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation .

Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem.

My original idea:  *make this a supervised learning problem*.
Suppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those *unlabelled data*, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to.

Question 2: The tricky part is: how to **identify and extract the keywords** ? Using the `tm` package in R ? what algorithm is the `tm`   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well.


Any ideas would be great. Thanks

 ', 1315, '2014-07-03 16:11:22.637', '2d4b4082-f46e-4bfe-b376-12b3de493edb', 662, 1708, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What algorithms should I use to perform job classification based on resume data?', 1315, '2014-07-03 16:11:22.637', '2d4b4082-f46e-4bfe-b376-12b3de493edb', 662, 1709, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><nlp><text-mining>', 1315, '2014-07-03 16:11:22.637', '2d4b4082-f46e-4bfe-b376-12b3de493edb', 662, 1710, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Not enough rep for a comment, but check out: http://www.rdatamining.com/examples/text-mining

Here, they will take you through loading unstructured text to creating a wordcloud.  You can adapt this strategy and instead of creating a wordcloud, you can create a frequency matrix of terms used.  The idea is to take the unstructured text and structure it somehow.  You change everything to lowercase (or uppercase), remove stop words, and find frequent terms for each job function, via Document Term Matrices.  You also have the option of stemming the words.  If you stem words you will be able to detect different forms of words as the same word.  For example, ''programed'' and ''programming'' could be stemmed to ''program''.  You can possibly add the occurrence of these frequent terms as a weighted feature in your ML model training.

You can also adapt this to frequent phrases, finding common groups of 2-3 words for each job function.

I would post an example, but I don''t have access to my text mining code right now.', 375, '2014-07-03 17:06:13.143', '717d0fe6-4273-4e4c-b252-8ba9abcbf5ac', 663, 1711, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R base function glm() uses Fishers Scoring for MLE, while the glmnet uses the coordinate descent method to solve the same equation ? Coordinate descent is more time efficient than Fisher Scoring as fisher scoring calculates the second order derivative matrix and some other matrix operation which makes it space and time expensive, while coordinate descent can do the same task in O(np) time.

Why R base function uses Fisher Scoring or this method has advantage over other optimization methods? What will be comparison between coordinate descent and Fisher Scoring ? I am relatively new to do this field so any help or resource will be helpfu', 1319, '2014-07-03 17:11:01.770', '39fde80d-951d-41ad-b3bf-1a2443a25a40', 664, 1712, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Fisher Scoring v/s Coordinate Descent for MLE in R', 1319, '2014-07-03 17:11:01.770', '39fde80d-951d-41ad-b3bf-1a2443a25a40', 664, 1713, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><algorithms><optimization>', 1319, '2014-07-03 17:11:01.770', '39fde80d-951d-41ad-b3bf-1a2443a25a40', 664, 1714, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I download and installed CDH 5 package succesfully on a single linux node in pseudo-distributed Mode on my CentOS 6.5

Starting Hadoop and verifying it is Working Properly as in [this link][1]

I succesfully finished the following steps

Step 1: Format the NameNode.

Step 2: Start HDFS

Step 3: Create the /tmp Directory

Step 4: Create the MapReduce system directories:

Step 5: Verify the HDFS File Structure

Step 6: Start MapReduce

while following command in step 7 I get the following error.

Step 7: Create User Directories

***$ sudo -u hdfs hadoop fs -mkdir -p /user/hadoopuser***

***mkdir: ''/user/hadoopuser'': No such file or directory***

(where hadoopuser is my linux login username)

If I create the directory manually as /user/hadoopuser in the filesystem, it is not accepting.

How to success the step 7:?

Please provide the sloution to procced the remaining installation.


  [1]: http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/CDH5-Quick-Start/cdh5qs_mrv1_pseudo.html', 434, '2014-07-03 17:45:59.843', '371aca4d-4046-44d1-a758-a60fbd0694cc', 661, 'deleted 306 characters in body; edited title', 1715, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Cannot make user directory on a new CDH5 installation (Hadoop)', 434, '2014-07-03 17:45:59.843', '371aca4d-4046-44d1-a758-a60fbd0694cc', 661, 'deleted 306 characters in body; edited title', 1716, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('At each split point, CART will choose the feature which "best" splits the observations. What qualifies as best varies, but generally the split is done so that the subsequent nodes are more homogenous/pure with respect to the target. There are different ways of measuring homogeneity, for example Gini, Entropy, Chi-square. If you are using software, it may allow you to choose the measure of homogenity that the tree algorithm will use.

Distance is not a factor with trees - what matters is whether the value is greater than or less than the split point, not the distance from the split point.', 1111, '2014-07-03 17:52:00.963', '3fca4e87-895d-4016-9127-0139be01f2df', 665, 1717, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As far as gathering data goes, you can check out **Quandl** (there''s a tutorial on using it with **R** on [DataCamp](https://www.datacamp.com/courses/how-to-work-with-quandl-in-r) if you''re interested).

In addition, Aswath Damodaran''s [site](http://people.stern.nyu.edu/adamodar/New_Home_Page/data.html) contains a lot of helpful datasets. Though they aren''t updated that frequently, they may still be useful, especially as a benchmark for comparing your own output (from the scripts you will inevitably need to write to calculate the necessary metrics).

And, again, **Quant SE** is probably a better place to be looking...', 786, '2014-07-03 18:36:36.050', '03bb1ff3-2c4e-4148-bc5e-7997f3a57301', 666, 1718, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is a tricky problem. There are many ways to handle it. I guess, resumes can be treated as semi-structured documents. Sometimes, it''s beneficial to have some minimal structure in the documents. I believe, in resumes you would see some tabular data. You might want to treat these as attribute value pairs. For example, you would get a list of terms for the attribute "Skill set".

The key idea is to manually configure a list of key phrases such as "skill", "education", "publication" etc. The next step is to extract terms which pertain to these key phrases either by exploiting the structure in some way (such as tables) or by utilizing the proximity of terms around these key phrases, e.g. the fact that the word "Java" is in close proximity to the term "skill" might indicate that the person is skilled in Java.

After you extract these information, the next step could be to build up a feature vector for each of these key phrases. You can then represent a document as a vector with different fields (one each for a key phrase). For example, consider the following two resumes represented with two fields, namely *project* and *education*.

Doc1: {project: (java, 3) (c, 4)}, {education: (computer, 2), (physics, 1)}

Doc2: {project: (java, 3) (python, 2)}, {education: (maths, 3), (computer, 2)}

In the above example, I show a term with the frequency. Of course, while extracting the terms you need to stem and remove stop-words. It is clear from the examples that the person whose resume is Doc1 is more skilled in C than that of D2. Implementation wise, it''s very easy to represent documents as field vectors in Lucene.

Now, the next step is to retrieve a ranked list of resumes given a job specification. In fact, that''s fairly straight forward if you represent queries (job specs) as field vectors as well. You just need to retrieve a ranked list of candidates (resumes) using Lucene from a collection of indexed resumes.


', 984, '2014-07-03 20:47:20.147', '5e6e8972-786e-4f5e-9300-7b870fa55476', 667, 1720, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve done some research in this area. I''ve found first order Markov chains work well for predicting within game scoring dynamics across a variety of sports.

You can read in more detail here:
http://www.epjdatascience.com/content/3/1/4', 1329, '2014-07-03 20:51:59.510', '073477ea-7830-4101-9574-0d7294b5865b', 668, 1721, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Note that I am doing everything in R.

The problem goes as follow:

Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don''t. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation .

Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem.

My original idea:  *make this a supervised learning problem*.
Suppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those *unlabelled data*, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to.

**Update**
Question 2: Would it be a good idea to create an text file by extracting everything in a resume and print these data out in the text file, so that each resume is associated with a text file,which contains unstructured strings, and then we applied text mining techniques to the text files and make the data become structured or even to create a  frequency matrix of terms used out of the text files ? Is this approach wrong ? Please correct me if you think my approach is wrong.

Question 3: The tricky part is: how to **identify and extract the keywords** ? Using the `tm` package in R ? what algorithm is the `tm`   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well.


Any ideas would be great. Thanks

 ', 1315, '2014-07-03 22:10:08.953', 'd65e227a-e75c-4316-b010-03ab1307e6fd', 662, 'Added new question: question 2', 1722, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Not enough rep for a comment, but check out: http://www.rdatamining.com/examples/text-mining

Here, they will take you through loading unstructured text to creating a wordcloud.  You can adapt this strategy and instead of creating a wordcloud, you can create a frequency matrix of terms used.  The idea is to take the unstructured text and structure it somehow.  You change everything to lowercase (or uppercase), remove stop words, and find frequent terms for each job function, via Document Term Matrices.  You also have the option of stemming the words.  If you stem words you will be able to detect different forms of words as the same word.  For example, ''programed'' and ''programming'' could be stemmed to ''program''.  You can possibly add the occurrence of these frequent terms as a weighted feature in your ML model training.

You can also adapt this to frequent phrases, finding common groups of 2-3 words for each job function.

Example:

1) Load libraries and build the example data

    library(tm)
    library(SnowballC)

    doc1 = "I am highly skilled in Java Programming.  I have spent 5 years developing bug-tracking systems and creating data managing system applications in C."
    job1 = "Software Engineer"
    doc2 = "Tested new software releases for major program enhancements.  Designed and executed test procedures and worked with relational databases.  I helped organize and lead meetings and work independently and in a group setting."
    job2 = "Quality Assurance"
    doc3 = "Developed large and complex web applications for client service center. Lead projects for upcoming releases and interact with consumers.  Perform database design and debugging of current releases."
    job3 = "Software Engineer"
    jobInfo = data.frame("text" = c(doc1,doc2,doc3),
                         "job" = c(job1,job2,job3))

2) Now we do some text structuring. I am positive there are quicker/shorter ways to do the following.

    # Convert to lowercase
    jobInfo$text = sapply(jobInfo$text,tolower)

    # Remove Punctuation
    jobInfo$text = sapply(jobInfo$text,function(x) gsub("[[:punct:]]"," ",x))

    # Remove extra white space
    jobInfo$text = sapply(jobInfo$text,function(x) gsub("[ ]+"," ",x))

    # Remove stop words
    jobInfo$text = sapply(jobInfo$text, function(x){
      paste(setdiff(strsplit(x," ")[[1]],stopwords()),collapse=" ")
    })

    # Stem words (Also try without stemming?)
    jobInfo$text = sapply(jobInfo$text, function(x)  {
      paste(setdiff(wordStem(strsplit(x," ")[[1]]),""),collapse=" ")
    })

3) Make a corpus source and document term matrix.

    # Create Corpus Source
    jobCorpus = Corpus(VectorSource(jobInfo$text))

    # Create Document Term Matrix
    jobDTM = DocumentTermMatrix(jobCorpus)

    # Create Term Frequency Matrix
    jobFreq = as.matrix(jobDTM)

Now we have the frequency matrix, jobFreq, that is a (3 by x) matrix, 3 entries and X number of words.

Where you go from here is up to you.  You can keep only specific (more common) words and use them as features in your model.  Another way is to keep it simple and have a percentage of words used in each job description, say "java" would have 80% occurrence in ''software engineer'' and only 50% occurrence in ''quality assurance''.

Now it''s time to go look up why ''assurance'' has 1 ''r'' and ''occurrence'' has 2 ''r''s.

', 375, '2014-07-03 22:52:29.253', 'f0e4f213-3511-48f3-b6ea-124c6851cdf6', 663, 'Added Example', 1723, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The results you''re seeing aren''t a byproduct of your training product, but rather that `neural nets` are not a great choice for this task. `Neural nets` are effectively a means to create a high order non-linear function by composing a number of simpler functions. This is often a really good thing, because it allows neural nets to fit very complex patterns.

However, in a stock exchange any complex pattern, when traded upon will quickly decay. Detecting a complicated pattern will generally not generate useful results, because it is typically complex patterns in the short term. Additionally, depending on the metric you choose, there are a number of ways of performing well that actually won''t pay off in investing (such as just predicting the last value in your example).

In addition the stock market is startlingly chaotic which can result in a `neural net` overfitting. This means that the patterns it learns will generalize poorly. Something along the lines of just seeing a stock decrease over a day and uniformly deciding that the stock will always decrease just because it was seen on a relatively short term. Instead techniques like `ridge` and `robust regression`, which will identify more general, less complex patterns, do better.

The winner of a similar Kaggle competition used `robust regression` for this very reason. You are likely to see better results if you switch to a shallow learning model that will find functions of a lower polynomial order, over the deep complex functions of a neural net.', 548, '2014-07-03 23:00:42.350', '11ff1261-e5fe-4ad3-8245-865435ad02bb', 669, 1724, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Note that I am doing everything in R.

The problem goes as follow:

Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don''t. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation .

Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem.

My original idea:  *make this a supervised learning problem*.
Suppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those *unlabelled data*, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to.

**Update**
Question 2: Would it be a good idea to create an text file by extracting everything in a resume and print these data out in the text file, so that each resume is associated with a text file,which contains unstructured strings, and then we applied text mining techniques to the text files and make the data become structured or even to create a  frequency matrix of terms used out of the text files ? For example, the text file may look something like this:

`I deployed ML algorithm in this project and... Skills: Java, Python, c++ ...`

This is what I meant by ''unstructured'', i.e. collapsing everything into a single line string.

Is this approach wrong ? Please correct me if you think my approach is wrong.

Question 3: The tricky part is: how to **identify and extract the keywords** ? Using the `tm` package in R ? what algorithm is the `tm`   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well.


Any ideas would be great. Thanks

 ', 1315, '2014-07-04 00:14:16.620', '6e222939-6f23-4da8-b227-00577075d170', 662, 'added 240 characters in body', 1725, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was building a model that predicts user churn for a website, where I have data on all users, both past and present.

I can build a model that only uses those users that have left, but then I''m leaving 2/3 of the total user population unused.

Is there a good way to incorporate data from these users into a model from a conceptual standpoint?', 1334, '2014-07-04 02:31:42.080', 'ff925324-4438-4f63-9f25-d78aac34e6cb', 670, 1726, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Dealing with events that have not yet happened', 1334, '2014-07-04 02:31:42.080', 'ff925324-4438-4f63-9f25-d78aac34e6cb', 670, 1727, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 1334, '2014-07-04 02:31:42.080', 'ff925324-4438-4f63-9f25-d78aac34e6cb', 670, 1728, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Dealing with events that have not yet happened when building a model', 1334, '2014-07-04 02:38:12.523', '65a6139d-dfd8-4f85-b14c-3279f112dbc6', 670, 'edited title', 1729, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve a linearly increasing time series dataset of a sensor with value ranges between 50 to 150. On which I''ve implemented [Simple Linear Regression][1] algorithm to fit a regression line, and I''m predicting the date when the series would reach 120.

All works fine when the series move upwards. But, there are cases when the sensor reaches around 110 or 115, the sensor would be reset and the values start over again at say 50 or 60...

This is where I start facing issues with the regression line as it starts moving downwards, and it starts predicting old date. I think I should be considering only the subset of data from where it was previously reset. However, I''m trying to understand if there are any algorithm available that considers this case.

I''m new to data science, would appreciate any pointers to move further.

Thanks
ArunDhaJ


  [1]: http://en.wikipedia.org/wiki/Simple_linear_regression', 870, '2014-07-04 05:12:44.707', '9b662400-be8f-4464-bd7a-40ed3c03212b', 671, 1730, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Linearly increasing data with manual reset', 870, '2014-07-04 05:12:44.707', '9b662400-be8f-4464-bd7a-40ed3c03212b', 671, 1731, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><statistics><time-series>', 870, '2014-07-04 05:12:44.707', '9b662400-be8f-4464-bd7a-40ed3c03212b', 671, 1732, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This setting is common in reliability, health care, and mortality. The statistical analysis method is called [Survival Analysis](http://en.wikipedia.org/wiki/Survival_analysis). All users are coded according to their start date (or week or month).  You use the empirical data to estimate the survival function, which is the probability that the time of defection is later than some specified time ***t***.

Your baseline model will estimate survival function for all users.  Then you can do more sophisticated modeling to estimate what factors or behaviors might predict defection (churn), given your baseline survival function.  Basically, any model that is predictive will yield a survival probability that is significantly lower than the baseline.

----

There''s another approach which involves attempting to identify precursor events patterns or user behavior pattern that foreshadow defection. Any given event/behavior pattern might occur for users that defect, or for users that stay. For this analysis, you may need to censor your data to only include users that have been members for some minimum period of time. The minimum time period can be estimated using your estimate of survival function, or even simple histogram analysis of the distribution of membership period for users who have defected. ', 609, '2014-07-04 06:46:19.180', 'c9120954-597c-435b-9a20-ad8ec13c867e', 672, 1733, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Just extract **keywords** and train **classifier** on them. That''s all, really.

Most of the text in CVs is not actually related to skills. E.g. consider sentense "I''m experienced and highly efficient in Java". Here only 1 out of 7 words is a skill name, the rest is just a noise that''s going to put your classification accuracy down.

Most of CVs are not really structured. Or structured too freely. Or use unusual names for sections. Or file formats that don''t preserve structure when translated to text. I have experience extracting dates, times, names, addresses and even people intents from unstructured text, but not skill (or university or anything) list, not even closely.

So just tokenize (and possibly [stem](http://stackoverflow.com/questions/16069406/text-mining-with-the-tm-package-word-stemming)) your CVs, select only words from predefined list (you can use LinkedIn or something similar to grab this list), create feature vector and try out a couple of classifiers (say, SVM and Naive Bayes).

(Note: I used similar approach to classify LinkedIn profiles into more than 50 classes with accuracy > 90%, so I''m pretty sure even naive implementation will work well.)', 1279, '2014-07-04 22:46:32.130', '9650a3d2-211e-494c-801a-9983e19a98c0', 675, 1741, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Just extract **keywords** and train a **classifier** on them. That''s all, really.

Most of the text in CVs is not actually related to skills. E.g. consider sentence "I''m experienced and highly efficient in Java". Here only 1 out of 7 words is a skill name, the rest is just a noise that''s going to put your classification accuracy down.

Most of CVs are not really structured. Or structured too freely. Or use unusual names for sections. Or file formats that don''t preserve structure when translated to text. I have experience extracting dates, times, names, addresses and even people intents from unstructured text, but not a skill (or university or anything) list, not even closely.

So just tokenize (and possibly [stem](http://stackoverflow.com/questions/16069406/text-mining-with-the-tm-package-word-stemming)) your CVs, select only words from predefined list (you can use LinkedIn or something similar to grab this list), create a feature vector and try out a couple of classifiers (say, SVM and Naive Bayes).

(Note: I used a similar approach to classify LinkedIn profiles into more than 50 classes with accuracy > 90%, so I''m pretty sure even naive implementation will work well.)', 434, '2014-07-05 08:31:38.717', '7d0161a8-539d-496c-8fcd-6e5a23ce36fa', 675, 'minor grammar and spelling', 1744, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In general regression models (any) can behave in an arbitrary way beyond the domain spanned by training samples. In particular, they are free to assume linearity of the modeled function, so if you for instance train a regression model with points:

    X     Y
    10    0
    20    1
    30    2

it is reasonable to build a model `f(x) = x/10-1`, which for `x<10` returns negative values.

The same applies "in between" your data points, it is always possible that due to the assumed famility of functions (which can be modeled by particular method) you will get values "out of your training samples".

You can think about this in another way - "what is so special about negative values?", why do you find existance of negative values weird (if not provided in training set) while you don''t get alarmed by existance of lets say... value 2131.23? Unless developed in such a way, no model will treat negative values "different" than positive ones. This is just a natural element of the real values which can be attained as any other value.', 1355, '2014-07-05 14:48:10.570', '2e9c5e37-cdef-4ee0-9916-0864b58b1505', 676, 1745, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to use the sklearn_pandas module to extend the work I do in pandas and dip a toe into machine learning but I''m struggling with an error I don''t really understand how to fix.

I was working through the following dataset on [Kaggle][1].

It''s essentially an unheadered table (1000 rows, 40 features) with floating point values.


    import pandas as pdfrom sklearn import neighbors
    from sklearn_pandas import DataFrameMapper, cross_val_score
    path_train ="../kaggle/scikitlearn/train.csv"
    path_labels ="../kaggle/scikitlearn/trainLabels.csv"
    path_test = "../kaggle/scikitlearn/test.csv"

    train = pd.read_csv(path_train, header=None)
    labels = pd.read_csv(path_labels, header=None)
    test = pd.read_csv(path_test, header=None)
    mapper_train = DataFrameMapper([(list(train.columns),neighbors.KNeighborsClassifier(n_neighbors=3))])
    mapper_train

Output:

    DataFrameMapper(features=[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], KNeighborsClassifier(algorithm=''auto'', leaf_size=30, metric=''minkowski'',
           n_neighbors=3, p=2, weights=''uniform''))])

So far so good. But then I try the fit

    mapper_train.fit_transform(train, labels)

Output:

    ---------------------------------------------------------------------------
    TypeError                                 Traceback (most recent call last)
    <ipython-input-6-e3897d6db1b5> in <module>()
    ----> 1 mapper_train.fit_transform(train, labels)

    //anaconda/lib/python2.7/site-packages/sklearn/base.pyc in fit_transform(self, X, y,     **fit_params)
        409         else:
        410             # fit method of arity 2 (supervised transformation)
    --> 411             return self.fit(X, y, **fit_params).transform(X)
        412
        413

    //anaconda/lib/python2.7/site-packages/sklearn_pandas/__init__.pyc in fit(self, X, y)
        116         for columns, transformer in self.features:
        117             if transformer is not None:
    --> 118                 transformer.fit(self._get_col_subset(X, columns))
        119         return self
        120

    TypeError: fit() takes exactly 3 arguments (2 given)`

What am I doing wrong? While the data in this case is all the same, I''m planning to work up a workflow for mixtures categorical, nominal and floating point features and sklearn_pandas seemed to be a logical fit.

  [1]: https://www.kaggle.com/c/data-science-london-scikit-learn/data', 974, '2014-07-05 15:01:43.940', '61c782b1-3f8a-49cc-9b17-52aa706c32b6', 677, 1746, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Struggling to integrate sklearn and pandas in simple Kaggle task', 974, '2014-07-05 15:01:43.940', '61c782b1-3f8a-49cc-9b17-52aa706c32b6', 677, 1747, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><pandas><sklearn>', 974, '2014-07-05 15:01:43.940', '61c782b1-3f8a-49cc-9b17-52aa706c32b6', 677, 1748, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When I say "document", I have in mind web pages like Wikipedia articles and news stories.  I prefer answers giving either vanilla lexical distance metrics or state-of-the-art semantic distance metrics, with stronger preference for the latter.', 1097, '2014-07-05 16:10:21.580', 'ceaeca18-ecfe-4157-87f2-2aa3dd7e53a5', 678, 1749, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are some standard ways of computing the distance between documents?', 1097, '2014-07-05 16:10:21.580', 'ceaeca18-ecfe-4157-87f2-2aa3dd7e53a5', 678, 1750, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><nlp><text-mining>', 1097, '2014-07-05 16:10:21.580', 'ceaeca18-ecfe-4157-87f2-2aa3dd7e53a5', 678, 1751, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I made a similar question asking about distance between "documents" (Wikipedia articles, news stories, etc.).  I made this a separate question because search queries are considerably smaller than documents and are considerably noisier.  I hence don''t know (and doubt) if the same distance metrics would be used here.

Either vanilla lexical distance metrics or state-of-the-art semantic distance metrics are preferred, with stronger preference for the latter.', 1097, '2014-07-05 16:20:17.963', '3d013f83-dfe3-4367-9d9c-44ebaa18a290', 679, 1752, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are some standard ways of computing the distance between individual search queries?', 1097, '2014-07-05 16:20:17.963', '3d013f83-dfe3-4367-9d9c-44ebaa18a290', 679, 1753, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><nlp><search>', 1097, '2014-07-05 16:20:17.963', '3d013f83-dfe3-4367-9d9c-44ebaa18a290', 679, 1754, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''s a number of semantic distance measures, each with its pros and cons. Here are just a few of them:

 * [**cosine distance**](http://en.wikipedia.org/wiki/Cosine_similarity), inner product between document feature vectors;
 * [**LSA**](http://en.wikipedia.org/wiki/Latent_semantic_analysis), another vector-based model, but utilizing SVD for de-noising original term-document matrix;
 * [**WordNet**](http://en.wikipedia.org/wiki/WordNet)-based, human verified, though hardly extensible.

Start with a simplest approach and then move further based on issues for your specific case. ', 1279, '2014-07-06 00:01:41.327', 'ad960bc3-af51-4904-b4b3-1b12b1df6cd8', 680, 1755, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Michael Maouboussin, in his book, "The Success Equation," looks at differentiating luck from skill in various endeavors, including sports.  He actually ranks sports by the amount of luck that contributes to performance in the different sports (p. 23) and about 2/3 of performance in football is attributable to skill.  By contrast, I used MM''s technique to analyze performance in Formula 1 racing, and found that 60% is attributable to skill (less than I was expecting.)

That said, it seems this kind of analysis would imply that a sufficiently detailed and crafted feature set would allow ML algorithms to predict performance of NFL teams, perhaps even to the play level, with the caveat that significant variance will still exist because of the influence of luck in the game.', 1360, '2014-07-06 14:01:57.467', '3e39a53a-203a-4ad0-8c4a-0fa791491591', 682, 1759, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have 2 datasets, one with positive instances of what I would like to detect, and one with unlabeled instances. What methods can I use ?

As an example, suppose we want to understand detect spam email based on a few structured email characteristics. We have one dataset of 10000 spam emails, and one dataset of 100000 emails for which we don''t know whether they are spam or not.

How can we tackle this problem (without labeling manually any of the unlabeled data) ?

What can we do if we have additional information about the proportion of spam in the unlabeled data (i.e. what if we estimate that between 20-40% of the 100000 unlabeled emails are spam) ?
', 906, '2014-07-07 09:34:36.950', '20a707b9-769d-4c75-a090-0d1a79f655a9', 683, 1767, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Build a binary classifier with only positive and unlabeled data', 906, '2014-07-07 09:34:36.950', '20a707b9-769d-4c75-a090-0d1a79f655a9', 683, 1768, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 906, '2014-07-07 09:34:36.950', '20a707b9-769d-4c75-a090-0d1a79f655a9', 683, 1769, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have never used `sklearn_pandas`, but from reading their source code, it looks like this is a bug on their side. If you look for [the function that is throwing the exception][1], you can notice that they are discarding the `y` argument (it does not even survive until the docstring), and the inner `fit` function expects one argument more, which is probably `y`:

<!-- language: python -->

    def fit(self, X, y=None):
        ''''''
        Fit a transformation from the pipeline

        X       the data to fit
        ''''''
        for columns, transformer in self.features:
            if transformer is not None:
                transformer.fit(self._get_col_subset(X, columns))
        return self

I would recommend that you open an issue in [their bug tracker][2].


  [1]: https://github.com/paulgb/sklearn-pandas/blob/master/sklearn_pandas/__init__.py
  [2]: https://github.com/paulgb/sklearn-pandas/issues', 1367, '2014-07-07 10:25:55.323', 'ba3db5d5-da3d-43aa-962e-cc58e8799bb7', 684, 1770, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('i am very new to machine learning and in my first project have stumbled across a lot of issues which i really want to get through.

i m using logistic regression using R''s glmnet package with alpha = 0 for ridge regression

i m using ridge regression actually since lasso deleted all my variables and gave very low Area under Curve (.52)

but with ridge also, there aint much of a difference (.61)

my dependent variable/output is probability of click, based on if there a click or not in historical data

the independent variables are state, city, device, user age, user gender, IP carrier, keyword, mobile manufacturer, ad template, browser version, browser family, OS version, OS family

of these, in prediction i m using state, device, user age, user gender, IP carrier, browser version, browser family, OS version, OS family, I am not using keyword or template since we want to reject a user request before deep diving in our system and select a keyword or template. I am not using city because they are too many, or mobile manufacturer cuz they are too few

is it okay or should i be using these the rejected variables?

to go about, i create a sparse matrix from my variables which are mapped against the column of clicks that has yes or no yes or no.

so after training the model, i save the coefficients and intercept. these are used for new incoming requests using the formula for logistic that is
1/(1+e^-1*sum(a+k(ith)*x(ith)))
where a is intercept, k is the ith  coefficient and x is the ith variable value

please let me know if my approach is correct so far.

now simple glm in R (that is where there is no regularised regression, right?) gave me .56 AUC
with regularization i get .61 but there is no distinct threshold that we could say that okay above 0.xx its mostly 1s and below it most 0s are covered, actually max probability of where click didnt happened is almost always > max probability where click happened

so basically what should i do?

i have read how stochastic gradient descent is an effective technique in logit
so how to implement stochastic gradient descent in R?
if its not straightforward, is there a way to implement this system in python?
is SGD implemented after generating a regularized logistic regression model or is it a different process all together?

Also there is an algo called follow the regularized leader (FTRL) that is used in ctr prediction. is there a sample code and use of that it i could go through?', 1273, '2014-07-07 11:43:48.430', '3c17c75c-dd17-490a-a11d-6d7126a6628d', 685, 1771, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('SGD in logistic regression', 1273, '2014-07-07 11:43:48.430', '3c17c75c-dd17-490a-a11d-6d7126a6628d', 685, 1772, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><logistic-regression>', 1273, '2014-07-07 11:43:48.430', '3c17c75c-dd17-490a-a11d-6d7126a6628d', 685, 1773, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('i am very new to machine learning and in my first project have stumbled across a lot of issues which i really want to get through.

i m using logistic regression using R''s glmnet package with alpha = 0 for ridge regression

i m using ridge regression actually since lasso deleted all my variables and gave very low Area under Curve (.52)

but with ridge also, there aint much of a difference (.61)

my dependent variable/output is probability of click, based on if there a click or not in historical data

the independent variables are state, city, device, user age, user gender, IP carrier, keyword, mobile manufacturer, ad template, browser version, browser family, OS version, OS family

of these, in prediction i m using state, device, user age, user gender, IP carrier, browser version, browser family, OS version, OS family, I am not using keyword or template since we want to reject a user request before deep diving in our system and select a keyword or template. I am not using city because they are too many, or mobile manufacturer cuz they are too few

is it okay or should i be using these the rejected variables?

to go about, i create a sparse matrix from my variables which are mapped against the column of clicks that have yes or no values.

so after training the model, i save the coefficients and intercept. these are used for new incoming requests using the formula for logistic that is 1/(1+e^-1*sum(a+k(ith)*x(ith))) where a is intercept, k is the ith coefficient and x is the ith variable value

please let me know if my approach is correct so far.

now simple glm in R (that is where there is no regularised regression, right?) gave me .56 AUC with regularization i get .61 but there is no distinct threshold that we could say that okay above 0.xx its mostly 1s and below it most 0s are covered, actually max probability of where click didnt happened is almost always > max probability where click happened

so basically what should i do?

i have read how stochastic gradient descent is an effective technique in logit so how to implement stochastic gradient descent in R? if its not straightforward, is there a way to implement this system in python? is SGD implemented after generating a regularized logistic regression model or is it a different process all together?

Also there is an algo called follow the regularized leader (FTRL) that is used in ctr prediction. is there a sample code and use of it that i could go through?', 1273, '2014-07-07 11:54:21.357', '0ccc1bc8-7b2a-4541-93a7-209110965fe9', 685, 'deleted 12 characters in body', 1774, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('State of the art appears to be "paragraph vectors" introduced in a recent paper: http://cs.stanford.edu/~quocle/paragraph_vector.pdf. Cosine/Euclidean distance between paragraph vectors would likely work better than any other approach. This probably isn''t feasible yet due to lack of open source implementations.

Next best thing is cosine distance between LSA vectors or cosine distance between raw BOW vectors. Sometimes it works better to choose different weighting schemes, like TF-IDF. ', 574, '2014-07-07 13:23:28.220', 'cd2a34ea-23be-450e-8b8a-194bb19d56e7', 686, 1775, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have never used `sklearn_pandas`, but from reading their source code, it looks like this is a bug on their side. If you look for [the function that is throwing the exception][1], you can notice that they are discarding the `y` argument (it does not even survive until the docstring), and the inner `fit` function expects one argument more, which is probably `y`:

<!-- language: python -->

    def fit(self, X, y=None):
        ''''''
        Fit a transformation from the pipeline

        X       the data to fit
        ''''''
        for columns, transformer in self.features:
            if transformer is not None:
                transformer.fit(self._get_col_subset(X, columns))
        return self

I would recommend that you open an issue in [their bug tracker][2].

**UPDATE**:

You can test this if you run your code from IPython. To summarize, if you use the `%pdb on` magic right before you run the problematic call, the exception is captured by the Python debugger, so you can play around a bit and see that calling the `fit` function with the label values `y[0]` does work  -- see the last line with the `pdb> ` prompt. (The CSV files are downloaded from Kaggle, except for the largest one which is just a part of the real file).

    In [1]: import pandas as pd

    In [2]: from sklearn import neighbors

    In [3]: from sklearn_pandas import DataFrameMapper, cross_val_score

    In [4]: path_train ="train.csv"

    In [5]: path_labels ="trainLabels.csv"

    In [6]: path_test = "test.csv"

    In [7]: train = pd.read_csv(path_train, header=None)

    In [8]: labels = pd.read_csv(path_labels, header=None)

    In [9]: test = pd.read_csv(path_test, header=None)

    In [10]: mapper_train = DataFrameMapper([(list(train.columns),neighbors.KNeighborsClassifier(n_neighbors=3))])

    In [13]: %pdb on

    In [14]: mapper_train.fit_transform(train, labels)
    ---------------------------------------------------------------------------
    TypeError                                 Traceback (most recent call last)
    <ipython-input-14-e3897d6db1b5> in <module>()
    ----> 1 mapper_train.fit_transform(train, labels)

    /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/base.pyc in fit_transform(self, X, y, **fit_params)
        409         else:
        410             # fit method of arity 2 (supervised transformation)
    --> 411             return self.fit(X, y, **fit_params).transform(X)
        412
        413

    /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn_pandas/__init__.pyc in fit(self, X, y)
        116         for columns, transformer in self.features:
        117             if transformer is not None:
    --> 118                 transformer.fit(self._get_col_subset(X, columns))
        119         return self
        120

    TypeError: fit() takes exactly 3 arguments (2 given)
    > /opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn_pandas/__init__.py(118)fit()
        117             if transformer is not None:
    --> 118                 transformer.fit(self._get_col_subset(X, columns))
        119         return self

    ipdb> l
        113
        114         X       the data to fit
        115         ''''''
        116         for columns, transformer in self.features:
        117             if transformer is not None:
    --> 118                 transformer.fit(self._get_col_subset(X, columns))
        119         return self
        120
        121
        122     def transform(self, X):
        123         ''''''
    ipdb> transformer.fit(self._get_col_subset(X, columns), y[0])
    KNeighborsClassifier(algorithm=''auto'', leaf_size=30, metric=''minkowski'',
               n_neighbors=3, p=2, weights=''uniform'')


  [1]: https://github.com/paulgb/sklearn-pandas/blob/master/sklearn_pandas/__init__.py
  [2]: https://github.com/paulgb/sklearn-pandas/issues', 1367, '2014-07-07 13:42:31.827', 'c267f08b-18f1-4655-956d-a0a95a3ec575', 684, 'Show debugging session', 1778, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My suggestion would be to attempt to build some kind of clustering on your unlabeled data that somewhat approximates a labelled dataset. The rationale is more or less as follows:

 - You have some feature vector for representing your documents
 - Based on that feature vector, you can come up with a number of different clusterings, with either fuzzy, rough, or class-based clustering methods
 - Knowing what a positive example looks like, you can quickly evaluate the overall similarity of a cluster to your positive cluster
 - Knowing that there should really only be two clusters, you can adjust the hyperparameters on your clustering method so that the above two metrics are closer and closer to satisfaction
 - With the two clusters, you have what is likely a close approximation of a labelled dataset, which you can then use as a silver-standard corpus of sorts to actually train your model

Hope that makes sense, if you''re specifically looking for clustering algorithms, a few that I personally enjoy that might be good in this scenario are [FLAME][1] and [tsne][2]. Alternately, looking at the spectacular [gensim][3] library in python will get you a long way toward the clustering you''re looking for.

Hope that helps and makes sense, leave a comment if you''ve got any questions.


  [1]: http://en.wikipedia.org/wiki/FLAME_clustering
  [2]: http://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding
  [3]: http://radimrehurek.com/gensim/', 548, '2014-07-07 14:15:51.117', 'c478b097-e601-4002-aded-0c3ac6f211f1', 687, 1779, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As @SpacedMan has noted [in a comment][1], the street layout will have a massive influence on the optimization of the walk list.  You have included only "latitude and longitude" in your question''s title; but solving that problem does not lead to a "walk list", but to a "as-the-crow-flies list".

Looking at your street layout as a graph, with edge weights describing distances, and trying to find the shortest traversal between all required addresses, will lead you to think of your problem as a "[Shortest path problem][2]". [Dijkstra''s algorithm][3] is the best known solution (there are others); in its naive implementation it converges in *O(n<sup>2</sup>)*, which may be acceptable if your lists of addresses are moderate in size.  Otherwise, look for optimized versions in the above links.

As for libraries and resources to start tackling the problem, since you do not specify languages or platforms, let me point to the [compilation of routing solvers in the Open Street Maps wiki][4] and in general [their frameworks and libraries page][5].


  [1]: http://datascience.stackexchange.com/questions/600/how-do-you-create-an-optimized-walk-list-given-longitude-and-latitude-coordinate#comment1670_600
  [2]: http://en.wikipedia.org/wiki/Shortest_path_problem
  [3]: http://en.wikipedia.org/wiki/Dijkstra%27s_algorithm
  [4]: http://wiki.openstreetmap.org/wiki/Routing#Developers
  [5]: http://wiki.openstreetmap.org/wiki/Frameworks', 1367, '2014-07-07 14:41:59.523', '3eb6736b-bf64-4197-8f8c-3d48342217fe', 688, 1780, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''s a number of different ways of going about this depending on exactly how much semantic information you want to retain and how easy your documents are to tokenize (html documents would probably be pretty difficult to tokenize, but you could conceivably do something with tags and context.)

Some of them have been mentioned by ffriend, and the paragraph vectors by user1133029 is a really solid one, but I just figured I would go into some more depth about plusses and minuses of different approaches.

 - [Cosine Distance][1] - Tried a true, cosine distance is probably the most common distance metric used generically across multiple domains. With that said, there''s very little information in cosine distance that can actually be mapped back to anything semantic, which seems to be non-ideal for this situation.
 - [Levenshtein Distance][2] - Also known as `edit distance`, this is usually just used on the individual token level (words, bigrams, etc...). In general I wouldn''t recommend this metric as it not only discards any semantic information, but also tends to treat very different word alterations very similarly, but it is an extremely common metric for this kind of thing
 - [LSA][3] - Is a part of a large arsenal of techniques when it comes to evaluating document similarity called `topic modeling`. LSA has gone out of fashion pretty recently, and in my experience, it''s not quite the strongest topic modeling approach, but it is relatively straightforward to implement and has a few open source implementations
 - [LDA][4] - Is also a technique used for `topic modeling`, but it''s different from `LSA` in that it actually learns internal representations that tend to be more smooth and intuitive. In general, the results you get from `LDA` are better for modeling document similarity than `LSA`, but not quite as good for learning how to discriminate strongly between topics.
 - [Pachinko Allocation][5] - Is a really neat extension on top of LDA. In general, this is just a significantly improved version of `LDA`, with the only downside being that it takes a bit longer to train and open-source implementations are a little harder to come by
 - [word2vec][6] - Google has been working on a series of techniques for intelligently reducing words and documents to more reasonable vectors than the sparse vectors yielded by techniques such as `Count Vectorizers` and `TF-IDF`. Word2vec is great because it has a number of open source implementations. Once you have the vector, any other similarity metric (like cosine distance) can be used on top of it with significantly more efficacy.
 - [doc2vec][7] - Also known as `paragraph vectors`, this is the latest and greatest in a series of papers by Google, looking into dense vector representations of documents. The `gensim` library in python has an implementation of `word2vec` that is straightforward enough that it can pretty reasonably be leveraged to build `doc2vec`, but make sure to keep the license in mind if you want to go down this route

Hope that helps, let me know if you''ve got any questions.


  [1]: http://en.wikipedia.org/wiki/Cosine_similarity
  [2]: http://en.wikipedia.org/wiki/Levenshtein_distance
  [3]: http://en.wikipedia.org/wiki/Latent_semantic_analysis
  [4]: http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation
  [5]: http://en.wikipedia.org/wiki/Pachinko_allocation
  [6]: https://code.google.com/p/word2vec/
  [7]: http://cs.stanford.edu/~quocle/paragraph_vector.pdf', 548, '2014-07-07 15:36:40.960', '98e684c7-917c-422d-ba44-881560cdad32', 689, 1781, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Train 2 generative models, one for each dataset (spam only, spam plus ham), that will give you the probability that a datapoint is drawn from the same probability distribution of the training data. Assign emails as spam or ham based on which model gives you the highest probability of the document arising from the training data used to train it. Example generative models are RBM''s, autoencoders (in that case, which model has the lowest reconstruction error). There are likely some bayesian generative models also that will assign a probability to a data point based on some training data.

The best option though would be to take time to curate a second dataset containing only ham. That will give you higher classification accuracy. Assuming a lower proportion of spam to ham emails, that should not be too hard. You can even use Mechanical Turk if you lack the time or resources (or interns \ graduates students or other cheap labor).', 1301, '2014-07-07 18:06:24.583', 'b0c3915c-31f5-4152-8738-5feebbed4571', 690, 1782, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Empirically I have found LSA vastly superior to LDA every time and on every dataset I have tried it on. I have talked to other people who have said the same thing. It''s also been used to win a number of the SemEval competitions for measuring semantic similarity between documents, often in combinations with a wordnet based measure, so I wouldn''t say it''s going out of fashion, or is definitely inferior to LDA, which is better for topic modelling and not semantic similarity in my experience, contrary to what some responders have stated.

If you use gensim (a python library), it has LSA, LDA and word2vec, so you can easily compare the 3. doc2vec is a cool idea, but does not scale very well and you will likely have to implement it yourself as I am unaware of any open source implementations. It does not scale well as for each document, a new and separate model has to be built using SGD, a slow machine learning algorithm. But it will probably give you the most accurate results. LSA and LDA also don''t scale well (word2vec does however), LDA scales worse in general. Gensim''s implementations are very fast however, as it uses iterative SVD.

One other note, if you use word2vec, you will still have to determine a way to compose vectors from documents, as it gives you a different vector per word. The simplest way to do this is to normalize each vector and take the mean over all word vectors in the document, or take a weighted mean by idf weighting of each word. So it''s not as simple as ''use word2vec'', you will need to do something further to compute document similarity.

I would personally go with LSA, as I have seen it work well empirically, and gensim''s library scales very well. However, there''s no free lunch, so preferably try each method and see which works better for your data.', 1301, '2014-07-07 18:20:36.090', '2c9db33a-02b0-4a13-99cd-c574373e8dea', 691, 1783, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The cosine similarity metric does a good (if not perfect) job of controlling for the document length, so comparing the similarity of 2 documents or 2 queries using the cosine metric and tf idf weights for the words should work well in either case. I would also recommend doing LSA first on tf idf weights, and then computing the cosine distance\similarities.

If you are trying to build a search engine, I would recommend using a free open source search engine like solr or elastic search, or just the raw lucene libraries, as they do most of the work for you, and have good built in methods for handling the query to document similarity problem.', 1301, '2014-07-07 18:28:50.420', '572437c1-bf5a-4519-8e48-6e82800c6470', 692, 1784, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I work for an online jobs site and we build solutions to recommend jobs based on resumes. Our approach take''s a person''s job title (or desired job title if a student and known), along with skills we extract from their resume, and their location (which is very important to most people) and find matches with jobs based on that.

in terms of document classification, I would take a similar approach. I would recommend computing a tf idf matrix for each resume as a standard bag of words model, extracting just the person''s job title and skills (for which you will need to define a list of skills to look for), and feed that into a ML algorithm. I would recommend trying knn, and an SVM, the latter works very well with high dimensional text data. Linear SVM''s tend to do better than non-linear (e.g. using RBf kernels). If you have that outputting reasonable results, I would then play with extracting features using a natural language parser \ chunker, and also some custom built phrases matched by regex''s.', 1301, '2014-07-07 18:36:23.430', '9ce5a69f-7baa-4368-8688-1f6e6e9f1209', 693, 1785, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m using Neural Networks to solve different Machine learning problems. I''m using Python and [pybrain][1] but this library is almost discontinued. Are there other good alternatives in Python?

Thanks


  [1]: http://pybrain.org/', 989, '2014-07-07 19:17:04.973', 'b07cc531-109e-4364-a55f-e945d118d409', 694, 1786, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Best python library for neural networks', 989, '2014-07-07 19:17:04.973', 'b07cc531-109e-4364-a55f-e945d118d409', 694, 1787, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><python><neuralnetwork>', 989, '2014-07-07 19:17:04.973', 'b07cc531-109e-4364-a55f-e945d118d409', 694, 1788, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Pylearn2](http://deeplearning.net/software/pylearn2/) is generally considered the library of choice for neural networks and deep learning in python.  Its designed for easy scientific experimentation rather than ease of use, so the learning curve is rather steep, but if you take your time and follow the tutorials I think you''ll be happy with the functionality it provides.  Everything from standard Multilayer Perceptrons to Restricted Boltzmann Machines to Convolutional Nets to Autoencoders are provided.  There''s great GPU support and everything is built on top of Theano, so performance is typically quite good.

Be aware that Pylearn2 has the opposite problem of pybrain at the moment -- rather than being abandoned, Pylearn2 is under active development and is subject to frequent changes.', 684, '2014-07-07 19:55:51.057', '83fad451-79eb-4c22-8607-18fbf0af94b3', 695, 1789, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am very new to machine learning and in my first project have stumbled across a lot of issues which I really want to get through.

I''m using logistic regression with R''s `glmnet` package and alpha = 0 for ridge regression.

I''m using ridge regression actually since lasso deleted all my variables and gave very low area under curve (0.52) but with ridge there isn''t much of a difference (0.61).

My dependent variable/output is probability of click, based on if there is a click or not in historical data.

The independent variables are state, city, device, user age, user gender, IP carrier, keyword, mobile manufacturer, ad template, browser version, browser family, OS version and OS family.

Of these, for prediction I''m using state, device, user age, user gender, IP carrier, browser version, browser family, OS version and OS family; I am not using keyword or template since we want to reject a user request before deep diving in our system and selecting a keyword or template. I am not using city because they are too many or mobile manufacturer because they are too few.

**Is that okay or should I be using the rejected variables?**

To start, I create a sparse matrix from my variables which are mapped against the column of clicks that have yes or no values.

After training the model, I save the coefficients and intercept. These are used for new incoming requests using the formula for logistic regression:

> ![1 / (1+e^-1*sum(a+k(ith)*x(ith)))][1]

Where `a` is intercept, `k` is the `i`th coefficient and `x` is the `i`th variable value.

**Is my approach correct so far?**

Simple GLM in R (that is where there is no regularized regression, right?) gave me 0.56 AUC. With regularization I get 0.61 but there is no distinct threshold where we could say that above 0.xx its mostly ones and below it most zeros are covered; actually, the max probability that a click didn''t happen is almost always greater than the max probability that a click happened.

**So basically what should I do?**

I have read how stochastic gradient descent is an effective technique in logit so how do I implement stochastic gradient descent in R? If it''s not straightforward, is there a way to implement this system in Python? Is SGD implemented after generating a regularized logistic regression model or is it a different process altogether?

Also there is an algorithm called follow the regularized leader (FTRL) that is used in click-through rate prediction. Is there a sample code and use of FTRL that I could go through?


  [1]: http://i.stack.imgur.com/AZBRq.png', 322, '2014-07-07 20:02:08.777', '0cdf5161-92ad-4c9d-8234-2a8f0363f85b', 685, 'grammar overhaul, well-formatted formula, add tags', 1791, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Stochastic gradient descent in logistic regression', 322, '2014-07-07 20:02:08.777', '0cdf5161-92ad-4c9d-8234-2a8f0363f85b', 685, 'grammar overhaul, well-formatted formula, add tags', 1792, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><r><logistic-regression><gradient-descent>', 322, '2014-07-07 20:02:08.777', '0cdf5161-92ad-4c9d-8234-2a8f0363f85b', 685, 'grammar overhaul, well-formatted formula, add tags', 1793, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-07 20:02:08.777', '0cdf5161-92ad-4c9d-8234-2a8f0363f85b', 685, 'Proposed by 322 approved by 434, 1273 edit id of 109', 1794, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to run some analysis with some big datasets (eg 400k rows vs. 400 columns) with R (e.g. using neural networks and recommendation systems).
But, it''s taking too long to process the data (with huge matrices, e.g. 400k rows vs. 400k columns).
What are some free/cheap ways to improve R performance?

I''m accepting packages or web services suggestions (other options are welcome).
', 199, '2014-07-07 21:26:36.830', '34a1f41f-2ec8-4b3d-a5c9-15ede509c5ef', 697, 1796, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Running huge datasets with R', 199, '2014-07-07 21:26:36.830', '34a1f41f-2ec8-4b3d-a5c9-15ede509c5ef', 697, 1797, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><r><optimization><processing>', 199, '2014-07-07 21:26:36.830', '34a1f41f-2ec8-4b3d-a5c9-15ede509c5ef', 697, 1798, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('They can''t predict, but they can tell you the most likely result. There''s an study about this kind of approach from **Etienne** - [Predicting Who Will Win the World Cup with Wolfram Language][1]. This is a very detailed study, so you can check all the methodology used to get the predictions.


Interesting enough, 11 from 15 matches were correct!

> As one might expect, Brazil is the favorite, with a probability to win of 42.5%. This striking result is due to the fact that Brazil has both the highest Elo ranking and plays at home.


(Let''s go Brazil!)


  [1]: http://blog.wolfram.com/2014/06/20/predicting-who-will-win-the-world-cup-with-wolfram-language/', 1379, '2014-07-08 02:08:04.560', '46435fa7-87b4-47b5-a43d-5d7e32d2bbd8', 698, 1800, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From my experience only some classes of queries can be classified on lexical features (due to ambiguity of natural language). Instead you can try to use boolean search results (sites or segments of sites, not documents, without ranking) as features for classification (instead on words). This approach works well in classes where there is a big lexical ambiguity in a query but exists a lot of good sites relevant to the query (e.g. movies, music, commercial queries and so on).

Also, for offline classification you can do LSI on query-site matrix. See "Introduction to Information Retrieval" book for details.', 1384, '2014-07-08 06:40:36.923', '7a197e5c-12e6-4386-b021-f1fa7ae9aa21', 699, 1801, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a set of datapoints from the unit interval (i.e. 1-dimensional dataset with numerical values). I receive some additional datapoints online, and moreover the value of some datapoints might change dynamically. I''m looking for an ideal clustering algorithm which can handle these issues efficiently.

I know [sequential k-means clustering](https://www.cs.princeton.edu/courses/archive/fall08/cos436/Duda/C/sk_means.htm) copes with the addition of new instances, and I suppose with minor modification it can work with dynamic instance values (i.e. first taking the modified instance from the respective cluster, then updating the mean of the cluster and finally giving the modified instance as an input to the algorithm just as the addition of an unseen instance).

My concern with using the k-means algorithm is the requirement of supplying the number of clusters as an input. I know that they beat other clustering algorithms (GAs, MSTs, Hierarchical Methods etc.) in time&space complexity. Honestly I''m not sure, but maybe I can get away with using one of the aforementioned algorithms. Even that my datasets are relatively large, the existence of a single dimension makes me wonder.

More specifically a typical test case of mine would contain about 10K-200K 1-dimensional datapoints. I would like to complete the clustering preferably under a second. The dynamic changes in the value points are assumed to be smooth, i.e. relatively small. Thus being able to use existing solutions (i.e. being able to continue clustering on the existing one when a value is changed or new one is added) is highly preferred.

So all in all:
> Can you think of an algorithm which will provide a sweet spot between computational efficiency and the accuracy of clusters wrt. the problem defined above?

> Are there some nice heuristics for the k-means algorithm to automatically compute the value of K beforehand?', 1386, '2014-07-08 07:29:34.167', 'fc33a836-7cb9-4a5d-8e6d-8cbe6a6e8e66', 700, 1802, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Efficient dynamic clustering', 1386, '2014-07-08 07:29:34.167', 'fc33a836-7cb9-4a5d-8e6d-8cbe6a6e8e66', 700, 1803, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><algorithms><clustering><k-means><hierarchical-data-format>', 1386, '2014-07-08 07:29:34.167', 'fc33a836-7cb9-4a5d-8e6d-8cbe6a6e8e66', 700, 1804, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have generated a dataset of pairwise distances as follows:

    id_1 id_2 dist_12
    id_2 id_3 dist_23

I want to cluster this data so as to identify the pattern. I have been looking at Spectral clustering and DBSCAN, but I haven''t been able to come to a conclusion and have been ambiguous on how to make use of the existing implementations of these algorithms. I have been looking at Python and Java implementations so far.

Could anyone point me to a tutorial or demo on how to make use of these clustering algorithms to handle the situation in hand?

Thanks,
TM', 645, '2014-07-08 07:37:57.123', '62f04c14-e734-47e7-a561-32e97ef813a4', 701, 1805, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Clustering pair-wise distance dataset', 645, '2014-07-08 07:37:57.123', '62f04c14-e734-47e7-a561-32e97ef813a4', 701, 1806, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering>', 645, '2014-07-08 07:37:57.123', '62f04c14-e734-47e7-a561-32e97ef813a4', 701, 1807, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In the scikit-learn implementation of Spectral clustering and DBSCAN you do not need to precompute the distances, you should input the sample coordinates for all `id_1` ... `id_n`.  Here is a simplification of the [documented example comparison of clustering algorithms][1]:

    import numpy as np
    from sklearn import cluster
    from sklearn.preprocessing import StandardScaler

    ## Prepare the data
    X = np.random.rand(1500, 2)
    # When reading from a file of the form: `id_n coord_x coord_y`
    # you will need this call instead:
    # X = np.loadtxt(''coords.csv'', usecols=(1, 2))
    X = StandardScaler().fit_transform(X)

    ## Instantiate the algorithms
    spectral = cluster.SpectralClustering(n_clusters=2,
                                          eigen_solver=''arpack'',
                                          affinity="nearest_neighbors")
    dbscan = cluster.DBSCAN(eps=.2)

    ## Use the algorithms
    spectral_labels = spectral.fit_predict(X)
    dbscan_labels = dbscan.fit_predict(X)


  [1]: http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html', 1367, '2014-07-08 09:18:17.990', '7ff769a1-d669-47d3-8b50-84d6a1513863', 702, 1808, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Pylearn is relies on Theano and as mentioned in other answer to use the library is really complicated, until you get the hold of it.

In the meantime I would suggest using [Theanets][1]. It aslo built on top of Theano, but is much more easier to work with. It might be true, that it doesn''t have all the features of Pylearn, but for the basic work it''s sufficient.

Also it''s open source, so you can add custom networks on the fly, if you dare. :)


  [1]: https://github.com/lmjohns3/theano-nets/', 1390, '2014-07-08 10:36:44.220', 'ffb23650-6e09-4487-9613-c1120bd62f20', 703, 1809, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In my Uni we have an HPC computing clusters. I use the cluster to train classifiers and so on. So usually to send a job to the cluster, (e.g. python scikit-learn script), I need to write a Bash scrip file that contains (among other lines) a line like this `qsub script.py`.

However I find this process very very frustrating. Usually what happens is that I write the python script on my laptop and then I login to the server and update the SVN repository so I get the same python script there. Then I write that Bash script or edit it. Then I run the bash script. As you see this is really frustrating, since for every little update for the python script I need to do many steps in order to have it executed at the computing clusters. Of course the task gets even more complicated when I have to put the data on the server and use the datasets'' path on the server.

I''m sure many people here are using computing clusters for their data science tasks. I just want to know how you guys manage sending the jobs to the clusters?', 728, '2014-07-08 13:45:07.583', '859d33ca-9a84-472b-b8c1-dc0ccf9b932a', 704, 1811, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Working with HPC clusters', 728, '2014-07-08 13:45:07.583', '859d33ca-9a84-472b-b8c1-dc0ccf9b932a', 704, 1812, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><data-mining>', 728, '2014-07-08 13:45:07.583', '859d33ca-9a84-472b-b8c1-dc0ccf9b932a', 704, 1813, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Stochastic gradient descent is a method of setting the parameters of the regressor; since the objective for logistic regression is convex (has only one maximum), this won''t be an issue and SGD is generally only needed to improve convergence speed with masses of training data.

What your numbers suggest to me is that your features are not adequate to separate the classes. Consider adding extra features if you can think any any that are useful. You might also consider interactions and quadratic features in your original feature space.', 1399, '2014-07-08 14:32:55.827', 'b5a5f8d2-3f45-46c9-a4bf-e61abfb4ca37', 705, 1814, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[Pylearn2](http://deeplearning.net/software/pylearn2/) is generally considered the library of choice for neural networks and deep learning in python.  Its designed for easy scientific experimentation rather than ease of use, so the learning curve is rather steep, but if you take your time and follow the tutorials I think you''ll be happy with the functionality it provides.  Everything from standard Multilayer Perceptrons to Restricted Boltzmann Machines to Convolutional Nets to Autoencoders are provided.  There''s great GPU support and everything is built on top of Theano, so performance is typically quite good.  The source for Pylearn2 is available [on github](https://github.com/lisa-lab/pylearn2).

Be aware that Pylearn2 has the opposite problem of pybrain at the moment -- rather than being abandoned, Pylearn2 is under active development and is subject to frequent changes.', 684, '2014-07-08 14:39:21.820', '035aa9ff-6a0d-4e41-8cdf-bf779a2a3e04', 695, 'added 89 characters in body', 1815, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have generated a dataset of pairwise distances as follows:

    id_1 id_2 dist_12
    id_2 id_3 dist_23

I want to cluster this data so as to identify the pattern. I have been looking at Spectral clustering and DBSCAN, but I haven''t been able to come to a conclusion and have been ambiguous on how to make use of the existing implementations of these algorithms. I have been looking at Python and Java implementations so far.

Could anyone point me to a tutorial or demo on how to make use of these clustering algorithms to handle the situation in hand?', 84, '2014-07-09 00:13:52.097', 'b9b4ae3c-5425-4d32-a9fb-febf7d8c9654', 701, 'Improving formatting.', 1820, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-mining><clustering>', 84, '2014-07-09 00:13:52.097', 'b9b4ae3c-5425-4d32-a9fb-febf7d8c9654', 701, 'Improving formatting.', 1821, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Note that I am doing everything in R.

The problem goes as follow:

Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don''t. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation .

Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem.

My original idea:  *make this a supervised learning problem*.
Suppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those *unlabelled data*, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to.

**Update**
Question 2: Would it be a good idea to create an text file by extracting everything in a resume and print these data out in the text file, so that each resume is associated with a text file,which contains unstructured strings, and then we applied text mining techniques to the text files and make the data become structured or even to create a  frequency matrix of terms used out of the text files ? For example, the text file may look something like this:

`I deployed ML algorithm in this project and... Skills: Java, Python, c++ ...`

This is what I meant by ''unstructured'', i.e. collapsing everything into a single line string.

Is this approach wrong ? Please correct me if you think my approach is wrong.

Question 3: The tricky part is: how to **identify and extract the keywords** ? Using the `tm` package in R ? what algorithm is the `tm`   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well.


Any ideas would be great.

 ', 1352, '2014-07-09 00:19:01.640', '86406d8f-6544-45fc-83c4-a378dddf2e57', 662, 'removed "thanks"', 1822, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-09 00:19:01.640', '86406d8f-6544-45fc-83c4-a378dddf2e57', 662, 'Proposed by 1352 approved by 84 edit id of 108', 1823, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am new to machine learning. I have a task at hand of predicting click probability given user information like city, state, OS version, OS family, device, browser family, browser version, etc. I have been advised to try logit since logit seems to be what MS and Google are using. I have some questions regarding logistic regression:

Click and non click is a very very unbalanced class and the simple GLM predictions do not look good. How can I make the data work better with the GLM?

All the variables I have are categorical and things like device and city can be numerous. Also the frequency of occurrence of some devices or some cities can be very very low. How can I deal with this distribution of categorical variables?

One of the variables that we get is device ID. This is a very unique feature that can be translated to a user''s identity. How can I make use of it in logit, or should it be used in a completely different model based on user identity?', 322, '2014-07-09 00:19:12.683', '9634e5fa-6841-4049-a47d-ba5de4758c2a', 636, 'proofreading grammar', 1824, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Data preparation and machine learning algorithm for click prediction', 322, '2014-07-09 00:19:12.683', '9634e5fa-6841-4049-a47d-ba5de4758c2a', 636, 'proofreading grammar', 1825, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-09 00:19:12.683', '9634e5fa-6841-4049-a47d-ba5de4758c2a', 636, 'Proposed by 322 approved by 434, 84 edit id of 103', 1826, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m working on a fraud detection system. In this field, new frauds appear regularly, so that new features have to be added to the model on ongoing basis.

I wonder what is the best way to handle it (from the development process perspective)? Just adding a new feature into the feature vector and re-training the classifier seems to be a naive approach, because too much time will be spent for re-learning of the old features.

I''m thinking along the way of training a classifier for each feature (or a couple of related features), and then combining the results of those classifiers with an overall classifier. Are there any drawbacks of this approach? How can I choose an algorithm for the overall classifier?', 322, '2014-07-09 00:19:42.423', '80e9a95e-b9f3-4f6d-bf96-025d103ca3bd', 634, 'More explicit title, some grammar', 1827, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Handling a regularly increasing feature set', 322, '2014-07-09 00:19:42.423', '80e9a95e-b9f3-4f6d-bf96-025d103ca3bd', 634, 'More explicit title, some grammar', 1828, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-09 00:19:42.423', '80e9a95e-b9f3-4f6d-bf96-025d103ca3bd', 634, 'Proposed by 322 approved by 434, 84 edit id of 104', 1829, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would like to use another type of data, not atomic data, as a feature for a prediction.
Suppose I have a Table with those Features:
<pre>
- Column 1: Categorical - House
- Column 2: Numerical - 23.22
- Column 3: A Vector - [ 12, 22, 32 ]
- Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]
- Column 5: A List [ 122, Boolean ]
</pre>
I would like to predict/classify ... Columns 2 ... for example....

I am making a Software to automatically respond questions... Any type...like "Where Foo was Born ?" ...

I first make a query to a search engine --->>> then I get some Text data as a Result.
So I do all the Parsing Staff... Tagging, Stemming, Parsing, Splitting...
My first approach was to make a table, each row with a line of text.. and a lot of Features...like ... First Word ... Tag of First Word.. Chunks, etc..
But with this approach I am missing the relationships between the Sentences.

I would like to know if there is an algorithm that look inside the Tree Structures... Vectors... and make the relations and extract whatever is relevant for predicting/classifying.
I rather know a library that does that then an algorithm that I have to implement...

Thank you very much !', 1163, '2014-07-09 00:20:49.460', '157056ee-5e53-4de5-bcc1-9b4fd009751f', 658, 'Formatted text', 1830, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-09 00:20:49.460', '157056ee-5e53-4de5-bcc1-9b4fd009751f', 658, 'Proposed by 1163 approved by 434, -1 edit id of 107', 1831, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would like to use another type of data, not atomic data, as a feature for a prediction.
Suppose I have a Table with those Features:
<pre>
- Column 1: Categorical - House
- Column 2: Numerical - 23.22
- Column 3: A Vector - [ 12, 22, 32 ]
- Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]
- Column 5: A List [ 122, Boolean ]
</pre>
I would like to predict/classify ... Columns 2 ... for example....

I am making a Software to automatically respond questions... Any type...like "Where Foo was Born ?" ...

I first make a query to a search engine --->>> then I get some Text data as a Result.
So I do all the Parsing Staff... Tagging, Stemming, Parsing, Splitting...
My first approach was to make a table, each row with a line of text.. and a lot of Features...like ... First Word ... Tag of First Word.. Chunks, etc..
But with this approach I am missing the relationships between the Sentences.

I would like to know if there is an algorithm that look inside the Tree Structures... Vectors... and make the relations and extract whatever is relevant for predicting/classifying. I''d prefer to know about a library that does that than an algorithm that I have to implement.', 84, '2014-07-09 00:20:49.460', 'a63486a3-ea30-49c0-9dd3-7afe49823480', 658, 'Formatted text', 1832, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In my university, we have an HPC computing cluster. I use the cluster to train classifiers and so on. So, usually, to send a job to the cluster, (e.g. python scikit-learn script), I need to write a Bash script that contains (among others) a command like `qsub script.py`.

However, I find this process very very frustrating. Usually what happens is that I write the python script on my laptop and then I login to the server and update the SVN repository, so I get the same python script there. Then I write that Bash script or edit it, so I can run the bash script.

As you see this is really frustrating since, for every little update for the python script, I need to do many steps to have it executed at the computing cluster. Of course the task gets even more complicated when I have to put the data on the server and use the datasets'' path on the server.

I''m sure many people here are using computing clusters for their data science tasks. I just want to know how you guys manage sending the jobs to the clusters?', 84, '2014-07-09 00:25:47.190', '91f6ce21-8205-425d-8e7b-9be38d1b3fcf', 704, 'Improving formatting.', 1833, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try a recurrent neural network, a model well suited for time series data. They''re notoriously difficult to train, but seem to perform well when trained properly: http://cs229.stanford.edu/proj2012/BernalFokPidaparthi-FinancialMarketTimeSeriesPredictionwithRecurrentNeural.pdf', 684, '2014-07-09 00:37:51.167', '7eed256f-7943-4bfc-a8fa-8ec1ce8f3168', 707, 1834, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are many solutions to ease the burden of copying the file from a local machine to the computing nodes in the clusters. A simple approach is to use an interface that allows multi-access to the machines in the cluster, like [clusterssh](http://sourceforge.net/projects/clusterssh/) (cssh). It allows you to type commands to multiple machines at once via a set of terminal screens (each one a ssh connection to a different machine in the cluster).

Since your cluster seem to have `qsub` set up, your problem may be rather related to *replicating* the data along the machines (other than simply running a command in each node). So, to address this point, you may either write an `scp` script, to copy things to and from each node in the cluster (which is surely better addressed with SVN), or you may set up a NFS. This would allow for a simple and transparent access to the data, and also reduce the need for replicating unnecessary data.

For example, you could access a node, copy the data to such place, and simply use the data *remotely*, via network communication. I''m not acquainted with how to set up a NFS, but you already have access to it (in case your home folder is the same across the machines you access). Then, the scripts and data could be sent to a single place, and later accessed from others. This is akin to the SVN approach, except it''s more transparent/straightforward.', 84, '2014-07-09 00:44:28.207', '8cbda1f9-fe35-4c68-b83b-702ac15495a1', 708, 1835, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a linearly increasing time series dataset of a sensor, with value ranges between 50 and 150. I''ve implemented a [Simple Linear Regression][1] algorithm to fit a regression line on such data, and I''m predicting the date when the series would reach 120.

All works fine when the series move upwards. But, there are cases in which the sensor reaches around 110 or 115, and it is reset; in such cases the values would start over again at, say, 50 or 60.

This is where I start facing issues with the regression line, as it starts moving downwards, and it starts predicting old date. I think I should be considering only the subset of data from where it was previously reset. However, I''m trying to understand if there are any algorithms available that consider this case.

I''m new to data science, would appreciate any pointers to move further.

  [1]: http://en.wikipedia.org/wiki/Simple_linear_regression', 84, '2014-07-09 06:38:11.923', '11ade90a-b22d-4f11-9c47-f8fbe0b9700a', 671, 'Improving writing.', 1836, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Since you mention you are building a recommendation system, I believe you have a sparse matrix which you are working on. Check [sparseMatrix](http://stat.ethz.ch/R-manual/R-devel/library/Matrix/html/sparseMatrix.html) from Matrix package. This should be able to help you with storing your large size matrix in memory and train your model. ', 1131, '2014-07-09 07:08:34.740', '78b49eef-7e20-44b6-aba2-4fb8438d59c2', 709, 1837, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m looking for commercial text summarization tools (APIs, Libraries,...) which are able to perform any of the following tasks:

1. Extractive Multi-Document Summarization (Generic or query-based)
2. Extractive Single-Document Summarization (Generic or query-based)
3. Generative Single-Document Summarization (Generic or query-based)
4. Generative Multi-Document Summarization (Generic or query-based)

Thank you in advance.

Regards,
pasmod
', 979, '2014-07-09 11:05:40.813', 'b7a45c64-cab7-4f3d-811b-a65e90176df1', 710, 1838, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Commercial Text Summarization Tools', 979, '2014-07-09 11:05:40.813', 'b7a45c64-cab7-4f3d-811b-a65e90176df1', 710, 1839, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining>', 979, '2014-07-09 11:05:40.813', 'b7a45c64-cab7-4f3d-811b-a65e90176df1', 710, 1840, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This question is in response to a comment I saw on another question.

The comment was regarding the Machine Learning course syllabus on Coursera, and along the lines of "SVMs are not used so much nowadays".

I have only just finished the relevant lectures myself, and my understanding of SVMs is that they are a robust and efficient learning algorithm for classification, and that when using a kernel, they have a "niche" covering number of features perhaps 10 to 1000 and number of training samples perhaps 100 to 10,000. The limit on training samples is because the core algorithm revolves around optimising results generated from a square matrix with dimensions based on number of training samples, not number of original features.

So does the comment I saw refer some real change since the course was made, and if so, what is that change: A new algorithm that covers SVM''s "sweet spot" just as well, better CPUs meaning SVM''s computational advantages are not worth as much? Or is it perhaps opinion or personal experience of the commenter?

I tried a search for e.g. "are support vector machines out of fashion" and found nothing to imply they were being dropped in favour of anything else.

And Wikipedia has this: http://en.wikipedia.org/wiki/Support_vector_machine#Issues . . . the main sticking point appears to be difficulty of interpreting the model. Which makes SVM fine for a black-box predicting engine, but not so good for generating insights.', 836, '2014-07-09 12:22:22.400', 'ede783de-aeca-4026-8bae-e5e20cb9476c', 711, 1841, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are Support Vector Machines still considered "state of the art" in their niche?', 836, '2014-07-09 12:22:22.400', 'ede783de-aeca-4026-8bae-e5e20cb9476c', 711, 1842, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<svm>', 836, '2014-07-09 12:22:22.400', 'ede783de-aeca-4026-8bae-e5e20cb9476c', 711, 1843, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('This question is in response to a comment I saw on another question.

The comment was regarding the Machine Learning course syllabus on Coursera, and along the lines of "SVMs are not used so much nowadays".

I have only just finished the relevant lectures myself, and my understanding of SVMs is that they are a robust and efficient learning algorithm for classification, and that when using a kernel, they have a "niche" covering number of features perhaps 10 to 1000 and number of training samples perhaps 100 to 10,000. The limit on training samples is because the core algorithm revolves around optimising results generated from a square matrix with dimensions based on number of training samples, not number of original features.

So does the comment I saw refer some real change since the course was made, and if so, what is that change: A new algorithm that covers SVM''s "sweet spot" just as well, better CPUs meaning SVM''s computational advantages are not worth as much? Or is it perhaps opinion or personal experience of the commenter?

I tried a search for e.g. "are support vector machines out of fashion" and found nothing to imply they were being dropped in favour of anything else.

And Wikipedia has this: http://en.wikipedia.org/wiki/Support_vector_machine#Issues . . . the main sticking point appears to be difficulty of interpreting the model. Which makes SVM fine for a black-box predicting engine, but not so good for generating insights. I don''t see that as a major issue, just another minor thing to take into account when picking the right tool for the job (along with nature of the training data and learning task etc).
', 836, '2014-07-09 12:32:51.360', 'b8ca092a-8fdb-4c24-b577-2353092b2a85', 711, 'added 187 characters in body', 1844, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('SVM is a powerful classifier. It has some nice advantages (which I guess were responsible for its popularity)... These are:

  - Efficiency: Only the support vectors play a role in determining the classification boundary. All other points from the training set needn''t be stored in memory.
  - The so-called power of kernels: With appropriate kernels you can transform feature space into a higher dimension so that it becomes linearly separable. The notion of kernels work with arbitrary objects on which you can define some notion of similarity with the help of inner products... and hence SVMs can classify arbitrary objects such as trees, graphs etc.

There are some significant disadvantages as well.

   -  Parameter sensitivity: The performance is highly sensitive to the choice of the regularization parameter C, which allows some variance in the model.
   -  Extra parameter for the Gaussian kernel: The radius of the Gaussian kernel can have a significant impact on classifier accuracy. Typically a grid search has to be conducted to find optimal parameters. LibSVM has a support for grid search.

SVMs generally belong to the class of "Sparse Kernel Machines". The sparse vectors in the case of SVM are the support vectors which are chosen from the maximum margin criterion. Other sparse vector machines such as the Relevance Vector Machine (RVM) perform better than SVM. The following figure shows a comparative performance of the two.

![RVM vs. SVM][1]

A very effective classifier, which is very popular nowadays is the Random Forest. The main advantages are:

   -  Only one parameter to tune (i.e. the number of trees in the forest)
   -  Not utterly parameter sensitive
   -  Can easily be extended to multiple classes
   -  Is based on probabilistic principles (maximizing mutual information gain with the help of decision trees)




  [1]: http://i.stack.imgur.com/zNYbt.png', 984, '2014-07-09 13:07:13.303', '21036688-6a74-4f08-a3c2-6424df094b3a', 712, 1845, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For example...if u have an agricultural land then selecting one particular area of that land would be feature selection.If u aim to find the affected plants in that area den u need to observe each plant based on a particular feature that is common in each plant so as to find the abnormalities...for this u would be considering feature extraction.In this example the original agricultural land corresponds to Dimensionality reduction.', 1015, '2014-07-09 17:18:44.043', 'df635743-4b70-469d-9336-aec3a86dd31b', 509, 'deleted 72 characters in body', 1847, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('SVM is a powerful classifier. It has some nice advantages (which I guess were responsible for its popularity)... These are:

  - Efficiency: Only the support vectors play a role in determining the classification boundary. All other points from the training set needn''t be stored in memory.
  - The so-called power of kernels: With appropriate kernels you can transform feature space into a higher dimension so that it becomes linearly separable. The notion of kernels work with arbitrary objects on which you can define some notion of similarity with the help of inner products... and hence SVMs can classify arbitrary objects such as trees, graphs etc.

There are some significant disadvantages as well.

   -  Parameter sensitivity: The performance is highly sensitive to the choice of the regularization parameter C, which allows some variance in the model.
   -  Extra parameter for the Gaussian kernel: The radius of the Gaussian kernel can have a significant impact on classifier accuracy. Typically a grid search has to be conducted to find optimal parameters. LibSVM has a support for grid search.

SVMs generally belong to the class of "Sparse Kernel Machines". The sparse vectors in the case of SVM are the support vectors which are chosen from the maximum margin criterion. Other sparse vector machines such as the Relevance Vector Machine (RVM) perform better than SVM. The following figure shows a comparative performance of the two. In the figure, the x-axis shows one dimensional data from two classes y={0,1}. The mixture model is defined as P(x|y=0)=Unif(0,1) and P(x|y=1)=Unif(.5,1.5) (Unif denotes uniform distribution). 1000 points were sampled from this mixture and an SVM and an RVM were used to estimate the posterior. The problem of SVM is that the predicted values are far off from the true log odds.

![RVM vs. SVM][1]

A very effective classifier, which is very popular nowadays is the Random Forest. The main advantages are:

   -  Only one parameter to tune (i.e. the number of trees in the forest)
   -  Not utterly parameter sensitive
   -  Can easily be extended to multiple classes
   -  Is based on probabilistic principles (maximizing mutual information gain with the help of decision trees)




  [1]: http://i.stack.imgur.com/zNYbt.png', 984, '2014-07-09 17:43:29.320', '8233ce02-9555-4320-b6a3-8e54f1c0d566', 712, 'Explained the figure', 1848, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have installed cloudera CDH5 Quick start VM on VM player. When I login through HUE in the first page I am the following error

Potential misconfiguration detected. Fix and restart Hue.![Potential misconfiguration detected. Fix and restart Hue][1]


  [1]: http://i.stack.imgur.com/vnq5P.png


How to solve this issue.

Thanks,
Green', 1314, '2014-07-09 17:51:40.583', '97e3729d-e807-4749-b176-3245766341b5', 713, 1849, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Cloudera QuickStart VM Error', 1314, '2014-07-09 17:51:40.583', '97e3729d-e807-4749-b176-3245766341b5', 713, 1850, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop>', 1314, '2014-07-09 17:51:40.583', '97e3729d-e807-4749-b176-3245766341b5', 713, 1851, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('SVM is a powerful classifier. It has some nice advantages (which I guess were responsible for its popularity)... These are:

  - Efficiency: Only the support vectors play a role in determining the classification boundary. All other points from the training set needn''t be stored in memory.
  - The so-called power of kernels: With appropriate kernels you can transform feature space into a higher dimension so that it becomes linearly separable. The notion of kernels work with arbitrary objects on which you can define some notion of similarity with the help of inner products... and hence SVMs can classify arbitrary objects such as trees, graphs etc.

There are some significant disadvantages as well.

   -  Parameter sensitivity: The performance is highly sensitive to the choice of the regularization parameter C, which allows some variance in the model.
   -  Extra parameter for the Gaussian kernel: The radius of the Gaussian kernel can have a significant impact on classifier accuracy. Typically a grid search has to be conducted to find optimal parameters. LibSVM has a support for grid search.

SVMs generally belong to the class of "Sparse Kernel Machines". The sparse vectors in the case of SVM are the support vectors which are chosen from the maximum margin criterion. Other sparse vector machines such as the **Relevance Vector Machine** (RVM) perform better than SVM. The following figure shows a comparative performance of the two. In the figure, the x-axis shows one dimensional data from two classes y={0,1}. The mixture model is defined as P(x|y=0)=Unif(0,1) and P(x|y=1)=Unif(.5,1.5) (Unif denotes uniform distribution). 1000 points were sampled from this mixture and an SVM and an RVM were used to estimate the posterior. The problem of SVM is that the predicted values are far off from the true log odds.

![RVM vs. SVM][1]

A very effective classifier, which is very popular nowadays, is the **Random Forest**. The main advantages are:

   -  Only one parameter to tune (i.e. the number of trees in the forest)
   -  Not utterly parameter sensitive
   -  Can easily be extended to multiple classes
   -  Is based on probabilistic principles (maximizing mutual information gain with the help of decision trees)




  [1]: http://i.stack.imgur.com/zNYbt.png', 984, '2014-07-09 20:23:12.380', '4543c6ea-281b-446c-a23f-34467672157c', 712, 'added 9 characters in body', 1854, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Although your question is not very specific so I''ll try to give you some generic solutions. There are couple of things you can do here:

 - Check sparseMatrix from Matrix package as mentioned by @Sidhha
 - Try running your model in parallel using packages like snowfall, [Parallel](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf). Check this [list of packages on Cran](http://cran.r-project.org/web/views/HighPerformanceComputing.html) which can help you runnning your model in multicore parallel mode.
 - You can also try [data.table](http://datatable.r-forge.r-project.org/) package. It is quite phenomenal in speed.

Good reads:

 1. [11 Tips on How to Handle Big Data in R (and 1 Bad Pun)](http://theodi.org/blog/fig-data-11-tips-how-handle-big-data-r-and-1-bad-pun)
 2. [Why R is slow & how to improve its Performance?](http://adv-r.had.co.nz/Performance.html)', 2433, '2014-07-10 08:38:43.353', 'e9221279-67ef-49c8-9a52-d434f47b4f69', 714, 1855, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Go into the other link from home - to the cloudera manager.

From there, you''ll see Hue can be restarted, but there is probably an alert that needs to be resolved in there first.

If I remember right there''s some initial configuration that needs to be done on the quickstart VM that''s spelled out as soon as you log into the manager application.', 434, '2014-07-10 09:16:39.937', 'a2b48fe4-2c77-4513-b0c2-a484cc06d4cb', 715, 1856, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I know that there is no a clear answer for this question, but let''s suppose that I have a huge neural network, with a lot of data and I want to add a new feature in input. The "best" way would be to test the network with the new feature and see the results, but is there a method to test if the feature IS UNLIKELY helpful? Like correlation measures (http://www3.nd.edu/~mclark19/learn/CorrelationComparison.pdf) etc?
', 989, '2014-07-10 10:07:13.523', '9946cab7-331f-4408-b610-641d3a0207bb', 716, 1857, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to choose the features for a neural network?', 989, '2014-07-10 10:07:13.523', '9946cab7-331f-4408-b610-641d3a0207bb', 716, 1858, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork>', 989, '2014-07-10 10:07:13.523', '9946cab7-331f-4408-b610-641d3a0207bb', 716, 1859, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am searching for the correct definition for an experimental design I am using to test the robustness of different classification methods.

I am creating different subsets of the full dataset by cutting away some samples. Each subset is created independently with respect to the others.

Then I run each classification method on every subset.

Finally, I estimate the accuracy of each method as how many classification on subsets are in agreement with the classification on full dataset.

Example:

    Classification-full     1    2    3    2    1    1    2

    Classification-subset1  1    2         2    3    1
    Classification-subset2       2    3         1    1    2
    ...

    Accuracy                1    1    1    1  0.5    1    1


Is there a correct name to this methodology? I thought it can fall under [bootstrapping][1] but I am unsure about this.

Thanks


  [1]: http://en.wikipedia.org/wiki/Bootstrapping_(statistics)', 133, '2014-07-10 11:55:49.637', '6f040623-8a64-420c-99da-b33c7055a239', 717, 1860, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to define a custom methodology', 133, '2014-07-10 11:55:49.637', '6f040623-8a64-420c-99da-b33c7055a239', 717, 1861, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><definitions><accuracy><sampling>', 133, '2014-07-10 11:55:49.637', '6f040623-8a64-420c-99da-b33c7055a239', 717, 1862, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to define a custom resampling methodology', 133, '2014-07-10 12:08:34.847', 'e39b7863-1347-472f-bc5d-75581b022ff6', 717, 'edited title', 1863, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A very strong correlation between the new feature and an existing feature is a fairly good sign that the new feature provides little new information.  A low correlation between the new feature and existing features is likely preferable.

If the new feature was manually constructed from a combination of existing features, consider leaving it out.  The beauty of neural networks is that little feature engineering and preprocessing is required -- features are instead learned by intermediate layers.  Whenever possible, prefer learning features to engineering them.', 684, '2014-07-10 15:43:53.177', 'eefeae51-bc95-4315-aeac-6824e3dfe867', 718, 1864, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork><feature-selection><feature-extraction>', 97, '2014-07-10 17:51:00.230', 'e497a222-a887-4807-918d-071ebf6495c2', 716, 'More relevant tags.', 1865, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-10 17:51:00.230', 'e497a222-a887-4807-918d-071ebf6495c2', 716, 'Proposed by 97 approved by 989 edit id of 110', 1866, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m using an experimental design to test the robustness of different classification methods, and now I''m searching for the correct definition of such design.

I''m creating different subsets of the full dataset by cutting away some samples. Each subset is created independently with respect to the others. Then, I run each classification method on every subset. Finally, I estimate the accuracy of each method as how many classifications on subsets are in agreement with the classification on the full dataset. For example:

    Classification-full     1    2    3    2    1    1    2

    Classification-subset1  1    2         2    3    1
    Classification-subset2       2    3         1    1    2
    ...

    Accuracy                1    1    1    1  0.5    1    1


Is there a correct name to this methodology? I thought it can fall under [bootstrapping][1] but I''m not sure about this.

  [1]: http://en.wikipedia.org/wiki/Bootstrapping_(statistics)', 84, '2014-07-10 18:04:59.523', 'a5f01dae-52c3-4a86-b6db-2c488ccf25f4', 717, 'Improving writing.', 1867, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A very strong correlation between the new feature and an existing feature is a fairly good sign that the new feature provides little new information.  A low correlation between the new feature and existing features is likely preferable.

A strong linear correlation between the new feature and the predicted variable is an good sign that a new feature will be valuable, but the absence of a high correlation is not necessary a sign of a poor feature, because neural networks are not restricted to linear combinations of variables.

If the new feature was manually constructed from a combination of existing features, consider leaving it out.  The beauty of neural networks is that little feature engineering and preprocessing is required -- features are instead learned by intermediate layers.  Whenever possible, prefer learning features to engineering them.', 684, '2014-07-10 19:18:05.697', '4374816c-37c2-4456-a45c-f2d236e3698b', 718, 'added 299 characters in body', 1868, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('nsl-
I''m a beginner at machine learning, so forgive the lay-like description here, but it sounds like you might be able to use topic modelling, like latent dirichlet analysis (LDA). It''s an algorithm widely used to classify documents, according to what topics they are about, based on the words found and the relative frequencies of those words in the overall corpus.  I bring it up mainly because, in LDA it''s not necessary to define the topics in advance.

Since the help pages on LDA are mostly written for text analysis, the analogy I would use, in order to apply it to your question, is:
- Treat each gene expression, or feature, as a ''word'' (sometimes called a token in typical LDA text-classification applications)
- Treat each sample as a document (ie it contains an assortment of words, or gene expressions)
- Treat the signatures as pre-existing topics

If I''m not mistaken, LDA should give weighted probabilities for each topic, as to how strongly it is present in each document.
', 2443, '2014-07-10 22:42:13.720', '08015365-f35e-4d92-8d8c-acde8d5a0ad8', 719, 1869, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are a couple of open source options I know of -

LibOTS - http://libots.sourceforge.net/

DocSum - http://docsum.sourceforge.net/docsum/web/about.php

A couple of commercial solutions -

Intellix Summarizer Pro - http://summarizer.intellexer.com/order_summarizer_pro.php

Copernic Summarizer - http://www.copernic.com/en/products/summarizer/

And this one is a web service -

TextTeaser - http://www.textteaser.com/


I''m sure there are plenty of others out there.  I have used Copernic a good deal and it''s pretty good, but I was hoping it could be automated easily, which it can''t - at least it couldn''t when I used it.  ', 434, '2014-07-10 23:38:58.153', 'b7a7136d-0895-4aed-8c9b-4bc3d0416cb4', 720, 1870, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When it comes to dealing with many disparate kinds of data, especially when the relationships between them are unclear, I would strongly recommend a technique based on [decision trees][1], the most popular ones today to the best of my knowledge are [random forest][2], and [extremely randomized trees][3].

Both have implementations in [sklearn][4], and they are pretty straightforward to use. At a very high level, the reason that a `decision tree`-based approach is advantageous for multiple disparate kinds of data is because decision trees are largely independent from the specific data they are dealing with, just so long as they are capable of understanding your representation.

You''ll still have to fit your data into a feature vector of some kind, but based on your example that seems like a pretty straightforward task, and if you''re willing to go a little deeper on your implementation you could certainly come up with a custom tree-splitting rule without actually having to change anything in the underlying algorithm. The [original paper][5] is a pretty decent place to start if you want to give that a shot.

If you want pseudo-structural data from your text data though, I might suggest looking into `doc2vec`, recently developed by Google. I don''t think there are any good open-source implementations now, but it''s a pretty straightforward improvement on the `word2vec` algorithm, which has implementations in at least `C` and `python`.

Hope that helps! Let me know if you''ve got any more questions.


  [1]: http://en.wikipedia.org/wiki/Decision_tree_learning
  [2]: http://en.wikipedia.org/wiki/Random_forest
  [3]: http://www.montefiore.ulg.ac.be/~ernst/uploads/news/id63/extremely-randomized-trees.pdf
  [4]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit_transform
  [5]: http://www.cs.nyu.edu/~roweis/csc2515-2006/readings/quinlan.pdf', 548, '2014-07-11 01:28:24.957', 'b62069ff-953a-4753-83fd-bc1247a3b630', 721, 1871, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R contains some *standard* functions for data manipulation, which can be used for data cleaning, in its **base** package (`gsub`, `transform`, etc.), as well as in various third-party packages, such as **stringr**, **reshape**, **reshape2**, and **plyr**. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.

Additionally, R offers some packages specifically *focused* on data cleaning and transformation (my current reputation on this site limits number of links that I can post within an answer, so please see corresponding URLs in the comments below):

- **editrules**
- **deducorrect**
- **StatMatch**
- **MatchIt**

A comprehensive and coherent approach to **data cleaning** in R, including examples and use of **editrules** and **deducorrect** packages, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf.', 2452, '2014-07-11 09:49:32.933', '7bce2775-9100-4376-b411-62ae4d3eb15f', 722, 1872, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('R contains some *standard* functions for data manipulation, which can be used for data cleaning, in its **base** package (`gsub`, `transform`, etc.), as well as in various third-party packages, such as **stringr**, **reshape**, **reshape2**, and **plyr**. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.

Additionally, R offers some packages specifically *focused* on data cleaning and transformation (my current reputation on this site limits number of links that I can post within an answer, so please see corresponding URLs in the comments below):

- **editrules**
- **deducorrect**
- **StatMatch**
- **MatchIt**
- **DataCombine**

A comprehensive and coherent approach to **data cleaning** in R, including examples and use of **editrules** and **deducorrect** packages, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf.', 2452, '2014-07-11 10:10:41.697', 'ea0aaa55-b9f2-490f-ba8a-469d13c22371', 722, 'Added info on DataCombine package.', 1873, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Random subsampling seems appropriate, bootstrapping is a bit more generic, but also correct.

Here are some references and synonyms: http://www.frank-dieterle.com/phd/2_4_3.html', 127, '2014-07-11 11:32:13.550', '9f1d7eac-76ff-42d3-aaf8-306721fef026', 723, 1874, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Your approach of using a source version repository is a good one and it actually allows you also working on the cluster and then copying everything back.

If you find yourself making minor edits to your Python script on your laptop, then updating your SVN directory on the cluster, why not work directly on the cluster frontend, make all needed minor edits, and then, at the end of the day, commit everything there and update on your laptop?

All you need is to get familiar with the environment there (OS, editor, etc.) or install your own environment (I usually install in my home directory the latest version of [Vim][1], [Tmux][2], etc. with the proper dotfiles so I feel at home there.)

Also, you can version your data, and even your intermediate results if size permits. My repositories often comprise code, data (original and cleaned versions), documentation, and paper sources for publishing (latex)

Finally, you can script your job submission to avoid modifying scripts manually. `qsub` accepts a script from stdin and also accepts all `#$` comments as command-line arguments.

  [1]: http://www.vim.org/
  [2]: http://tmux.sourceforge.net/', 172, '2014-07-11 14:13:30.403', 'b63867ea-94d1-4d37-96ce-5f3a1d8812b4', 724, 1875, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What you describe falls in the category of [concept drift][1] in machine learning.
You might find interesting and actionable ideas in this [summary paper][2] and you''ll find a taxonomy of the possible approaches in [these slides][3].


  [1]: http://en.wikipedia.org/wiki/Concept_drift
  [2]: http://arxiv.org/pdf/1010.4784.pdf
  [3]: http://www.cs.waikato.ac.nz/~abifet/PAKDD2011/PAKDD11Tutorial_Handling_Concept_Drift.pdf', 172, '2014-07-11 14:27:01.603', 'aa15d9e3-f0c3-4541-ba2d-7b84886f0ed7', 725, 1876, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to understand a neuroscience article by Karl Friston. In it he gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear as equation (5) in the article at http://www.fil.ion.ucl.ac.uk/spm/doc/papers/Action_and_behavior_A_free-energy_formulation.pdf (DOI 10.1007/s00422-010-0364-z). Here they are (The tilda in "s" below is supposed to be over the s rather than in front of it.):

 Energy minus entropy: F = {ln p(s,|m)}q + {ln q(|)}q  Divergence plus surprise: = D(q(|)||p(|s,m))  ln p (s|m)  Complexity minus accuracy: = D(q(|)||p(|m))  {ln p(s|,m)}q

The things I am struggling with at this point are 1) the meaning of the || in the 2nd and 3rd versions of the equations, 2) the negative logs. Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?', 2458, '2014-07-11 21:09:58.873', '7463cf8c-1ddf-4e61-bd9b-676e8e7e7cb1', 726, 1877, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Trying to understand the equations in an Karl Friston article', 2458, '2014-07-11 21:09:58.873', '7463cf8c-1ddf-4e61-bd9b-676e8e7e7cb1', 726, 1878, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork><accuracy>', 2458, '2014-07-11 21:09:58.873', '7463cf8c-1ddf-4e61-bd9b-676e8e7e7cb1', 726, 1879, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('R contains some *standard* functions for data manipulation, which can be used for data cleaning, in its **base** package (`gsub`, `transform`, etc.), as well as in various third-party packages, such as **stringr**, **reshape**, **reshape2**, and **plyr**. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.

Additionally, R offers some packages specifically *focused* on data cleaning and transformation:

- **editrules** (http://cran.r-project.org/web/packages/editrules/index.html)
- **deducorrect** (http://cran.r-project.org/web/packages/deducorrect/index.html)
- **StatMatch** (http://cran.r-project.org/web/packages/StatMatch/index.html)
- **MatchIt** (http://cran.r-project.org/web/packages/MatchIt/index.html)
- **DataCombine** (http://cran.r-project.org/web/packages/DataCombine)

A comprehensive and coherent approach to **data cleaning** in R, including examples and use of **editrules** and **deducorrect** packages, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf.', 2452, '2014-07-11 21:34:06.603', 'fd5f3cbf-db83-4153-a6f9-a04b5cbbc18f', 722, 'Added info on DataCombine. Moved URLs from comments to the answer.', 1880, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Your problem is that the resets aren''t part of your linear model. You either have to cut your data into different fragments at the resets, so that no reset occurs within each fragment, and you can fit a linear model to each fragment. Or you can build a more complicated model that allows for resets. In this case, either the time of occurrence of the resets has to be put into the model manually, or the time of resets has to be a free parameter in the model that is determined by fitting the model to the data.', 2459, '2014-07-11 21:35:22.677', 'ea42f294-4762-447c-b3d3-2daa45318f10', 727, 1881, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In addition to excellent previous answers, I''d like to recommend two papers on **data cleaning**. They are not specific to *manual* data cleaning, but, considering the benefits and advice (which I completely agree with) of expressing even **manual** data transformations **in code**, these resources can be as valuable. Also, despite the fact that following papers are somewhat R-focused, I believe that general *ideas* and *workflows* for data cleaning can be easily extracted and are equally applicable to non-R environments, as well.

The first paper presents the concept of *tidy data*, as well as examples and best practices of use of standard and specific R packages in data cleaning: http://vita.had.co.nz/papers/tidy-data.pdf.

A comprehensive and coherent approach to data cleaning in R, including examples, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf.', 2452, '2014-07-11 22:03:31.950', '0bd550f6-10c1-4138-aebd-09bfc34c7816', 728, 1882, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I thought this was an interesting problem, so I wrote a sample data set and a linear slope estimator in R.  I hope it helps you with your problem.  I''m going to make some assumptions, the biggest is that you want to estimate a constant slope, given by some segments in your data.

This code creates the sample data set.  It will consist of 100 points, random noise with a slope of 4 (Hopefully we will estimate this).  When the y-values reach a cutoff, they reset to 50.  The cutoff is randomly chosen between 115 and 120 for each reset.  Here is the R code to create the data set.

    # Create Sample Data
    set.seed(1001)
    x_data = 1:100 # x-data
    y_data = rep(0,length(x_data)) # Initialize y-data
    y_data[1] = 50
    reset_level = sample(115:120,1) # Select initial cutoff
    for (i in x_data[-1]){ # Loop through rest of x-data
      if(y_data[i-1]>reset_level){ # check if y-value is above cutoff
        y_data[i] = 50             # Reset if it is and
        reset_level = sample(115:120,1) # rechoose cutoff
      }else {
        y_data[i] = y_data[i-1] + 4 + (10*runif(1)-5) # Or just increment y with random noise
      }
    }
    plot(x_data,y_data) # Plot data
![Sample Data][1]


  [1]: http://i.stack.imgur.com/2dC1w.png

Now we find the breaks and fit each set of y-values and record the slopes.

    # Find the differences between adjacent points
    diffs = y_data[-1] - y_data[-length(y_data)]
    # Find the break points (here I use 4 s.d.''s)
    break_points = c(0,which(diffs < (mean(diffs) - 4*sd(diffs))),length(y_data))
    # Create the lists of y-values
    y_lists = sapply(1:(length(break_points)-1),function(x){
      y_data[(break_points[x]+1):(break_points[x+1])]
    })
    # Create the lists of x-values
    x_lists = lapply(y_lists,function(x) 1:length(x))
    #Find all the slopes for the lists of points
    slopes = unlist(lapply(1:length(y_lists), function(x) lm(y_lists[[x]] ~ x_lists[[x]])$coefficients[2]))

Here are the slopes:
(3.309110, 4.419178, 3.292029, 4.531126, 3.675178, 4.294389)

And we can just take the mean to find the expected slope (3.920168).', 375, '2014-07-11 23:08:16.267', 'ca6bf796-44f2-40d7-a7c1-d3915c766dce', 729, 1883, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am trying to understand a neuroscience article by Karl Friston. In it he gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear as equation (5) in the article at http://www.fil.ion.ucl.ac.uk/spm/doc/papers/Action_and_behavior_A_free-energy_formulation.pdf (DOI 10.1007/s00422-010-0364-z). Here they are (The tilda in "s" below is supposed to be over the s rather than in front of it.):

 Energy minus entropy: F = {ln p(s,|m)}q + {ln q(|)}q

 Divergence plus surprise: = D(q(|)||p(|s,m))  ln p (s|m)

 Complexity minus accuracy: = D(q(|)||p(|m))  {ln p(s|,m)}q

The things I am struggling with at this point are 1) the meaning of the || in the 2nd and 3rd versions of the equations, 2) the negative logs. Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?', 2458, '2014-07-12 03:18:03.923', '902a48e0-3043-4274-9ae8-6037896845de', 726, 'added 4 characters in body', 1884, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am trying to understand a neuroscience article by Karl Friston. In it he gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear as equation (5) in the article at http://www.fil.ion.ucl.ac.uk/spm/doc/papers/Action_and_behavior_A_free-energy_formulation.pdf (DOI 10.1007/s00422-010-0364-z). Here they are (The tilda in "s" below is supposed to be over the s rather than in front of it AND the { & } are angle brackets in the original.):

 Energy minus entropy: F = {ln p(s,|m)}q + {ln q(|)}q

 Divergence plus surprise: = D(q(|)||p(|s,m))  ln p (s|m)

 Complexity minus accuracy: = D(q(|)||p(|m))  {ln p(s|,m)}q

The things I am struggling with at this point are 1) the meaning of the || in the 2nd and 3rd versions of the equations, 2) the negative logs. Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?', 2458, '2014-07-12 03:38:07.083', 'db1cc94b-5947-4910-b83b-f1dbff405dd0', 726, 'added 49 characters in body', 1885, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As far as I know the development of algorithms to solve the [Frequent Pattern Mining]() (FPM) problem, the road of improvements have some main checkpoints. Firstly, the [Apriori](http://en.wikipedia.org/wiki/Apriori_algorithm) algorithm was proposed in 1993, by [Agrawal et al.](http://dl.acm.org/citation.cfm?id=170072), along with the formalization of the problem. The algorithm was able to *strip-off* some sets from the `2^n - 1` sets (powerset) by using a lattice to maintain the data. A drawback of the approach was the need to re-read the database to compute the frequency of each set expanded.

Later, on year 1997, [Zaki et al.](http://www.computer.org/csdl/trans/tk/2000/03/k0372-abs.html) proposed the algorithm [Eclat](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm), which *inserted* the resulting frequency of each set inside the lattice. This was done by adding, at each node of the lattice, the set of transaction-ids that had the items from root to the referred node. The main contribution is that one does not have to re-read the entire dataset to know the frequency of each set, but the memory required to keep such data structure built may exceed the size of the dataset itself.

In 2000, [Han et al.](http://dl.acm.org/citation.cfm?doid=335191.335372) proposed an algorithm named [FPGrowth](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm), along with a prefix-tree data structure named FPTree. The algorithm was able to provide significant data compression, while also granting that only frequent itemsets would be yielded (without candidate itemset generation). This was done mainly by sorting the items of each transaction in decreasing order, so that the most frequent items are the ones with the least repetitions in the tree data structure. Since the frequency only descends while traversing the tree in-depth, the algorithm is able to *strip-off* non-frequent itemsets.

As far as I know, this may be considered a state-of-the-art algorithm, but I''d like to know about other proposed solutions. What algorithms for FPM are considered "state-of-the-art"? What is the *intuition*/*main-contribution* of such algorithms?', 84, '2014-07-12 17:25:52.907', 'c4180c50-4e0c-4032-9f32-58d9c747eb53', 730, 1886, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('State-of-the-art algorithms in frequent pattern mining', 84, '2014-07-12 17:25:52.907', 'c4180c50-4e0c-4032-9f32-58d9c747eb53', 730, 1887, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><data-mining><efficiency><state-of-the-art>', 84, '2014-07-12 17:25:52.907', 'c4180c50-4e0c-4032-9f32-58d9c747eb53', 730, 1888, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-mining><svm><state-of-the-art>', 84, '2014-07-12 17:26:32.727', 'cf62dc8b-9ab4-4fa5-92a3-d80e263c4eff', 711, 'Adding tags', 1889, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As far as I know the development of algorithms to solve the [Frequent Pattern Mining]() (FPM) problem, the road of improvements have some main checkpoints. Firstly, the [Apriori](http://en.wikipedia.org/wiki/Apriori_algorithm) algorithm was proposed in 1993, by [Agrawal et al.](http://dl.acm.org/citation.cfm?id=170072), along with the formalization of the problem. The algorithm was able to *strip-off* some sets from the `2^n - 1` sets (powerset) by using a lattice to maintain the data. A drawback of the approach was the need to re-read the database to compute the frequency of each set expanded.

Later, on year 1997, [Zaki et al.](http://www.computer.org/csdl/trans/tk/2000/03/k0372-abs.html) proposed the algorithm [Eclat](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm), which *inserted* the resulting frequency of each set inside the lattice. This was done by adding, at each node of the lattice, the set of transaction-ids that had the items from root to the referred node. The main contribution is that one does not have to re-read the entire dataset to know the frequency of each set, but the memory required to keep such data structure built may exceed the size of the dataset itself.

In 2000, [Han et al.](http://dl.acm.org/citation.cfm?doid=335191.335372) proposed an algorithm named [FPGrowth](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm), along with a prefix-tree data structure named FPTree. The algorithm was able to provide significant data compression, while also granting that only frequent itemsets would be yielded (without candidate itemset generation). This was done mainly by sorting the items of each transaction in decreasing order, so that the most frequent items are the ones with the least repetitions in the tree data structure. Since the frequency only descends while traversing the tree in-depth, the algorithm is able to *strip-off* non-frequent itemsets.

As far as I know, this may be considered a state-of-the-art algorithm, but I''d like to know about other proposed solutions. What other algorithms for FPM are considered "state-of-the-art"? What is the *intuition*/*main-contribution* of such algorithms?', 84, '2014-07-12 17:31:42.567', '5be84dc7-6d0c-4248-b462-c7405547a993', 730, 'added 6 characters in body', 1890, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As far as I know the development of algorithms to solve the [Frequent Pattern Mining]() (FPM) problem, the road of improvements have some main checkpoints. Firstly, the [Apriori](http://en.wikipedia.org/wiki/Apriori_algorithm) algorithm was proposed in 1993, by [Agrawal et al.](http://dl.acm.org/citation.cfm?id=170072), along with the formalization of the problem. The algorithm was able to *strip-off* some sets from the `2^n - 1` sets (powerset) by using a lattice to maintain the data. A drawback of the approach was the need to re-read the database to compute the frequency of each set expanded.

Later, on year 1997, [Zaki et al.](http://www.computer.org/csdl/trans/tk/2000/03/k0372-abs.html) proposed the algorithm [Eclat](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm), which *inserted* the resulting frequency of each set inside the lattice. This was done by adding, at each node of the lattice, the set of transaction-ids that had the items from root to the referred node. The main contribution is that one does not have to re-read the entire dataset to know the frequency of each set, but the memory required to keep such data structure built may exceed the size of the dataset itself.

In 2000, [Han et al.](http://dl.acm.org/citation.cfm?doid=335191.335372) proposed an algorithm named [FPGrowth](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm), along with a prefix-tree data structure named FPTree. The algorithm was able to provide significant data compression, while also granting that only frequent itemsets would be yielded (without candidate itemset generation). This was done mainly by sorting the items of each transaction in decreasing order, so that the most frequent items are the ones with the least repetitions in the tree data structure. Since the frequency only descends while traversing the tree in-depth, the algorithm is able to *strip-off* non-frequent itemsets.

**Edit**:

<strike>As far as I know, this may be considered a state-of-the-art algorithm, but I''d like to know about other proposed solutions. What other algorithms for FPM are considered "state-of-the-art"? What is the *intuition*/*main-contribution* of such algorithms?</strike>

Is the FPGrowth algorithm still considered "state of the art" in frequent pattern mining? If not, what algorithm(s) may extract frequent itemsets from large datasets more efficiently?', 84, '2014-07-13 03:05:46.660', 'eaf0d598-a638-4c85-b245-7429719a0b7c', 730, 'Repharsing the question.', 1893, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Is FPGrowth still considered "state of the art" in frequent pattern mining?', 84, '2014-07-13 03:05:46.660', 'eaf0d598-a638-4c85-b245-7429719a0b7c', 730, 'Repharsing the question.', 1894, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When I started with ANN I thought I''d have to fight overfitting as the main problem. But in practice I can''t even get my NN to pass the 20% error rate barrier. I can''t even nearly beat my score on random forest!

I''m seeking some very general or not so general advice on what should one do to make his NN start capturing trends in data.

For implementing NN I use Theano Stacked Auto Encoder, [the code from tutorial][1] that works great (less than 5% error rate) on classifying MNIST dataset.
It is a multilayer perceptron, with softmax layer on top with each hidden later being pre-trained as autoencoder (fully described at [tutorial][2], chapter 8).
The number of input features are ~50 and output classes ~10. NN has sigmoid neurons and all data normalized to [0,1]. I tried lots of different configurations: number of hidden layers and neurons in them (100->100->100, 60->60->60, 60->30->15, etc,etc), different learning and pre-train rates, etc.

And the best thing I can get is %20 error rate on validation set and %40 error rate on test set.

On the contrary, when I try to use Random Forest (from scikit-learn) I easily get %12 error rate on validation set and %25(!) on test set.

How can that be that my deep NN with pre-training behaves so badly? What should I try?



  [1]: https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/SdA.py
  [2]: http://deeplearning.net/tutorial/deeplearning.pdf', 2471, '2014-07-13 09:04:39.703', '787a51d5-1db3-414f-8662-5688a55483fc', 731, 1895, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to fight underfitting in deep neural net', 2471, '2014-07-13 09:04:39.703', '787a51d5-1db3-414f-8662-5688a55483fc', 731, 1896, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork>', 2471, '2014-07-13 09:04:39.703', '787a51d5-1db3-414f-8662-5688a55483fc', 731, 1897, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The problem with deep networks is that they have lots of hyperparameters to tune and very small solution space. Thus, finding good ones is more like an art rather than engineering task. I would start with working example from tutorial and play around with its parameters to see how results change - this give a good intuition (though not formal explanation) about dependencies between parameters and results (both - final and intermediate).

Also I found following papers very useful:

 * [Visually Debugging Restricted Boltzmann Machine Training
with a 3D Example](http://yosinski.com/media/papers/Yosinski2012VisuallyDebuggingRestrictedBoltzmannMachine.pdf)
 * [A Practical Guide to Training Restricted Boltzmann
Machines](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)

They both describe RBMs, but contain some insights on deep networks in general. For example, one of key points is that networks need to be debugged layer-wise - if previous layer doesn''t provide good representation of features, further layers have almost no chance to fix it. ', 1279, '2014-07-13 09:58:16.387', '585d734d-cd1c-465e-bc7b-974d4cb0269b', 732, 1898, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Ask your grid administrator to add your local machine as a "submit host", and install SGE (which we assume you are using, you don''t actually say) so then you can `qsub` from your machine.

OR....

Use emacs, then you can edit on your HPC via emacs''s "tramp" ssh-connection facilities, and keep a shell open in another emacs window. You don''t say what editor/operating system you like to use. You can even configure emacs to save a file in two places, so you could save to your local machine for running tests and to the HPC file system simultaneously for big jobs.

', 471, '2014-07-13 12:24:36.430', '817f05d4-f07c-424c-88e1-8e7254e9ebfe', 733, 1902, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would like to use non-atomic data, as a feature for a prediction.
Suppose I have a Table with these features:
<pre>
- Column 1: Categorical - House
- Column 2: Numerical - 23.22
- Column 3: A Vector - [ 12, 22, 32 ]
- Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]
- Column 5: A List [ 122, Boolean ]
</pre>
I would like to predict/classify ... Column 2 ... for example....

I am making something to automatically respond to questions, any type of question, like "Where was Foo Born?" ...

I first make a query to a search engine --->>> then I get some Text data as a Result.
So I do all the Parsing Staff... Tagging, Stemming, Parsing, Splitting...
My first approach was to make a table, each row with a line of text.. and a lot of Features...like ... First Word ... Tag of First Word.. Chunks, etc..
But with this approach I am missing the relationships between the Sentences.

I would like to know if there is an algorithm that look inside the Tree Structures... Vectors... and make the relations and extract whatever is relevant for predicting/classifying. I''d prefer to know about a library that does that than an algorithm that I have to implement.', 548, '2014-07-13 15:45:48.960', '298e1ff7-4a4b-4161-bf40-5ad17a3b4fd9', 658, 'General Grammar and punctuation clean up', 1903, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Prediction with non-atomic features', 548, '2014-07-13 15:45:48.960', '298e1ff7-4a4b-4161-bf40-5ad17a3b4fd9', 658, 'General Grammar and punctuation clean up', 1904, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-13 15:45:48.960', '298e1ff7-4a4b-4161-bf40-5ad17a3b4fd9', 658, 'Proposed by 548 approved by 434, 84 edit id of 111', 1905, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The problem with deep networks is that they have lots of hyperparameters to tune and very small solution space. Thus, finding good ones is more like an art rather than engineering task. I would start with working example from tutorial and play around with its parameters to see how results change - this gives a good intuition (though not formal explanation) about dependencies between parameters and results (both - final and intermediate).

Also I found following papers very useful:

 * [Visually Debugging Restricted Boltzmann Machine Training
with a 3D Example](http://yosinski.com/media/papers/Yosinski2012VisuallyDebuggingRestrictedBoltzmannMachine.pdf)
 * [A Practical Guide to Training Restricted Boltzmann
Machines](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)

They both describe RBMs, but contain some insights on deep networks in general. For example, one of key points is that networks need to be debugged layer-wise - if previous layer doesn''t provide good representation of features, further layers have almost no chance to fix it. ', 1279, '2014-07-13 16:46:23.883', 'b34521d7-fad9-4f08-814b-1eeceed396ac', 732, 'added 1 character in body', 1907, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m not qualified to understand almost all of that paper, but, I might be able to give some intuitions from information theory that help you parse the paper.

`||` denotes the [Kullback-Leibler divergence][1]. It measures an information gain between two distributions. I suppose you could say it indicates the information in the real distribution of data that a model fails to capture.

When you see "negative log" think ["entropy"][2].

In the first equation, think of it as "-ln(...) - -ln(...)". This may help think of it as the difference of entropies. Likewise in the second, read it as "D(...) + -ln(...)". This may help think of it as "plus entropy".

If you look at the divergence definition, you''ll see it is defined as the log of the ratio of the PDFs. This may help connect it to logs and negative logs. Look at the definition that writes it as cross-entropy minus entropy. Then this is all a question of differences of entropies of things which may be clearer.


  [1]: http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence
  [2]: http://en.wikipedia.org/wiki/Information_entropy', 21, '2014-07-14 12:08:13.373', '40f0f1be-ab99-4b72-9a72-c43fa372bb8a', 734, 1908, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From your question''s wording I assume that you have a local machine and a remote machine where you update two files &mdash; a Python script and a Bash script.  Both files are under SVN control, and both machines have access to the same SVN server.

I am sorry I do not have any advice specific to your grid system, but let me list some general points I have found important for any deployment.

**Keep production changes limited to configuration changes**. You write that you have to "use the datasets'' path on the server"; this sounds to me like you have the paths hardcoded into your Python script.  This is not a good idea, precisely because you will need to change those paths in every other machine where you move the script to.  If you commit those changes back to SVN, then on your local machine you will have the remote paths, and on and on ...  (What if there are not only paths, but also passwords?  You should not have production passwords in an SVN server.)

So, keep paths and other setup informations in a `.ini` file and use [ConfigParser][1] to read it, or use a `.json` file and use the [json][2] module. Keep one copy of the file locally and one remotely, both under the same path, both without SVN control, and just keep the path to that configuration file in the Python script (or get it from the command line if you can''t keep both configurations under the same path).

**Keep configuration as small as possible**. Any configuration is a "moving part" of your application, and any system is more robust the less it has moving parts.  A good indicator of something that belongs into configuration is exactly that you have to edit it every time you move the code; things that have not needed editing can remain as constants in the code.

**Automate your deployment**.  You can do it via a Bash script on your local machine; note that you can [run any command on a remote machine][3] through `ssh`.  For instance:

<!-- language: bash -->

    svn export yourprojectpath /tmp/exportedproject
    tar czf /tmp/yourproject.tgz /tmp/exportedproject
    scp /tmp/myproject.tgz youruser@remotemachine:~/dev

    ## Remote commands are in the right hand side, between ''''
    ssh youruser@remotemachine ''tar xzf ~/dev/yourproject.tgz''
    ssh youruser@remotemachine ''qsub ~/dev/yourproject/script.py''

For this to work, you need of course to have a [passwordless login][4], based on public/private keys, set up between your local and the remote machine.

If you need more than this, you can think of using Python''s [Fabric][5] or the higher-level [cuisine][6].


  [1]: https://docs.python.org/2/library/configparser.html
  [2]: https://docs.python.org/2/library/json.html
  [3]: http://malcontentcomics.com/systemsboy/2006/07/send-remote-commands-via-ssh.html
  [4]: http://www.linuxproblem.org/art_9.html
  [5]: http://www.fabfile.org
  [6]: https://github.com/sebastien/cuisine', 1367, '2014-07-14 13:06:05.523', 'b70f286d-7614-4f2d-8602-ea7660b13b8d', 735, 1909, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a dataset which contains ~100,000 samples of 50 classes. I have been using SVM with an RBF kernel to train and predict new data. The problem though is the dataset is skewed towards different classes.

For example, Class 1 - 30 (~3% each), Class 31 - 45 (~0.6% each), Class 46 - 50 (~0.2% each)

I see that the model tends to very rarely predict the classes which occur less frequent in the training set, even though the test set has the same class distribution as the training set.

I am aware that there are technique such as ''undersampling'' where the majority class is scaled down to the minor class. However, is this applicable here where there are so many different classes? Are there other methods to help handle this case?', 802, '2014-07-14 13:53:28.437', '5e27e7a2-35b5-43cf-bffa-458bb818b5a3', 736, 1910, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Skewed multi-class data', 802, '2014-07-14 13:53:28.437', '5e27e7a2-35b5-43cf-bffa-458bb818b5a3', 736, 1911, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><svm>', 802, '2014-07-14 13:53:28.437', '5e27e7a2-35b5-43cf-bffa-458bb818b5a3', 736, 1912, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I thought this was an interesting problem, so I wrote a sample data set and a linear slope estimator in R.  I hope it helps you with your problem.  I''m going to make some assumptions, the biggest is that you want to estimate a constant slope, given by some segments in your data.  Another assumption to separate the blocks of linear data is that the natural ''reset'' will be found by comparing consecutive differences and finding ones that are X-standard deviations below the mean. (I chose 4 sd''s, but this can be changed)

Here is a plot of the data, and the code to generating it is at the bottom.
![Sample Data][1]


  [1]: http://i.stack.imgur.com/2dC1w.png

For starters, we find the breaks and fit each set of y-values and record the slopes.

    # Find the differences between adjacent points
    diffs = y_data[-1] - y_data[-length(y_data)]
    # Find the break points (here I use 4 s.d.''s)
    break_points = c(0,which(diffs < (mean(diffs) - 4*sd(diffs))),length(y_data))
    # Create the lists of y-values
    y_lists = sapply(1:(length(break_points)-1),function(x){
      y_data[(break_points[x]+1):(break_points[x+1])]
    })
    # Create the lists of x-values
    x_lists = lapply(y_lists,function(x) 1:length(x))
    #Find all the slopes for the lists of points
    slopes = unlist(lapply(1:length(y_lists), function(x) lm(y_lists[[x]] ~ x_lists[[x]])$coefficients[2]))

Here are the slopes:
(3.309110, 4.419178, 3.292029, 4.531126, 3.675178, 4.294389)

And we can just take the mean to find the expected slope (3.920168).


----------


How the sample data was created:

The sample data will consist of 100 points, random noise with a slope of 4 (Hopefully we will estimate this).  When the y-values reach a cutoff, they reset to 50.  The cutoff is randomly chosen between 115 and 120 for each reset.  Here is the R code to create the data set.

    # Create Sample Data
    set.seed(1001)
    x_data = 1:100 # x-data
    y_data = rep(0,length(x_data)) # Initialize y-data
    y_data[1] = 50
    reset_level = sample(115:120,1) # Select initial cutoff
    for (i in x_data[-1]){ # Loop through rest of x-data
      if(y_data[i-1]>reset_level){ # check if y-value is above cutoff
        y_data[i] = 50             # Reset if it is and
        reset_level = sample(115:120,1) # rechoose cutoff
      }else {
        y_data[i] = y_data[i-1] + 4 + (10*runif(1)-5) # Or just increment y with random noise
      }
    }
    plot(x_data,y_data) # Plot data

', 375, '2014-07-14 15:21:03.373', 'be757d7e-d405-4d1c-abb5-5ad60b7d3f46', 729, 'Suggested Edits', 1913, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have faced this problem many times while using SVM with Rbf kernel. Using Linear kernel instead of Rbf kernel solved my problem, but I dealt with lesser number of classes. The results were less skewed and more accurate with the linear kernel. Hope this solves your problem.', 2485, '2014-07-14 15:58:42.517', 'd14cf11b-b7ec-4a1f-a812-0fa9fb34bd03', 737, 1914, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am an MSc student at the University of Edinburgh, specialized in machine learning and natural language processing. I had some practical courses focused on data mining, and others dealing with machine learning, bayesian statistics and graphical models. My background is a BSc in Computer Science.

I did some software engineering and I learnt the basic concepts, such as design patterns, but I have never been involved in a large software development project. However, I had a data mining project in my MSc. My question is, if I want to go for a career as Data Scientist, should I apply for a graduate data scientist position first, or should I get a position as graduate software engineer first, maybe something related to data science, such as big data infrastructure or machine learning software development?

My concern is that I might need good software engineering skills for data science, and I am not sure if these can be obtained by working as a graduate data scientist directly.

Moreover, at the moment I like Data Mining, but what if I want to change my career to software engineering in the future? It might be difficult if I specialised so much in data science.

I have not been employed yet, so my knowledge is still limited. Any clarification or advice are welcome, as I am about to finish my MSc and I want to start applying for graduate positions in early October.

Thank you very much', 2489, '2014-07-14 19:02:01.670', '90d1fb48-d16c-4b0a-937c-5c94383ca0b9', 739, 1918, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Starting my career as Data Scientist, is Software Engineering experience required?', 2489, '2014-07-14 19:02:01.670', '90d1fb48-d16c-4b0a-937c-5c94383ca0b9', 739, 1919, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 2489, '2014-07-14 19:02:01.670', '90d1fb48-d16c-4b0a-937c-5c94383ca0b9', 739, 1920, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would suggest you to use libsvm, which already has adjustable class weights implemented in it. Rather than replicating the training samples, one modifies the C parameter for different classes in the SVM optimization. For example if your data has 2 classes, and the first class is only 10% of the data, you would choose class weights to be 10 and 1 for class 1 and 2 respectively. Therefore, margin violations of the first class would cost 10 times more than the margin violations for second class, and per-class accuracies would be more balanced.', 1350, '2014-07-14 19:06:44.090', '5b1fb498-371f-4734-acb5-974c4e45eed4', 740, 1921, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Absolutely. Keep your software skills sharp. You can do this in an academic program if you simply implement by yourself all the algorithms you learn about.

Good selection of courses, btw. Consider getting an internship too.', 381, '2014-07-15 04:39:11.670', '65a57967-596e-4e5d-a212-365c61b47ca3', 741, 1923, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('1) I think that there''s no need to question whether your background is adequate for a career in data science. CS degree IMHO is **more than enough** for data scientist from software engineering point of view. Having said that, theoretical knowledge is not very helpful without matching **practical experience**, so I would definitely try to **enrich** my experience through participating in *additional school projects, internships or open source projects* (maybe ones, focused on data science / machine learning / artificial intelligence).

2) I believe your concern about **focusing** on data science **too early** is unfounded, as long as you will be practicing software engineering either as a part of your data science job, or additionally in your spare time.

3) I find the following **definition of a data scientist** rather accurate and hope it will be helpful in your future career success:

> A *data scientist* is someone who is better at statistics than any
> software engineer and better at software engineering than any
> statistician.

P.S. Today''s **enormous** number of various resources on data science topics is mind-blowing, but this **open source curriculum for learning data science** might fill some gaps between your BSc/MSc respective curricula and reality of the data science career (or, at least, provide some direction for further research and maybe answer some of your concerns): http://datasciencemasters.org, or on GitHub: https://github.com/datasciencemasters/go.', 2452, '2014-07-15 06:19:31.820', '224d77bc-9fd1-4bff-9bf9-4a440237ce8b', 742, 1924, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From the job ads I have seen, the answer depends: There are jobs which are more technical in nature (designing big data projects, doing some analysis) or the exact opposite (doing analysis, storage etc. is someone elses job).

So I would say that SOME software design skills are extremely useful , but you don''t need the abillity to build a huge program in C# / Java or whatever.
Why I like some SW skills is simply that your code probably looks way better than code from someone who never programmed for the sake of programming. Most of the time, the latter code is very hard do understand / debug for outsiders. Also, sometimes your analysis needs to be integrated in a bigger program,an understand of the needs of the programms certainly helps.', 791, '2014-07-15 09:30:02.183', '87d2dbb3-a36c-4665-a8f4-5b24bcf6b3b7', 743, 1925, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It looks like the cosine similarity of two features is just their dot product scaled by the product of their magnitudes. When does cosine similarity make a better distance metric than the dot product? I.e. do the dot product and cosine similarity have different strengths or weaknesses in different situations?
', 2507, '2014-07-15 21:30:11.600', '733dabc8-cad4-4937-8eec-e59a366bbb69', 744, 1926, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Cosine similarity versus dot product as distance metrics', 2507, '2014-07-15 21:30:11.600', '733dabc8-cad4-4937-8eec-e59a366bbb69', 744, 1927, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 2507, '2014-07-15 21:30:11.600', '733dabc8-cad4-4937-8eec-e59a366bbb69', 744, 1928, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to use ARMA/ARIMA with the [statsmodel Python package][1], in order to predict the gas consumption. I tried with [a dataset][2] of this format:

![with this format](https://i.imgur.com/ZUvBlUP.png)

Using only the gas column.

    from pandas.tseries.offsets import *

    arma_mod20 = sm.tsa.ARMA(januaryFeb[[''gas [m3]'']], (5,3)).fit()
    predict_sunspots = arma_mod20.predict(''2012-01-13'', ''2012-01-14'', dynamic=True)
    ax = januaryFeb.ix[''2012-01-13 00:00:00'':''2012-01-15 22:00:00''][''gas [m3]''].plot(figsize=(12,8))
    ax = predict_sunspots.plot(ax=ax, style=''r--'', label=''Dynamic Prediction'');
    ax.legend();

![result](https://i.imgur.com/oCPonu7.png)

Why is the prediction so bad?


  [1]: http://statsmodels.sourceforge.net/devel/tsa.html#descriptive-statistics-and-tests
  [2]: https://github.com/denadai2/Gas-consumption-outliers/blob/master/exportWeb.csv', 989, '2014-07-16 00:06:02.160', '57c151aa-bb71-4a3e-9014-84e5d42bc383', 745, 1929, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('ARMA/ARIMA on energy forecasts timeseries: strange prediction', 989, '2014-07-16 00:06:02.160', '57c151aa-bb71-4a3e-9014-84e5d42bc383', 745, 1930, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><python>', 989, '2014-07-16 00:06:02.160', '57c151aa-bb71-4a3e-9014-84e5d42bc383', 745, 1931, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m not an expert on time series, but I have a **general advice**: may I suggest you to try other packages (and various parameters) to see, if there are any differences in results.

Also, unless you have to use `Python`, I''d recommend to take a look at the `R`''s extensive *ecosystem* for **time series analysis**: see http://www.statmethods.net/advstats/timeseries.html and http://cran.r-project.org/web/views/TimeSeries.html.

In particular, you may want to check the standard `stats` package (including functions `arima()` and `arima0`), as well as some other packages: `FitARMA` (http://cran.r-project.org/web/packages/FitARMA), `forecast` (http://cran.r-project.org/web/packages/forecast) and education-focused `fArma` (cran.r-project.org/web/packages/fArma), to mention just a few. I hope this is helpful.', 2452, '2014-07-16 03:32:16.270', '5b5748da-abad-483c-8df0-859048bae0f6', 746, 1932, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('About automatic cleaning: You really cannot clean data automatically, because the number of errors and the definition of an error is often dependent on the data. E.g.: Your column "Income" might contain negative values, which are an error - you have to do something about the cases. On the other hand a column "monthly savings" could reasonably contain negative values.

Such errors are highly domain dependent - so to find them, you must have domain knowledge, something at which humans excel, automated processes not so much.

Where you can and should automate is repeated projects. E.g. a report which has to produced monthly. If you spot errors, you should place some automated process which can spot  these kinds of errors in subsequent months, freeing your time. ', 791, '2014-07-16 06:45:36.740', 'cbf2d712-1519-473d-8b3b-19706117bc15', 747, 1933, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I asked a data science question regarding how to decide on the best variation of a split test on the Statistics section of StackExchange. I hope I will have better luck here. The question is basically, "Why is mean revenue per user the best metric to make your decision on in a split test?"

The original question is here: http://stats.stackexchange.com/questions/107599/better-estimator-of-expected-sum-than-mean

Since it was not well received/understood I simplified the problem to a discrete set of purchases and phrased it as a classical probability problem. That question is here: http://stats.stackexchange.com/questions/107848/drawing-numbered-balls-from-an-urn

The mean may be the best metric for such a decision but I am not convinced. We often have a lot of prior information so a Bayesian method would likely improve our estimates. I realize that this is a difficult question but Data Scientists are doing such split tests everyday. ', 2511, '2014-07-16 07:47:48.603', '930bf6c7-a160-4474-8b64-1a7fcdd1ee29', 748, 1935, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why use mean revinue in a split test?', 2511, '2014-07-16 07:47:48.603', '930bf6c7-a160-4474-8b64-1a7fcdd1ee29', 748, 1936, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<research><cross-validation>', 2511, '2014-07-16 07:47:48.603', '930bf6c7-a160-4474-8b64-1a7fcdd1ee29', 748, 1937, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m learning about matrix factorization for recommending systems and I''m seeing the term `latent features` occurring too frequently but I''m unable to understand what it means. I know what a feature is but I don''t understand the idea of latent features. Could please explain it? Or at least point me to a paper/place where I can read about it?', 728, '2014-07-16 09:24:51.780', 'cb66685e-13ef-4cde-b8d9-f5494dff8013', 749, 1938, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Meaning of latent features?', 728, '2014-07-16 09:24:51.780', 'cb66685e-13ef-4cde-b8d9-f5494dff8013', 749, 1939, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><recommendation>', 728, '2014-07-16 09:24:51.780', 'cb66685e-13ef-4cde-b8d9-f5494dff8013', 749, 1940, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am using OpenCV letter_recog.cpp example to experiment on random trees and other classifiers. This example has implementations of six classifiers - random trees, boosting, MLP, kNN, naive Bayes and SVM. UCI letter recognition dataset with 20000 instances and 16 features is used, which I split in half for training and testing. I have experience with SVM so I quickly set its recognition error to 3.3%. After some experimentation what I got was:

UCI letter recognition:

 - RTrees - 5.3%
 - Boost - 13%
 - MLP - 7.9%
 - kNN(k=3) - 6.5%
 - Bayes - 11.5%
 - SVM - 3.3%

Parameters used:

 - RTrees - max_num_of_trees_in_the_forrest=200, max_depth=20,
   min_sample_count=1

 - Boost - boost_type=REAL, weak_count=200, weight_trim_rate=0.95,
   max_depth=7

 - MLP - method=BACKPROP, param=0.001, max_iter=300 (default values - too
   slow to experiment)

 - kNN(k=3) - k=3

 - Bayes - none

 - SVM - RBF kernel, C=10, gamma=0.01

After that I used same parameters and tested on Digits and MNIST datasets by extracting gradient features first (vector size 200 elements):

Digits:

 - RTrees - 5.1%
 - Boost - 23.4%
 - MLP - 4.3%
 - kNN(k=3) - 7.3%
 - Bayes - 17.7%
 - SVM - 4.2%

MNIST:

 - RTrees - 1.4%
 - Boost - out of memory
 - MLP - in progress
 - kNN(k=3) - 1.2%
 - Bayes - 34.33%
 - SVM - 0.6%

I am new to all classifiers except SVM and kNN, for these two I can say the results seem fine. What about others? I expected more from random trees, on MNIST kNN gives better accuracy, any ideas how to get it higher? Boost and Bayes give very low accuracy. In the end I''d like to use these classifiers to make a multiple classifier system. Any advice?', 1387, '2014-07-16 09:49:15.933', '4b358d4a-5056-4e53-8857-11b5944ad03a', 750, 1941, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to increase accuracy of classifiers?', 1387, '2014-07-16 09:49:15.933', '4b358d4a-5056-4e53-8857-11b5944ad03a', 750, 1942, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><svm><random-forest>', 1387, '2014-07-16 09:49:15.933', '4b358d4a-5056-4e53-8857-11b5944ad03a', 750, 1943, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You are right, cosine similarity has a lot of common with dot product of vectors. Indeed, it is a dot product, scaled by magnitude. And because of scaling it is normalized between 0 and 1. CS is preferable because it takes into account variability of data and features'' relative frequencies. On the other hand, plain dot product is a little bit "cheaper" (in terms of complexity and implementation).', 941, '2014-07-16 10:42:55.793', 'a7b20b81-bf9d-4081-8a0d-a88f2cbec909', 751, 1944, '2');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (122, '2014-07-16 11:00:39.170', '4cfef48a-36ee-4f64-a513-2b281056b3b8', 224, '5', 1945, '33');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It seems to me that *latent features* is a term used to describe criteria for **classifying entities** by their structure, in other words, by *features* (traits) they contain, instead of *classes* they belong to. Meaning of the word "latent" here is most likely similar to its meaning in social sciences, where very popular term latent variable (http://en.wikipedia.org/wiki/Latent_variable) means unobservable variable (concept).

Section "Introduction" in the following paper provides a good explanation of latent features'' **meaning** and use in **modeling** of social sciences phenomena: http://papers.nips.cc/paper/3846-nonparametric-latent-feature-models-for-link-prediction.pdf.', 2452, '2014-07-16 11:07:56.467', 'd3a1c4e4-6c7b-40ca-a2c8-4f3ada7b9927', 752, 1946, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I could try to explain you with words, but these slides explain it very well with pictures. Hope it helps.
http://www.inf.ed.ac.uk/teaching/courses/mt/lectures/phrase-model.pdf

Note this slides correspond to the chapter 5 of "Statistical Machine Translation" by Philipp Koehn, highly recommended if you are working on machine translation, and it is easy to read.', 2489, '2014-07-16 13:05:41.123', 'ddc29c6a-717b-48dd-965e-e2b54a9544bb', 753, 1947, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Most of the recent Frequent Pattern approaches that I''ve seen in the literature are based on optimizing FPGrowth. I have to admit, I haven''t seen many developments within the literature in FPM in many years.

[This wikibook](http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm) highlights many of the variants on FPGrowth that are out there.', 2513, '2014-07-16 14:22:38.677', '012f8664-0245-46bf-aad3-9ffaf6cc2c3c', 754, 1948, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am using OpenCV letter_recog.cpp example to experiment on random trees and other classifiers. This example has implementations of six classifiers - random trees, boosting, MLP, kNN, naive Bayes and SVM. UCI letter recognition dataset with 20000 instances and 16 features is used, which I split in half for training and testing. I have experience with SVM so I quickly set its recognition error to 3.3%. After some experimentation what I got was:

UCI letter recognition:

 - RTrees - 5.3%
 - Boost - 13%
 - MLP - 7.9%
 - kNN(k=3) - 6.5%
 - Bayes - 11.5%
 - SVM - 3.3%

Parameters used:

 - RTrees - max_num_of_trees_in_the_forrest=200, max_depth=20,
   min_sample_count=1

 - Boost - boost_type=REAL, weak_count=200, weight_trim_rate=0.95,
   max_depth=7

 - MLP - method=BACKPROP, param=0.001, max_iter=300 (default values - too
   slow to experiment)

 - kNN(k=3) - k=3

 - Bayes - none

 - SVM - RBF kernel, C=10, gamma=0.01

After that I used same parameters and tested on Digits and MNIST datasets by extracting gradient features first (vector size 200 elements):

Digits:

 - RTrees - 5.1%
 - Boost - 23.4%
 - MLP - 4.3%
 - kNN(k=3) - 7.3%
 - Bayes - 17.7%
 - SVM - 4.2%

MNIST:

 - RTrees - 1.4%
 - Boost - out of memory
 - MLP - 1.0%
 - kNN(k=3) - 1.2%
 - Bayes - 34.33%
 - SVM - 0.6%

I am new to all classifiers except SVM and kNN, for these two I can say the results seem fine. What about others? I expected more from random trees, on MNIST kNN gives better accuracy, any ideas how to get it higher? Boost and Bayes give very low accuracy. In the end I''d like to use these classifiers to make a multiple classifier system. Any advice?', 1387, '2014-07-16 15:09:44.907', '00d3584e-9215-48b0-8e12-0d18c61f6aa7', 750, 'deleted 7 characters in body; edited tags', 1950, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><classification><svm><accuracy><random-forest>', 1387, '2014-07-16 15:09:44.907', '00d3584e-9215-48b0-8e12-0d18c61f6aa7', 750, 'deleted 7 characters in body; edited tags', 1951, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<education><definitions>', 97, '2014-07-16 15:35:15.697', 'f9d259f3-e80c-4704-9133-56a222e8ca35', 739, 'Re-tag to more relevant.', 1952, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-16 15:35:15.697', 'f9d259f3-e80c-4704-9133-56a222e8ca35', 739, 'Proposed by 97 approved by 434, -1 edit id of 114', 1953, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am an MSc student at the University of Edinburgh, specialized in machine learning and natural language processing. I had some practical courses focused on data mining, and others dealing with machine learning, bayesian statistics and graphical models. My background is a BSc in Computer Science.

I did some software engineering and I learnt the basic concepts, such as design patterns, but I have never been involved in a large software development project. However, I had a data mining project in my MSc. My question is, if I want to go for a career as Data Scientist, should I apply for a graduate data scientist position first, or should I get a position as graduate software engineer first, maybe something related to data science, such as big data infrastructure or machine learning software development?

My concern is that I might need good software engineering skills for data science, and I am not sure if these can be obtained by working as a graduate data scientist directly.

Moreover, at the moment I like Data Mining, but what if I want to change my career to software engineering in the future? It might be difficult if I specialised so much in data science.

I have not been employed yet, so my knowledge is still limited. Any clarification or advice are welcome, as I am about to finish my MSc and I want to start applying for graduate positions in early October.', 84, '2014-07-16 15:35:15.697', '87a0b5ea-32f4-4c45-aeab-f03a56202725', 739, 'Re-tag to more relevant.', 1954, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would like to use non-atomic data, as a feature for a prediction.
Suppose I have a Table with these features:

    - Column 1: Categorical - House
    - Column 2: Numerical - 23.22
    - Column 3: A Vector - [ 12, 22, 32 ]
    - Column 4: A Tree - [ [ 2323, 2323 ],[2323, 2323] , [ Boolean, Categorical ] ]
    - Column 5: A List [ 122, Boolean ]

I would like to predict/classify, for instance, Column 2.

I am making something to automatically respond to questions, any type of question, like "Where was Foo Born?" ...

I first make a query to a search engine, then I get some text data as a result, then I do all the parsing stuff (tagging, stemming, parsing, splitting ... )

My first approach was to make a table, each row with a line of text and a lot of features, like "First Word", "Tag of First Word", "Chunks", etc...

But with this approach I am missing the relationships between the sentences.

I would like to know if there is an algorithm that looks inside the tree structures (or vectors) and makes the relations and extract whatever is relevant for predicting/classifying. I''d prefer to know about a library that does that than an algorithm that I have to implement.', 1367, '2014-07-16 15:35:31.410', 'c3f1660e-97e3-4f1f-86ca-525f65381312', 658, 'Remove capitals and simplify the formulation', 1955, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-16 15:35:31.410', 'c3f1660e-97e3-4f1f-86ca-525f65381312', 658, 'Proposed by 1367 approved by 434, 84 edit id of 112', 1956, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am trying to understand a neuroscience article:

- Friston, Karl J., et al. "Action and behavior: a free-energy formulation." *Biological cybernetics* 102.3 (2010): 227-260. ([DOI 10.1007/s00422-010-0364-z](http://link.springer.com/article/10.1007%2Fs00422-010-0364-z))

In this article, Friston gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear on page 231 of the article as equation (5):

>The resulting expression for free-energy can be expressed in three ways (with the use of the Bayes rules and simple rearrangements):
>
> Energy minus entropy
>
> Divergence plus surprise
>
> Complexity minus accuracy
>
>Mathematically, these correspond to:
>
>![\begin {aligned} F &= - \langle \ln p(\tilde s, \Psi \vert m) \rangle _q + \langle \ln q(\Psi \vert \mu) \rangle _q \\ &= D(q(\Psi \vert \mu) \parallel p(\Psi \vert \tilde s, m)) - \ln p(\tilde s \vert m) & \qquad \qquad & (5) \\ &= D(q(\Psi \vert \mu) \parallel p(\Psi \vert m)) - \langle \ln p(\tilde s \vert \Psi, m) \rangle _q \end{aligned}][1]

The things I am struggling with at this point are 1) the meaning of the || in the 2nd and 3rd versions of the equations and 2) the negative logs. Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?


  [1]: http://i.stack.imgur.com/0eQgg.png', 322, '2014-07-16 16:18:18.573', '666b2591-8722-48a2-8842-464f0a678ac4', 726, 'make the title, citation, and quoted material all explicit and internal; fix equation formatting', 1957, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Trying to understand free-energy equations in a Karl Friston neuroscience article', 322, '2014-07-16 16:18:18.573', '666b2591-8722-48a2-8842-464f0a678ac4', 726, 'make the title, citation, and quoted material all explicit and internal; fix equation formatting', 1958, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<neuralnetwork>', 322, '2014-07-16 16:18:18.573', '666b2591-8722-48a2-8842-464f0a678ac4', 726, 'make the title, citation, and quoted material all explicit and internal; fix equation formatting', 1959, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-16 16:18:18.573', '666b2591-8722-48a2-8842-464f0a678ac4', 726, 'Proposed by 322 approved by 434, -1 edit id of 113', 1960, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am trying to understand a neuroscience article:

- Friston, Karl J., et al. "Action and behavior: a free-energy formulation." *Biological cybernetics* 102.3 (2010): 227-260. ([DOI 10.1007/s00422-010-0364-z](http://link.springer.com/article/10.1007%2Fs00422-010-0364-z))

In this article, Friston gives three equations that are, as I understand him, equivalent or inter-convertertable and refer to both physical and Shannon entropy. They appear on page 231 of the article as equation (5):

>The resulting expression for free-energy can be expressed in three ways (with the use of the Bayes rules and simple rearrangements):
>
> Energy minus entropy
>
> Divergence plus surprise
>
> Complexity minus accuracy
>
>Mathematically, these correspond to:
>
>![\begin {aligned} F &= - \langle \ln p(\tilde s, \Psi \vert m) \rangle _q + \langle \ln q(\Psi \vert \mu) \rangle _q \\ &= D(q(\Psi \vert \mu) \parallel p(\Psi \vert \tilde s, m)) - \ln p(\tilde s \vert m) & \qquad \qquad & (5) \\ &= D(q(\Psi \vert \mu) \parallel p(\Psi \vert m)) - \langle \ln p(\tilde s \vert \Psi, m) \rangle _q \end{aligned}][1]

The things I am struggling with at this point are:

1. the meaning of the || in the 2nd and 3rd versions of the equations;
2. and the negative logs.

Any help in understanding how these equations are actually what Fristen claims them to be would be greatly appreciated. For example, in the 1st equation, in what sense is the first term energy, etc?


  [1]: http://i.stack.imgur.com/0eQgg.png', 84, '2014-07-16 16:18:18.573', '1e001584-ee66-4c21-af7c-f0ebe3f67488', 726, 'make the title, citation, and quoted material all explicit and internal; fix equation formatting', 1961, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I thought this was an interesting problem, so I wrote a sample data set and a linear slope estimator in R.  I hope it helps you with your problem.  I''m going to make some assumptions, the biggest is that you want to estimate a constant slope, given by some segments in your data.  Another assumption to separate the blocks of linear data is that the natural ''reset'' will be found by comparing consecutive differences and finding ones that are X-standard deviations below the mean. (I chose 4 sd''s, but this can be changed)

Here is a plot of the data, and the code to generating it is at the bottom.
![Sample Data][1]


For starters, we find the breaks and fit each set of y-values and record the slopes.

    # Find the differences between adjacent points
    diffs = y_data[-1] - y_data[-length(y_data)]
    # Find the break points (here I use 4 s.d.''s)
    break_points = c(0,which(diffs < (mean(diffs) - 4*sd(diffs))),length(y_data))
    # Create the lists of y-values
    y_lists = sapply(1:(length(break_points)-1),function(x){
      y_data[(break_points[x]+1):(break_points[x+1])]
    })
    # Create the lists of x-values
    x_lists = lapply(y_lists,function(x) 1:length(x))
    #Find all the slopes for the lists of points
    slopes = unlist(lapply(1:length(y_lists), function(x) lm(y_lists[[x]] ~ x_lists[[x]])$coefficients[2]))

Here are the slopes:
(3.309110, 4.419178, 3.292029, 4.531126, 3.675178, 4.294389)

And we can just take the mean to find the expected slope (3.920168).

----------
**Edit: Predicting when series reaches 120**

I realized I didn''t finish predicted when series reaches 120.  If we estimate the slope to be m and we see a reset at time t to a value x (x<120), we can predict how much longer it would take to reach 120 by some simple algebra.

![enter image description here][2]

Here, t is the time it would take to reach 120 after a reset, x is what it resets to, and m is the estimated slope.  I''m not going to even touch the subject of units here, but it''s good practice to work them out and make sure everything makes sense.

----------

**Edit: Creating The Sample Data**

The sample data will consist of 100 points, random noise with a slope of 4 (Hopefully we will estimate this).  When the y-values reach a cutoff, they reset to 50.  The cutoff is randomly chosen between 115 and 120 for each reset.  Here is the R code to create the data set.

    # Create Sample Data
    set.seed(1001)
    x_data = 1:100 # x-data
    y_data = rep(0,length(x_data)) # Initialize y-data
    y_data[1] = 50
    reset_level = sample(115:120,1) # Select initial cutoff
    for (i in x_data[-1]){ # Loop through rest of x-data
      if(y_data[i-1]>reset_level){ # check if y-value is above cutoff
        y_data[i] = 50             # Reset if it is and
        reset_level = sample(115:120,1) # rechoose cutoff
      }else {
        y_data[i] = y_data[i-1] + 4 + (10*runif(1)-5) # Or just increment y with random noise
      }
    }
    plot(x_data,y_data) # Plot data


  [1]: http://i.stack.imgur.com/2dC1w.png
  [2]: http://i.stack.imgur.com/DixZv.gif', 375, '2014-07-16 16:36:02.310', '8daca80c-9e82-496b-9c48-1ff3148973fa', 729, 'Answered the Question', 1962, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('*I expected more from random trees*:

  - With random forests, typically for N features, sqrt(N) features are used for each decision tree construction. Since in your case *N*=20, you could try setting *max_depth* (the number of sub-features to construct each decision tree) to 5.

  - Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes. This might improve your accuracy.


*On MNIST kNN gives better accuracy, any ideas how to get it higher?*

  - Try with a higher value of *K* (say 5 or 7). A higher value of K would give you more supportive evidence about the class label of a point.
  - You could run PCA or Fisher''s Linear Discriminant Analysis before running k-nearest neighbour. By this you could potentially get rid of correlated features while computing distances between the points, and hence your k neighbours would be more robust.
  - Try different K values for different points based on the variance in the distances between the K neighbours.






', 984, '2014-07-16 17:34:24.880', '63c5c61f-4267-46ca-a17e-4cdcfe7b0cc1', 755, 1963, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('At the expense of over-simplication, latent features are ''hidden'' features to distinguish them from observed features. Latent features are computed from observed features using matrix factorization. An example would be text document analysis. ''words'' extracted from the documents are features. If you factorize the data of words you can find ''topics'', where ''topic'' is a group of words with semantic relevance. Low-rank matrix factorization maps several rows (observed features) to a smaller set of rows (latent features).
To elaborate, the document could have observed features (words) like [sail-boat, schooner, yatch, steamer, cruiser] which would ''factorize'' to latent feature (topic) like ''ship'' and ''boat''.

[sail-boat, schooner, yatch, steamer, cruiser, ...] -> [ship, boat]

The underlying idea is that latent features are semantically relevant ''aggregates'' of observered features. When you have large-scale, high-dimensional, and noisy observered features, it makes sense to build your classifier on latent features.

This is a of course a simplified description to elucidate the concept. You can read the details on Latent Dirichlet Allocation (LDA) or probabilistic Latent Semantic Analysis (pLSA) models for an accurate description.', 2515, '2014-07-16 18:15:42.343', 'cb1999e1-135d-4522-9369-8f5d109fc2ff', 756, 1964, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Gas usage has a daily cycle but there are also secondary weekly and annual cycles that the ARIMA may not be able to capture.

There is a very noticeable difference between the weekday and Saturday data. Try creating a subset of the data for each day of the week or splitting the data into weekday and weekend and applying the model.

If you can obtain temperature data for the same period check if there is a correlation between the temperature and gas usage.

As @Aleksandr Blekh said R does have good packages for ARIMA models', 325, '2014-07-16 18:26:54.233', 'f6fa3a99-56be-4e6c-8ef5-2e9fc6bca998', 757, 1965, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am working on a data science project using Python.
The project has several stages.
Each stage comprises of taking a data set, using Python scripts, auxiliary data, configuration and parameters, and creating another data set.
I store the code in git, so that part is covered.
I would like to hear about:

 1. Tools for data version control.
 2. Tools enabling to reproduce stages and experiments.
 3. Protocol and suggested directory structure for such a project.
 4. Automated build/run tools.

 ', 895, '2014-07-16 20:09:08.640', 'c9f8a8ad-657c-4431-9ad3-a00f5d595dd0', 758, 1966, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Tools and protocol for reproducible data science using Python', 895, '2014-07-16 20:09:08.640', 'c9f8a8ad-657c-4431-9ad3-a00f5d595dd0', 758, 1967, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><tools>', 895, '2014-07-16 20:09:08.640', 'c9f8a8ad-657c-4431-9ad3-a00f5d595dd0', 758, 1968, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This topic of *reproducible research* (RR) is **very popular** today and, consequently, is **huge**, but I hope that my answer will be **comprehensive enough** as an answer and will provide enough information for **further research**, should you decide to do so.

While Python-specific tools for RR certainly exist out there, I think it makes more sense to focus on more **universal tools** (you never know for sure what programming languages and computing environments you will be working with in the future). Having said that, let''s take a look what tools are available per your list.

1) **Tools for data version control**. Unless you plan to work with (very) *big data*, I guess, it would make sense to use the same `git`, which you use for source code version control. The infrastructure is already there. Even if your files are binary and big, this advice might be helpful: http://stackoverflow.com/questions/540535/managing-large-binary-files-with-git.

2) **Tools for managing RR workflows and experiments**. Here''s a list of most popular tools in this category, to the best of my knowledge (in the descending order of popularity):

- *Taverna Workflow Management System* (http://www.taverna.org.uk) - very solid, if a little too complex, set of tools. The major tool is a Java-based desktop software. However, it is compatible with online workflow repository portal myExperiment (http://www.myexperiment.org), where user can store and share their RR workflows. Web-based RR portal, fully compatible with Taverna is called Taverna Online, but it is being developed and maintained by totally different organization in Russia (referred there to as OnlineHPC: http://onlinehpc.com).

- *The Kepler Project* (https://kepler-project.org)

- *VisTrails* (http://vistrails.org)

- *Madagascar* (http://www.reproducibility.org)

**EXAMPLE**. Here''s an interesting article on scientific workflows with an example of the **real** workflow design and data analysis, based on using *Kepler* and *myExperiment* projects: http://f1000research.com/articles/3-110/v1.

There are many RR tools that implement *literate programming* paradigm, exemplified by `LaTeX` software family. Tools that help in report generation and presentation is also a large category, where `Sweave` and `knitr` are probably the most well-known ones. `Sweave` is a tool, focused on R, but it can be integrated with Python-based projects, albeit with some additional effort (http://stackoverflow.com/questions/2161152/sweave-for-python). I think that `knitr` might be a better option, as it''s modern, has extensive support by popular tools (such as `RStudio`) and is language-neutral (http://yihui.name/knitr/demo/engines).

3) **Protocol and suggested directory structure**. If I understood correctly what you implied by using term *protocol* (*workflow*), generally I think that standard RR data analysis workflow consists of the following sequential phases: *data collection* => *data preparation* (cleaning, transformation, merging, sampling) => *data analysis* => *presentation of results* (generating reports and/or presentations). Nevertheless, every workflow is project-specific and, thus, some specific tasks might require adding additional steps.

For sample directory structure, you may take a look at documentation for R package `ProjectTemplate` (http://projecttemplate.net), as an attempt to automate data analysis workflows and projects:

![enter image description here][1]

4) **Automated build/run tools**. Since my answer is focused on universal (language-neutral) RR tools, the most popular tools is `make`. Read the following article for some reasons to use `make` as the preferred RR workflow automation tool: http://bost.ocks.org/mike/make. Certainly, there are other **similar** tools, which either improve some aspects of `make`, or add some additional features. For example: `ant` (officially, Apache Ant: http://ant.apache.org), `Maven` ("next generation `ant`": http://maven.apache.org), `rake` (https://github.com/ruby/rake), `Makepp` (http://makepp.sourceforge.net). For a comprehensive list of such tools, see Wikipedia: http://en.wikipedia.org/wiki/List_of_build_automation_software.


  [1]: http://i.stack.imgur.com/0B2vo.png', 2452, '2014-07-17 06:02:04.813', 'f1bb9bb1-291f-4656-924b-2285cfd56666', 759, 1969, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am given a time series data vector (ordered by months and years),which contains only `0`s and `1`s. `1` s represent a person changes his job at a particular a month.

**Questions:** What model can i use to determine model how frequently this person change his job ? In addition, this model should be able to predict the probability of this person changing his in the next 6 months.

A poisson process ? (I have studied poisson process before however I have no idea when and how to apply it). Any assumptions that data need to meet before applying the poisson process ?

Would love to gather more information on how to model something like this. Thanks', 1315, '2014-07-17 09:26:11.833', 'db27118c-6e1e-4e07-a928-14244d2ea693', 760, 1970, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Given time series data, how to model the frequency of someone changes his job?', 1315, '2014-07-17 09:26:11.833', 'db27118c-6e1e-4e07-a928-14244d2ea693', 760, 1971, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><time-series>', 1315, '2014-07-17 09:26:11.833', 'db27118c-6e1e-4e07-a928-14244d2ea693', 760, 1972, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the right approach and clustering algorithm for geo location clustering?

I''m using the following code to cluster geolocation coordinates :

    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.cluster.vq import kmeans2, whiten

    coordinates= np.array([
               [lat, long],
               [lat, long],
                ...
               [lat, long]
               ])
    x, y = kmeans2(whiten(coordinates), 3, iter = 20)
    plt.scatter(coordinates[:,0], coordinates[:,1], c=y);
    plt.show()

Is it right to use Kmeans for location clustering, as it uses Euclidean distance and not Haversine formula as a distance function?', 2533, '2014-07-17 09:50:41.437', '6f98ad34-c53f-4499-918b-251754f61341', 761, 1973, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Clustering geo location coordinates (lat,long pairs)', 2533, '2014-07-17 09:50:41.437', '6f98ad34-c53f-4499-918b-251754f61341', 761, 1974, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><python><clustering><k-means><clusters>', 2533, '2014-07-17 09:50:41.437', '6f98ad34-c53f-4499-918b-251754f61341', 761, 1975, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('t-SNE, as in [1], works by progressively reducing the Kullback-Leibler (KL) divergence, until a certain condition is met.
The creators of t-SNE suggests to use KL divergence as a performance criterion for the visualizations:

> you can compare the Kullback-Leibler divergences that t-SNE reports. It is perfectly fine to run t-SNE ten times, and select the solution with the lowest KL divergence [2]

I tried two implementations of t-SNE:

* **python**: sklearn.manifold.TSNE().
* **R**: tsne, from library(tsne).

Both these implementations, when verbosity is set, print the error (Kullback-Leibler divergence) for each iteration. However, they don''t allow the user to get this information, which looks a bit strange to me.

For example, the code:

    import numpy as np
    from sklearn.manifold import TSNE
    X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
    model = TSNE(n_components=2, verbose=2, n_iter=200)
    t = model.fit_transform(X)
produces:

    [t-SNE] Computing pairwise distances...
    [t-SNE] Computed conditional probabilities for sample 4 / 4
    [t-SNE] Mean sigma: 1125899906842624.000000
    [t-SNE] Iteration 10: error = 6.7213750, gradient norm = 0.0012028
    [t-SNE] Iteration 20: error = 6.7192064, gradient norm = 0.0012062
    [t-SNE] Iteration 30: error = 6.7178683, gradient norm = 0.0012114
    ...
    [t-SNE] Error after 200 iterations: 0.270186

Now, as far as I understand, **0.270186** should be the KL divergence. However i cannot get this information, neither from **model** nor from **t** (which is a simple numpy.ndarray).

To solve this problem I could: i) Calculate KL divergence by my self, ii) Do something nasty in python for capturing and parsing TSNE() function''s output [3]. However: i) would be quite stupid to re-calculate KL divergence, when TSNE() has already computed it, ii) would be a bit unusual in terms of code.

Do you have any other suggestion? Is there a standard way to get this information using this library?

I mentioned I tried *R*''s tsne library, but I''d prefer the answers to focus on the *python* sklearn implementation.

------------
References

[1] http://nbviewer.ipython.org/urls/gist.githubusercontent.com/AlexanderFabisch/1a0c648de22eff4a2a3e/raw/59d5bc5ed8f8bfd9ff1f7faa749d1b095aa97d5a/t-SNE.ipynb

[2] http://homepage.tudelft.nl/19j49/t-SNE.html

[3] http://stackoverflow.com/questions/16571150/how-to-capture-stdout-output-from-a-python-function-call', 131, '2014-07-17 10:04:29.797', '2412e6b0-2c0d-4625-9b7a-eb265718cd57', 762, 1976, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('t-SNE Python/R implementations getting final divergence', 131, '2014-07-17 10:04:29.797', '2412e6b0-2c0d-4625-9b7a-eb265718cd57', 762, 1977, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><python>', 131, '2014-07-17 10:04:29.797', '2412e6b0-2c0d-4625-9b7a-eb265718cd57', 762, 1978, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A simple and perhaps somewhat naive approach would be to assume that a person changes jobs at a constant rate and that previous job changes have no influence on future ones. Under these assumptions you could model the job changes as a Poisson process and estimate the rate parameter using MLE (http://en.wikipedia.org/wiki/Poisson_process and http://en.wikipedia.org/wiki/Poisson_distribution).

Of course one should explore how well these assumptions hold in the data. To do this, you could study whether or not job changes are independent of one another by computing the correlation between events at various lags (http://en.wikipedia.org/wiki/Correlation). You could also plot the distribution of time between job change events. If the process is Poisson-like then you should observe little to no correlation between events at any number of lags and the distribution of time between job change events should be exponentially distributed (http://en.wikipedia.org/wiki/Exponential_distribution).

', 1329, '2014-07-17 10:26:45.547', '2d5339aa-cf02-4853-a15d-923726711ef7', 763, 1979, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('This topic of *reproducible research* (RR) is **very popular** today and, consequently, is **huge**, but I hope that my answer will be **comprehensive enough** as an answer and will provide enough information for **further research**, should you decide to do so.

While Python-specific tools for RR certainly exist out there, I think it makes more sense to focus on more **universal tools** (you never know for sure what programming languages and computing environments you will be working with in the future). Having said that, let''s take a look what tools are available per your list.

1) **Tools for data version control**. Unless you plan to work with (very) *big data*, I guess, it would make sense to use the same `git`, which you use for source code version control. The infrastructure is already there. Even if your files are binary and big, this advice might be helpful: http://stackoverflow.com/questions/540535/managing-large-binary-files-with-git.

2) **Tools for managing RR workflows and experiments**. Here''s a list of most popular tools in this category, to the best of my knowledge (in the descending order of popularity):

- *Taverna Workflow Management System* (http://www.taverna.org.uk) - very solid, if a little too complex, set of tools. The major tool is a Java-based desktop software. However, it is compatible with online workflow repository portal *myExperiment* (http://www.myexperiment.org), where user can store and share their RR workflows. Web-based RR portal, fully compatible with *Taverna* is called *Taverna Online*, but it is being developed and maintained by totally different organization in Russia (referred there to as *OnlineHPC*: http://onlinehpc.com).

- *The Kepler Project* (https://kepler-project.org)

- *VisTrails* (http://vistrails.org)

- *Madagascar* (http://www.reproducibility.org)

**EXAMPLE**. Here''s an interesting article on scientific workflows with an example of the **real** workflow design and data analysis, based on using *Kepler* and *myExperiment* projects: http://f1000research.com/articles/3-110/v1.

There are many RR tools that implement *literate programming* paradigm, exemplified by `LaTeX` software family. Tools that help in report generation and presentation is also a large category, where `Sweave` and `knitr` are probably the most well-known ones. `Sweave` is a tool, focused on R, but it can be integrated with Python-based projects, albeit with some additional effort (http://stackoverflow.com/questions/2161152/sweave-for-python). I think that `knitr` might be a better option, as it''s modern, has extensive support by popular tools (such as `RStudio`) and is language-neutral (http://yihui.name/knitr/demo/engines).

3) **Protocol and suggested directory structure**. If I understood correctly what you implied by using term *protocol* (*workflow*), generally I think that standard RR data analysis workflow consists of the following sequential phases: *data collection* => *data preparation* (cleaning, transformation, merging, sampling) => *data analysis* => *presentation of results* (generating reports and/or presentations). Nevertheless, every workflow is project-specific and, thus, some specific tasks might require adding additional steps.

For sample directory structure, you may take a look at documentation for R package `ProjectTemplate` (http://projecttemplate.net), as an attempt to automate data analysis workflows and projects:

![enter image description here][1]

4) **Automated build/run tools**. Since my answer is focused on universal (language-neutral) RR tools, the most popular tools is `make`. Read the following article for some reasons to use `make` as the preferred RR workflow automation tool: http://bost.ocks.org/mike/make. Certainly, there are other **similar** tools, which either improve some aspects of `make`, or add some additional features. For example: `ant` (officially, Apache Ant: http://ant.apache.org), `Maven` ("next generation `ant`": http://maven.apache.org), `rake` (https://github.com/ruby/rake), `Makepp` (http://makepp.sourceforge.net). For a comprehensive list of such tools, see Wikipedia: http://en.wikipedia.org/wiki/List_of_build_automation_software.


  [1]: http://i.stack.imgur.com/0B2vo.png', 2452, '2014-07-17 11:28:13.340', '626937c4-d005-44d1-84ac-8a6d97cdcbd1', 759, 'Minor updates: added emphasis (italics) for some software project titles.', 1980, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The topic of *reproducible research* (RR) is **very popular** today and, consequently, is **huge**, but I hope that my answer will be **comprehensive enough** as an answer and will provide enough information for **further research**, should you decide to do so.

While Python-specific tools for RR certainly exist out there, I think it makes more sense to focus on more **universal tools** (you never know for sure what programming languages and computing environments you will be working with in the future). Having said that, let''s take a look what tools are available per your list.

1) **Tools for data version control**. Unless you plan to work with (very) *big data*, I guess, it would make sense to use the same `git`, which you use for source code version control. The infrastructure is already there. Even if your files are binary and big, this advice might be helpful: http://stackoverflow.com/questions/540535/managing-large-binary-files-with-git.

2) **Tools for managing RR workflows and experiments**. Here''s a list of most popular tools in this category, to the best of my knowledge (in the descending order of popularity):

- *Taverna Workflow Management System* (http://www.taverna.org.uk) - very solid, if a little too complex, set of tools. The major tool is a Java-based desktop software. However, it is compatible with online workflow repository portal *myExperiment* (http://www.myexperiment.org), where user can store and share their RR workflows. Web-based RR portal, fully compatible with *Taverna* is called *Taverna Online*, but it is being developed and maintained by totally different organization in Russia (referred there to as *OnlineHPC*: http://onlinehpc.com).

- *The Kepler Project* (https://kepler-project.org)

- *VisTrails* (http://vistrails.org)

- *Madagascar* (http://www.reproducibility.org)

**EXAMPLE**. Here''s an interesting article on scientific workflows with an example of the **real** workflow design and data analysis, based on using *Kepler* and *myExperiment* projects: http://f1000research.com/articles/3-110/v1.

There are many RR tools that implement *literate programming* paradigm, exemplified by `LaTeX` software family. Tools that help in report generation and presentation is also a large category, where `Sweave` and `knitr` are probably the most well-known ones. `Sweave` is a tool, focused on R, but it can be integrated with Python-based projects, albeit with some additional effort (http://stackoverflow.com/questions/2161152/sweave-for-python). I think that `knitr` might be a better option, as it''s modern, has extensive support by popular tools (such as `RStudio`) and is language-neutral (http://yihui.name/knitr/demo/engines).

3) **Protocol and suggested directory structure**. If I understood correctly what you implied by using term *protocol* (*workflow*), generally I think that standard RR data analysis workflow consists of the following sequential phases: *data collection* => *data preparation* (cleaning, transformation, merging, sampling) => *data analysis* => *presentation of results* (generating reports and/or presentations). Nevertheless, every workflow is project-specific and, thus, some specific tasks might require adding additional steps.

For sample directory structure, you may take a look at documentation for R package `ProjectTemplate` (http://projecttemplate.net), as an attempt to automate data analysis workflows and projects:

![enter image description here][1]

4) **Automated build/run tools**. Since my answer is focused on universal (language-neutral) RR tools, the most popular tools is `make`. Read the following article for some reasons to use `make` as the preferred RR workflow automation tool: http://bost.ocks.org/mike/make. Certainly, there are other **similar** tools, which either improve some aspects of `make`, or add some additional features. For example: `ant` (officially, Apache Ant: http://ant.apache.org), `Maven` ("next generation `ant`": http://maven.apache.org), `rake` (https://github.com/ruby/rake), `Makepp` (http://makepp.sourceforge.net). For a comprehensive list of such tools, see Wikipedia: http://en.wikipedia.org/wiki/List_of_build_automation_software.


  [1]: http://i.stack.imgur.com/0B2vo.png', 2452, '2014-07-17 11:42:25.753', '48b149f5-3bcf-4963-b76b-57182cf8f7bd', 759, 'Fixed typo.', 1981, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('t-SNE Python implementation: Kullback-Leibler divergence', 131, '2014-07-17 12:20:20.850', 'bf161695-5ab1-4334-8169-c3f64220b70a', 762, 'edited title', 1982, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('K-means should be right in this case. Since k-means tries to group based solely on euclidean distance between objects you will get back clusters of locations that are close to each other.

To find the optimal number of clusters you can try making an ''elbow'' plot of the within group sum of square distance. This may be helpful (http://nbviewer.ipython.org/github/nborwankar/LearnDataScience/blob/master/notebooks/D3.%20K-Means%20Clustering%20Analysis.ipynb)', 802, '2014-07-17 12:34:11.397', '5bbff1e7-9e39-453f-a5dd-5a54f13fe870', 764, 1983, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve seen the same thing: My target variables are non-negative, but sklearn''s GBR sometimes makes negative predictions.  To grossly oversimplify, GBR is just using averages of subsets of the target variable to make predictions, so if the target variables are non-negative, then the predictions should be non-negative.

Also, I can provide an example, if anyone''s interested.


I''d make this a comment, but I don''t have enough reputation.
', 2543, '2014-07-17 13:19:46.817', '0e9c3d8d-81c5-4178-a688-c7770b7ee5d8', 765, 1984, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The TSNE source in scikit-learn is in pure Python. Fit `fit_transform()` method is actually calling a private `_fit()` function which then calls a private `_tsne()` function. That `_tsne()` function has a local variable `error` which is printed out at the end of the fit. Seems like you could pretty easily change one or two lines of source code to have that value returned to `fit_transform()`.', 159, '2014-07-17 14:07:12.643', '152aca45-5368-44c0-bf89-c6b9beb063ad', 766, 1985, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A great reproducibility tool for Python is of course [IPython Notebook][1] (don''t forget the [%logon and %logstart][2] magics to keep a history of all your experiments).

Another more general tool working with any language (with a Python API on [pypi][3]) is [Sumatra][4], which is specifically designed to produce **replicable** research (which aims to produce the same results given the exact same code and softwares, whereas reproducibility aims to produce the same results given any medium).


  [1]: http://ipython.org/
  [2]: https://damontallen.github.io/IPython-quick-ref-sheets/
  [3]: https://pypi.python.org/pypi/Sumatra
  [4]: http://neuralensemble.org/sumatra/', 2544, '2014-07-17 14:28:38.207', 'a61030fd-4b33-49f9-9653-8207b19035d3', 767, 1986, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A great reproducibility tool for Python with a low learning curve is of course [**IPython Notebook**][1] (don''t forget the [%logon and %logstart][2] magics to keep a history of all your experiments).

Another more general tool working with any language (with a Python API on [pypi][3]) is [**Sumatra**][4], which is specifically designed to produce **replicable** research (which aims to produce the same results given the exact same code and softwares, whereas reproducibility aims to produce the same results given any medium). Here is how Sumatra works: for each experiment that you conduct through Sumatra, this software will act like a "save game state" often found in videogames. More precisely, it will will save:

- all the parameters you provided;
- the exact sourcecode state of your whole experimental application and config files;
- the output/plots/results and also any file produced by your experimental application.

It will then construct a database with the timestamp and other metadatas for each of your experiments, that you can later crawl using the webGUI. Since Sumatra saved the full state of your application for a specific experiment at one specific point in time, you can restore the code that produced a specific result at any moment you want, thus you have replicable research at a low cost (except for storage if you work on huge datasets, but you can configure exceptions if you don''t want to save everything everytime).


  [1]: http://ipython.org/
  [2]: https://damontallen.github.io/IPython-quick-ref-sheets/
  [3]: https://pypi.python.org/pypi/Sumatra
  [4]: http://neuralensemble.org/sumatra/', 2544, '2014-07-17 14:35:57.760', 'e1670543-a442-4974-93c2-357075788172', 767, 'Added more infos about Sumatra', 1987, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Since I started doing research in academia I was constantly looking for a satisfactory workflow.
I think that I finally found something I am happy with:

1) Put everything under version control, e.g., Git:

For hobby research projects I use GitHub, for research at work I use the private GitLab server that is provided by our university. I also keep my datasets there.

2) I do most of my analyses along with the documentation on IPython notebooks. It is very organized (for me) to have the code, the plots, and the discussion/conclusion all in one document
If I am running larger scripts, I would usually put them into separate script .py files, but I would still execute them from the IPython notebook via the %run magic to add information about the purpose, outcome, and other parameters.

I have written a small cell-magic extension for IPython and IPython notebooks, called "watermark" that I use to conveniently create time stamps and keep track of the different package versions I used and also Git hashs

For example

<br>

    %watermark

    29/06/2014 01:19:10

    CPython 3.4.1
    IPython 2.1.0

    compiler   : GCC 4.2.1 (Apple Inc. build 5577)
    system     : Darwin
    release    : 13.2.0
    machine    : x86_64
    processor  : i386
    CPU cores  : 2
    interpreter: 64bit
<br>

    %watermark -d -t

    29/06/2014 01:19:11

<br>

    %watermark -v -m -p numpy,scipy

    CPython 3.4.1
    IPython 2.1.0

    numpy 1.8.1
    scipy 0.14.0

    compiler   : GCC 4.2.1 (Apple Inc. build 5577)
    system     : Darwin
    release    : 13.2.0
    machine    : x86_64
    processor  : i386
    CPU cores  : 2
    interpreter: 64bit

For more info, see the [documentation here](http://nbviewer.ipython.org/github/rasbt/python_reference/blob/master/ipython_magic/watermark.ipynb).


', 2556, '2014-07-17 15:19:32.237', '8f174ddc-0ef1-437a-91b7-76e8f046aa81', 768, 1988, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Dimensionality Reduction**

Another important procedure is to compare the error rates on training and test dataset to see if you are overfitting (due to the "curse of dimensionality"). E.g., if your error rate on the test dataset is much larger than the error on the training data set, this would be one indicator.
In this case, you could try dimensionality reduction techniques, such as PCA or LDA.

If you are interested, I have written about PCA, LDA and some other techniques here: http://sebastianraschka.com/index.html#machine_learning and in my GitHub repo here: https://github.com/rasbt/pattern_classification

**Cross validation**

Also you may want to take a look at cross-validation techniques in order to evaluate the performance of your classifiers in a more objective manner', 2556, '2014-07-17 15:28:25.940', '59b43a3b-dff1-41ee-bbda-ee3f23dbe722', 769, 1989, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am not an export in using SVMs, but usually (if you are using a machine learning library like Python''s `scikit-learn` or R''s `libsvm`, there is the `class_weight` parameter, or `class.weights`, respectively.

Or if you''d use a Bayes classifier, you would take this "skew" into account via the "prior (class) probabilities" P(&omega;<sub>j</sub>)', 2556, '2014-07-17 15:35:17.487', '306f4428-c77d-4e1c-abac-dd6fce2e2988', 770, 1990, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From what I heard, Pylearn2 might be currently the library of choice for most people. This reminds me of a recent blog post a few month ago that lists all the different machine learning libraries with a short explanation

https://www.cbinsights.com/blog/python-tools-machine-learning

The section you might be interested in here would be "Deep Learning". About Pylearn2, he writes

> PyLearn2
>
> There is another library built on top of Theano, called PyLearn2 which
> brings modularity and configurability to Theano where you could create
> your neural network through different configuration files so that it
> would be easier to experiment different parameters. Arguably, it
> provides more modularity by separating the parameters and properties
> of neural network to the configuration file.', 2556, '2014-07-17 18:25:44.683', '7d554780-209e-48e4-847c-8f20378551d3', 771, 1992, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Why use mean revenue in a split test?', 1333, '2014-07-17 19:58:22.467', '34bb614a-4a9c-4314-9dc5-20eb6a093fbc', 748, 'spelling error in title', 1995, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-17 19:58:22.467', '34bb614a-4a9c-4314-9dc5-20eb6a093fbc', 748, 'Proposed by 1333 approved by 434, 2511 edit id of 116', 1996, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Think geometrically. Cosine similarity only cares about angle difference, while dot product cares about angle and magnitude. If you normalize your data to have the same magnitude, the two are indistinguishable. Sometimes it is desirable to ignore the magnitude, hence cosine similarity is nice, but if magnitude plays a role, dot product would be better as a similarity measure. Note that neither of them is a "distance metric".', 154, '2014-07-17 20:02:11.227', 'c228f7c0-338f-4c3e-b625-2d35f500bea0', 773, 1997, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The following **general** answer is my uneducated guess, so take it with grain of salt. Hopefully, it makes sense. I think that the best way to describe or analyze *experiments* (as any other *systems*, in general) is to build their **statistical (multivariate) models** and evaluate them. Depending on whether *environments* for your set of experiments are represented by the **same model or different**, I see the following **approaches**:

1) **Single model approach.** *Define* experiments'' statistical model for all environments (dependent and independent variables, data types, assumptions, constraints). *Analyze* it (most likely, using *regression analysis*). *Compare* results **across variables**, which determine (influence) different environments.

2) **Multiple models approach.** The same steps as previous case, but *compare* results **across models**, corresponding to different environments.', 2452, '2014-07-18 04:50:58.287', 'c396573e-97ba-47cf-82d6-46663fdf3c07', 774, 1999, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Be sure to check out [docker][1]! And in general, all the other good things that software engineering has created along decades for ensuring isolation and reproductibility.

I would like to stress that it is not enough to have *just* reproducible workflows, but also *easy* to reproduce workflows. Let me show what I mean. Suppose that your project uses Python, a database X and Scipy. Most surely you will be using a specific library to connect to your database from Python, and Scipy will be in turn using some sparse algebraic routines. This is by all means a very simple setup, but not entirely simple to setup, pun intended. If somebody wants to execute your scripts, she will have to install all the dependencies. Or worse, she might have incompatible versions of it already installed. Fixing those things takes time. It will also take time to you if you at some moment need to move your computations to a cluster, to a different cluster, or to some cloud servers.

Here is where I find docker useful. Docker is a way to formalize and compile recipes for binary environments. You can write the following in a dockerfile (I''m using here plain English instead of the Dockerfile syntax):

* Start with a basic binary environment, like Ubuntu''s
* Install libsparse-dev
* (Pip) Install numpy and scipy
* Install X
* Install libX-dev
* (Pip) Install python-X
* Install IPython-Notebook
* Copy my python scripts/notebooks to my binary environment, these datafiles, and these configurations to do other miscellaneous things. To ensure reproductibility, copy them from a named url instead of a local file.
* Maybe run IPython-Notebook.

Some of the lines will be installing things in Python using pip, since pip can do a very clean work in selecting specific package versions. Check it out too!

And that''s it. If after you create your Dockerfile it can be built, then it can be built anywhere, by anybody (provided they also have access to your project-specific files, e.g. because you put them in a public url referenced from the Dockerfile). What is best, you can upload the resulting environment (called an "image") to a public or private server (called a "register") for other people to use. So, when you publish your workflow, you have both a fully reproducible recipe in the form of a Dockerfile, and an easy way for you or other people to reproduce what you do:

    docker run dockerregistery.thewheezylab.org/nowyouwillbelieveme

Or if they want to poke around in your scripts and so forth:

    docker run -i -t dockerregistery.thewheezylab.org/nowyouwillbelieveme /bin/bash

  [1]: https://www.docker.com/', 2575, '2014-07-18 07:43:56.823', '08922425-28ee-4535-887a-0c6ea6e098cd', 775, 2000, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A great reproducibility tool for Python with a low learning curve is of course [**IPython Notebook**][1] (don''t forget the [%logon and %logstart][2] magics and/or [Git][3] to keep a history of all your experiments).

Another more general tool working with any language (with a Python API on [pypi][4]) is [**Sumatra**][5], which is specifically designed to produce **replicable** research (which aims to produce the same results given the exact same code and softwares, whereas reproducibility aims to produce the same results given any medium). Here is how Sumatra works: for each experiment that you conduct through Sumatra, this software will act like a "save game state" often found in videogames. More precisely, it will will save:

- all the parameters you provided;
- the exact sourcecode state of your whole experimental application and config files;
- the output/plots/results and also any file produced by your experimental application.

It will then construct a database with the timestamp and other metadatas for each of your experiments, that you can later crawl using the webGUI. Since Sumatra saved the full state of your application for a specific experiment at one specific point in time, you can restore the code that produced a specific result at any moment you want, thus you have replicable research at a low cost (except for storage if you work on huge datasets, but you can configure exceptions if you don''t want to save everything everytime).

/EDIT: [dsign][6] touched a very important point here: the replicability of your setup is as important as the replicability of your application. In other words, you should at least provide a **full list of the libraries and compilers** you used along with their exact **versions** and the details of your **platform**.

Personally, in scientific computing with Python, I have found that packaging an application along with the libraries is just too painful, thus I now just use an all-in-one scientific python package such as [Anaconda][7] (with the great package manager [conda][8]), and just advise users to use the same package. Another solution could be to provide a script to automatically generate a [virtualenv][9], or to package everything using the [Docker application as cited by dsign][10].


  [1]: http://ipython.org/
  [2]: https://damontallen.github.io/IPython-quick-ref-sheets/
  [3]: https://en.wikipedia.org/wiki/Git_(software)
  [4]: https://pypi.python.org/pypi/Sumatra
  [5]: http://neuralensemble.org/sumatra/
  [6]: http://datascience.stackexchange.com/a/775
  [7]: https://store.continuum.io/cshop/anaconda/
  [8]: http://www.continuum.io/blog/conda
  [9]: http://docs.python-guide.org/en/latest/dev/virtualenvs/
  [10]: http://datascience.stackexchange.com/a/775', 2544, '2014-07-18 09:47:43.920', 'bc139639-9bcc-4f2c-91b1-0f79c5b2c592', 767, 'Added talk about setup replicability', 2001, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am far from an expert, but my understanding of the subject tells me that R (superb in statistics) and e.g. Python (superb in several of those things where R is lacking) complements each other quite well (as pointed out by previous posts). ', 2583, '2014-07-18 15:24:38.007', 'ee936e7a-b365-4844-9baa-0ed5beb486b0', 776, 2002, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<education><definitions><career>', 553, '2014-07-18 19:29:05.073', '00f60075-0136-4752-a7e8-560b95d85763', 739, 'adding a better tag for career-related questions', 2003, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-18 19:29:05.073', '00f60075-0136-4752-a7e8-560b95d85763', 739, 'Proposed by 553 approved by 434, 2489 edit id of 115', 2004, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('New to the Data Science forum, and first poster here!

This may be kind of a specific question (hopefully not too much so), but one I''d imagine others might be interested in.

I''m looking for a way to basically query GitHub with something like this:

    Give me a collection of all of the public repositories that have more than 10 stars, at
    least two forks, and more than three committers.

The result could take any viable form: a JSON data dump, a URL to the web page, etc. It more than likely will consist of information from 10,000 repos or something large.

Is this sort of thing possible using the API or some other pre-built way, or am I going to have to build out my own custom solution where I try to scrape every page? If so, how feasible is this and how might I approach it?', 2599, '2014-07-18 22:29:05.017', '5bf76b73-30f6-4b4f-a8b8-32dc30cfc663', 777, 2006, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Getting GitHub repository information by different criteria', 2599, '2014-07-18 22:29:05.017', '5bf76b73-30f6-4b4f-a8b8-32dc30cfc663', 777, 2007, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><data-mining><python><dataset>', 2599, '2014-07-18 22:29:05.017', '5bf76b73-30f6-4b4f-a8b8-32dc30cfc663', 777, 2008, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I read in this post https://datascience.stackexchange.com/questions/41/is-the-r-language-suitable-for-big-data that big data constitutes `5TB`, and while it does a good job of providing information about the feasibility of working with this type of data in `R` it provides very little information about `Python`. I was wondering if `Python` can work with this much data as well. ', 890, '2014-07-18 22:34:48.080', 'f8a40563-efcb-4bbd-a51e-8cb7d4c4b289', 778, 2009, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is Python suitable for big data', 890, '2014-07-18 22:34:48.080', 'f8a40563-efcb-4bbd-a51e-8cb7d4c4b289', 778, 2010, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><python>', 890, '2014-07-18 22:34:48.080', 'f8a40563-efcb-4bbd-a51e-8cb7d4c4b289', 778, 2011, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I believe the language itself has little to do with performance capabilities, when it comes to large data. What matters is:

 - How large is the data actually
 - What processing are you going to perform on it
 - What hardware are you going to use
 - Which are the specific libraries that you plan to use

Anyway, Python is well adopted in data science communities.', 2600, '2014-07-18 22:59:39.100', 'c5c6e364-6605-419b-aa8c-d730e69e7907', 779, 2012, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Some good answers here. I would like to join the discussion by adding the following two **notes**:

1) The question''s emphasis on the **volume of data** while referring to *Big Data* is certainly understandable and valid, especially considering the **problem** of data volume growth **outpacing** technological capacities'' exponential growth per *Moore''s Law* (http://en.wikipedia.org/wiki/Moore%27s_law).

2) Having said that, it is important to remember about other aspects of big data concept, based on *Gartner*''s definition (emphasis mine - AB): "*Big data* is high **volume**, high **velocity**, and/or high **variety** information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization." (usually referred to as the "**3Vs model**"). I mention this, because it forces data scientists and other analysts to look for and use R packages that focus on **other than volume** aspects of big data (enabled by the **richness** of enormous *R ecosystem*).

2) While existing answers mention some R packages, related to big data, for a more **comprehensive coverage**, I''d recommend to refer to *CRAN Task View* **"High-Performance and Parallel Computing with R"** (http://cran.r-project.org/web/views/HighPerformanceComputing.html), in particular, sections **"Parallel computing: Hadoop"** and **"Large memory and out-of-memory data"**.', 2452, '2014-07-19 02:19:46.530', '5460b8c4-59b8-42bf-bc47-d0bce4022d84', 780, 2013, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To clarify, I feel like the original question references by OP probably isn''t be best for a SO-type format, but I will certainly represent `python` in this particular case.

Let me just start by saying that regardless of your data size, `python` shouldn''t be your limiting factor. In fact, there are just a couple main issues that you''re going to run into dealing with large datasets:

 - **Reading data into memory** - This is by far the most common issue faced in the world of big data. Basically, you can''t read in more data than you have memory (RAM) for. The best way to fix this is by making atomic operations on your data instead of trying to read everything in at once.
 - **Storing data** - This is actually just another form of the earlier issue, by the time to get up to about `1TB`, you start having to look elsewhere for storage. AWS S3 is the most common resource, and `python` has the fantastic `boto` library to facilitate leading with large pieces of data.
 - **Network latency** - Moving data around between different services is going to be your bottleneck. There''s not a huge amount you can do to fix this, other than trying to pick co-located resources and plugging into the wall.', 548, '2014-07-19 03:29:02.647', '45c1894e-8578-4eec-807e-6e7f699941c5', 781, 2014, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My limited understanding, based on brief browsing *GitHub API* documentation, is that currently there is NO **single API request** that supports **all** your listed criteria **at once**. However, I think that you could use the following sequence in order to achieve the goal from your example (at least, I would use this approach):

1) **Request** information on all public repositories (API returns *summary representations* only): https://developer.github.com/v3/repos/#list-all-public-repositories;

2) **Loop** through the list of all public repositories retrieved in step 1, requesting individual resources, and save it as new (detailed) list (this returns *detailed representations*, in other words, all attributes): https://developer.github.com/v3/repos/#get;

3) **Loop** through the detailed list of all repositories, filtering corresponding fields by your criteria. For your example request, you''d be interested in the following attributes of the **parent** object: *stargazers_count*, *forks_count*. In order to filter the repositories by number of committers, you could use a separate API: https://developer.github.com/v3/repos/#list-contributors.

Updates or comments from people more familiar with GitHub API are welcome!', 2452, '2014-07-19 03:42:34.433', 'f2d1fadf-da9e-4e97-af6c-eba59342b605', 782, 2015, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to plot the bytes from a disk image in order to understand a pattern in them. This is mainly an academic task, since I''m almost sure this pattern was created by a disk testing program, but I''d like to reverse-engineer it anyway.

I already know that the pattern is aligned, with a periodicity of 256 characters.

I can envision two ways of visualizing this information: either a 16x16 plane viewed through time (3 dimensions), where each pixel''s color is the ASCII code for the character, or a 256 pixel line for each period (2 dimensions).

This is a snapshot of the pattern (you can see more than one), seen through `xxd` (32x16):

![Pattern to analyze][1]

Either way, I am trying to find a way of visualizing this information. This probably isn''t hard for anyone into signal analysis, but I can''t seem to find a way using open-source software.

I''d like to avoid Matlab or Mathematica and I''d prefer an answer in R, since I have been learning it recently, but nonetheless, any language is welcome.


  [1]: http://i.stack.imgur.com/zOFSK.gif', 2604, '2014-07-19 05:27:22.773', 'c8ff1782-7ec8-46ba-bcce-3b7a57142e12', 783, 2016, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data visualization for pattern analysis (language-independent, but R preferred)', 2604, '2014-07-19 05:27:22.773', 'c8ff1782-7ec8-46ba-bcce-3b7a57142e12', 783, 2017, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><visualization>', 2604, '2014-07-19 05:27:22.773', 'c8ff1782-7ec8-46ba-bcce-3b7a57142e12', 783, 2018, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a timeseries with hourly gas consumption. I want to use ARMA/ARIMA to forecast the consumption on the next hour, basing on the previous. Why should I analyze/find the seasonality (with STL?)?

![enter image description here][1]


  [1]: http://i.stack.imgur.com/hYyH8.png', 989, '2014-07-19 18:31:36.573', 'ad29df50-b321-463c-8345-722cd8de5ffd', 784, 2019, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why should I care about seasonal data when I forecast?', 989, '2014-07-19 18:31:36.573', 'ad29df50-b321-463c-8345-722cd8de5ffd', 784, 2020, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 989, '2014-07-19 18:31:36.573', 'ad29df50-b321-463c-8345-722cd8de5ffd', 784, 2021, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I know almost nothing about signal analysis, but 2-dimensional visualization could be easily done using R. Particularly you will need `reshape2` and `ggplot2` packages. Assuming your data is **wide** (e.g. [n X 256] size), first you need to transform it to [**long**][1] format using `melt()` function from `reshape2` package. Then use [`geom_tile`][2] geometry from `ggplot2`. Here is a nice [recipe][3] with [gist][4].


  [1]: http://www.cookbook-r.com/Manipulating_data/Converting_data_between_wide_and_long_format/
  [2]: http://docs.ggplot2.org/current/geom_tile.html
  [3]: http://www.r-bloggers.com/simplest-possible-heatmap-with-ggplot2/
  [4]: https://gist.github.com/dsparks/3710171', 941, '2014-07-19 18:47:38.787', 'ea8ddfa8-24f5-4b05-bb56-66d7892f03df', 785, 2022, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to find stock data to practice with, is there a good resource for this? I found this: ftp://emi.nasdaq.com/ITCH/ but it only has the current year.

I already have a way of parsing the protocol, but would like to have some more data to compare with. It doesn''t have to be in the same format, as long as it has price, trades, and date statistics.', 2567, '2014-07-19 20:46:52.740', 'e62e1112-9944-4e97-be28-29507c9f2c92', 786, 2023, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('NASDAQ Trade Data', 2567, '2014-07-19 20:46:52.740', 'e62e1112-9944-4e97-be28-29507c9f2c92', 786, 2024, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><dataset>', 2567, '2014-07-19 20:46:52.740', 'e62e1112-9944-4e97-be28-29507c9f2c92', 786, 2025, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am a relatively new user to Hadoop (using version 2.4.1). I installed hadoop on my first node without a hitch, but I can''t seem to get the Resource Manager to start on my second node.

I cleared up some "shared library" problems by adding this to yarn-env.sh and hadoop-env.sh:

>  export HADOOP_HOME="/usr/local/hadoop"
>
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"

I also added this to hadoop-env.sh:

> export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native

based on the advice of this post at horton works http://hortonworks.com/community/forums/topic/hdfs-tmp-dir-issue/

That cleared up all of my error messages; when I run /sbin/start-yarn.sh I get this:
>
starting yarn daemons
>
starting resourcemanager,
> logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-HdNode.out
>
localhost: starting nodemanager,
> logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-HdNode.out

The only problem is, JPS says that the Resource Manager isn''t running.

What''s going on here?', 2614, '2014-07-19 20:51:58.527', 'eb091561-9142-42a6-915a-94d1c83bdf8d', 787, 2026, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hadoop Resource Manager Won''t Start', 2614, '2014-07-19 20:51:58.527', 'eb091561-9142-42a6-915a-94d1c83bdf8d', 787, 2027, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop>', 2614, '2014-07-19 20:51:58.527', 'eb091561-9142-42a6-915a-94d1c83bdf8d', 787, 2028, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a timeseries with hourly gas consumption. I want to use [ARMA](http://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model)/[ARIMA](http://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) to forecast the consumption on the next hour, basing on the previous. Why should I analyze/find the seasonality (with [Seasonal and Trend decomposition using Loess](https://www.otexts.org/fpp/6/5) (STL)?)?

![enter image description here][1]


  [1]: http://i.stack.imgur.com/hYyH8.png', 84, '2014-07-20 19:34:08.117', '7d2c939c-151f-4c1a-a1fb-9a5361070727', 784, 'Improving question.', 2032, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><time-series>', 84, '2014-07-20 19:34:08.117', '7d2c939c-151f-4c1a-a1fb-9a5361070727', 784, 'Improving question.', 2033, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Check your version of JPS and make sure it''s the same as the version of java that you are running.  Sometimes you start out with an out of the box jdk install, upgrade, set alternatives for the java bin, but still have the original jps binary being referenced.

Run `ps -ef |grep java` and look for the resource manager threads.  Maybe it''s actually running.  If it is, try `update-alternatives --config jps` to see what binary jps is pointing at.

If the resource manager is not running, it''s time to do some basic linux troubleshooting.  Check log files and barring that check actual command output.

On the system I''m looking at now, the log files for resource manager are placed in the `hadoop-install/logs` directory in `yarn-username-resourcemanager-hostname.log` and `yarn-user-resourcemanager-hostname.out`.  Your configuration may place them in /var/logs or what have you.  Also, have a look at the syslog.

If the logs don''t yield any good information, which can happen, my process is to generally try to figure out the command line from the startup script (usually by prefixing the command line with `echo`), and then trying to run the command directly to see the output as it comes out.

---
I have actually run into this problem before, but I can''t remember the specific issue.  I''m sure the same result can manifest itself from a variety of problems.  Considering that you are as far as you are in the process of getting set up, I believe it''s likely to be a minor configuration issue.', 434, '2014-07-20 21:37:27.857', '1f1f84d6-de5d-4cf9-8534-60aa2ad860fa', 788, 2034, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Check your version of JPS and make sure it''s the same as the version of java that you are running.  Sometimes you start out with an out of the box jdk install, upgrade, set alternatives for the java bin, but still have the original jps binary being referenced.

Run `ps -ef |grep java` and look for the resource manager threads.  Maybe it''s actually running.  If it is, try `update-alternatives --config jps` to see what binary jps is pointing at and compare it with the java binary that you are using.

If the resource manager is not running, it''s time to do some basic linux troubleshooting.  Check log files and barring that check actual command output.

On the system I''m looking at now, the log files for resource manager are placed in the `hadoop-install/logs` directory in `yarn-username-resourcemanager-hostname.log` and `yarn-user-resourcemanager-hostname.out`.  Your configuration may place them in /var/logs or what have you.  Also, have a look at the syslog.

If the logs don''t yield any good information, which can happen, my process is to generally try to figure out the command line from the startup script (usually by prefixing the command line with `echo`), and then trying to run the command directly to see the output as it comes out.

---
I have actually run into this problem before, but I can''t remember the specific issue.  I''m sure the same result can manifest itself from a variety of problems.  Considering that you are as far as you are in the process of getting set up, I believe it''s likely to be a minor configuration issue.', 434, '2014-07-20 21:48:25.257', 'a315ce39-e5c5-4b0d-b8dc-a09791e46432', 788, 'added 55 characters in body', 2035, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can pull stock data very easyly in python and R (probably other languages as well) with the following packages:

In python with: https://pypi.python.org/pypi/ystockquote

This is also a really nice tutorial in iPython which shows you how to pull the stock data and play with it: http://nbviewer.ipython.org/github/twiecki/financial-analysis-python-tutorial/blob/master/1.%20Pandas%20Basics.ipynb

In R with: http://www.quantmod.com/

HTH.
', 802, '2014-07-20 23:54:09.070', '81a40b9c-4aca-440c-80c6-e588e1cae8fd', 789, 2036, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Absolutely. When you''re working with data at that scale it''s common to use a big data framework, in which case python or whatever language you''re using is merely an interface. See for example [Spark''s Python Programming Guide](http://spark.apache.org/docs/0.9.1/python-programming-guide.html). What kind of data do you have and what do you want to do with it?', 381, '2014-07-21 00:35:13.440', '58f77885-f005-42cb-a2dd-7cd3116a4ce3', 790, 2037, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to define a metric between job titles in IT field. For this I need some metric between words of job titles that are not appearing together in the same job title, e.g. metric between the words

> senior, primary, lead, head, vp, director, stuff, principal, chief,

or the words

> analyst, expert, modeler, researcher, scientist.


 1. So, how to get all such possible words with their distance ?

 2. Can you post this type of words with their distance as an answer based on your own experience ?', 921, '2014-07-21 09:00:04.917', '5b30ca0a-3cee-46c6-b406-798b43bf91eb', 791, 2038, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Job title similarity', 921, '2014-07-21 09:00:04.917', '5b30ca0a-3cee-46c6-b406-798b43bf91eb', 791, 2039, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><dataset>', 921, '2014-07-21 09:00:04.917', '5b30ca0a-3cee-46c6-b406-798b43bf91eb', 791, 2040, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To handle such amount of data, programming language is not the main concern but the programming framework is. Frameworks such as MapReduce or Spark have bindings to many languages including Python. These frameworks certainly have many ready-to-use packages for data analysis tasks. But in the end it all comes to your requirement, i.e., what is your task? People have different definitions of data analysis tasks, some of them can be easily solved with relational databases. In that case, SQL is much better than all other alternatives.', 743, '2014-07-21 11:59:49.203', 'abedd327-c5b1-4dbb-b856-52674b104d17', 792, 2041, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the major uses of NOSQL databases like mongodb in the field of data science?', 2643, '2014-07-21 13:41:13.427', '43d01a84-e725-48f2-ab11-f6cc00a2b857', 793, 2042, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Uses of NOSQL database in data science', 2643, '2014-07-21 13:41:13.427', '43d01a84-e725-48f2-ab11-f6cc00a2b857', 793, 2043, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><nosql><mongodb>', 2643, '2014-07-21 13:41:13.427', '43d01a84-e725-48f2-ab11-f6cc00a2b857', 793, 2044, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Pylearn2 seems to be the library of choice, however I find their YAML configuration files off-putting.

Python itself was designed to be an easy language for prototyping, why would you **not** use it to define the network properties themselves? We have great editors with autocompletion that would make your life much easier and Python is not like C++ where you have to wait for long builds to finish before you can run your code.

YAML files on the other hand you have to edit using a standard text editor with no assistance whatsoever and this makes the learning curve even steeper.

I may be missing the big picture but I still don''t understand what were they thinking, I don''t think prototyping in code would be much slower. For that reason I''m considering Theanets or using Theano directly.', 2648, '2014-07-21 15:35:59.540', '78c1e705-e63f-4e0b-a230-18e1674b987b', 794, 2045, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Not sure if this is exactly what you''re looking for, but r-base has a function called "adist" which creates a distance matrix of approximate string distances (according to the Levenshtein distance). Type ''?adist'' for more.

    words = c("senior", "primary", "lead", "head", "vp", "director", "stuff", "principal", "chief")
    adist(words)

          [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
     [1,]    0    6    5    5    6    5    5    7    5
     [2,]    6    0    6    6    7    7    7    6    6
     [3,]    5    6    0    1    4    7    5    8    5
     [4,]    5    6    1    0    4    7    5    8    4
     [5,]    6    7    4    4    0    8    5    8    5
     [6,]    5    7    7    7    8    0    8    8    7
     [7,]    5    7    5    5    5    8    0    9    4
     [8,]    7    6    8    8    8    8    9    0    8
     [9,]    5    6    5    4    5    7    4    8    0

Also, if R isn''t an option, the Levenshtein distance algorithm is implemented in many languages here:
http://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance', 375, '2014-07-21 16:15:07.010', '00c3a861-89a9-486f-afec-de4a76389360', 795, 2046, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are couple off things you need to understand when dealing with Big data -

###What is Big data?
You might be aware of famous V''s of Big data - Volume, Velocity, Variety... So, Python may not be suitable for all. And it goes with all data science tools available. You need to know which tool is good for what purpose.

If dealing with large Volume of data:

 - Pig/Hive/Shark - Data cleaning and ETL work
 - Hadoop/Spark - Distributed parallel computing
 - Mahout/ML-Lib - Machine Learning

Now, you can use R/Python in intermediate stages but you''ll realize that they become bottleneck in your entire process.

If dealing with Velocity of data:

 - Kafka/Storm - High throughput system

People are trying to R/Python here but again it depends on kind of parallelism you want and your model complexity.

###What sort of analysis you wish to do?
If your model demands the entire data to be first brought into memory then your model should not be complex because if the intermediate data is large then the code will break. And if you think of writing it into disk then you''ll face additional delay because disk read/write is slow as compared to RAM.

##Conclusion
You can definitely use Python in Big data space (Definitely, since people are trying with R, why not Python) but know your data and business requirement first. There may be better tools available for same and always remember:

> Your tools shouldnt determine how you answer questions. Your questions should determine what tools you use.', 2433, '2014-07-21 16:58:22.660', 'e7d17d05-6fc1-4032-8b74-daa2ec065856', 796, 2047, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To be perfectly honest, most NOSQL databases are not very well suited to applications in big data. For the vast majority of all big data applications, the performance of `MongoDB` compared to a relational database like `MySQL` is [significantly][1] is poor enough to warrant staying away from something like `Mongo` entirely.

With that said, there are a couple really useful properties of NOSQL databases that certainly work in your favor when you''re working with large data sets, though the chance of those benefits outweighing the generally poor performance of `NOSQL` compared to `SQL` for read-intensive operations (most similar to typical big data use cases) is low.

 - **No Schema** - If you''re working with a lot of unstructured data, it might be hard to actually decide on and rigidly apply a schema. `NOSQL` databases in general are very supporting of this, and will allow you to insert schema-less documents on the fly, which is certainly not something a `SQL` database will support.
 - **JSON** - If you happen to be working with `JSON`-style documents instead of with `csv`''s, then you''ll see a lot of advantage in using something like `mongo` for a database-layer. Generally the workflow savings don''t outweigh the increased query-times though.
 - **Ease of Use** - I''m not saying that `SQL` databases are always hard to use, or that `cassandra` is the easiest thing in the world to set up, but in general `NOSQL` databases are easier to set up and use than `SQL` databases. `Mongo` is a particularly strong example of this, known for being one of the easiest database layers to use (outside of `sqlite`). `SQL` also deals with a lot of normalization and there''s a large legacy of `SQL` best practices that just generally bogs down the development process.

Personally I might suggest you also check out [graph][2] databases such as [neo4j][3] that show really good performance for certain types of queries if you''re looking into picking out a backend for your data science applications.


  [1]: http://www.moredevs.ro/mysql-vs-mongodb-performance-benchmark/
  [2]: http://en.wikipedia.org/wiki/Graph_database
  [3]: http://www.neo4j.org/', 548, '2014-07-21 19:06:43.223', '81672812-80be-4d1f-864d-2357832d76b7', 797, 2048, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I understand your question, you can look at the co-occurrence matrix formed using the terms following the title; e.g., senior FOO, primary BAR, etc. Then you can compute the similarity between any pair of terms, such as "senior" and "primary", using a suitable metric; e.g., the cosine similarity.', 381, '2014-07-21 20:42:00.143', 'e3d1f492-2524-4373-867f-50f9ddb37edf', 798, 2049, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would use a visual analysis. Since you know there is a repetition every 256 bytes, create an image 256 pixels wide by however many deep, and encode the data using brightness. In (i)python it would look like this:

    import os, numpy, matplotlib.pyplot as plt

    %matplotlib inline

    def read_in_chunks(infile, chunk_size=256):
        while True:
            chunk = infile.read(chunk_size)
            if chunk:
                yield chunk
            else:
                # The chunk was empty, which means we''re at the end
                # of the file
                return

    fname = ''enter something here''
    srcfile = open(fname, ''rb'')
    height = 1 + os.path.getsize(fname)/256
    data = numpy.zeros((height, 256), dtype=numpy.uint8)

    data.shape
    for i, line in enumerate(read_in_chunks(srcfile)):
        vals = list(map(int, line))
        data[i,:len(vals)] = vals

    plt.imshow(data, aspect=1e-2);

This is what a PDF looks like:

![A PDF file visualized][1]

A 256 byte periodic pattern would have manifested as vertical lines. Except for the header and tail it looks pretty noisy.


  [1]: http://i.stack.imgur.com/bicgF.png', 381, '2014-07-21 21:17:07.303', '33db8fc8-d3df-476c-8541-73cc1d154239', 799, 2050, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One benefit of the schema-free NoSQL approach is that you don''t commit prematurely and you can apply the right schema at query time using an appropriate tool like [Apache Drill](http://incubator.apache.org/drill/). See [this presentation](http://wiki.apache.org/incubator/DrillProposal?action=AttachFile&do=get&target=Drill+slides.pdf) for details. MySQL wouldn''t be my first choice in a big data setting.', 381, '2014-07-21 21:29:26.270', '313ba9e4-182b-4f4d-bd85-495d74e4eb78', 800, 2051, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('That''s an interesting problem, thanks for bring out here on stack.

I think this problem is similar to when we apply [LSA(Latent Semantic Analysis)](http://en.wikipedia.org/wiki/Latent_semantic_analysis) in sentiment analysis to find list of positive and negative words with polarity with respect to some predefined positive and negative words.

*Good reads:*

 - [Learning Word Vectors for Sentiment Analysis](http://cs.stanford.edu/people/ang/papers/acl11-WordVectorsSentimentAnalysis.pdf)
 - [Unsupervised Learning of Semantic Orientation from a Hundred-Billion-Word Corpus](http://arxiv.org/pdf/cs/0212012.pdf)

So, according to me LSA is your best approach to begin with in this situation as it learns the underlying relation between the words from the corpus and probably that''s what you are looking for.', 2433, '2014-07-21 21:32:12.963', '838e9e67-a8e7-4f1b-a60f-139d92d460f0', 801, 2052, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m working on an application which requires creating a very large database of n-grams that exist in a large text corpus.

I need three efficient operation types: Lookup and insertion indexed by the n-gram itself, and querying for all n-grams that contain a sub-n-gram.

This sounds to me like the database should be a gigantic document tree, and document databases, e.g. Mongo, should be able to do the job well, but I''ve never used those at scale.

Knowing the Stack Exchange question format, I''d like to clarify that I''m not asking for suggestions on specific technologies, but rather a type of database that I should be looking for to implement something like this at scale.', 1163, '2014-07-21 23:53:11.120', 'b29b7353-8769-4069-8a77-4b6fd3cc1c33', 802, 2053, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Efficient database model for storing data indexed by n-grams', 1163, '2014-07-21 23:53:11.120', 'b29b7353-8769-4069-8a77-4b6fd3cc1c33', 802, 2054, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><databases>', 1163, '2014-07-21 23:53:11.120', 'b29b7353-8769-4069-8a77-4b6fd3cc1c33', 802, 2055, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I haven''t done this before but it sounds like a job for a graph database given the functionality you want. [Here''s a demo in neo4j](http://www.rene-pickhardt.de/download-google-n-gram-data-set-and-neo4j-source-code-for-storing-it/).', 381, '2014-07-22 00:06:10.500', 'c96e4980-7583-4759-ab5f-139ad514efc0', 803, 2056, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would use a visual analysis. Since you know there is a repetition every 256 bytes, create an image 256 pixels wide by however many deep, and encode the data using brightness. In (i)python it would look like this:

    import os, numpy, matplotlib.pyplot as plt

    %matplotlib inline

    def read_in_chunks(infile, chunk_size=256):
        while True:
            chunk = infile.read(chunk_size)
            if chunk:
                yield chunk
            else:
                # The chunk was empty, which means we''re at the end
                # of the file
                return

    fname = ''enter something here''
    srcfile = open(fname, ''rb'')
    height = 1 + os.path.getsize(fname)/256
    data = numpy.zeros((height, 256), dtype=numpy.uint8)

    for i, line in enumerate(read_in_chunks(srcfile)):
        vals = list(map(int, line))
        data[i,:len(vals)] = vals

    plt.imshow(data, aspect=1e-2);

This is what a PDF looks like:

![A PDF file visualized][1]

A 256 byte periodic pattern would have manifested itself as vertical lines. Except for the header and tail it looks pretty noisy.


  [1]: http://i.stack.imgur.com/bicgF.png', 381, '2014-07-22 00:13:21.200', '2722106e-8bcd-41ba-a13e-c2433a80a99f', 799, 'deleted 9 characters in body', 2057, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Basically, both are software systems that are based on data and algorithms.', 1117, '2014-07-22 01:12:03.860', '28e336d2-d721-4e23-a7e7-f3c1b374912e', 804, 2058, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What''s the difference between data products and intelligent systems?', 1117, '2014-07-22 01:12:03.860', '28e336d2-d721-4e23-a7e7-f3c1b374912e', 804, 2059, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<tools><education><definitions>', 1117, '2014-07-22 01:12:03.860', '28e336d2-d721-4e23-a7e7-f3c1b374912e', 804, 2060, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is very vague question. However, I will try to make sense of it. Considering your statement that both *entities* are "software systems that are based on data and algorithms" and *logic* rules, it appears that **data products** are *intelligent systems* and **intelligent systems** are, to some degree, *data products*. Therefore, the difference between the terms "data products" and "intelligent systems" is purely in the **focus** (*source* of information or *purpose* of system dimensions) of each type of systems (*data* **vs.** *intelligence/algorithms*).', 2452, '2014-07-22 02:27:43.710', '256b22f7-53b8-440c-9ad6-efbdc3a1fb23', 805, 2061, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was starting to look into area under curve(AUC) and am a little confused about its usefulness. When first explained to me, AUC seemed to be a great measure of performance but in my research I''ve found that some claim its advantage is mostly marginal in that it is best for catching ''lucky'' models with high standard accuracy measurements and low AUC.

So should I avoid relying on AUC for validating models or would a combination be best? Thanks for all your help.', 2653, '2014-07-22 03:43:20.327', '8e075c7b-6722-4a70-a96d-04bc31c46994', 806, 2062, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Advantages of AUC vs standard accuracy', 2653, '2014-07-22 03:43:20.327', '8e075c7b-6722-4a70-a96d-04bc31c46994', 806, 2063, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><accuracy>', 2653, '2014-07-22 03:43:20.327', '8e075c7b-6722-4a70-a96d-04bc31c46994', 806, 2064, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Really great question, and one that I find that most people don''t really understand on an intuitive level. `AUC` is in fact often predicted over accuracy for binary classification for a number of different reasons. First though, let''s talk about exactly what `AUC` is. Honestly, for being one of the most widely used efficacy metrics, it''s surprisingly obtuse to figure out exactly how `AUC` works.

`AUC` stands for `Area Under the Curve`, which curve you ask? Well that would be the `ROC` curve. `ROC` stands for [Receiver Operating Characteristic][1], which is actually slightly non-intuitive. The implicit goal of `AUC` is to deal with situations where you have a very skewed sample distribution, and don''t want to overfit to a single class.

A great example is in spam detection. Generally spam data sets are STRONGLY biased towards ham, or not-spam. If your data set is 90% ham, you can get a pretty damn good accuracy by just saying that every single email is ham, which is obviously something that indicates a non-ideal classifier. Let''s start with a couple of metrics that are a little more useful for us, specifically the true positive rate (`TPR`) and the false positive rate (`FPR`):

![ROC axes][2]

Now in this graph, `TPR` is specifically the ratio of true positive to all positives, and `FPR` is the ratio of false positives to all negatives. (Keep in mind, this is only for binary classification.) On a graph like this, it should be pretty straightforward to figure out that a prediction of all 0''s or all 1''s will result in the points of `(0,0)` and `(1,1)` respectively. If you draw a line through these lines you get something like this:

![Kind of like a triangle][3]

Which looks basically like a diagonal line (it is), and by some easy geometry, you can see that the `AUC` of such a model would be `0.5` (height and base are both 1). Similarly, if you predict a random assortment of 0''s and 1''s, let''s say 90% 1''s, you could get the point `(0.9, 0.9)`, which again falls along that diagonal line.

Now comes the interesting part. What if we weren''t only predicting 0''s and 1''s? What if instead we wanted to say that, theoretically we were going to set a cutoff, above which every result was a 1, and below which every result were a 0. This would mean that at the extremes you get the original situation where you have all 0''s and all 1''s (at a cutoff of 0 and 1 respectively), but also a series of intermediate states that fall within the `1x1` graph that contains your `ROC`. In practice you get something like this:
![Courtesy of Wikipedia][4]

So basically, what you''re actually getting when you do an `AUC` over accuracy is something that will strongly discourage people going for models that are representative, but not discriminative, as this will only actually select for models that achieve false positive and true positive rates that are significantly above random chance, which is not guaranteed for accuracy.


  [1]: http://en.wikipedia.org/wiki/Receiver_operating_characteristic
  [2]: http://i.stack.imgur.com/hNxTl.png
  [3]: http://i.stack.imgur.com/B1WT1.png
  [4]: http://i.stack.imgur.com/13McM.png', 548, '2014-07-22 04:10:18.353', '4943da37-2105-4a82-924e-2f508821e682', 807, 2065, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('This is a very vague question. However, I will try to make sense of it. Considering your statement that both *entities* are "software systems that are based on data and algorithms" and *logic* rules, it appears that **data products** are *intelligent systems* and **intelligent systems** are, to some degree, *data products*. Therefore, the difference between the terms "data products" and "intelligent systems" is purely in the **focus** (*source* of information or *purpose* of system dimensions) of each type of systems (*data* **vs.** *intelligence/algorithms*).', 2452, '2014-07-22 05:58:59.537', '100eb78e-1651-4a52-8a43-a37abb004e59', 805, 'Fixed typo.', 2067, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('This is a very vague question. However, I will try to make sense of it. Considering rules of *logic* as well as your statement that both *entities* are "software systems that are based on data and algorithms", it appears that **data products** are *intelligent systems* and **intelligent systems** are, to some degree, *data products*. Therefore, it can be argued that the difference between the terms "data products" and "intelligent systems" is purely in the **focus** (*source* of information or *purpose* of system dimensions) of each type of systems (*data* **vs.** *intelligence/algorithms*).', 2452, '2014-07-22 06:06:31.513', '13d6ef44-f209-427a-93c4-a48c756618d1', 805, 'Re-phrased sentences for more clarity.', 2068, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('i want to become a **data scientist**. I studied applied **statistics** (actuarial science), so i have a great statistical background (regression, stochastic process, time series, just for mention a few). But now,  I am going to do a master degree in **Computer Science** focus in Intelligent Systems.

Here is my study plan:
-Machine learning
-Advanced machine learning
-Data mining
-Fuzzy logic
-Recommendation Systems
-Distributed Data Systems
-Cloud Computing
-Knowledge discovery
-Business Intelligence
-Information retrieval
-Text mining

At the end, with all my statistical  and  computer science knowledge, can i call myself a data scientist? , or am i wrong?

Thanks for the answers.', 1117, '2014-07-22 08:39:33.810', '71908968-28dc-42b5-9205-268e21856f20', 808, 2069, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Statistics + Computer Science = Data Science?', 1117, '2014-07-22 08:39:33.810', '71908968-28dc-42b5-9205-268e21856f20', 808, 2070, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><statistics>', 1117, '2014-07-22 08:39:33.810', '71908968-28dc-42b5-9205-268e21856f20', 808, 2071, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m trying to define a metric between job titles in IT field. For this I need some metric between words of job titles that are not appearing together in the same job title, e.g. metric between the words

> senior, primary, lead, head, vp, director, stuff, principal, chief,

or the words

> analyst, expert, modeler, researcher, scientist.

How can I get all such possible words with their distance ?
', 809, '2014-07-22 09:00:27.183', '3a8b3de6-fda8-404f-bc8b-218cabd0418b', 791, 'Removing off-topic request to provide dataset (e.g., do his work for him) and slightly fixing grammar.', 2072, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-22 09:00:27.183', '3a8b3de6-fda8-404f-bc8b-218cabd0418b', 791, 'Proposed by 809 approved by 434, -1 edit id of 118', 2073, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m trying to define a metric between job titles in IT field. For this I need some metric between words of job titles that are not appearing together in the same job title, e.g. metric between the words

> senior, primary, lead, head, vp, director, stuff, principal, chief,

or the words

> analyst, expert, modeler, researcher, scientist, developer, engineer, architect.

How can I get all such possible words with their distance ?
', 921, '2014-07-22 09:00:27.183', '445a1c41-4d21-445c-aa59-335052cb2ca1', 791, 'Removing off-topic request to provide dataset (e.g., do his work for him) and slightly fixing grammar.', 2074, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think that you''re on the right track toward becoming an **expert** *data scientist*. Recently I have answered related question here on Data Science StackExchange: http://datascience.stackexchange.com/a/742/2452 (pay attention to the *definition* I mention there, as it essentially answers your question by itself). I hope that you will find all that useful. Good luck in your career!', 2452, '2014-07-22 09:03:49.270', '5cde8495-d532-4ef1-889b-c08dc63244da', 809, 2075, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I think that you''re on the right track toward becoming an **expert** *data scientist*. Recently I have answered related question here on Data Science StackExchange: http://datascience.stackexchange.com/a/742/2452 (pay attention to the *definition* I mention there, as it essentially answers your question by itself, as well as to aspects of **practicing** *software engineering* and **applying** knowledge to solving *real-world* problems). I hope that you will find all that useful. Good luck in your career!', 2452, '2014-07-22 09:11:35.213', 'cd27daf5-bfdd-490d-b1b7-9015b6bdc10c', 809, 'Improved wording.', 2076, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My ''machine learning'' task is of separating benign Internet traffic from malicious traffic. In the real world scenario, most (say 90% or more) of Internet traffic is benign. Thus I felt that I should go with the similar kind of data for training my models as well. But then I did come across a research paper or two (in my area of work) which have used a balanced data to train models, implying equal number of instances of benign and malicious traffic?

Can someone shed more light on the *pros* and *cons* of both the choices, and which one to go for?', 2661, '2014-07-22 12:29:10.050', '961211f2-c75e-4250-baec-3c33a5106a58', 810, 2077, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Should I go for a ''balanced'' dataset or a ''representative'' dataset?', 2661, '2014-07-22 12:29:10.050', '961211f2-c75e-4250-baec-3c33a5106a58', 810, 2078, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><dataset>', 2661, '2014-07-22 12:29:10.050', '961211f2-c75e-4250-baec-3c33a5106a58', 810, 2079, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('My ''machine learning'' task is of separating benign Internet traffic from malicious traffic. In the real world scenario, most (say 90% or more) of Internet traffic is benign. Thus I felt that I should go with the similar kind of data for training my models as well. But then I did come across a research paper or two (in my area of work) which have used a balanced data to train models, implying equal number of instances of benign and malicious traffic.

In general, if I am building ML models, should I go for a dataset which is representative of the real world problem, or is a balanced dataset better suited for building the models (since certain classifiers do not behave well with class imbalance, or due to other reasons not known to me)?

Can someone shed more light on the *pros* and *cons* of both the choices, and how to decide which one to go for?', 2661, '2014-07-22 12:35:30.687', 'cd4d92ee-eec2-41c8-b5bc-5f4b9f736ba2', 810, 'added 307 characters in body', 2080, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would say the answer depends on your use case. Based on my experience:

 - If you''re trying to build a representative model -- one that describes the data rather than necessarily predicts -- then I would suggest using a representative sample of your data.
 - If you want to build a predictive model, particularly one that performs well by measure of AUC or rank-order and plan to use a basic ML framework (i.e. Decision Tree, SVM, Naive Bayes, etc), then I would suggest you feed the framework a balanced dataset. Much of the literature on class imbalance finds that random undersampling (down sampling the majority class to the size of the minority class) can drive performance gains.
 - If you''re building a predictive model, but are using a more advanced framework (i.e. something that determines sampling parameters via wrapper or a modification of a bagging framework that samples to class equivalence), then I would suggest again feeding the representative sample and letting the algorithm take care of balancing the data for training.', 2513, '2014-07-22 14:22:49.137', '55361c59-2143-419b-95bb-7a948c5654ab', 811, 2081, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the best tool to use to visualize (draw the vertices and edges) a graph with 1000000 vertices? There are about 50000 edges in the graph. And I can compute the location of individual vertices and edges.

I am thinking about writing a program to generate a svg. Any other suggestions?  ', 192, '2014-07-22 15:17:04.810', '4e1b4950-a1aa-4e64-a072-2706b36e1074', 812, 2082, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Visualizating a graph with 1000000 vertices', 192, '2014-07-22 15:17:04.810', '4e1b4950-a1aa-4e64-a072-2706b36e1074', 812, 2083, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization><graphs>', 192, '2014-07-22 15:17:04.810', '4e1b4950-a1aa-4e64-a072-2706b36e1074', 812, 2084, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('https://gephi.github.io/ says it can handle a million edges. If your graph has 1000000 vertices and only 50000 edges then most of your vertices won''t have any edges anyway.


In fact the Gephi spec is the dual of your example: "Networks up to 50,000 nodes and 1,000,000 edges"

', 471, '2014-07-22 15:44:04.160', '855143c2-0e32-41df-9cc0-68b826ab7dde', 813, 2085, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I also suggest `Gephi` software (https://gephi.github.io), which seems to be quite powerful. Some additional information on using `Gephi` with **large networks** can be found [here][1] and, more generally, [here][2]. `Cytoscape` (http://www.cytoscape.org) is an alternative to `Gephi`, being an another popular platform for complex network analysis and visualization.

If you''d like to work with networks **programmatically** (including visualization) in R, Python or C/C++, you can check `igraph` collection of libraries. Speaking of R, you may find interesting the following blog posts: on **using R with Cytoscape** (http://www.vesnam.com/Rblog/viznets1) and on **using R with Gephi** (http://www.vesnam.com/Rblog/viznets2).

For **extensive lists** of *network analysis and visualization software*, including some comparison and reviews, you might want to check the following pages: 1) http://wiki.cytoscape.org/Network_analysis_links; 2) http://www.kdnuggets.com/software/social-network-analysis.html; 3) http://www.activatenetworks.net/social-network-analysis-sna-software-review.


  [1]: https://forum.gephi.org/viewtopic.php?t=1554
  [2]: https://forum.gephi.org/viewforum.php?f=25', 2452, '2014-07-22 16:40:17.733', '81582efc-e8a8-4b49-a8f6-7345c8edc1c5', 814, 2086, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How can NOSQL databases like  mongodb be used for data analysis? What are the features in them that can make data analysis faster and powerful?', 2643, '2014-07-22 17:02:29.310', '6bd11059-6c21-4739-abbd-0a3bc2a08204', 793, 'added 58 characters in body', 2087, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Visualizing a graph with a million vertices', 471, '2014-07-22 18:34:44.740', '7debc3d3-eded-4298-97b0-d6e3fe8059fa', 812, 'Made the title correct and saved ppl from counting zeroes.', 2091, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-22 18:34:44.740', '7debc3d3-eded-4298-97b0-d6e3fe8059fa', 812, 'Proposed by 471 approved by 434, 192 edit id of 119', 2092, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m currently facing a project that I could solve with a relational database in a relatively painful way. Having heard so much about NOSQL, I''m wondering if there is not a more appropriate way of tackling it:

Suppose we are tracking a group of animals in a forest (n ~ 500) and would like to keep a record of a set of observations (this is a fictional scenario).

We would like to store the following information in a database:

* a unique identifier for each animal
* a description of the animal with structured fields: Species, Genus, Family, ...
* a free text field with additional information
* each time-point at which it was detected close to a reference point
* a picture of the animal
* an indication whether two given animals are siblings

And:

* there might be additional features appearing later as more data comes in

We would like to be able to execute the following types of queries:

* return all the animals spotted between in a given time interval
* return all the animals of a given Species or Family
* perform a text search on the free text field

Which particular database system would you recommend ? Is there any tutorial / examples that I could use as a starting point ?

', 906, '2014-07-22 19:20:47.580', '4113cf3c-63b9-42b0-bc21-030955ed9356', 815, 2093, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('is this a good case for NOSQL?', 906, '2014-07-22 19:20:47.580', '4113cf3c-63b9-42b0-bc21-030955ed9356', 815, 2094, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nosql><databases>', 906, '2014-07-22 19:20:47.580', '4113cf3c-63b9-42b0-bc21-030955ed9356', 815, 2095, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Well it depends on what kind of "Data Science" you wish to get in to.  For basic analytics and reporting statistics will certainly help, but for Machine Learning and Artificial Intelligence then you''ll want a few more skills

 - **Probability theory** - you must have a solid background in pure probability so that you can decompose any problem, whether seen before or not, into probabilistic principles.  Statistics helps a lot for already solved problems, but new and unsolved problems require a deep understanding of probability so that you can design appropriate techniques.

 - **Information Theory** - this (relative to statistics) is quite a new field (though still decades old), the most important work was by Shannon, but even more important and often neglected note in literature is work by Hobson that proved that Kullback-Leibler Divergence is the only mathematical definition that truly captures the notion of a *"measure of information"*. Now fundamental to artificial intellgence is being able to quantify information.  Suggest reading "Concepts in Statistical Mechanics" - Arthur Hobson (very expensive book, only available in academic libraries).

 - **Complexity Theory** - A big problem many Data Scientists face that do not have a solid complexity theory background is that their algorithms do not scale, or just take an extremely long time to run on large data.  Take PCA for example, many peoples favourite answer to the interview question "how do you reduce the number of features in our dataset", but even if you tell the candidate "the data set is really really really large" they still propose various forms of PCA that are O(n^3).  If you want to stand out, you want to be able to solve each problem on it''s own, NOT throw some text book solution at it designed a long time ago before Big Data was such a hip thing.  For that you need to understand how long things take to run, not only theoretically, but practically - so how to use a cluster of computers to distribute an algorithm, or which data structures take up less memory.

 - **Communication Skills** - A huge part of Data Science is understanding business.  Whether it''s inventing a product driven by data science, or giving business insight driven by data science, being able to communicate well with both the Project and Product Managers, the tech teams, and your fellow data scientists is very important.  You can have an amazing idea, say an awesome AI solution, but if you cannot effectively (a) communicate WHY that will make the business money, (b) convince your collegues it will work and (c) explain to tech people how you need their help to build it, then it wont get done.', 2668, '2014-07-22 21:12:08.333', '1222ac47-e855-4c3a-9649-e046655f1674', 816, 2096, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data scientist (to me) a big umbrella term. I would see a data scientist as a person who can proficiently use techniques from the fields of data mining, machine learning, pattern classification, and statistics.

However, those terms are intertwined to: machine learning is tied together with pattern classification, and also data mining overlaps when it comes finding patterns in data. And all techniques have their underlying statistical principles. I always picture this as a Venn diagram with a huge intersection.

Computer sciences is related to all those fields too. I would say that you need "data science" techniques to do computer-scientific research, but computer science knowledge is not necessarily implied in "data science". However, programming skills - I see programming and computer science as different professions, where programming is more the tool in order solve problems - are also important to work with the data and to conduct data analysis.

You have a really nice study plan, and it all makes sense. But I am not sure if you "want" to call yourself just "data scientist", I have the impression that "data scientist" is such a ambiguous term that can mean everything or nothing. What I want to convey is that you will end up being something more - more "specialized" - than "just" a data scientist.', 2556, '2014-07-22 21:24:33.820', '9dc14af4-af10-403e-a731-3ca8ee4a8e51', 817, 2097, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think, that `Gephi` could face with lack-of-memory issues, you will need at least 8Gb of RAM. Though number of edges is not extremely huge.

Possibly, more appropriate tool in this case would be [`GraphViz`][1]. It''s a command line tool for network visualizations, and presumably would be more tolerant to graph size. Moreover, as I remember, in `GraphViz` it is possible to use precomputed coordinates to facilitate computations.

I''ve tried to find a real-world examples of using `GraphViz` with huge graphs, but didn''t succeed. Though I found similar discussion on [**Computational Science**][2].


  [1]: http://www.graphviz.org/
  [2]: http://scicomp.stackexchange.com/questions/3315/visualizing-very-large-link-graphs', 941, '2014-07-23 05:29:06.903', '9db346b5-5da6-4cc9-a16e-fe39c5f2385b', 818, 2098, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Three tables: **animal**, **observation**, and **sibling**. The observation has an **animal_id** column which links to the animal table, and the sibling table has **animal_1_id** and **animal_2_id** columns that indicates two animals are siblings for each row.

Even with 5000 animals and 100000 observations I don''t think query time will be a problem for something like PostgreSQL for most reasonable queries (obviously you can construct unreasonable queries but you can do that in any system).

So I don''t see how this is "relatively painful". Relative to what? The only complexity is the sibling table. In NOSQL you might store the full list of siblings in the record for each animal, but then when you add a sibling relationship you have to add it to both sibling''s animal records. With the relational table approach I''ve outlined, it only exists once, but at the expense of having to test against both columns to find an animal''s siblings.

I''d use PostgreSQL, and that gives you the option of using PostGIS if you have location data - this is a geospatial extension to PostgreSQL that lets you do spatial queries (point in polygon, points near a point etc) which might be something for you.

I really don''t think the properties of NOSQL databases are a problem here for you - you aren''t changing your schema every ten minutes, you probably **do** care that your database is ACID-compliant, and you don''t need something web-scale.

http://www.mongodb-is-web-scale.com/ [warning: strong language]

', 471, '2014-07-23 07:01:01.140', '25af8766-1911-484f-9f4b-00359925302d', 819, 2099, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Apologies if this is very broad question, what I would like to know is how effective is A/B testing (or other methods) of effectively measuring the effects of a design decision.

For instance we can analyse user interactions or click results, purchase/ browse decisions and then modify/tailor the results presented to the user.

We could then test the effectiveness of this design change by subjecting 10% of users to the alternative model randomly but then how objective is this?

How do we avoid influencing the user by the model change, for instance we could decided that search queries for ''David Beckham'' are probably about football so search results become biased towards this but we could equally say that his lifestyle is just as relevant but this never makes it into the top 10 results that are returned.

I am curious how this is dealt with and how to measure this effectively.

My thoughts are that you could be in danger of pushing a model that you think is correct and the user obliges and this becomes a self-fulfilling prophecy.

I''ve read an article on this: http://techcrunch.com/2014/06/29/ethics-in-a-data-driven-world/ and also the book: http://shop.oreilly.com/product/0636920028529.do which discussed this so it piqued my interest.', 95, '2014-07-23 08:06:21.417', '1fb62cce-3c05-4ccf-aa89-e350a29915e4', 820, 2100, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How can we effectively measure the impact of our data decisions', 95, '2014-07-23 08:06:21.417', '1fb62cce-3c05-4ccf-aa89-e350a29915e4', 820, 2101, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<search>', 95, '2014-07-23 08:06:21.417', '1fb62cce-3c05-4ccf-aa89-e350a29915e4', 820, 2102, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A great reproducibility tool for Python with a low learning curve is of course [**IPython Notebook**][1] (don''t forget the [%logon and %logstart][2] magics and/or [Git][3] to keep a history of all your experiments).

Another more general tool working with any language (with a Python API on [pypi][4]) is [**Sumatra**][5], which is specifically designed to produce **replicable** research (which aims to produce the same results given the exact same code and softwares, whereas reproducibility aims to produce the same results given any medium). Here is how Sumatra works: for each experiment that you conduct through Sumatra, this software will act like a "save game state" often found in videogames. More precisely, it will will save:

- all the parameters you provided;
- the exact sourcecode state of your whole experimental application and config files;
- the output/plots/results and also any file produced by your experimental application.

It will then construct a database with the timestamp and other metadatas for each of your experiments, that you can later crawl using the webGUI. Since Sumatra saved the full state of your application for a specific experiment at one specific point in time, you can restore the code that produced a specific result at any moment you want, thus you have replicable research at a low cost (except for storage if you work on huge datasets, but you can configure exceptions if you don''t want to save everything everytime).

/EDIT: [dsign][6] touched a very important point here: the replicability of your setup is as important as the replicability of your application. In other words, you should at least provide a **full list of the libraries and compilers** you used along with their exact **versions** and the details of your **platform**.

Personally, in scientific computing with Python, I have found that packaging an application along with the libraries is just too painful, thus I now just use an all-in-one scientific python package such as [Anaconda][7] (with the great package manager [conda][8]), and just advise users to use the same package. Another solution could be to provide a script to automatically generate a [virtualenv][9], or to package everything using the commercial [Docker application as cited by dsign][10] or the opensource [Vagrant][11] (with for example [pylearn2-in-a-box][12] which use Vagrant to produce an easily redistributable virtual environment package).


  [1]: http://ipython.org/
  [2]: https://damontallen.github.io/IPython-quick-ref-sheets/
  [3]: https://en.wikipedia.org/wiki/Git_(software)
  [4]: https://pypi.python.org/pypi/Sumatra
  [5]: http://neuralensemble.org/sumatra/
  [6]: http://datascience.stackexchange.com/a/775
  [7]: https://store.continuum.io/cshop/anaconda/
  [8]: http://www.continuum.io/blog/conda
  [9]: http://docs.python-guide.org/en/latest/dev/virtualenvs/
  [10]: http://datascience.stackexchange.com/a/775
  [11]: http://www.vagrantup.com/
  [12]: http://deeplearning.net/software/pylearn2/#other-methods', 2544, '2014-07-23 12:00:44.207', 'a8e30a5c-1260-43c9-9913-6390f7642fe2', 767, 'added Vagrant as an opensource alternative to Docker', 2103, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('"Because its there".

The data has a seasonal pattern. So you model it. The data has a trend. So you model it. Maybe the data is correlated with the number of sunspots. So you model that. Eventually you hope to get nothing left to model than uncorrelated random noise.

But I think you''ve screwed up your STL computation here. Your residuals are clearly not serially uncorrelated. I rather suspect you''ve not told the function that your "seasonality" is a 24-hour cycle rather than an annual one. But hey you haven''t given us any code or data so we don''t really have a clue what you''ve done, do we? What do you think "seasonality" even means here? Do you have any idea?

Your data seems the have three peaks every 24 hours. Really? Is this ''gas''=''gasoline''=''petrol'' or gas in some heating/electric generating system? Either way if you know a priori there''s an 8 hour cycle, or an 8 hour cycle on top of a 24 hour cycle on top of what looks like a very high frequency one or two hour cycle you **put that in your model**.

Actually you don''t even say what your x-axis is so maybe its days and then I''d fit a daily cycle, a weekly cycle, and then an annual cycle. But given how it all changes at time=85 or so I''d not expect a model to do well on both sides of that.

With statistics (which is what this is, sorry to disappoint you but you''re not a data scientist yet) you don''t just robotically go "And.. Now.. I.. Fit.. An... S TL model....". You look at your data, try and get some understanding, then propose a model, fit it, test it, and use the parameters it make inferences about the data. Fitting cyclic seasonal patterns is part of that.




', 471, '2014-07-23 12:57:37.073', '5f4eee5d-793b-4126-b6d1-80031a66094a', 821, 2104, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In A/B testing, bias is handled very well by ensuring visitors are randomly assigned to either version A or version B of the site.  This creates independent samples drawn from the same population.  Because the groups are independent and, on average, only differ in the version of the site seen, the test measures the effect of the design decision.

*Slight aside*: Now you might argue that the A group or B group may differ in some demographic.  That commonly happens by random chance.  To a certain degree this can be taken care of by covariate adjusted randomization.  It can also be taken care of by adding covariates to the model that tests the effect of the design decision.  It should be noted that there is still some discussion about the proper way to do this within the statistics community.  Essentially A/B testing is an application of a [Randomized Control Trial](http://en.wikipedia.org/wiki/Randomized_controlled_trial) to website design.  Some people disagree with adding covariates to the test.  Others, such as Frank Harrel (see [Regression Modeling Strategies](http://www.amazon.com/exec/obidos/ASIN/0387952322/)) argue for the use of covariates in such models.

I would offer the following suggestions:

- Design the study in advance so as to take care of as much sources of bias and variation as possible.
- Let the data speak for itself.  As you get more data (like about searches for David Beckham), let it dominate your assumptions about how the data should be (as how the posterior dominates the prior in Bayesian analysis when the sample size becomes large).
- Make sure your data matches the assumptions of the model.', 178, '2014-07-23 13:49:01.423', 'cb1e7149-e45d-4b66-9ef6-16ac2b891812', 822, 2105, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There was a recent furore with [facebook experimenting on their users to see if they could alter user''s emotions](http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840).

Whilst I am not a professional data scientist I read about [data science ethics](http://columbiadatascience.com/2013/11/25/data-science-ethics/) from [Cathy O''Neill''s book ''Doing Data Science''](http://shop.oreilly.com/product/0636920028529.do) and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science *accidentally*.', 95, '2014-07-23 14:04:31.057', '4436c95f-b0ca-4815-9197-d12c91557aa3', 823, 2106, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How should moral ethics be applied in data science', 95, '2014-07-23 14:04:31.057', '4436c95f-b0ca-4815-9197-d12c91557aa3', 823, 2107, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<social-network-analysis>', 95, '2014-07-23 14:04:31.057', '4436c95f-b0ca-4815-9197-d12c91557aa3', 823, 2108, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There was a recent furore with [facebook experimenting on their users to see if they could alter user''s emotions](http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840).

Whilst I am not a professional data scientist I read about [data science ethics](http://columbiadatascience.com/2013/11/25/data-science-ethics/) from [Cathy O''Neill''s book ''Doing Data Science''](http://shop.oreilly.com/product/0636920028529.do) and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science *accidentally*.

By the way I am not making any judgements here or saying that all data scientists behave like this, I''m interested in what is taught academically and practised professionally.', 95, '2014-07-23 14:13:03.410', '8f4598af-4f9d-441e-80d2-0ca6b4bc903b', 823, 'added 179 characters in body', 2109, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<social-network-analysis>', 178, '2014-07-23 14:34:58.483', '5c008180-a822-4d9d-819f-706a0bebb6bb', 823, 'Add ethics tag', 2110, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-23 14:34:58.483', '5c008180-a822-4d9d-819f-706a0bebb6bb', 823, 'Proposed by 178 approved by 95 edit id of 120', 2111, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('(too long for a comment)

Basically, @Emre''s answer is correct: simple correlation matrix and cosine distance should work well*. There''s one subtlety, though - job titles are too short to carry important context. Let me explain this.

Imagine LinkedIn profiles (which is pretty good source for data). Normally, they contain 4-10 sentences describing person''s skills and qualifications. It''s pretty likely that you find phrases like "lead data scientist" and "professional knowledge of Matlab and R" in a same profile, but it''s very unlikely to also see "junior Java developer" in it. So we may say that "lead" and "professional" (as well as "data scientist" and "Matlab" and "R") often occur in same contexts, but they are rarely found together with "junior" and "Java".

Co-occurrence matrix shows exactly this. The more 2 words occur in same context, the more similar their vectors in the matrix will look like. And cosine distance is just a good way to measure this similarity.

But what about job titles? Normally they are much shorter and don''t actually create enough context to catch similarities. Luckily, you don''t need source data to be titles themselves - you need to find similarities between skills in general, not specifically in titles. So you can simply build co-occurrence matrix from (long) profiles and then use it to measure similarity of titles.

*** - in fact, it''s already worked for me on a similar project. ', 1279, '2014-07-23 15:46:48.977', 'd7612507-cdbc-42e6-9399-807751af606f', 824, 2112, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Most vehicle license/number plate extractors I''ve found involve reading a plate from an image (OCR) but I''m interested in something that could tag instances of license plates in a body of text. Are there any such annotators out there?', 1192, '2014-07-24 00:01:40.760', '70ac2f67-50c9-468e-88c8-b43878fdf582', 825, 2113, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are there any annotators or Named Entity Recognition for license plate numbers?', 1192, '2014-07-24 00:01:40.760', '70ac2f67-50c9-468e-88c8-b43878fdf582', 825, 2114, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining>', 1192, '2014-07-24 00:01:40.760', '70ac2f67-50c9-468e-88c8-b43878fdf582', 825, 2115, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This can be done using `regular expressions`.

2 letters followed by a number (\d denotes digits) would be

    [A-Z]{2} \d*

2 or 3 letters followed by a number is

    [A-Z]{2,3} \d*', 325, '2014-07-24 00:34:41.130', '0757c84d-ab18-419f-bb5f-c180b7df4673', 826, 2116, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are a lot of pretty decent tools out there for text annotation in general, and given the broad nature of the task you''re approaching (license plates are about as general as words), the annotation tools you are looking at should probably come from the more classical tools for annotation.

There was actually a pretty good discussion about annotation tools on [this][1] question, which should actually apply to this problem. The most relied-upon thing in annotation right now is probably `brat`. You can learn more about `brat` [here][2].

Hope that helps! Let me know if you''ve got any more questions.


  [1]: http://datascience.stackexchange.com/questions/223/how-to-annotate-text-documents-with-meta-data/404#404
  [2]: http://brat.nlplab.org/', 548, '2014-07-24 02:23:23.703', '94a34d6c-8887-48b9-a501-e20000a6057b', 827, 2117, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Reporting back: I ended up coding graphml and using yEd for visualization (just because I am familiar with this combination. I bet gephi or graphviz would work fine and might even be better). Since I computed the location of all nodes, memory was not such big of an issue. Coding graphml is a little easier comparing to coding svg, since I don''t have to explicitly specify the placement of edges.', 192, '2014-07-24 03:31:32.087', '7fae1fe0-4572-4e9f-b1b4-7cffb4bb80b9', 828, 2118, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('While running the below pig script I am getting error in line4:
If it is GROUP then I am getting error.
If I change from ''GROUP'' TO ''group'' in line4, then the script is running.
What is the difference between group and GROUP.


LINES = LOAD ''/user/cloudera/datapeople.csv'' USING PigStorage('','') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );

WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;

WORDSGROUPED = GROUP WORDS BY ZIPS;

WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);

WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;

DUMP WORDSSORT;', 1314, '2014-07-24 06:26:07.290', 'e5f6c137-7351-44ab-86d2-7587a2441992', 829, 2119, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Pig script code error?', 1314, '2014-07-24 06:26:07.290', 'e5f6c137-7351-44ab-86d2-7587a2441992', 829, 2120, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop>', 1314, '2014-07-24 06:26:07.290', 'e5f6c137-7351-44ab-86d2-7587a2441992', 829, 2121, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('''group'' in strictly lower case in the FOREACH is the thing you are looping/grouping over.

http://squarecog.wordpress.com/2010/05/11/group-operator-in-apache-pig/ says:

> When you group a relation, the result is a new relation with two
> columns: group and the name of the original relation.

Column names are case sensitive, so you have to use lower-case ''group'' in your FOREACH.

''GROUP'' in upper case is the grouping operator. You can''t mix them. So don''t do that.

', 471, '2014-07-24 07:00:13.187', '5c1d608a-93c8-42cc-bdd1-00b62609ea35', 830, 2122, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Have you heard of the "Data Science Association"? Do you expect it to become a professional body like the Actuaries Institute?
If yes, then why?
If no, then why not and do you see anyone else becoming the professional body?

Lastly, is this question "on-topic" ?', 366, '2014-07-24 07:38:07.163', 'b739a6e5-29fd-4fee-8594-f21b7a5ec60b', 831, 2124, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data Science Association?', 366, '2014-07-24 07:38:07.163', 'b739a6e5-29fd-4fee-8594-f21b7a5ec60b', 831, 2125, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<knowledge-base>', 366, '2014-07-24 07:38:07.163', 'b739a6e5-29fd-4fee-8594-f21b7a5ec60b', 831, 2126, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Have you heard of the "Data Science Association"?
<br>URL: http://www.datascienceassn.org/
<br>Do you expect it to become a professional body like the Actuaries Institute?
<br>If yes, then why?
<br>If no, then why not and do you see anyone else becoming the professional body?

Lastly, is this question "on-topic" ?', 366, '2014-07-24 09:32:20.100', '4e954e01-3262-4ffe-8850-6acfba11569c', 831, 'Added URL', 2127, '5');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-07-24 12:43:43.173', '0e804b2f-058f-4559-b7d8-7414ce7803b6', 224, '5', 2129, '34');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My data contains a set of start times and duration for an action. I would like to plot this so that for a given time slice I can see how many actions are active. I''m currently thinking of this as a histogram with time on the x axis and number of active actions on the y axis.

My question is, how should I adjust the data so that this is able to be plotted?

The times for an action can be between 2 seconds and a minute. Ideally a single plot would be able to show hours of data. The accuracy of the data is in milliseconds.

In the past the way that I have done this is to count for each second how many actions started , ended, or were active. This gave me a count of active actions for each second. The issue I found with this technique was that it made it difficult to adjust the time slice that I was looking at. Looking at a time slice of a minute was difficult to compute and looking at time slices of less than a second was impossible.

I''m open to any advice on how to think about this issue.

Thanks in advance!', 2702, '2014-07-24 14:04:09.533', '5241b6e8-ba5c-4262-9f3c-4787ca42cc5b', 832, 2130, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How do you plot overlapping durations?', 2702, '2014-07-24 14:04:09.533', '5241b6e8-ba5c-4262-9f3c-4787ca42cc5b', 832, 2131, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization>', 2702, '2014-07-24 14:04:09.533', '5241b6e8-ba5c-4262-9f3c-4787ca42cc5b', 832, 2132, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This can be done in `R` using `ggplot`. Based on [this][1] question, it could be done with this code where `limits` is the date range of the plot.

    tasks <- c("Task1", "Task2")
    dfr <- data.frame(
    name        = factor(tasks, levels = tasks),
    start.date  = c("2014-08-07 09:03:25.815", "2014-08-07 09:03:25.956"),
    end.date    = c("2014-08-07 09:03:28.300", "2014-08-07 09:03:30.409")
    )

    mdfr <- melt(dfr, measure.vars = c("start.date", "end.date"))


    mdfr$time<-as.POSIXct(mdfr$value)

    ggplot(mdfr, aes(time,name)) +
    geom_line(size = 6) +
    xlab("") + ylab("") +
    theme_bw()+
    scale_x_datetime(breaks=date_breaks("2 sec"),
    limits = as.POSIXct(c(''2014-08-07 09:03:24'',''2014-08-07 09:03:29'')))

![enter image description here][2]


  [1]: http://stackoverflow.com/questions/18102224/drawing-gantt-charts-with-r-to-sub-second-accuracy
  [2]: http://i.stack.imgur.com/Bwwod.png', 325, '2014-07-24 21:14:45.793', '0821a23b-00b8-4a4e-bc30-9c7620167b6a', 833, 2133, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The output of my word alignment file looks as such:

    I wish to say with regard to the initiative of the Portuguese Presidency that we support the spirit and the political intention behind it . In bezug auf die Initiative der portugiesischen Präsidentschaft möchte ich zum Ausdruck bringen , daß wir den Geist und die politische Absicht , die dahinter stehen , unterstützen . 0-0 5-1 5-2 2-3 8-4 7-5 11-6 12-7 1-8 0-9 9-10 3-11 10-12 13-13 13-14 14-15 16-16 17-17 18-18 16-19 20-20 21-21 19-22 19-23 22-24 22-25 23-26 15-27 24-28
    It may not be an ideal initiative in terms of its structure but we accept Mr President-in-Office , that it is rooted in idealism and for that reason we are inclined to support it . Von der Struktur her ist es vielleicht keine ideale Initiative , aber , Herr amtierender Ratspräsident , wir akzeptieren , daß sie auf Idealismus fußt , und sind deshalb geneigt , sie mitzutragen . 0-0 11-2 8-3 0-4 3-5 1-6 2-7 5-8 6-9 12-11 17-12 15-13 16-14 16-15 17-16 13-17 14-18 17-19 18-20 19-21 21-22 23-23 21-24 26-25 24-26 29-27 27-28 30-29 31-30 33-31 32-32 34-33

How can I produce the phrase tables that are used by MOSES from this output?', 322, '2014-07-24 22:08:39.407', '89284752-2d65-435f-a9ea-32bde043d010', 224, 'correct misspelled tag, remove tag from title, grammar', 2134, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to get phrase tables from word alignments?', 322, '2014-07-24 22:08:39.407', '89284752-2d65-435f-a9ea-32bde043d010', 224, 'correct misspelled tag, remove tag from title, grammar', 2135, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<untagged>', 322, '2014-07-24 22:08:39.407', '89284752-2d65-435f-a9ea-32bde043d010', 224, 'correct misspelled tag, remove tag from title, grammar', 2136, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-24 22:08:39.407', '89284752-2d65-435f-a9ea-32bde043d010', 224, 'Proposed by 322 approved by 122 edit id of 122', 2137, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to build a recommendation engine using collaborative filtering. I have the usual [user, movie, rating] information. I would like to incorporate an additional feature like ''language'' or ''duration of movie''. I am not sure what techniques I could use for such a problem. Please suggest references or packages in python/R. ', 1131, '2014-07-25 00:58:12.253', '8c0c8a4a-1f62-4f97-95d2-badf93fa8cb9', 834, 2138, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Recommending movies with additional features using collaborative filtering', 1131, '2014-07-25 00:58:12.253', '8c0c8a4a-1f62-4f97-95d2-badf93fa8cb9', 834, 2139, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><r><recommendation>', 1131, '2014-07-25 00:58:12.253', '8c0c8a4a-1f62-4f97-95d2-badf93fa8cb9', 834, 2140, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Consider, try, and perhaps even use multiple databases. It''s not just a "performance" issue at play here. It''s really going to come down to your requirements. How much data are you talking about? what kind of data? how fast do you need it? Are you more read heavy or write heavy?

Here''s one thing you can''t do in a SQL database: Calculate sentiment. http://www.slideshare.net/shift8/mongodb-machine-learning

Of course the speed in that case may not be fast enough for your needs, but it is something that''s possible. With some caching of specific aggregate values, it was quite acceptable even. Why would you do this? Convenience.

Convenience really is something that you''re going to be persuaded by. That''s exactly why (in my opinion) NoSQL databases were created. Performance too of course, but I''m trying to discount benchmarks and focus more on other concerns.

MongoDB (and some other NoSQL) databases have some very powerful features such as built-in map/reduce. This could result in a savings both in cost and time over using something like Hadoop. Or it could provide a prototype or MVP to launch a larger business.

What about graph databases? They''re "NoSQL" too. Look at databases like OrientDB. If you want to argue performance ...I don''t think you''re gonna show me a SQL database that''s faster there =) ...and graph databases have some really amazing application based on what you need to do.

Rule of technology (and the internet) don''t get too comfortable with one thing. You''re gonna be limited and set yourself up for failure.', 2711, '2014-07-25 03:05:38.107', 'ec66ce2c-4838-47f1-bff5-67f74472cadf', 835, 2141, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I want to plot the bytes from a disk image in order to understand a pattern in them. This is mainly an academic task, since I''m almost sure this pattern was created by a disk testing program, but I''d like to reverse-engineer it anyway.

I already know that the pattern is aligned, with a periodicity of 256 characters.

I can envision two ways of visualizing this information: either a 16x16 plane viewed through time (3 dimensions), where each pixel''s color is the ASCII code for the character, or a 256 pixel line for each period (2 dimensions).

This is a snapshot of the pattern (you can see more than one), seen through `xxd` (32x16):

![Pattern to analyze][1]

Either way, I am trying to find a way of visualizing this information. This probably isn''t hard for anyone into signal analysis, but I can''t seem to find a way using open-source software.

I''d like to avoid Matlab or Mathematica and I''d prefer an answer in R, since I have been learning it recently, but nonetheless, any language is welcome.

<hr>

Update, 2014-07-25: given Emre''s answer below, this is what the pattern looks like, given the first 30MB of the pattern, aligned at 512 instead of 256 (this alignment looks better):

![Graphical pattern][2]

Any further ideas are welcome!


  [1]: http://i.stack.imgur.com/zOFSK.gif
  [2]: http://i.stack.imgur.com/4tDIA.png', 2604, '2014-07-25 03:26:52.427', 'e96bc5f4-7efb-4b26-b067-406fdb91bbf7', 783, 'Update 2014-07-25', 2142, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here some resources that might be helpful:

- **Recommenderlab** - a framework and open source software for developing and testing recommendation algorithms: http://lyle.smu.edu/IDA/recommenderlab. Corresponding `R` package `recommenderlab`: http://cran.r-project.org/package=recommenderlab.

- The following blog post illustrates the use of `recommenderlab` package (which IMHO can be generalized for any open source recommendation engine) for building **movie recommendation** application, based on *collaborative filtering*: http://scn.sap.com/community/developer-center/hana/blog/2013/11/06/movie-recommendation-by-leveraging-r.

- **Research on recommender systems** - a nice webpage with resources on the topic, maintained by Recommenderlab''s lead developer Michael Hahsler: http://michael.hahsler.net/research/recommender.

- **Mortar Recommendation Engine** - an open source customizable recommendation engine for `Hadoop` and `Pig`, written in `Python` and `Java`: https://github.com/mortardata/mortar-recsys. Company, sponsoring the development of this project, **Mortar Data**, offers general commercial *cloud platform* for development and hosting *data science* software projects, including ones based on the `Mortar Recommendation Engine` (development and hosting of public projects are free): http://www.mortardata.com. Mortar Data provides help in form of **public Q&A forum** (https://answers.mortardata.com) as well as a **comprehensive tutorial** on building recommendation engine using open technologies (http://help.mortardata.com/data_apps/recommendation_engine).

- **"Introduction to Recommender Systems"** - a relevant Coursera course (MOOC), which content and description provide additional resources on the topic: https://www.coursera.org/course/recsys.

- **PredictionIO** - an open source machine learning server software, which allows building *data science applications*, including *recommendation systems*: http://prediction.io (source code is available on GitHub: https://github.com/PredictionIO). `PredictionIO` includes a built-in *recommendation engine* (http://docs.prediction.io/current/engines/itemrec/index.html) and supports a wide range of programming languages and frameworks via `RESTful` APIs as well as SDKs/plug-ins. PredictionIO maintains an `Amazon Machine Image` on **AWS Marketplace** for deploying applications on the AWS infrastructure: https://aws.amazon.com/marketplace/pp/B00ECGJYGE.

- **Additional open source software projects**, relevant to the topic (discovered via `MLOSS` website on *machine learning* open source software: http://www.mloss.org):
  + **Jubatus**: http://jubat.us/en
  + **MyMediaLite**: http://mymedialite.net
  + **TBEEF**: https://github.com/ChrisRackauckas/TBEEF
  + **PREA**: http://prea.gatech.edu
  + **CofiRank**: http://www.cofirank.org

- The following relevant `R` **blog posts** are also interesting:
  + "Simple tools for building a recommendation engine" (http://blog.revolutionanalytics.com/2012/04/simple-tools-for-building-a-recommendation-engine.html)
  + "Recommendation System in R" (http://blog.yhathq.com/posts/recommender-system-in-r.html)', 2452, '2014-07-25 04:54:07.787', '6345363e-ae43-42ea-acfa-48bf8bc8dae3', 836, 2143, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would look at the `raster` package for this, which can read in raw binary data and present it as NxM grids. It can even extract subsets of large binary grids without having to read in the whole file (the R raster object itself is just a proxy to the data, not the data itself).

', 471, '2014-07-25 12:59:00.850', '28b6fd61-3879-4923-abb2-e9630719940d', 837, 2144, '2');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-07-25 15:02:21.217', '9c875ded-47b3-4260-ae64-75b304d9e419', 838, 'from http://stackoverflow.com/questions/24926631/pig-latin-code-error', 2146, '36');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-07-25 15:02:21.217', '453ebd8b-128d-40db-a6bb-64356fc16543', 839, 'from http://stackoverflow.com/questions/24926631/pig-latin-code-error/24951968#24951968', 2147, '36');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('abhishek bafna', 'When we do grouping of the data, Pig creates a new key named "group" and puts all the tuple matching that key into a bag and associates the bag with the key. So after the group operation schema of the grouped data will be something like

    raw = load ''$input'' using PigStorage(''\u0001'') as (id1:int, name:chararray);
    groupdata1 = group raw by (id1,name);
    describe groupdata1;
    {group: (id1: int,name: chararray),raw: {(id1: int,name: chararray)}}

The ''GROUP'' in line 4 you are trying to access is one of the attribute of the schema from the last statement. These attribute name are case sensitive. It will produce the error saying it doesn''t exist in the schema. So you need to use ''group'' only to access it.', '2014-07-25 09:05:09.493', '291345e3-ddd8-4147-8ee8-6967e90ed122', 839, 2148, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('
While running the below pig script I am getting error in line4: If it is `GROUP` then I am getting error. If I change from `GROUP` TO `group` in line4, then the script is running. What is the difference between group and GROUP.

    LINES = LOAD ''/user/cloudera/datapeople.csv'' USING PigStorage('','') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );

    WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;

    WORDSGROUPED = GROUP WORDS BY ZIPS;

    WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);

    WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;

    DUMP WORDSSORT;

', 1314, '2014-07-24 06:34:50.083', 'cf2fa9f9-d89e-4edd-87a2-81485b9e48d0', 838, 2149, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Pig latin code error', 1314, '2014-07-24 06:34:50.083', 'cf2fa9f9-d89e-4edd-87a2-81485b9e48d0', 838, 2150, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop><bigdata>', 1314, '2014-07-24 06:34:50.083', 'cf2fa9f9-d89e-4edd-87a2-81485b9e48d0', 838, 2151, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"OriginalQuestionIds":[829],"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-07-25 16:26:00.903', '002807ec-c52e-473c-8a79-dc01ecac858f', 838, '101', 2152, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve been trying to create a similarity matrix in Pandas from with a matrix multiplication operation on a document-term count matrix with 2264 rows and 20475 columns.

The calculation completes in IPython but inspection shows the results all come back as NaN.

I''ve also tried doing the same job in numpy, tried converting the original matrix to_sparse and even re-casting the values as integers, but still no joy.

Can anyone suggest the best approach to tackle the problem?', 974, '2014-07-25 17:18:21.393', '3b7901f6-ac11-4af8-8ba0-e607cde30c42', 840, 2153, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to fix similarity matrix in Pandas returning all NaNs?', 974, '2014-07-25 17:18:21.393', '3b7901f6-ac11-4af8-8ba0-e607cde30c42', 840, 2154, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<pandas><similarity>', 974, '2014-07-25 17:18:21.393', '3b7901f6-ac11-4af8-8ba0-e607cde30c42', 840, 2155, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve been trying to create a similarity matrix in Pandas from with a matrix multiplication operation on a document-term count matrix with 2264 rows and 20475 columns.

The calculation completes in IPython but inspection shows the results all come back as NaN.

I''ve also tried doing the same job in numpy, tried converting the original matrix to_sparse and even re-casting the values as integers, but still no joy.

Can anyone suggest the best approach to tackle the problem?

EDIT: Here''s my code thus far:

    path = "../../reuters.db"
    %pylab inline
    import pandas as pd
    import numpy as np
    import pandas.io.sql as psql
    import sqlite3 as lite
    con = lite.connect(path)
    with con:
        sql = "SELECT * FROM Frequency"
        df = psql.frame_query(sql, con)
        print df.shape
    df = df.rename(columns={"term":"term_id", "count":"count_id"})
    pivoted = df.pivot(''docid'', ''term_id'', ''count_id'')
    pivoted.to_sparse()
    similarity_matrix = pivoted.dot(pivoted.T)', 974, '2014-07-25 17:56:59.177', '043ebd49-0390-4919-9620-f50045b27268', 840, 'Asked for code', 2156, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Instead of collaborative filtering I would use the matrix factorization approach, wherein users and movies alike a represented by vectors of latent features whose dot products yield the ratings. Normally one merely selects the rank (number of features) without regard to what the features represent, and the algorithm does the rest. Like PCA, the result is not immediately interpretable but it yields good results. What you want to do is extend the movie matrix to include the additional features you mentioned and make sure that they stay fixed as the algorithm estimates the two matrices using regularizastion. The corresponding entries in the user matrix will be initialized randomly, then estimated by the matrix factorization algorithm. It''s a versatile and performant approach but it takes some understanding of machine learning, or linear algebra at least.

I saw a nice ipython notebook a while back but I can''t find it right now, so I''ll refer you to [another one](http://nbviewer.ipython.org/github/diktat/CPSC540machinelearning/blob/master/1.4%20Collaborative%20Filtering%20for%20Movie%20Recommendation.ipynb) which, while not as nice, still clarifies some of the maths.', 381, '2014-07-25 18:12:49.847', 'baa5ae44-d8a1-4123-a1dd-00af219f5a4f', 841, 2157, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('My data contains a set of start times and duration for an action. I would like to plot this so that for a given time slice I can see how many actions are active. I''m currently thinking of this as a histogram with time on the x axis and number of active actions on the y axis.

My question is, how should I adjust the data so that this is able to be plotted?

The times for an action can be between 2 seconds and a minute. And, at any given time I would estimate there could be about 100 actions taking place. Ideally a single plot would be able to show hours of data. The accuracy of the data is in milliseconds.

In the past the way that I have done this is to count for each second how many actions started , ended, or were active. This gave me a count of active actions for each second. The issue I found with this technique was that it made it difficult to adjust the time slice that I was looking at. Looking at a time slice of a minute was difficult to compute and looking at time slices of less than a second was impossible.

I''m open to any advice on how to think about this issue.

Thanks in advance!', 2702, '2014-07-25 18:30:26.490', 'c86effde-d559-410b-a2d3-88cfccdf9787', 832, 'added 87 characters in body', 2158, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I don''t know if this is a right place to ask this question, but a community dedicated to Data Science should be the most apt place in my opinion.

I have just started with Data Science and Machine learning. I am looking for long term project ideas which I can work on for like 8 months.

A mix of Data Science and Machine learning would be great.

A project big enough to help me understand the core concepts and also implement them at the same time would be very beneficial.', 2725, '2014-07-25 18:36:31.340', '9fd8d0a3-ae7f-428e-8f1f-3c796e93733e', 842, 2159, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data Science Project Ideas', 2725, '2014-07-25 18:36:31.340', '9fd8d0a3-ae7f-428e-8f1f-3c796e93733e', 842, 2160, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><dataset>', 2725, '2014-07-25 18:36:31.340', '9fd8d0a3-ae7f-428e-8f1f-3c796e93733e', 842, 2161, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would try to analyze and solve one or more of the problems published on **Kaggle Competitions** (https://www.kaggle.com/competitions). Note that the competitions are grouped by their expected *complexity*, from `101` (bottom of the list) to `Research` and `Featured` (top of the list). A color-coded vertical band is a *visual guideline* for grouping. You can **assess time** you could spend on a project by **adjusting** the expected *length* of corresponding competition, based on your *skills* and *experience*.

A number of **data science project ideas** can be found by browsing the following `Coursolve` webpage: https://www.coursolve.org/browse-needs?query=Data%20Science.

If you have skills and desire to work on a **real data science project**, focused on **social impacts**, visit `DataKind` projects page: http://www.datakind.org/projects. More projects with social impacts focus can be found at `Data Science for Social Good` fellowship webpage: http://dssg.io/projects.

**Science Project Ideas** page at `My NASA Data` site looks like another place to visit for inspiration: http://mynasadata.larc.nasa.gov/804-2.

If you would like to use **open data**, this long list of applications on `Data.gov` can provide you with some interesting *data science* project ideas: http://www.data.gov/applications.', 2452, '2014-07-25 20:50:14.540', '01e4aa54-eeb0-4a83-a50e-5a462a1fe7f4', 843, 2162, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('so I''m using Spark to do sentiment analysis, and I keep getting errors with the serializers it uses (I think) to pass python objects around.

    PySpark worker failed with exception:
    Traceback (most recent call last):
      File "/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/worker.py", line 77, in main
        serializer.dump_stream(func(split_index, iterator), outfile)
      File "/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py", line 191, in dump_stream
        self.serializer.dump_stream(self._batched(iterator), stream)
      File "/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py", line 123, in dump_stream
        for obj in iterator:
      File "/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py", line 180, in _batched
        for item in iterator:
    TypeError: __init__() takes exactly 3 arguments (2 given)

and the code for serializers is available [here][1]


  [1]: https://spark.apache.org/docs/latest/api/python/pyspark.serializers-pysrc.html#PickleSerializer.dumps', 2726, '2014-07-25 21:03:44.663', '5137cf9c-796e-49f8-b06e-6661ab5c7039', 844, 2163, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using Apache Spark to do ML. Keep getting serializing errors', 2726, '2014-07-25 21:03:44.663', '5137cf9c-796e-49f8-b06e-6661ab5c7039', 844, 2164, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop><distributed><scalability><map-reduce>', 2726, '2014-07-25 21:03:44.663', '5137cf9c-796e-49f8-b06e-6661ab5c7039', 844, 2165, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('so I''m using Spark to do sentiment analysis, and I keep getting errors with the serializers it uses (I think) to pass python objects around.

    PySpark worker failed with exception:
    Traceback (most recent call last):
      File "/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/worker.py", line 77, in main
        serializer.dump_stream(func(split_index, iterator), outfile)
      File "/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py", line 191, in dump_stream
        self.serializer.dump_stream(self._batched(iterator), stream)
      File "/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py", line 123, in dump_stream
        for obj in iterator:
      File "/Users/abdul/Desktop/RSI/spark-1.0.1-bin-    hadoop1/python/pyspark/serializers.py", line 180, in _batched
        for item in iterator:
    TypeError: __init__() takes exactly 3 arguments (2 given)

and the code for serializers is available [here][1]

and my code is [here][2]


  [1]: https://spark.apache.org/docs/latest/api/python/pyspark.serializers-pysrc.html#PickleSerializer.dumps
  [2]: https://github.com/seashark97/Scalable-Sentiment-Analysis/blob/master/spark_test.py', 2726, '2014-07-25 21:11:03.587', 'bb7414b8-3b72-47fb-bdc3-ad29d9037c65', 844, 'added 120 characters in body', 2166, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Most often serialization error in (Py)Spark means that some part of your distributed code (e.g. functions passed to `map`) has **dependencies** on **non-serializable data**. Consider following example:

    rdd = sc.parallelize(range(5))
    rdd = rdd.map(lambda x: x + 1)
    rdd.collect()

Here you have distributed collection and lambda function to send to all workers. Lambda is completely self-containing, so it''s easy to copy its binary representation to other nodes without any worries.

Now let''s make things a bit more interesting:

    f = open("/etc/hosts")
    rdd = sc.parallelize(range(100))
    rdd = rdd.map(lambda x: f.read())
    rdd.collect()
    f.close()

Boom! Strange error in serialization module! What just happened is that we had attempted to pass `f`, which is a file object, to workers. Obviously, file object is a handle to _local_ data and thus cannot be sent to other machines.

----

So what''s happening in your specific code? Without actual data and knowing record format, I cannot debug it completely, but I guess that problem goes from this line:

    def vectorizer(text, vocab=vocab_dict):

In Python, keyword arguments are initialized when function is called for the first time. When you call `sc.parallelize(...).map(vectorizer)` just after its definition, `vocab_dict` is available _locally_, but _remote_ workers know absolutely nothing about it. Thus function is called with fewer parameters than it expects which results in `__init__() takes exactly 3 arguments (2 given)` error.

Also note, that you follow very bad pattern of  `sc.parallelize(...)...collect()` calls. First you spread your collection to entire cluster, do some computations, and then pull the result. But sending data back and forth is pretty pointless here. Instead, you can just do these computations locally, and run Spark''s parallel processes only when you work with really big datasets (like you main `amazon_dataset`, I guess).', 1279, '2014-07-26 00:11:03.637', 'c989dcd9-2f94-41e9-9e9d-22c2cb2a23f5', 845, 2167, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Take something from your everyday life. Create predictor of traffic jams in your region, craft personalised music recommender, analyse car market, etc. Choose **real problem** that you **want to solve** - this will not only keep you motivated, but also make you go through the whole development circle from data collection to hypothesis testing. ', 1279, '2014-07-26 01:12:08.167', '07f30f72-4e58-45a6-be0f-29e94fa2a687', 846, 2168, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Introduction to Data Science][1] course that is being run on Coursera now includes real-world project assignment where companies post their problems and students are encouraged to solve them. This is done via [coursolve.com][2] (already mentioned here).

More information [here][3] (you have to be enrolled in the course to see that link)


  [1]: https://www.coursera.org/course/datasci
  [2]: https://www.coursolve.org/
  [3]: https://class.coursera.org/datasci-002/wiki/OptionalRealWorldProject', 816, '2014-07-26 08:37:52.823', 'd47390b6-4187-4cee-af47-7b7bc21cc08c', 847, 2170, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":62,"DisplayName":"AsheeshR"}]}', 62, '2014-07-26 15:09:56.510', '808152a4-d0eb-4487-83aa-814da2369d65', 842, '105', 2172, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<hadoop><r>', 62, '2014-07-26 15:10:51.000', '7190494a-16dd-46c3-a3bc-e485e1e28ec7', 59, 'Tag not relevant.', 2173, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":62,"DisplayName":"AsheeshR"}]}', 62, '2014-07-26 15:11:29.480', '8e5926ce-a596-4081-874c-78ff75e3c1c1', 831, '105', 2174, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('K-means is not the most appropriate algorithm here.

The reason is that k-means is designed to **minimize variance**. This is, of course, appearling from a statistical and signal procssing point of view, but your data is not "linear".

Since your data is in latitude, longitude format, you should use an algorithm that can handle *arbitrary* distance functions, in particular geodetic distance functions. Hierarchical clustering, PAM, CLARA, and DBSCAN are popular examples of this.

The problems of k-means are easy to see when you consider points close to the +-180 degrees wrap-around. Even if you hacked k-means to use Haversine distance, in the update step when it recomputes the *mean* the result will be badly screwed. **Worst case is, k-means will never converge!**', 924, '2014-07-26 16:04:25.363', 'a8313402-ac0a-4a4d-9819-19ae6f808b7b', 848, 2175, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Since you want to show so much data, I think that your best choice is going interactive. Check out this [demo][1], it is close to what you want but not quite.

It is very difficult to show a lot of data in a single diagram, together with the finest details and the bird-eyes view. But you can let the user interact and look for the details. To show counts, one option is to use color-coding. Take a look at this image (code [here][2]): ![image][3].

Here rgb channels have been used to encode (the logarithm of) the number of active events (red), events starting (green) and events ending (blue) for windows of different size. The X axis is time, and the Y axis represents window size, that is, duration. Thus, a point with coordinates (10, 4) represents the interval of time that goes from 10 to 14.

To make a lot of data more detailed, it could be a good idea to make the diagram zoomable (like in the demo before), and to give the user the possibility of visualizing just one channel/magnitude.


  [1]: http://demo.zunzun.se/intervals/index.html
  [2]: https://gist.github.com/dsign/4c598cfbfc81f6d491d5
  [3]: http://i.stack.imgur.com/xmgHi.png', 2575, '2014-07-26 21:03:13.470', '29028da1-a74f-42f6-9220-64a790fb9fd3', 849, 2176, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('To be perfectly honest, most NoSQL databases are not very well suited to applications in big data. For the vast majority of all big data applications, the performance of MongoDB compared to a relational database like [MySQL][1] is [significantly][2] is poor enough to warrant staying away from something like MongoDB entirely.

With that said, there are a couple of really useful properties of NoSQL databases that certainly work in your favor when you''re working with large data sets, though the chance of those benefits outweighing the generally poor performance of NoSQL compared to [SQL][3] for read-intensive operations (most similar to typical big data use cases) is low.

 - **No Schema** - If you''re working with a lot of unstructured data, it might be hard to actually decide on and rigidly apply a schema. NoSQL databases in general are very supporting of this, and will allow you to insert schema-less documents on the fly, which is certainly not something an SQL database will support.
 - **[JSON][4]** - If you happen to be working with JSON-style documents instead of with [CSV][5] files, then you''ll see a lot of advantage in using something like MongoDB for a database-layer. Generally the workflow savings don''t outweigh the increased query-times though.
 - **Ease of Use** - I''m not saying that SQL databases are always hard to use, or that [Cassandra][6] is the easiest thing in the world to set up, but in general NoSQL databases are easier to set up and use than SQL databases. MongoDB is a particularly strong example of this, known for being one of the easiest database layers to use (outside of [SQLite][7]). SQL also deals with a lot of normalization and there''s a large legacy of SQL best practices that just generally bogs down the development process.

Personally I might suggest you also check out [graph databases][8] such as [Neo4j][9] that show really good performance for certain types of queries if you''re looking into picking out a backend for your data science applications.

  [1]: http://en.wikipedia.org/wiki/MySQL
  [2]: http://www.moredevs.ro/mysql-vs-mongodb-performance-benchmark/
  [3]: http://en.wikipedia.org/wiki/SQL
  [4]: http://en.wikipedia.org/wiki/JSON
  [5]: http://en.wikipedia.org/wiki/Comma-separated_values
  [6]: http://en.wikipedia.org/wiki/Apache_Cassandra
  [7]: http://en.wikipedia.org/wiki/SQLite
  [8]: http://en.wikipedia.org/wiki/Graph_database
  [9]: http://en.wikipedia.org/wiki/Neo4j

', 2523, '2014-07-27 03:34:44.953', 'c995dfa4-af19-48d6-a7bf-e9276c2311fb', 797, 'Copy edited (e.g. ref. <http://en.wikipedia.org/wiki/NoSQL>). Added some context. Removed unnecessary formatting. ', 2182, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-27 03:34:44.953', 'c995dfa4-af19-48d6-a7bf-e9276c2311fb', 797, 'Proposed by 2523 approved by 548 edit id of 125', 2183, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><statistics><career>', 553, '2014-07-27 03:35:00.173', '1cbf3b5c-17e1-4824-bc37-86b23836c446', 808, 'adding a career tag', 2184, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-27 03:35:00.173', '1cbf3b5c-17e1-4824-bc37-86b23836c446', 808, 'Proposed by 553 approved by 434, 548 edit id of 121', 2185, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I don''t know if this is a right place to ask this question, but a community dedicated to Data Science should be the most appropriate place in my opinion.

I have just started with Data Science and Machine learning. I am looking for long term project ideas which I can work on for like 8 months.

A mix of Data Science and Machine learning would be great.

A project big enough to help me understand the core concepts and also implement them at the same time would be very beneficial.', 1352, '2014-07-27 03:35:06.853', '2551aa1b-7f01-4ba1-8177-fbc868339a91', 842, 'apt = appropriate', 2186, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-27 03:35:06.853', '2551aa1b-7f01-4ba1-8177-fbc868339a91', 842, 'Proposed by 1352 approved by 434, 548 edit id of 123', 2187, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you''ve got prior information then you should certainly not use simple mean in a split test. I assume you''re trying to just predict which group will produce the greatest amount of revenue overall, by trying to emulate the underlying distribution.

Firstly, it''s worth noting that any metrics you choose will actually reduce to mean in a pretty trivial way. Eventually mean will necessarily work out, though using a standard bayesian method to estimate the mean is probably your best bet.

If you''ve got a prior then using a standard bayesian approach to update the prior on your mean revenue is probably the best way to do it. Basically, just take the individual results you get and update a multinomial distribution representing your prior in each case.

If you want some more full background on multinomial distributions as bayesian priors are pretty well, [this][1] Microsoft paper does a pretty good job of outlining it. In general, I wouldn''t care so much about the fact that your distribution is technically discrete, as a multinomial distribution will effectively interpolate across your solution space, giving you a continuous distribution that is a very good approximation of your discrete space.


  [1]: http://research.microsoft.com/en-us/um/people/minka/papers/minka-multinomial.pdf', 548, '2014-07-27 03:58:22.907', 'bd749343-3210-4942-b69f-342d5f3a6c6e', 850, 2188, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How can [NoSQL][1] databases like [MongoDB][2] be used for data analysis? What are the features in them that can make data analysis faster and powerful?

  [1]: http://en.wikipedia.org/wiki/NoSQL
  [2]: http://en.wikipedia.org/wiki/MongoDB
', 2523, '2014-07-27 07:36:51.510', '195c2898-f876-426d-b774-a63ac2a2a244', 793, 'Copy edited. Added some context.', 2190, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Uses of NoSQL database in data science', 2523, '2014-07-27 07:36:51.510', '195c2898-f876-426d-b774-a63ac2a2a244', 793, 'Copy edited. Added some context.', 2191, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-27 07:36:51.510', '195c2898-f876-426d-b774-a63ac2a2a244', 793, 'Proposed by 2523 approved by 548, 434 edit id of 124', 2192, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('## Image Similarity based on Color Palette Distribution ##

I am trying to compute similarity between two images based on their color palette distribution, let''s say I have two sets of key value pairs as follows,

Img1: `{''Brown'': 14, ''White'': 13, ''Black'': 40, ''Gray'': 31}`

Img2: `{''Pink'': 82, ''Brown'': 8, ''White'': 7}`

Where the numbers denote the % of that color present in the image. What would be the best way to compute similarity on a scale of 0-100 between the two images?', 2744, '2014-07-27 21:54:05.003', '8220e1a6-9dc9-47da-af5c-c537c14edab1', 851, 2193, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Computing Image Similarity based on Color Distribution', 2744, '2014-07-27 21:54:05.003', '8220e1a6-9dc9-47da-af5c-c537c14edab1', 851, 2194, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><clustering><similarity>', 2744, '2014-07-27 21:54:05.003', '8220e1a6-9dc9-47da-af5c-c537c14edab1', 851, 2195, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have implemented NER system with the use of CRF algorithm with my handcrafted features that gave quite good results. The thing is that I used lots of different features including POS tags and lemmas.

Now I want to make the same NER for different language. The problem here is that I can''t use POS tags and lemmas. I started reading articles about deep learning and unsupervised feature learning.

My question is, if it''s possible to use methods for unsupervised feature learning with CRF algorithm. Did anyone try this and got any good result? Is there any article or tutorial about this matter.

I still don''t completely understand this way of feature creation so I don''t want to spend to much time for something that won''t work. So any information would be really helpful. To create whole NER system based on deep learning is a bit to much for now.

Thank you in advance.', 2750, '2014-07-28 07:19:49.877', 'cb48a3ee-b41f-493e-915c-002979b88f41', 853, 2199, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Unsupervised Feature Learning for NER', 2750, '2014-07-28 07:19:49.877', 'cb48a3ee-b41f-493e-915c-002979b88f41', 853, 2200, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><text-mining><feature-extraction>', 2750, '2014-07-28 07:19:49.877', 'cb48a3ee-b41f-493e-915c-002979b88f41', 853, 2201, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I like @Kallestad answer very much, but I would like to add a meta-step: Make sure that you understand how the data where collected, and what types of constraints there are.
I think it is very common to think that there where no non-obvious steps when the data where collected, but this is not the case: Most of the time, some process or indivudal did somethink with the data, and these steps can and will influence the shape of the data.

Two examples:
I had a study recently where the data where collected by various con
tractors worldwide. I was not at the briefing, so that was opaque to me. Unfortunately, the measurements where off for some parts of france: People all liked ice cram, but we expected a random distribution. There was no obvious reason for this uniformity, so I began to hunt the errors. When I queried the contractors, one had misunderstood the briefing and selected only ice-cream lovers from his database.

The second error was more challenging: When doing some geographic analysis, I found that a lot of people had extremely large movement patterns, which suggested that a lot of them traveled from Munich to Hamburg in minutes. When I spoke with ppeople upstream, they found a subtle bug in their data aggregation software, which was unnoticed before.

Conclusions:

- Do not assume that your data was collected by perfect processes /humans.
- Do try to understand the limits of your data providers.
- Look at individual patterns / values and try to determine if they are logical (easy for movement / geographic data)', 791, '2014-07-28 08:36:53.633', '92f89317-dc69-485d-abbb-0bf218018a90', 854, 2202, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Below you can find a copy of my answer to a related (however, focused on data cleaning aspect) question here on *Data Science StackExchange* (http://datascience.stackexchange.com/a/722/2452), provided in its entirety for readers'' convenience. I believe that it partially answers your question as well and hope it is helpful. While the answer is focused on `R` ecosystem, similar packages and/or libraries can be found for other *data analysis environments*. Moreover, while the two **cited papers** on data preparation also contain examples in R, these papers present **general** **workflow (framework)** and **best practices** that are applicable to **any** data analysis environment.

R contains some *standard* functions for data manipulation, which can be used for data cleaning, in its **base** package (`gsub`, `transform`, etc.), as well as in various third-party packages, such as **stringr**, **reshape**, **reshape2**, and **plyr**. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.

Additionally, R offers some packages specifically *focused* on data cleaning and transformation:

- **editrules** (http://cran.r-project.org/web/packages/editrules/index.html)
- **deducorrect** (http://cran.r-project.org/web/packages/deducorrect/index.html)
- **StatMatch** (http://cran.r-project.org/web/packages/StatMatch/index.html)
- **MatchIt** (http://cran.r-project.org/web/packages/MatchIt/index.html)
- **DataCombine** (http://cran.r-project.org/web/packages/DataCombine)

A comprehensive and coherent approach to **data cleaning** in R, including examples and use of **editrules** and **deducorrect** packages, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf.', 2452, '2014-07-28 12:07:25.573', 'bea11309-544c-4bf1-b126-f2ec5b84e238', 855, 2203, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes, it is entirely possible to combine unsupervised learning with the CRF model.  In particular, I would recommend that you explore the possibility of using [word2vec](https://code.google.com/p/word2vec/) features as inputs to your CRF.

Word2vec trains a  to distinguish between words that are appropriate for a given context and words that are randomly selected.  Select weights of the model can then be interpreted as a dense vector representation of a given word.

These dense vectors have the appealing property that words that are semantically or syntactically similar have similar vector representations.  Basic vector arithmetic even reveals some interesting learned relationships between words.
For example, vector("Paris") - vector("France") + vector("Italy") yields a vector that is quite similar to vector("Rome").

At a high level, you can think of word2vec representations as being similar to LDA or LSA representations, in the sense that you can convert a sparse input vector into a dense output vector that contains word similarity information.

For English text Google distributes word2vec models pretrained on a huge 100 billion word Google News dataset, but for other languages you''ll have to train your own model.
', 684, '2014-07-28 14:48:37.743', '44bbf08a-3135-46c3-b8c2-296e9e23a527', 858, 2209, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the different classes of data science problems that can be solved using mapreduce found in hadoop?', 2643, '2014-07-28 16:17:49.823', 'bad074a4-6698-47f9-b811-13fe2f515d9d', 859, 2210, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data science and Hadoop Map Reduce', 2643, '2014-07-28 16:17:49.823', 'bad074a4-6698-47f9-b811-13fe2f515d9d', 859, 2211, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop><map-reduce>', 2643, '2014-07-28 16:17:49.823', 'bad074a4-6698-47f9-b811-13fe2f515d9d', 859, 2212, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ll add one thing- if possible, do a reasonableness check by comparing you data against some other source. It seems that whenever I fail to do this, I get burnt:(', 1241, '2014-07-28 16:23:39.600', '5c3baf92-ce10-48ca-a65a-16eeb9c23cb1', 860, 2213, '2');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('user1361', 'Data Science has many different sub-areas as described in [my post][1]). Nearly for each area, scientists and developer has significant contributions. To learn more about what can be done, please look at following websites:

 * Data Mining Algorithms & Machine Learning -> [Apache Mahout][3]
 * Statistics -> [RHadoop][4]
 * Data Warehousing & Database Querying -> [SQL-MapReduce][5]
 * Social Network Analysis -> [Article][6]
 * Bio-informatics -> [Article - 1 ][7], [Article - 2][8]

Also, there are some work on MapReduce + Excel + Cloud combination but I have not found the link.

Do not forget that knowing what MapReduce can do is not enough for Data Science. You should also aware of [What MapReduce can''t do][2], too.



[1]: http://www.datasciencecentral.com/profiles/blogs/ingredients-of-data-science-1
[2]: http://www.analyticbridge.com/profiles/blogs/what-mapreduce-can-t-do
[3]: https://mahout.apache.org/
[4]: https://github.com/RevolutionAnalytics/RHadoop/wiki
[5]: http://www.teradata.com.tr/Teradata-Aster-SQL-MapReduce/?LangType=1055&LangSelect=true
[6]: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=5636636
[7]: http://abhishek-tiwari.com/post/mapreduce-and-hadoop-algorithms-in-bioinformatics-papers?ModPagespeed=noscript
[8]: http://www.biomedcentral.com/1471-2105/11/S12/S1
', '2014-07-28 16:39:45.703', '5dfab5ec-1b22-4427-aefc-629721a6e520', 861, 2214, '2');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('user1361', 'Data Science has many different sub-areas as described in [my post][1]). Nearly for each area, scientists and developer has significant contributions. To learn more about what can be done, please look at following websites:

 * Data Mining Algorithms & Machine Learning -> [Apache Mahout][3]
 * Statistics -> [RHadoop][4]
 * Data Warehousing & Database Querying -> [SQL-MapReduce][5]
 * Social Network Analysis -> [Article][6]
 * Bio-informatics -> [Article - 1 ][7], [Article - 2][8]

Also, there are some work on MapReduce + Excel + Cloud combination but I have not found the link.

> What are the different classes of Data Science problems ...

Each "classes" is not purely homogeneous problem domain, i.e. some problem cannot be solved via map and reduce approach due to its communication cost, or algorithm behavior. What I mean by behavior is that some problem wants to have control on all data sets instead of chunks. Thus, I refuse to list type of problem "classes".


Do not forget that knowing what MapReduce can do is not enough for Data Science. You should also aware of [What MapReduce can''t do][2], too.



[1]: http://www.datasciencecentral.com/profiles/blogs/ingredients-of-data-science-1
[2]: http://www.analyticbridge.com/profiles/blogs/what-mapreduce-can-t-do
[3]: https://mahout.apache.org/
[4]: https://github.com/RevolutionAnalytics/RHadoop/wiki
[5]: http://www.teradata.com.tr/Teradata-Aster-SQL-MapReduce/?LangType=1055&LangSelect=true
[6]: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=5636636
[7]: http://abhishek-tiwari.com/post/mapreduce-and-hadoop-algorithms-in-bioinformatics-papers?ModPagespeed=noscript
[8]: http://www.biomedcentral.com/1471-2105/11/S12/S1
', '2014-07-28 16:49:55.573', '12d7b3ac-9675-411d-a173-a14a94a831b0', 861, 'added 397 characters in body', 2215, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think that there is no universal technique for "cleaning" data before doing actual research. On the other hand, I''m aiming for doing as much [reproducible research][1] as possible. By doing reproducible research, if you used cleaning techniques with bugs or with poor parameters/assumptions it could be spot by others.

There is nice R package [knitr][2] which helps a lot in reproducible research.

Of course, not all research could be fully reproduced (for example live Twitter data) , but at least you can document cleaning, formating and preprocessing steps easily.

You can check my [assessment][3] prepared for [Reproducible Research course at Coursera][4].


  [1]: https://en.wikipedia.org/wiki/Reproducibility#Reproducible_research
  [2]: http://yihui.name/knitr/
  [3]: http://rpubs.com/QuatnumDamage/storm
  [4]: https://www.coursera.org/course/repdata', 82, '2014-07-28 17:33:43.280', '03316c70-70aa-4649-8b7b-9e335b7cdd65', 862, 2216, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There was a recent furore with [facebook experimenting on their users to see if they could alter user''s emotions](http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840).

Whilst I am not a professional data scientist I read about [data science ethics](http://columbiadatascience.com/2013/11/25/data-science-ethics/) from [Cathy O''Neill''s book ''Doing Data Science''](http://shop.oreilly.com/product/0636920028529.do) and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science *accidentally*.

Whilst the linked article touched on data integrity, the book also discussed the moral ethics behind understanding the impact of the data models that are created and the impact of those models which can have adverse effects when used inappropriately (sometimes unwittingly) or when the models are inaccurate, again producing adverse results.

The article discusses a code of practice and mentions the [Data Science Association''s Code of conduct](http://www.datascienceassn.org/code-of-conduct.html), is this something that is in use? Rule 7 is of particular interest (quoted from their website):

> (a) A person who consults with a data scientist about the possibility
> of forming a client-data scientist relationship with respect to a
> matter is a prospective client.
>
> (b) Even when no client-data scientist relationship ensues, a data
> scientist who has learned information from a prospective client shall
> not use or reveal that information.
>
> (c) A data scientist subject to paragraph (b) shall not provide
> professional data science services for a client with interests
> materially adverse to those of a prospective client in the same or a
> substantially related industry if the data scientist received
> information from the prospective client that could be significantly
> harmful to that person in the matter

Is this something that is practiced professionally? Many users blindly accept that we get some free service (mail, social network, image hosting, blog platform etc..) and agree to an EULA in order to have ads pushed at us.

Finally how is this regulated, I often read about users being up in arms when the terms of a service change but it seems that it requires some liberty organisation or class action to react to such things before something happens.

By the way I am not making any judgements here or saying that all data scientists behave like this, I''m interested in what is taught academically and practiced professionally.', 95, '2014-07-28 20:53:13.387', 'cd8c123b-9bb0-43f8-b78c-474ef536f882', 823, 'added 1811 characters in body', 2218, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Yes, it is entirely possible to combine unsupervised learning with the CRF model.  In particular, I would recommend that you explore the possibility of using [word2vec](https://code.google.com/p/word2vec/) features as inputs to your CRF.

Word2vec trains a  to distinguish between words that are appropriate for a given context and words that are randomly selected.  Select weights of the model can then be interpreted as a dense vector representation of a given word.

These dense vectors have the appealing property that words that are semantically or syntactically similar have similar vector representations.  Basic vector arithmetic even reveals some interesting learned relationships between words.
For example, vector("Paris") - vector("France") + vector("Italy") yields a vector that is quite similar to vector("Rome").

At a high level, you can think of word2vec representations as being similar to LDA or LSA representations, in the sense that you can convert a sparse input vector into a dense output vector that contains word similarity information.

For that matter, LDA and LSA are also valid options for unsupervised feature learning -- both attempt to represent words as combinations of "topics" and output dense word representations.

For English text Google distributes word2vec models pretrained on a huge 100 billion word Google News dataset, but for other languages you''ll have to train your own model.
', 684, '2014-07-29 03:29:42.940', '6bfe1731-e473-4fa7-b845-3e5574bead11', 858, 'added 193 characters in body', 2224, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How should ethics be applied in data science', 95, '2014-07-29 12:39:59.630', '9430f2fa-53b9-458d-a562-85e4bca53caf', 823, 'edited title', 2227, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There was a recent furore with [facebook experimenting on their users to see if they could alter user''s emotions](http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840) and now [okcupid](http://www.bbc.co.uk/news/technology-28542642).

Whilst I am not a professional data scientist I read about [data science ethics](http://columbiadatascience.com/2013/11/25/data-science-ethics/) from [Cathy O''Neill''s book ''Doing Data Science''](http://shop.oreilly.com/product/0636920028529.do) and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science *accidentally*.

Whilst the linked article touched on data integrity, the book also discussed the moral ethics behind understanding the impact of the data models that are created and the impact of those models which can have adverse effects when used inappropriately (sometimes unwittingly) or when the models are inaccurate, again producing adverse results.

The article discusses a code of practice and mentions the [Data Science Association''s Code of conduct](http://www.datascienceassn.org/code-of-conduct.html), is this something that is in use? Rule 7 is of particular interest (quoted from their website):

> (a) A person who consults with a data scientist about the possibility
> of forming a client-data scientist relationship with respect to a
> matter is a prospective client.
>
> (b) Even when no client-data scientist relationship ensues, a data
> scientist who has learned information from a prospective client shall
> not use or reveal that information.
>
> (c) A data scientist subject to paragraph (b) shall not provide
> professional data science services for a client with interests
> materially adverse to those of a prospective client in the same or a
> substantially related industry if the data scientist received
> information from the prospective client that could be significantly
> harmful to that person in the matter

Is this something that is practiced professionally? Many users blindly accept that we get some free service (mail, social network, image hosting, blog platform etc..) and agree to an EULA in order to have ads pushed at us.

Finally how is this regulated, I often read about users being up in arms when the terms of a service change but it seems that it requires some liberty organisation, class action or a [senator](http://www.cnet.com/news/senator-asks-ftc-to-investigate-facebooks-mood-study/) to react to such things before something happens.

By the way I am not making any judgements here or saying that all data scientists behave like this, I''m interested in what is taught academically and practiced professionally.', 95, '2014-07-29 12:39:59.630', '9430f2fa-53b9-458d-a562-85e4bca53caf', 823, 'edited title', 2228, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Let''s first split it into parts.

**Data Science** is about making knowledge from raw data. It uses machine learning, statistics and other fields to simplify (or even automate) decision making. Data science techniques may work with any data size, but more data means better predictions and thus more precise decisions.

**Hadoop** is a common name for a set of tools intended to work with large amounts of data. Two most important components in Hadoop are HDFS and MapReduce.

**HDFS**, or Hadoop Distributed File System, is a special distributed storage capable of holding really large data amounts. Large files on HDFS are split into blocks, and for each block HDFS API exposes its _location_.

**MapReduce** is framework for running computations on nodes with data. MapReduce heavily uses _data locality_ exposed by HDFS: when possible, data is not transferred between nodes, but instead code is copied to the nodes with data.

So basically any problem (including data science tasks) that doesn''t break data locality principle may be efficiently implemented using MapReduce (and a number of other problems may be solved not that efficiently, but still simply enough).

-----

Let''s take some examples. Very often analyst only needs some simple statistics over his tabular data. In this case [**Hive**](https://hive.apache.org/), which is basically SQL engine over MapReduce, works pretty well (there are also Impala, Shark and others, but they don''t use Hadoop''s MapReduce, so more on them later).

In other cases analyst (or developer) may want to work with previously unstructured data. Pure MapReduce is pretty good for **transforming** and **standardizing** data.

Some people are used to exploratory statistics and visualization using tools like R. It''s possible to apply this approach to big data amounts using [**RHadoop**](https://github.com/RevolutionAnalytics/RHadoop/wiki) package.

And when it comes to MapReduce-based machine learning [**Apache Mahout**](https://mahout.apache.org/) is the first to mention.


-----

There''s, however, one type of algorithms that work pretty slowly on Hadoop even in presence of data locality, namely, iterative algorithms. Iterative algorithms tend to have multiple Map and Reduce stages. Hadoop''s MR framework **reads** and **writes** data to disk on each stage (and sometimes in between), which makes iterative (as well as any multi-stage) tasks terribly slow.

Fortunately, there are alternative frameworks that can both - use data locality and keep data in memory between stages. Probably, the most notable of them is [**Apache Spark**](https://spark.apache.org/). Spark is complete replacement for Hadoop''s MapReduce that uses its own runtime and exposes pretty rich API for manipulating your distributed dataset. Spark has several sub-projects, closely related to data science:

 * [**Shark**](http://shark.cs.berkeley.edu/) and [**Spark SQL**](https://spark.apache.org/sql/)  provide alternative SQL-like interfaces to data stored on HDFS
 * [**Spark Streaming**](https://spark.apache.org/streaming/) makes it easy to work with continuous data streams (e.g. Twitter feed)
 * [**MLlib**](https://spark.apache.org/mllib/) implements a number of machine learning algorithms with a pretty simple and flexible API
 * [**GraphX**](https://spark.apache.org/graphx/) enables large-scale graph processing

So there''s pretty large set of data science problems that you can solve with Hadoop and related projects. ', 1279, '2014-07-29 13:43:39.060', '61ec9a4d-c296-426a-84bb-576149312d70', 863, 2229, '2');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('user1361', 'What are the different classes of data science problems that can be solved using mapreduce programming model?', '2014-07-30 03:13:17.827', 'b52a07cb-3963-4217-a63d-b0bbc00788d5', 859, 'edit title and question body', 2230, '5');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('user1361', 'Data science and MapReduce programming model of Hadoop', '2014-07-30 03:13:17.827', 'b52a07cb-3963-4217-a63d-b0bbc00788d5', 859, 'edit title and question body', 2231, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-30 03:13:17.827', 'b52a07cb-3963-4217-a63d-b0bbc00788d5', 859, 'Proposed by 1361 approved by 434, 84 edit id of 126', 2232, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('map/reduce is most appropriate for parallelizable offline computations. To be more precise, it works best when the result can be found from the result of some function of a partition of the input. Averaging is a trivial example; you can do this with map/reduce by summing each partition, returning the sum and the number of elements in the partition, then computing the overall mean using these intermediate results. It is less appropriate when the intermediate steps depend on the state of the other partitions.', 381, '2014-07-30 07:49:14.467', '5c01c4c2-a20e-47fe-833c-95946cc66b13', 864, 2234, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I need to build parse tree for some source code (on Python or any program language that describe by CFG).

So, I have source code on some programming language and BNF this language.

Can anybody give some advice how can I build parse tree in this case?
Preferably, with tools for Python.', 988, '2014-07-30 10:24:54.180', '8ecbf926-0439-4cc8-84ce-c04db89c25f3', 865, 2237, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to build parse tree with BNF', 988, '2014-07-30 10:24:54.180', '8ecbf926-0439-4cc8-84ce-c04db89c25f3', 865, 2238, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><parsing>', 988, '2014-07-30 10:24:54.180', '8ecbf926-0439-4cc8-84ce-c04db89c25f3', 865, 2239, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am, admittedly, very new to data science. I have spent the last 8 months or so learning as much as I can about the field and its methods.  I am having issues choosing which methods to apply.

I am currently working with a large set of health insurance claims data that includes some laboratory and pharmacy claims. The most consistent information in the data set, however, is made up of diagnosis (ICD-9CM) and procedure codes (CPT, HCSPCS, ICD-9CM).

My goals are to:

 1. Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease;
 2. Identify the likelihood (or probability) that a patient will develop a medical condition based on the conditions they have had in the past;
 3. Do the same as 1 and 2, but with procedures and/or diagnoses.
 4. Preferably, the results would be interpretable by a doctor

I have looked at things like the [Heritage Health Prize Milestone papers][1] and have learned a lot from them, but they are focused on predicting hospitalizations.

I have thrown a number of algorithms at the problem (random forests, logistic regression, CART, Cox regressions) and it''s been an amazing learning experience.  I have not been able to decide on what "works" or "doesn''t work," if you know what I mean.  I have enough knowledge and skills to be misled by my own excitement and naivete; what I need is to be able to get excited about something real.

So here are my questions: What methods do you think work well for problems like this? And, what resources would be most useful for learning about data science applications and methods relevant to healthcare and clinical medicine?




  [1]: https://www.heritagehealthprize.com/c/hhp/details/milestone-winners', 2781, '2014-07-30 11:45:08.313', '37113054-0551-40f3-a23e-0a567b54ba25', 866, 2240, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Predicting next medical condition from past conditions in claims data', 2781, '2014-07-30 11:45:08.313', '37113054-0551-40f3-a23e-0a567b54ba25', 866, 2241, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><beginner>', 2781, '2014-07-30 11:45:08.313', '37113054-0551-40f3-a23e-0a567b54ba25', 866, 2242, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('does anyone know what (from your experience) is the best open source natural language generators out there? What are the relative merits of each? I''m looking to do sophisticated text summarization and would like to use theme extraction/semantic modeling in conjunction with NLG tools to create accurate, context-aware, and natural-sounding text summaries.', 2785, '2014-07-30 13:55:36.187', '9f3fcd05-b307-4421-a81f-cfe684743e0c', 867, 2243, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Relative merits of different open source natural language generators', 2785, '2014-07-30 13:55:36.187', '9f3fcd05-b307-4421-a81f-cfe684743e0c', 867, 2244, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><text-mining>', 2785, '2014-07-30 13:55:36.187', '9f3fcd05-b307-4421-a81f-cfe684743e0c', 867, 2245, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am, admittedly, very new to data science. I have spent the last 8 months or so learning as much as I can about the field and its methods.  I am having issues choosing which methods to apply.

I am currently working with a large set of health insurance claims data that includes some laboratory and pharmacy claims. The most consistent information in the data set, however, is made up of diagnosis (ICD-9CM) and procedure codes (CPT, HCSPCS, ICD-9CM).

My goals are to:

 1. Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease;
 2. Identify the likelihood (or probability) that a patient will develop a medical condition based on the conditions they have had in the past;
 3. Do the same as 1 and 2, but with procedures and/or diagnoses.
 4. Preferably, the results would be interpretable by a doctor

I have looked at things like the [Heritage Health Prize Milestone papers][1] and have learned a lot from them, but they are focused on predicting hospitalizations.

I have thrown a number of algorithms at the problem (random forests, logistic regression, CART, Cox regressions) and it''s been an amazing learning experience.  I have not been able to decide on what "works" or "doesn''t work," if you know what I mean.  I have enough knowledge and skills to be misled by my own excitement and naivete; what I need is to be able to get excited about something real.

So here are my questions: What methods do you think work well for problems like this? And, what resources would be most useful for learning about data science applications and methods relevant to healthcare and clinical medicine?

EDIT to add sample data frame:

    structure(list(gender = structure(c(1L, 2L, 2L, 2L, 1L, 2L), .Label = c("Male",         "Female"), class = "factor"), patient_age = c(31, 29, 31, 53,
    47, 48), anx.any = c(1, 1, 0, 1, 0, 0), art.any = c(0, 0, 1,
    1, 1, 1), ast.any = c(1, 1, 1, 1, 0, 1), bpa.any = c(1, 1, 1,
    1, 0, 1), can.any = c(0, 0, 0, 1, 0, 1), cer.any = c(0, 0, 0,
    0, 0, 0), chf.any = c(0, 0, 0, 0, 0, 0), ckd.any = c(0, 0, 0,
    0, 0, 0), dep.any = c(1, 1, 0, 1, 0, 0), dia.any = c(0, 0, 1,
    0, 0, 0), end.any = c(1, 1, 0, 1, 0, 1), flu.any = c(1, 0, 0,
    0, 0, 0), hrt.any = c(1, 0, 0, 1, 0, 1), hyp.any = c(1, 0, 0,
    0, 1, 0), inf.any = c(0, 0, 0, 1, 0, 1), men.any = c(1, 0, 1,
    0, 1, 0), ren.any = c(0, 0, 0, 0, 0, 0), sdp.any = c(0, 0, 0,
    0, 0, 0), skn.any = c(1, 0, 0, 1, 0, 1), tra.any = c(1, 1, 0,
    0, 1, 0), anx.isbefore.ckd = c(0, 0, 0, 0, 0, 0), art.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), ast.isbefore.ckd = c(0, 0, 0, 0, 0, 0), bpa.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), can.isbefore.ckd = c(0, 0, 0, 0, 0, 0), cer.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), chf.isbefore.ckd = c(0, 0, 0, 0, 0, 0), ckd.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), dep.isbefore.ckd = c(0, 0, 0, 0, 0, 0), dia.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), end.isbefore.ckd = c(0, 0, 0, 0, 0, 0), flu.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), hrt.isbefore.ckd = c(0, 0, 0, 0, 0, 0), hyp.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), inf.isbefore.ckd = c(0, 0, 0, 0, 0, 0), men.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), ren.isbefore.ckd = c(0, 0, 0, 0, 0, 0), sdp.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), skn.isbefore.ckd = c(0, 0, 0, 0, 0, 0), tra.isbefore.ckd = c(0,
    0, 0, 0, 0, 0)), .Names = c("gender", "patient_age", "anx.any",
    "art.any", "ast.any", "bpa.any", "can.any", "cer.any", "chf.any",
    "ckd.any", "dep.any", "dia.any", "end.any", "flu.any", "hrt.any",
    "hyp.any", "inf.any", "men.any", "ren.any", "sdp.any", "skn.any",
    "tra.any", "anx.isbefore.ckd", "art.isbefore.ckd", "ast.isbefore.ckd",
    "bpa.isbefore.ckd", "can.isbefore.ckd", "cer.isbefore.ckd", "chf.isbefore.ckd",
    "ckd.isbefore.ckd", "dep.isbefore.ckd", "dia.isbefore.ckd", "end.isbefore.ckd",
    "flu.isbefore.ckd", "hrt.isbefore.ckd", "hyp.isbefore.ckd", "inf.isbefore.ckd",
    "men.isbefore.ckd", "ren.isbefore.ckd", "sdp.isbefore.ckd", "skn.isbefore.ckd",
    "tra.isbefore.ckd"), row.names = c(NA, 6L), class = "data.frame")


  [1]: https://www.heritagehealthprize.com/c/hhp/details/milestone-winners', 2781, '2014-07-30 13:58:31.410', '1c185cb4-792d-467f-b1dd-529465e1d973', 866, 'Added an example data frame', 2246, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is a paper you should look into:

[MapReduce: Distributed Computing for Machine Learning](http://cs.smith.edu/dftwiki/images/6/68/MapReduceDistributedComputingMachineLearning.pdf)

They distinguish 3 classes of machine-learning problems that are reasonable to address with MapReduce:

1. Single pass algorithms
2. Iterative algorithms
3. Query based algorithms

They also give examples for each class.

', 2787, '2014-07-30 15:32:44.557', 'd9a3b14d-5711-4ba5-af39-51dfaf5094fe', 868, 2247, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><r><beginner>', 84, '2014-07-30 16:26:27.757', '9283bf84-f6a4-4935-b3af-61fdab0519c7', 866, 'Adding programming language.', 2248, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('So, I''m just starting to learn how a neural network can operate to recognize patterns and categorize inputs, and I''ve seen how an artificial neural network can parse image data and categorize the images ([demo with convnetjs](http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html)), and the key there is to downsample the image and each pixel stimulates one input neuron into the network.

However, I''m trying to wrap my head around if this is possible to be done with string inputs? The use-case I''ve got is a "recommendation engine" for movies a user has watched. Movies have lots of string data (title, plot, tags), and I could imagine "downsampling" the text down to a few key words that describe that movie, but even if I parse out the top five words that describe this movie, I think I''d need input neurons for every english word in order to compare a set of movies? I could limit the input neurons just to the words used in the set, but then could it grow/learn by adding new movies (user watches a new movie, with new words)? Most of the libraries I''ve seen don''t allow adding new neurons after the system has been trained?

Is there a standard way to map string/word/character data to inputs into a neural network? Or is a neural network really not the right tool for the job of parsing string data like this (what''s a better tool for pattern-matching in string data)?', 2790, '2014-07-30 16:27:45.177', 'd5053bbf-bce7-43e0-b111-128ffd833263', 869, 2249, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Neural Network parse string data?', 2790, '2014-07-30 16:27:45.177', 'd5053bbf-bce7-43e0-b111-128ffd833263', 869, 2250, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork>', 2790, '2014-07-30 16:27:45.177', 'd5053bbf-bce7-43e0-b111-128ffd833263', 869, 2251, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is not a problem about neural networks per se, but about representing textual data in machine learning. You can represent the movies, cast, and theme as categorical variables. The plot is more complicated; you''d probably want a [topic model](http://en.wikipedia.org/wiki/Topic_model) for that, but I''d leave that out until you get the hang of things. It does precisely that textual "downsampling" you mentioned.

Take a look at [this](http://visualstudiomagazine.com/articles/2013/07/01/neural-network-data-normalization-and-encoding.aspx) tutorial to learn how to encode categorical variables for neural networks. And good luck!', 381, '2014-07-30 17:42:09.797', '875de795-b315-44eb-941c-ccaaacfbbcc4', 870, 2252, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using a neural network for prediction on natural language data can be a tricky task, but there are tried and true methods for making it possible.

In the Natural Language Processing (NLP) field, text is often represented using the bag of words model.  In other words, you have a vector of length n, where n is the number of words in your vocabulary, and each word corresponds to an element in the vector.  In order to convert text to numeric data, you simply count the number of occurrences of each word and place that value at the index of the vector that corresponds to the word. [Wikipedia does an excellent job of describing this conversion process.](http://en.wikipedia.org/wiki/Bag-of-words_model.)  Because the length of the vector is fixed, its difficult to deal with new words that don''t map to an index, but there are ways to help mitigate this problem (lookup [feature hashing](http://en.wikipedia.org/wiki/Feature_hashing)).

This method of representation has many disadvantages -- it does not preserve the relationship between adjacent words, and results in very sparse vectors.  Looking at [n-grams](http://en.wikipedia.org/wiki/N-gram) helps to fix the problem of preserving word relationships, but for now let''s focus on the second problem: sparsity.

It''s difficult to deal directly with these sparse vectors (many linear algebra libraries do a poor job of handling sparse inputs), so often the next step is dimensionality reduction. For that we can refer to the field of [topic modeling](http://en.wikipedia.org/wiki/Topic_model):  Techniques like [Latent Dirichlet Allocation](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA) and [Latent Semantic Analysis](http://en.wikipedia.org/wiki/Latent_semantic_analysis) (LSA) allow the compression of these sparse vectors into dense vectors by representing a document as a combination of topics.  You can fix the number of topics used, and in doing so fix the size of the output vector producted by LDA or LSA. This dimensionality reduction process drastically reduces the size of the input vector while attempting to lose a minimal amount of information.

Finally, after all of these conversions, you can feed the outputs of the topic modeling process into the inputs of your neural network.     ', 684, '2014-07-30 17:53:40.330', '4f4ee11e-6a94-455d-a6d0-b5c5133d4c99', 871, 2253, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am attempting to use the tm package to convert a vector of text strings to a corpus element.

My code looks something like this

Corpus(d1$Yes)

where d1$Yes is a factor with 124 levels, each containing a text string.

For example, d1$Yes[246] = "So we can get the boat out!"

I''m receiving the following error:
"Error: inherits(x, "Source") is not TRUE"

I''m not sure how to remedy this.', 2792, '2014-07-30 18:45:13.790', '047e6332-4cc5-4e05-8192-e6a021e10d3c', 872, 2254, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R error using package tm (text-mining)', 2792, '2014-07-30 18:45:13.790', '047e6332-4cc5-4e05-8192-e6a021e10d3c', 872, 2255, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><text-mining>', 2792, '2014-07-30 18:45:13.790', '047e6332-4cc5-4e05-8192-e6a021e10d3c', 872, 2256, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You have to tell Corpus what kind of source you are using.  Try:

    Corpus(VectorSource(d1$Yes))', 375, '2014-07-30 19:15:09.383', '3c08a959-3e9d-4976-8fd1-40cf203b1d15', 873, 2257, '2');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('user1361', 'I have some thoughts about your question. I hope it may help you to solve your problem.

> I''m planning to run some experiments with very large data sets, and I''d like to distribute the computation.

In [one of my posts][1], I have done research on topic of _evaluation methods of Data Science_. With __Learning Curve__, you can evaluate your experiments _learning ability_. To talk a bit more, you will __fix__ commodity configuration, and then will run same experiment on the same number of machines with different size of data set you have (from starting from small chunk in size, incrementally increase the size until you reach the whole data set).

To point on, you should avoid having _power distribution_ for result of performance test being run with different size of data sets. To avoid, you should carefully choose _step size_ (step size = amount of increments).

> I have about ten machines available, each with 200 GB of free space on hard disk. However, I would like to perform experiments on a greater number of nodes, to measure scalability more precisely.

For this type question, I have intuitively searched and read materials; afterwards, published as a [blog post][2]. At the end of the post, I have briefly talked about how to test your hypothesis on real complex system. If you let me, I want to briefly talk about;

First of all, __base requirement__ should be formed in order to run data set as a whole. The minimum requirement will build your __baseline evaluation score__ which is calculated with one/combination of evaluation metrics you have chosen, or with one/combination of methods being used to calculate `Running Time = Computation complexity + Communication cost + Synchronization cost`.

After those steps, with an _evaluation strategy_, add new elements, e.g. new node, to the system you have doing scalability test; meanwhile, for each addition, measure performance w.r.t new system configuration.

Just to note, _evaluation strategy_ must be planned along with considerations of default behavior of parallel and distributed systems. For example, what I mean by behavior is that adding just more cores will, after some point, automatically drop performance of the system not due to your algorithm characteristics. It is because more cores need more RAMs, more hard driver, or etc. In other words, there is a __N-Way__ relationship between hardware components. As a second example, adding more nodes to the distributed system will punished you with more communication and synchronization costs.

As a last step, you will sketch two different graphs with your evaluation results via data analysis program or language (As a recommendation, use GNU Plot or R programming language). Print out and put those results at your desktop, and start to examine them, carefully. According to your investigation, modify/erase + rebuild _evaluation strategy_ and re-do the performance test.

> Are there commodity services which would grant me that only my application would be running at a given time? Has anyone used such services yet?

I have no much experiment on commodity services, but I can easily say whether it grants or not depends on your configuration of services. If you configured say Hadoop to your node as an only service, Hadoop will grant your code will be only running at any time.

[1]: http://www.datasciencecentral.com/profiles/blogs/meeting-justice-of-data-science
[2]: http://www.datasciencecentral.com/profiles/blogs/scale-your-vision', '2014-07-30 21:53:13.790', '7399861b-2041-4572-9443-0e45b03a6d27', 874, 2258, '2');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('user1361', 'I''m planning to run experiments with large datasets on distributed system in order to evaluate efficiency gains in comparison with previous proposals.

I have limited number of machines nearly ten machines having 200 GB of free space on hard disk on each. On the contrary, I wished to perform experiments on more than available nodes in order to measure scalability, __more precisely__. Since I don''t have any, I thought about using a commodity cluster. However, I''m not sure about the policies of usage, and I need to reliably measure execution times.

Are there commodity services which will grant me that only my application would be running at a given time?', '2014-07-30 22:36:59.310', 'e2b9f16b-1454-417e-83fd-f16c8fb07bb4', 423, 'edit title, and body of question', 2259, '5');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('user1361', 'How to measure execution time on distributed system', '2014-07-30 22:36:59.310', 'e2b9f16b-1454-417e-83fd-f16c8fb07bb4', 423, 'edit title, and body of question', 2260, '4');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('user1361', '<bigdata><scalability><distributed>', '2014-07-30 22:36:59.310', 'e2b9f16b-1454-417e-83fd-f16c8fb07bb4', 423, 'edit title, and body of question', 2261, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-07-30 22:36:59.310', 'e2b9f16b-1454-417e-83fd-f16c8fb07bb4', 423, 'Proposed by 1361 approved by 84 edit id of 129', 2262, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I suggest you use [ANTLR][1], which is a very powerful parser generator. It has a good GUI for entering your BNF. It has a [Python target][2] capability.


  [1]: http://www.antlr.org/
  [2]: https://theantlrguy.atlassian.net/wiki/display/ANTLR4/Python+Target', 609, '2014-07-30 23:40:54.700', '6f5d71f9-49dd-43c3-9845-4d4760249b8a', 875, 2263, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am, admittedly, very new to data science. I have spent the last 8 months or so learning as much as I can about the field and its methods.  I am having issues choosing which methods to apply.

I am currently working with a large set of health insurance claims data that includes some laboratory and pharmacy claims. The most consistent information in the data set, however, is made up of diagnosis (ICD-9CM) and procedure codes (CPT, HCSPCS, ICD-9CM).

My goals are to:

 1. Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease;
 2. Identify the likelihood (or probability) that a patient will develop a medical condition based on the conditions they have had in the past;
 3. Do the same as 1 and 2, but with procedures and/or diagnoses.
 4. Preferably, the results would be interpretable by a doctor

I have looked at things like the [Heritage Health Prize Milestone papers][1] and have learned a lot from them, but they are focused on predicting hospitalizations.

I have thrown a number of algorithms at the problem (random forests, logistic regression, CART, Cox regressions) and it''s been an amazing learning experience.  I have not been able to decide on what "works" or "doesn''t work," if you know what I mean.  I have enough knowledge and skills to be misled by my own excitement and naivete; what I need is to be able to get excited about something real.

So here are my questions: What methods do you think work well for problems like this? And, what resources would be most useful for learning about data science applications and methods relevant to healthcare and clinical medicine?

EDIT #2 to add plaintext table:

CKD is the target condition, "chronic kidney disease", ".any" denotes that they have acquired that condition at any time, ".isbefore.ckd" means they had that condition before their frist diagnosis of CKD.  The other abbreviations correspond with other conditions identified by ICD-9CM code groupings.  This grouping occurs in SQL during the import process. Each variable, with the exception of patient_age, is binary.

      gender patient_age anx.any art.any ast.any bpa.any can.any cer.any chf.any ckd.any dep.any dia.any end.any flu.any hrt.any hyp.any inf.any men.any ren.any sdp.any
    1   Male          31       1       0       1       1       0       0       0       0       1       0       1       1       1       1       0       1       0       0
    2 Female          29       1       0       1       1       0       0       0       0       1       0       1       0       0       0       0       0       0       0
    3 Female          31       0       1       1       1       0       0       0       0       0       1       0       0       0       0       0       1       0       0
    4 Female          53       1       1       1       1       1       0       0       0       1       0       1       0       1       0       1       0       0       0
    5   Male          47       0       1       0       0       0       0       0       0       0       0       0       0       0       1       0       1       0       0
    6 Female          48       0       1       1       1       1       0       0       0       0       0       1       0       1       0       1       0       0       0
      skn.any tra.any anx.isbefore.ckd art.isbefore.ckd ast.isbefore.ckd bpa.isbefore.ckd can.isbefore.ckd cer.isbefore.ckd chf.isbefore.ckd ckd.isbefore.ckd
    1       1       1                0                0                0                0                0                0                0                0
    2       0       1                0                0                0                0                0                0                0                0
    3       0       0                0                0                0                0                0                0                0                0
    4       1       0                0                0                0                0                0                0                0                0
    5       0       1                0                0                0                0                0                0                0                0
    6       1       0                0                0                0                0                0                0                0                0
      dep.isbefore.ckd dia.isbefore.ckd end.isbefore.ckd flu.isbefore.ckd hrt.isbefore.ckd hyp.isbefore.ckd inf.isbefore.ckd men.isbefore.ckd ren.isbefore.ckd
    1                0                0                0                0                0                0                0                0                0
    2                0                0                0                0                0                0                0                0                0
    3                0                0                0                0                0                0                0                0                0
    4                0                0                0                0                0                0                0                0                0
    5                0                0                0                0                0                0                0                0                0
    6                0                0                0                0                0                0                0                0                0
      sdp.isbefore.ckd skn.isbefore.ckd tra.isbefore.ckd
    1                0                0                0
    2                0                0                0
    3                0                0                0
    4                0                0                0
    5                0                0                0
    6                0                0                0

EDIT to add sample data frame:

    structure(list(gender = structure(c(1L, 2L, 2L, 2L, 1L, 2L), .Label = c("Male",         "Female"), class = "factor"), patient_age = c(31, 29, 31, 53,
    47, 48), anx.any = c(1, 1, 0, 1, 0, 0), art.any = c(0, 0, 1,
    1, 1, 1), ast.any = c(1, 1, 1, 1, 0, 1), bpa.any = c(1, 1, 1,
    1, 0, 1), can.any = c(0, 0, 0, 1, 0, 1), cer.any = c(0, 0, 0,
    0, 0, 0), chf.any = c(0, 0, 0, 0, 0, 0), ckd.any = c(0, 0, 0,
    0, 0, 0), dep.any = c(1, 1, 0, 1, 0, 0), dia.any = c(0, 0, 1,
    0, 0, 0), end.any = c(1, 1, 0, 1, 0, 1), flu.any = c(1, 0, 0,
    0, 0, 0), hrt.any = c(1, 0, 0, 1, 0, 1), hyp.any = c(1, 0, 0,
    0, 1, 0), inf.any = c(0, 0, 0, 1, 0, 1), men.any = c(1, 0, 1,
    0, 1, 0), ren.any = c(0, 0, 0, 0, 0, 0), sdp.any = c(0, 0, 0,
    0, 0, 0), skn.any = c(1, 0, 0, 1, 0, 1), tra.any = c(1, 1, 0,
    0, 1, 0), anx.isbefore.ckd = c(0, 0, 0, 0, 0, 0), art.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), ast.isbefore.ckd = c(0, 0, 0, 0, 0, 0), bpa.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), can.isbefore.ckd = c(0, 0, 0, 0, 0, 0), cer.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), chf.isbefore.ckd = c(0, 0, 0, 0, 0, 0), ckd.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), dep.isbefore.ckd = c(0, 0, 0, 0, 0, 0), dia.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), end.isbefore.ckd = c(0, 0, 0, 0, 0, 0), flu.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), hrt.isbefore.ckd = c(0, 0, 0, 0, 0, 0), hyp.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), inf.isbefore.ckd = c(0, 0, 0, 0, 0, 0), men.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), ren.isbefore.ckd = c(0, 0, 0, 0, 0, 0), sdp.isbefore.ckd = c(0,
    0, 0, 0, 0, 0), skn.isbefore.ckd = c(0, 0, 0, 0, 0, 0), tra.isbefore.ckd = c(0,
    0, 0, 0, 0, 0)), .Names = c("gender", "patient_age", "anx.any",
    "art.any", "ast.any", "bpa.any", "can.any", "cer.any", "chf.any",
    "ckd.any", "dep.any", "dia.any", "end.any", "flu.any", "hrt.any",
    "hyp.any", "inf.any", "men.any", "ren.any", "sdp.any", "skn.any",
    "tra.any", "anx.isbefore.ckd", "art.isbefore.ckd", "ast.isbefore.ckd",
    "bpa.isbefore.ckd", "can.isbefore.ckd", "cer.isbefore.ckd", "chf.isbefore.ckd",
    "ckd.isbefore.ckd", "dep.isbefore.ckd", "dia.isbefore.ckd", "end.isbefore.ckd",
    "flu.isbefore.ckd", "hrt.isbefore.ckd", "hyp.isbefore.ckd", "inf.isbefore.ckd",
    "men.isbefore.ckd", "ren.isbefore.ckd", "sdp.isbefore.ckd", "skn.isbefore.ckd",
    "tra.isbefore.ckd"), row.names = c(NA, 6L), class = "data.frame")


  [1]: https://www.heritagehealthprize.com/c/hhp/details/milestone-winners', 2781, '2014-07-31 12:17:27.037', 'f923efc7-d2f3-47fa-a465-09b117f60976', 866, 'Added a plaintext version of the table and an explanation for the codes.', 2267, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('"Identify the most influential precursor conditions (comorbidities) for a medical condition like chronic kidney disease"

I''m not sure that it''s possible to ID _the_ most influential conditions; I think it will depend on what model you''re using. Just yesterday I fit a random forest and a boosted regression tree to the same data, and the order and relative importance each model gave for the variables were quite different.', 1241, '2014-07-31 16:39:37.127', 'f77446ce-9026-40c3-8b46-20407da114e6', 876, 2268, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Well this looks like the most suited place for this question.

Every website collect data of the user, some just for usability and personalization, but the majority like social networks track every move on the web, some free apps on your phone scan text messages, call history and so on.

All this data siphoning is just for selling your profile for advertisers?
', 2798, '2014-07-31 18:52:56.307', '68f94e47-fd90-469e-b009-2ede54899dbf', 877, 2269, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the use of user data collection besides serving ads?', 2798, '2014-07-31 18:52:56.307', '68f94e47-fd90-469e-b009-2ede54899dbf', 877, 2270, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 2798, '2014-07-31 18:52:56.307', '68f94e47-fd90-469e-b009-2ede54899dbf', 877, 2271, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A couple of days ago developers from one product company asked me how they can understand why new users were leaving their website. My first question to them was what these users'' profiles looked like and how they were different from those who stayed.

Advertising is only top of an iceberg. User profiles (either filled by users themselves or computed from users'' behaviour) hold information about:

 - **user categories**, i.e. what kind of people tend to use your website/product
 - **paying client portraits**, i.e. who is more likely to use your paid services
 - **UX component performance**, e.g. how long it takes people to find the button they need
 - **action performance comparison**, e.g. what was more efficient - lower price for a weekend or propose gifts with each buy, etc.

So it''s more about improving product and making better user experience rather than selling this data to advertisers. ', 1279, '2014-07-31 23:03:33.767', '8ecc6c3d-c305-4cba-9d1b-ee041e18bf52', 878, 2272, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Most companies won''t sell the data, not on any small scale anyways.  Most will use it internally.

User tracking data is important for understanding a lot of things.  There''s basic A/B testing where you provide different experiences to see which is more effective.  There is understanding how your UI is utilized.  Categorizing your end users in different ways for a variety of reasons.  Figuring out where your end user base is, and within that group where the end users that matter are.  Correlating user experiences with social network updates.  Figuring out what will draw people to your product and what drives them away.  The list of potential for data mining and analysis projects could go on for days.

Data storage is cheap.  If you track everything out of the gate, you can figure out what you want to do with that data later.

Scanning text messages is sketchy territory when there isn''t a good reason for it.  Even when there is a good reason it''s sketchy territory.  I''d love to say that nobody does it, but there have been instances where big companies have done it and there are a lot of cases where no-name apps at least require access to that kind of data for installation.  I generally frown on that kind of thing myself as a consumer, but the data analyst in me would love to see if I could build anything useful from a set of information like that.', 434, '2014-08-01 06:03:47.700', 'c5ca8056-c8f4-4c19-bb9d-e3734de518c9', 879, 2273, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There always is the solution to try both approaches and keep the one that maximizes the expected performances.

In your case, I would assume you prefer minimizing false negatives at the cost of some false positive, so you want to bias your classifier against the strong negative prior, and address the imbalance by reducing the number of negative examples in your training set.

Then compute the precision/recall, or sensitivity/specificity, or whatever criterion suits you on the full, imbalanced, dataset to make sure you haven''t ignored a significant pattern present in the real data while building the model on the reduced data.', 172, '2014-08-01 12:27:41.580', 'efc6a71f-ea1f-4bcb-92ef-46aa8a1b4aad', 880, 2274, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve never worked with medical data, but from general reasoning I''d say that relations between variables in healthcare are pretty complicated. Different models, such as random forests, regression, etc. could capture only part of relations and ignore others. In such circumstances it makes sense to use general **statistical exploration** and **modelling**.

For example, the very first thing I would do is finding out **correlations** between possible precursor conditions and diagnoses. E.g. in what percent of cases chronic kidney disease was preceded by long flu? If it is high, it [doesn''t always mean causality](http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation), but gives pretty good food for thought and helps to better understand relations between different conditions.

Another important step is data visualisation. Does CKD happens in males more often than in females? What about their place of residence? What is distribution of CKD cases by age? It''s hard to grasp large dataset as a set of numbers, plotting them out makes it much easier.

When you have an idea of what''s going on, perform [**hypothesis testing**](http://en.wikipedia.org/wiki/Statistical_hypothesis_testing) to check your assumption. If you reject null hypothesis (basic assumption) in favour of alternative one, congratulations, you''ve made "something real".

Finally, when you have a good understanding of your data, try to create complete **model**. It may be something general like [PGM](https://www.coursera.org/course/pgm) (e.g. manually-crafted Bayesian network), or something more specific like linear regression or [SVM](http://en.wikipedia.org/wiki/Support_vector_machine), or anything. But in any way you will already know how this model corresponds to your data and how you can measure its efficiency.

----

As a good starting resource for learning statistical approach I would recommend [Intro to Statistics](https://www.udacity.com/course/st101) course by Sebastian Thrun. While it''s pretty basic and doesn''t include advanced topics, it describes most important concepts and gives systematic understanding of probability theory and statistics. ', 1279, '2014-08-01 14:08:13.043', 'e628b95d-e967-4d3f-a67d-a217f7b5c9d6', 881, 2275, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here''s a practical example of using web data for something other than advertising. Distil Networks (disclaimer, I work there) uses network traffic to determine whether page accesses are from humans or bots - scrapers, click fraud, form spam, etc.

Another example is some of the work that Webtrends is doing. They allow site users to build a model for each visitor to predict whether they''ll leave, buy, add to cart, etc. Then based on the probability of each action you can change the users experience (e.g. if they''re about to leave, give them a coupon). Here''s the slides from a talk by them: http://www.oscon.com/oscon2014/public/schedule/detail/34809', 403, '2014-08-01 14:10:38.267', 'adf0fffe-fa80-4517-b3be-6eb35c972ab0', 882, 2276, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am working on a data set that has multiple traffic speed measurements per day. My data is from the city of chicago, and it is taken every minute for about six months. I wanted to consolidate this data into days only, so this is what I did:

    traffic <- read.csv("path.csv",header=TRUE)
    traffic2 <- aggregate(SPEED~DATE, data=traffic, FUN=MEAN)

this was perfect because it took all of my data and averaged it by date. For example, my original data looked something like this:

    DATE        SPEED
    12/31/2012   22
    12/31/2012   25
    12/31/2012   23
    ...

and the final looked like this:

    DATE        SPEED
    10/1/2012    22
    10/2/2012    23
    10/3/2012    22
    ...

The only problem, is my data is supposed to start at 9/1/2012. I plotted this data, and it turns out the data goes from 10/1/2012-12/31/2012 and then 9/1/2012-9/30/2012.

What in the world is going on here?', 2614, '2014-08-01 18:13:06.063', 'ba66fbea-968d-49f9-bbd7-5fb798ce357b', 883, 2277, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R aggregate() with dates', 2614, '2014-08-01 18:13:06.063', 'ba66fbea-968d-49f9-bbd7-5fb798ce357b', 883, 2278, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><dataset><beginner>', 2614, '2014-08-01 18:13:06.063', 'ba66fbea-968d-49f9-bbd7-5fb798ce357b', 883, 2279, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('So I was never able to find the error by looking through my logs. I ended up reinstalling it with CDH5 (which was MUCH easier than installing "poor" Hadoop)
Now everything runs fine!

I''m still having trouble getting things to save to the hdfs, but thats a question for another day... ', 2614, '2014-08-01 18:16:57.543', 'e706efd6-d705-4f21-a9b1-9b99ad70c4a3', 884, 2280, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am going to agree with @user1683454''s comment. After importing, your DATE column is of either `character`, or `factor` class (depending on your settings for `stringsAsFactors`). Therefore, I think that you can solve this issue in at least several ways, as follows:

1) **Convert data** to correct type **during import**. To do this, just use the following options of `read.csv()`: `stringsAsFactors` (or `as.is`) and `colClasses`. By default, you can specify conversion to `Date` or `POSIXct` classes. If you need a non-standard format, you have two options. First, if you have a single Date column, you can use `as.Date.character()` to pass the desired format to `colClasses`. Second, if you have multiple Date columns, you can write a function for that and pass it to `colClasses` via `setAs()`. Both options are discussed here: http://stackoverflow.com/questions/13022299/specify-date-format-for-colclasses-argument-in-read-table-read-csv.

2) **Convert data** to correct format **after import**. Thus, after calling `read.csv()`, you would have to execute the following code: `dateColumn <- as.Date(dateColumn, "%m/%d/%Y")` or `dateColumn <- strptime(dateColumn, "%m/%d/%Y")` (adjust the format to whatever Date format you need).', 2452, '2014-08-01 23:21:34.360', 'ae203d9d-752b-4657-af1d-ccce4946f464', 886, 2284, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am currently on a project that will build a model (train and test) on Client-side Web data, but evaluate this model on Sever-side Web data.  Unfortunately building the model on Server-side data is not an option, nor is it an option to evaluate this model on Client-side data.

This model will be based on metrics collected on specific visitors.  This is a real time system that will be calculating a likelihood based on metrics collected while visitors browse the website.

I am looking for approaches to ensure the highest possible accuracy on the model evaluation.

So far I have the following ideas,

 1. Clean the Server-side data by removing webpages that are never seen Client-side.
 2. Collect additional data Server-side data to make the Server-side data more closely resemble Client-side data.
 3. Collect data on the Client and send this data to the Server.  This is possible and may be the best solution, but is currently undesirable.
 4. Build one or more models that estimate Client-side Visitor metrics from Server-side Visitor metrics and use these estimates in the Likelihood model.

Any other thoughts on evaluating over one Population while training (and testing) on another Population?


', 776, '2014-08-02 00:07:09.267', '9f9262aa-1eee-4610-add4-ea7d03d9d0b0', 887, 2285, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Modelling on one Population and Evaluating on another Population', 776, '2014-08-02 00:07:09.267', '9f9262aa-1eee-4610-add4-ea7d03d9d0b0', 887, 2286, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-cleaning><evaluation>', 776, '2014-08-02 00:07:09.267', '9f9262aa-1eee-4610-add4-ea7d03d9d0b0', 887, 2287, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m not an expert on this, so take my advice with a grain of salt. It''s not clear for me what is the relationship between server-side and client-side data. Are they both **representative** of the same population? If Yes, I think it''s OK to use different data sets for testing/training and evaluating your models. If No, I think it might be a good idea to use some **resampling** technique, such as *bootstrapping*, *jackknifing* or *cross-validation*.', 2452, '2014-08-02 04:21:49.927', '392cede8-56f6-4316-99a8-cb8fe593449d', 888, 2288, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If the users who you are getting client-side data from are from the same population of users who you would get server-side data from. If that is true, then you aren''t really training on one population and applying to another. The main difference is that the client side data happened in the past (by necessity unless you are constantly refitting your model) and the server side data will come in the future.

Let''s reformulate the question in terms of models rather than web clients and servers.

You are fitting a model on one dataset and applying it to another. That is the classic use of predictive modeling/machine learning. Models use features from the data to make estimates of some parameter or parameters. Once you have a fitted (and tested) model, all that you need is the same set of features to feed into the model to get your estimates.

Just make sure to model on a set of features (aka variables) that are available on the client-side and server-side. If that isn''t possible, ask that question separately.', 2666, '2014-08-02 04:54:14.757', '9e5e2816-3d1a-409e-83e9-b1ec42ec31b6', 889, 2289, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('R contains some *standard* functions for data manipulation, which can be used for data cleaning, in its **base** package (`gsub`, `transform`, etc.), as well as in various third-party packages, such as **stringr**, **reshape**/**reshape2**, and **plyr**/**dplyr**. Examples and best practices of usage for these packages and their functions are described in the following paper: http://vita.had.co.nz/papers/tidy-data.pdf.

Additionally, R offers some packages specifically *focused* on data cleaning and transformation:

- **editrules** (http://cran.r-project.org/web/packages/editrules/index.html)
- **deducorrect** (http://cran.r-project.org/web/packages/deducorrect/index.html)
- **StatMatch** (http://cran.r-project.org/web/packages/StatMatch/index.html)
- **MatchIt** (http://cran.r-project.org/web/packages/MatchIt/index.html)
- **DataCombine** (http://cran.r-project.org/web/packages/DataCombine)
- **data.table** (http://cran.r-project.org/web/packages/data.table)

A comprehensive and coherent approach to **data cleaning** in R, including examples and use of **editrules** and **deducorrect** packages, as well as a description of *workflow* (*framework*) of data cleaning in R, is presented in the following paper, which I highly recommend: http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf.', 2452, '2014-08-02 08:16:08.640', '1c381e49-1437-4014-b392-6f671983956f', 722, 'Added info on ''dplyr'' and ''data.table'' packages.', 2290, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If your work is parallelizable enough for a distributed network of cpus to make a difference, why not try to run it on the gpu instead? That will require rather less investment than a large network of cpus with individual software licenses and still provide parallel processing which you can do runtime tracking on yourself.', 2809, '2014-08-02 14:55:37.347', '230f608e-4499-492a-a0b7-f505d7f27e01', 890, 2291, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As others have pointed out, these are not distance "metrics", because they do not satisfy the metric criteria. Say instead "distance measure".

Anyway, what are you measuring and why? That information will help us give a more useful answer for your situation.', 2809, '2014-08-02 15:02:42.437', '8cb72728-8d5a-41bc-827a-88915ef25486', 891, 2292, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":62,"DisplayName":"AsheeshR"}]}', 62, '2014-08-03 06:18:00.257', '31347440-6d70-4283-983a-b4380b1e1ab0', 739, '102', 2293, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":62,"DisplayName":"AsheeshR"}]}', 62, '2014-08-03 06:18:34.900', '3d0c2814-6676-41fd-97ee-2af85dd8ddef', 808, '102', 2294, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am building a regression model and I need to calculate the below to check for correlations

 1. Correlation between 2 Multi level categorical variables
 2. Correlation between a Multi level categorical variable and
    continuous variable
 3. VIF(variance inflation factor) for a Multi
    level categorical variables

I believe its wrong to use Pearson correlation coefficient for the above scenarios because Pearson only works for 2 continuous variables.

Please answer the below questions

 1. Which correlation coefficient works best for the above cases ?
 2. VIF calculation only works for continuous data so what is the
    alternative?
 3. What are the assumptions I need to check before I use the correlation coefficient you suggest?
 4. How to implement them in SAS & R?', 1151, '2014-08-03 13:07:24.143', 'b95f0f54-b303-4fe8-93fc-1d2fe7a1d34c', 893, 2298, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to get correlation between two categorical variable and a categorical variable and continuous variable?', 1151, '2014-08-03 13:07:24.143', 'b95f0f54-b303-4fe8-93fc-1d2fe7a1d34c', 893, 2299, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><statistics>', 1151, '2014-08-03 13:07:24.143', 'b95f0f54-b303-4fe8-93fc-1d2fe7a1d34c', 893, 2300, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I assume that each person on Facebook is represented as a node (of a Graph) in Facebook, and relationship/friendship between each person(node) is represented as an edge between the involved nodes.

Given that there are millions of people on Facebook, how is the Graph stored?', 2756, '2014-08-03 16:09:35.750', 'adad3fec-871f-4d39-a048-fe3f2d88751e', 895, 2302, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Facebook''s Huge Database', 2756, '2014-08-03 16:09:35.750', 'adad3fec-871f-4d39-a048-fe3f2d88751e', 895, 2303, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<graphs>', 2756, '2014-08-03 16:09:35.750', 'adad3fec-871f-4d39-a048-fe3f2d88751e', 895, 2304, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to implement the Brown Clustering Algorithm [(link to paper)](http://delivery.acm.org/10.1145/180000/176316/p467-brown.pdf?ip=197.78.151.216&id=176316&acc=OPEN&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&CFID=522814165&CFTOKEN=99738065&__acm__=1407081874_5a9649adaf4a9d43b8faa5c1a2da47f8)

The algorithm is supposed to in O(|V|k^2) where |V| is the size of the vocabulary and k is the number of clusters. I am unable to implement it this efficiently. In fact, the best I can manage is O(|V|k^3) which is too slow. My current implementation for the main part of the algorithm is as follows:

    for w = number of clusters + 1 to |V|
    {
       word = next most frequent word in the corpus

       assign word to a new cluster

       initialize MaxQuality to 0

       initialize ArgMax vector to (0,0)

       for i = 0 to number of clusters - 1
       {
          for j = i to number of clusters
          {
             Quality = Mutual Information if we merge cluster i and cluster j

             if Quality > MaxQuality
             {
                MaxQuality = Quality

                ArgMax = (i,j)
             }
          }
       }
    }

I compute quality as follows:

    1. Before entering the second loop compute the pre-merge quality i.e. quality before doing any merges.
    2. Every time a cluster-pair merge step is considered:
        i. assign quality := pre-merge quality
       ii. quality = quality - any terms in the mutual information equation that contain cluster i or cluster j (pre-merge)
      iii. quality = quality + any terms in the mutual information equation that contain (cluster i U cluster j)  (post-merge)


In my implementation, the first loop has approx |V| iterations, the second and third loop approx k iterations each. To compute quality at each step requires approx a further k iterations. In total it runs in (|V|k^3) time.

How do you get it to run in (|V|k^2)?
', 2817, '2014-08-03 16:38:38.853', 'cc403f7e-d9fe-4f2d-aac9-7b1a49c2c8e4', 896, 2305, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to implement Brown Clustering Algorithm in O(|V|k^2)', 2817, '2014-08-03 16:38:38.853', 'cc403f7e-d9fe-4f2d-aac9-7b1a49c2c8e4', 896, 2306, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><efficiency><clustering>', 2817, '2014-08-03 16:38:38.853', 'cc403f7e-d9fe-4f2d-aac9-7b1a49c2c8e4', 896, 2307, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Having worked with Facebook data a bit (harvested from Facebook users) we stored it just as a pair of values: USER_ID, FRIEND_USER_ID.

But I guess your questions is a bit deeper? You can store it in different ways, depending on your research question. One interesting option is triads for example - http://mypersonality.org/wiki/doku.php?id=list_of_variables_available#triads', 587, '2014-08-03 20:21:08.150', '518edbd3-ab0e-49da-80d7-1a518244aced', 897, 2308, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('## Two Categorical Variables

Checking if two categorical variables are independent can be done with Chi-Squared test of independence.

This is a typical [Chi-Square test][1]: if we assume that two variables are independent, then the values of the contingency table for these variables should be distributed uniformly. And then we check how far away from uniform the actual values are


### Example

Suppose we have two variables

 - gender: male and female
 - city: Blois and Tours

We observed the following data:

![observed values][2]

Are gender and city independent? Let''s perform a Chi-Squred test. Null hypothesis: they are independent, Alternative hypothesis is that they are correlated in some way.

Under the Null hypothesis, we assume uniform distribution. So our expected values are the following

![expected value][3]

So we run the chi-squared test and the resulting p-value here can be seen as a measure of correlation between these two variables.


### R

    tbl = matrix(data=c(55, 45, 20, 30), nrow=2, ncol=2, byrow=T)
    dimnames(tbl) = list(City=c(''B'', ''T''), Gender=c(''M'', ''F''))

    chi2 = chisq.test(tbl, correct=F)
    c(chi2$statistic, chi2$p.value)

Here the p value is 0.08 - quite small, but still not enough to reject the hypothesis of independence. So we can say that the "correlation" here is 0.08


Consider another dataset

        Gender
    City  M  F
       B 51 49
       T 24 26

For this, it would give the following


    tbl = matrix(data=c(51, 49, 24, 26), nrow=2, ncol=2, byrow=T)
    dimnames(tbl) = list(City=c(''B'', ''T''), Gender=c(''M'', ''F''))

    chi2 = chisq.test(tbl, correct=F)
    c(chi2$statistic, chi2$p.value)

The p-value is 0.72 which is far closer to 1.



## Categorical vs Numerical Variables

For this type we typically perform [One-way ANOVA test][4]: we calculate in-group variance and intra-group variance and then compare them.



### Example

We want to study the relationship between absorbed fat from donuts vs the type of fat used to produce donuts (example is taken from [here][5])

![donuts][6]

Is there any dependence between the variables?
For that we conduct ANOVA test and see that the p-value is just 0.007 - there''s no correlation between these variables.


### R

    t1 = c(164, 172, 168, 177, 156, 195)
    t2 = c(178, 191, 197, 182, 185, 177)
    t3 = c(175, 193, 178, 171, 163, 176)
    t4 = c(155, 166, 149, 164, 170, 168)

    val = c(t1, t2, t3, t4)
    fac = gl(n=4, k=6, labels=c(''type1'', ''type2'', ''type3'', ''type4''))

    aov1 = aov(val ~ fac)
    summary(aov1)

Output is


                Df Sum Sq Mean Sq F value  Pr(>F)
    fac          3   1636   545.5   5.406 0.00688 **
    Residuals   20   2018   100.9
    ---
    Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1


So we can take the p-value as the measure of correlation here as well.

## References

 - https://en.wikipedia.org/wiki/Chi-square_test
 - http://0agr.ru/wiki/index.php/Chi-square_Test_of_Independence
 - http://courses.statistics.com/software/R/R1way.htm
 - http://0agr.ru/wiki/index.php/One-Way_ANOVA_F-Test


  [1]: https://en.wikipedia.org/wiki/Chi-square_test
  [2]: http://i.stack.imgur.com/zcCfV.png
  [3]: http://i.stack.imgur.com/H8bKJ.png
  [4]: http://en.wikipedia.org/wiki/F_test#One-way_ANOVA_example
  [5]: http://courses.statistics.com/software/R/R1way.htm
  [6]: http://i.stack.imgur.com/LMOS3.png', 816, '2014-08-04 09:42:03.590', 'c3428f00-3669-4d12-ba83-f3b90b04d8e0', 898, 2311, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When I worked with social network data, we stoted the "friendship" relation in a database in the table `Friends(friend_a, friend_b, ...)` with a B-Tree index on `(friend_a, friend_b)` plus also some partitioning.

In our case it was a little bit different since the graph was directed, so it wasn''t really "friendship", but rather "following/follower" relationship. But for friendship I would just store two edges: both `(friend_a, friend_b)` and `(friend_b, friend_a)`

We used MySQL to store the data, if it matters, but I guess it shouldn''t.', 816, '2014-08-04 09:48:09.660', '82e6d756-4ab7-467c-b4b9-cfced49bc363', 899, 2312, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think hierarchical clustering would be more time efficient in your case (with a single dimension).
Depending on your task, you may implement something like this:

Having N datapoints d<sub>i</sub> with their 1-dimension value x<sub>i</sub>:

1. Sort datapoints based on their x<sub>i</sub> value.
2. Calculate distances between adjacent datapoints (N-1 distances). Each distance must be assigned a pair of original datapoints (d<sub>i</sub>, d<sub>j</sub>).
3. Sort distances in descending order to generate list of datapoint pairs (d<sub>i</sub>, d<sub>j</sub>), starting from the closest one.
4. Iteratively unite datapoints (d<sub>i</sub>, d<sub>j</sub>) into clusters, starting from beginning of the list (the closest pair). (Depending on current state of d<sub>i</sub> and d<sub>j</sub>, uniting them means: (a) creating new cluster for two unclustered datapoints, (b) adding a datapoint to existing cluster and (c) uniting two clusters.)
5. Stop uniting, if the distance is over some threshold.
6. Create singleton clusters for datapoints which did not get into clusters.

This algorithm implements [single linkage][1] clustering. It can be tuned easily to implement average linkage. [Complete linkage][2] will be less efficient, but maybe easier ones will give good results depending on your data and task.

I believe for 200K datapoints it must take under second, if you use proper data structures for above operations.


  [1]: http://en.wikipedia.org/wiki/Single-linkage_clustering
  [2]: http://en.wikipedia.org/wiki/Complete_linkage_clustering
', 2574, '2014-08-04 15:48:29.757', '19ba47d3-c8d6-447d-a426-758ec0d0f44a', 900, 2313, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Strange as it sounds, graphs and graph databases are typically implemented as [linked lists][1]. As alluded to [here][2], even the most popular/performant graph database out there (neo4j), is secretly using something akin to a doubly-linked list.

Representing a graph this way has a number of significant benefits, but also a few drawbacks. Firstly, representing a graph this way means that you can do edge-based insertions in near-constant time. Secondly, this means that traversing the graph can happen extremely quickly, if we''re only looking to either step up or down a linked list.

The biggest drawback of this though comes from something sometimes called The Justin Bieber Effect, where nodes with a large number of connections tend to be extremely slow to evaluate. Imagine having to traverse a million semi-redundant links every time someone linked to Justin Bieber.

I know that the awesome folks over at Neo4j are working on the second problem, but I''m not sure how they''re going about it, or how much success they''ve had.



  [1]: http://docs.oracle.com/javase/7/docs/api/java/util/LinkedList.html
  [2]: http://docs.neo4j.org/chunked/stable/cypher-cookbook-newsfeed.html', 548, '2014-08-04 15:59:12.573', 'cedadb73-43bc-48ac-9d76-d5f8392a8e4b', 901, 2314, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are there any general rules that one can use to infer what can be learned/generalized from a particular data set?  Suppose the dataset was taken from a sample of people.  Can these rules be stated as functions of the sample or total population?

I understand the above may be vague, so a case scenario: Users participate in a search task, where the data are their queries, clicked results, and the HTML content (text only) of those results.  Each of these are tagged with their user and timestamp.  A user may generate a few pages - for a simple fact-finding task - or hundreds of pages - for a longer-term search task, like for class report.

Theory and paper references are a plus!', 1097, '2014-08-04 19:10:57.187', 'ffacf40c-7d21-4420-b077-7df86474b0d1', 902, 2315, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When is there enough data for generalization?', 1097, '2014-08-04 19:10:57.187', 'ffacf40c-7d21-4420-b077-7df86474b0d1', 902, 2316, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><statistics><search>', 1097, '2014-08-04 19:10:57.187', 'ffacf40c-7d21-4420-b077-7df86474b0d1', 902, 2317, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Are there any general rules that one can use to infer what can be learned/generalized from a particular data set?  Suppose the dataset was taken from a sample of people.  Can these rules be stated as functions of the sample or total population?

I understand the above may be vague, so a case scenario: Users participate in a search task, where the data are their queries, clicked results, and the HTML content (text only) of those results.  Each of these are tagged with their user and timestamp.  A user may generate a few pages - for a simple fact-finding task - or hundreds of pages - for a longer-term search task, like for class report.

Edit:  In addition to generalizing about a population, given a sample, I''m interested in generalizing about an individual''s overall search behavior, given a time slice.  Theory and paper references are a plus!', 1097, '2014-08-04 19:23:09.483', '0486f797-1576-4cd6-a558-e28b766789c5', 902, 'added 170 characters in body', 2318, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a set of results from an A/B test (one control group, one feature group) which do not fit a Normal Distribution.
In fact the distribution resembles more closely the Landau Distribution.

I believe the independent t-test requires that the samples be at least approximately normally distributed, which discourages me using the t-test as a valid method of significance testing.

But my question is:
**At what point can one say that the t-test is not a good method of significance testing?**

Or put another way, how can one qualify how reliable the p-values of a t-test are, given only the data set?', 2830, '2014-08-04 22:27:10.837', 'b622da73-8801-4229-a3a6-bebeba5686ef', 903, 2319, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Analyzing A/B test results which are not normally distributed, using independent t-test', 2830, '2014-08-04 22:27:10.837', 'b622da73-8801-4229-a3a6-bebeba5686ef', 903, 2320, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics><dataset>', 2830, '2014-08-04 22:27:10.837', 'b622da73-8801-4229-a3a6-bebeba5686ef', 903, 2321, '3');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-08-04 22:59:23.877', '822a0735-9cac-41dd-93ac-f89e75b611b9', 904, 'from http://stats.stackexchange.com/questions/110604/what-do-you-use-to-generate-a-dashboard-in-r', 2322, '36');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-08-04 22:59:23.877', '98e74a0d-be54-4c73-acd7-0079b9551a22', 905, 'from http://stats.stackexchange.com/questions/110604/what-do-you-use-to-generate-a-dashboard-in-r/110605#110605', 2323, '36');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Shiny][1] is a framework for generating HTML-based apps that execute R code dynamically. Shiny apps can stand alone or be built into Markdown documents with `knitr`, and Shiny development is fully integrated into RStudio. There''s even a free service called [shinyapps.io][2] for hosting Shiny apps, the `shiny` package has functions for deploying Shiny apps directly from R, and RStudio has a GUI interface for calling those functions. There''s plenty more info in the Tutorial section of the site.

Since it essentially "compiles" the whole thing to JavaScript and HTML, you can use CSS to freely change the formatting and layout, although Shiny has decent wrapper functionality for this. But it just so happens that their default color scheme is similar to the one in the screenshot you posted.

  [1]: http://shiny.rstudio.com
  [2]: https://www.shinyapps.io

edit: I just realized you don''t need them to be dynamic. Shiny still makes very nice-looking webpages out of the box, with lots of options for rearranging elements. There''s also functionality for downloading plots, so you can generate your dashboard every month by just updating your data files in the app, and then saving the resulting image to PDF.', 1156, '2014-08-04 19:28:38.173', 'ad27813b-29c3-4bb6-99c9-cbac1687e299', 905, 2324, '2');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('aiolias', 'I need to generate periodic (daily, monthly) web analytics dashboard reports. They will be static and don''t require interaction, so imagine a PDF file as the target output. The reports will mix tables and charts (mainly sparkline and bullet graphs created with ggplot2). Think Stephen Few/Perceptual Edge style dashboards, such as: ![sample dashboard][1]

but applied to web analytics.

Any suggestions on what packages to use creating these dashboard reports?

My first intuition is to use R markdown and knitr, but perhaps you''ve found a better solution. I can''t seem to find rich examples of dashboards generated from R.


  [1]: http://i.stack.imgur.com/Edh2e.png', '2014-08-04 19:21:45.067', '34bd5df0-ece1-4be5-9430-30735b425640', 904, 2325, '2');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('aiolias', 'What do you use to generate a dashboard in R?', '2014-08-04 19:21:45.067', '34bd5df0-ece1-4be5-9430-30735b425640', 904, 2326, '1');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('aiolias', '<untagged>', '2014-08-04 19:21:45.067', '34bd5df0-ece1-4be5-9430-30735b425640', 904, 2327, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[PajekXXL][1] is designed to handle enormous networks. But Pajek is also kind of a bizarre program with an unintuitive interface.


  [1]: http://mrvar.fdv.uni-lj.si/pajek/PajekXXL.htm', 1156, '2014-08-04 23:37:27.877', '33d304c9-2864-4b6a-83d7-74d668d9741e', 906, 2328, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think that `Shiny` is an overkill in this situation and doesn''t match your requirement of *dashboard reports* to be **static**. I guess, that your use of the term "dashboard" is a bit confusing, as some people might consider that it has more emphasis of **interactivity** (*real-time dashboards*), rather than **information layout**, as is my understanding (confirmed by the "static" requirement).

My recommendation to you is to use **R Markdown** and **knitr**, especially since these packages have much lower learning curve than **Shiny**. Moreover, I have recently run across an R package, which, in my view, ideally suits your requirement of embedding small charts/plots in a report, as presented on your picture above. This package generates static or dynamic *graphical tables* and is called **sparkTable** (http://cran.r-project.org/web/packages/sparkTable). Its vignette is available here (there is no link to it on the package''s home page): http://publik.tuwien.ac.at/files/PubDat_228663.pdf. Should you ever need some *interactivity*, `sparkTable` provides some via its simple interface to `Shiny`.', 2452, '2014-08-05 07:17:34.750', 'c03a7433-246d-4fab-90eb-4b711d93105d', 907, 2329, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It is my understanding that *random sampling* is a **mandatory condition** for making any *generalization* statements. IMHO, other parameters, such as sample size, just affect probability level (confidence) of generalization. Furthermore, clarifying the @ffriend''s comment, I believe that you have to **calculate** needed *sample size*, based on desired values of *confidence interval*, *effect size*, *statistical power* and *number of predictors* (this is based on Cohen''s work - see References section at the following link). For multiple regression, you can use the following calculator: http://www.danielsoper.com/statcalc3/calc.aspx?id=1.

More information on **how to select, calculate and interpret effect sizes** can be found in the following nice and comprehensive paper, which is freely available: http://jpepsy.oxfordjournals.org/content/34/9/917.full.

If you''re using `R` (and even, if you don''t), you may find the following Web page on **confidence intervals and R** interesting and useful: http://osc.centerforopenscience.org/static/CIs_in_r.html.

Finally, the following **comprehensive guide** to survey **sampling** can be helpful, even if you''re not using survey research designs. In my opinion, it contains a wealth of useful information on *sampling methods*, *sampling size determination* (including calculator) and much more: http://home.ubalt.edu/ntsbarsh/stat-data/Surveys.htm.', 2452, '2014-08-05 08:09:17.240', 'fe657e42-9024-40b1-8796-364591f8d6c6', 908, 2330, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The distribution of your data doesn''t need to be normal, it''s the [Sampling Distribution][1] that has to be nearly normal. If your sample size is big enough, then the sampling distribution of means from Landau Distribution should to be nearly normal, due to the [Central Limit Theorem][2].

So it means you should be able to safely use t-test with your data.


### Example

Let''s consider this example: suppose we have a population with [Lognormal distribution][3] with mu=0 and sd=0.5 (it looks a bit similar to Landau)

![lognormal density][4]

So we sample 30 observations 5000 times from this distribution each time calculating the mean of the sample

And this is what we get

![sampling distribution][5]

Looks quite normal, doesn''t it? If we increase the sample size, it''s even more apparent

![sampling distribution][6]


### R code

    x = seq(0, 4, 0.05)
    y = dlnorm(x, mean=0, sd=0.5)
    plot(x, y, type=''l'', bty=''n'')


    n = 30
    m = 1000

    set.seed(0)
    samp = rep(NA, m)

    for (i in 1:m) {
      samp[i] = mean(rlnorm(n, mean=0, sd=0.5))
    }

    hist(samp, col=''orange'', probability=T, breaks=25, main=''sample size = 30'')
    x = seq(0.5, 1.5, 0.01)
    lines(x, dnorm(x, mean=mean(samp), sd=sd(samp)))


    n = 300
    samp = rep(NA, m)

    for (i in 1:m) {
      samp[i] = mean(rlnorm(n, mean=0, sd=0.5))
    }

    hist(samp, col=''orange'', probability=T, breaks=25, main=''sample size = 300'')
    x = seq(1, 1.25, 0.005)
    lines(x, dnorm(x, mean=mean(samp), sd=sd(samp)))


  [1]: https://en.wikipedia.org/wiki/Sampling_distribution
  [2]: https://en.wikipedia.org/wiki/Central_limit_theorem
  [3]: https://en.wikipedia.org/wiki/Log-normal_distribution
  [4]: http://i.stack.imgur.com/Hw5mM.png
  [5]: http://i.stack.imgur.com/wjueS.png
  [6]: http://i.stack.imgur.com/M0FQS.png', 816, '2014-08-05 08:12:15.647', '8b61bcff-6432-4e9b-b50e-c7b3e5505b08', 909, 2331, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here are the basic Natural Language Processing capabilities (or annotators) that are usually necessary to extract language units from textual data for sake of search and other applications:

[Sentence breaker][1] - to split text (usually, text paragraphs) to sentences. Even in English it can be hard for some cases like "Mr. and Mrs. Brown stay in room no. 20."

[Tokenizer][2] - to split text or sentences to words or word-level units. This task is not trivial for languages with no spaces and no stable understanding of word boundaries (e.g. Chinese, Japanese)

[Part-of-speech Tagger][3] - to guess part of speech of each word in the context of sentence; usually each word is assigned a so-called POS-tag from a tagset developed in advance to serve your final task (for example, parsing).

[Lemmatizer][4] - to convert a given word into its canonical form ([lemma][5]). Usually you need to know the word''s POS-tag. For example, word "heating" as gerund must be converted to "heat", but as noun it must be left unchanged.

[Parser][6] - to perform syntactic analysis of the sentence and build a syntactic tree or graph. There''re two main ways to represent syntactic structure of sentence: via [constituency or dependency][7].

[Summarizer][8] - to generate a short summary of the text by selecting a set of top informative sentences of the document, representing its main idea. However can be done in more intelligent manner than just selecting the sentences from existing ones.

[Named Entity Recognition][9] - to extract so-called named entities from the text. Named entities are the chunks of words from text, which refer to an entity of certain type. The types may include: geographic locations (countries, cities, rivers, ...), person names, organization names etc. Before going into NER task you must understand what do you want to get and, possible, predefine a taxonomy of named entity types to resolve.

[Coreference Resolution][10] - to group named entities (or, depending on your task, any other text units) into clusters corresponding to a single real object/meaning. For example, "B. Gates", "William Gates", "Founder of Microsoft" etc. in one text may mean the same person, referenced by using different expressions.

There''re many other interesting NLP applications/annotators (see [NLP tasks category][11]), sentiment analysis, machine translation etc.). There''re many books on this, the classical book: "Speech and Language Processing" by Daniel Jurafsky and James H. Martin., but it can be too detailed for you.


  [1]: http://en.wikipedia.org/wiki/Sentence_boundary_disambiguation
  [2]: http://en.wikipedia.org/wiki/Tokenization
  [3]: http://en.wikipedia.org/wiki/POS_tagger
  [4]: http://en.wikipedia.org/wiki/Lemmatization
  [5]: http://en.wikipedia.org/wiki/Lemma_(morphology)
  [6]: http://en.wikipedia.org/wiki/Parser
  [7]: http://en.wikipedia.org/wiki/Dependency_grammar#Dependency_vs._constituency
  [8]: http://en.wikipedia.org/wiki/Automatic_summarization
  [9]: http://en.wikipedia.org/wiki/Named-entity_recognition
  [10]: http://en.wikipedia.org/wiki/Coreference_resolution
  [11]: http://en.wikipedia.org/wiki/Category:Tasks_of_natural_language_processing', 2574, '2014-08-05 09:07:42.393', 'a2f27326-6eb6-4e54-a635-93e11e588e52', 910, 2332, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Basically an independent t-test or a 2 sample t-test is used to check if the averages of the two samples are significantly different. Or, to put in another words, if there is a significant difference between the means of the two samples.

Now, the means of those 2 samples are two statistics, which according with CLT, have a normal distribution, if provided enough samples. Note that CLT works no matter of the distribution from which the mean statistic is built.

Normally one can use a z-test, but if the variances are estimated from the sample (because it is unknown), some additional uncertainty is introduced, which is incorporated in t distribution. That''s why 2-sample t-test applies here.  ', 108, '2014-08-05 10:15:33.713', 'ee5323d5-1fa6-4be5-9a5f-388c2cae53ab', 911, 2333, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is an excellent comparison of the common inner-product-based similarity metrics [here][1].

In particular, Cosine Similarity is normalized to lie within [0,1], unlike the dot product which can be any real number, but, as everyone else is saying, that will require ignoring the magnitude of the vectors. Personally, I think that''s a good thing. I think of magnitude as an internal (within-vector) structure, and angle between vectors as external (between vector) structure. They are different things and (in my opinion) are often best analyzed separately. I can''t imagine a situation where I would rather compute inner products than compute cosine similarities and just compare the magnitudes afterward.


  [1]: http://brenocon.com/blog/2012/03/cosine-similarity-pearson-correlation-and-ols-coefficients/', 1156, '2014-08-05 12:29:05.300', '07ebc09d-dbf5-43cd-88e0-33fbeb530311', 912, 2334, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are two rules for generalizability:

 1. The sample must be **representative**. In expectation, at least, the distribution of features in your sample must match the distribution of features in the population. When you are fitting a model with a response variable, *this includes features that you do not observe, but that affect any response variables in your model*. Since it is, in many cases, impossible to know what you do not observe, **random sampling** is used.

 The idea with randomization is that a random sample, up to sampling error, *must* accurately reflect the distribution of all features in the population, observed and otherwise. This is why **randomization is the "gold standard,"** but if sample control is available by some other technique, or it is defensible to argue that there are no omitted features, then it isn''t always necessary.

 2. Your sample must be **large enough** that the effect of **sampling error** on the feature distribution is relatively small. This is, again, to ensure representativeness. But deciding who to sample is different from deciding how many people to sample.

Since it sounds like you''re fitting a model, there''s the additional consideration that certain important combinations of features could be relatively rare in the population. This is not an issue for generalizability, but it bears heavily on your considerations for sample size. For instance, I''m working on a project now with (non-big) data that was originally collected to understand the experiences of minorities in college. As such, it was critically important to ensure that **statistical power** was high *specifically in the minority subpopulation*. For this reason, blacks and Latinos were deliberately **oversampled**. However, the proportion by which they were oversampled was also recorded. These are used to compute survey weights. These can be used to  re-weight the sample so as to reflect the estimated population proportions, in the event that a representative sample is required.

An additional consideration arises if your model is hierarchical. A canonical use for a hierarchical model is one of children''s behavior in schools. Children are "grouped" by school and share school-level traits. Therefore a representative sample of schools is required, and within each school a representative sample of children is required. This leads to **stratified sampling**. This and some other sampling designs are reviewed in surprising depth on [Wikipedia][1].


  [1]: http://en.wikipedia.org/wiki/Sampling_(statistics)#Sampling_methods', 1156, '2014-08-05 12:58:16.000', 'e56e5ad6-71c9-46ca-b417-22aad8b00816', 913, 2335, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('## Two Categorical Variables

Checking if two categorical variables are independent can be done with Chi-Squared test of independence.

This is a typical [Chi-Square test][1]: if we assume that two variables are independent, then the values of the contingency table for these variables should be distributed uniformly. And then we check how far away from uniform the actual values are.

There also exists a [Crammer''s V][2] that is a measure of correlation that follows from this test


### Example

Suppose we have two variables

 - gender: male and female
 - city: Blois and Tours

We observed the following data:

![observed values][3]

Are gender and city independent? Let''s perform a Chi-Squred test. Null hypothesis: they are independent, Alternative hypothesis is that they are correlated in some way.

Under the Null hypothesis, we assume uniform distribution. So our expected values are the following

![expected value][4]

So we run the chi-squared test and the resulting p-value here can be seen as a measure of correlation between these two variables.


To compute Crammer''s V we first find the normalizing factor chi-squared-max which is typically the size of the sample, divide the chi-square by it and take a square root

![crammers v][5]


### R

    tbl = matrix(data=c(55, 45, 20, 30), nrow=2, ncol=2, byrow=T)
    dimnames(tbl) = list(City=c(''B'', ''T''), Gender=c(''M'', ''F''))

    chi2 = chisq.test(tbl, correct=F)
    c(chi2$statistic, chi2$p.value)

Here the p value is 0.08 - quite small, but still not enough to reject the hypothesis of independence. So we can say that the "correlation" here is 0.08

We also compute V:

    sqrt(chi2$statistic / sum(tbl))

And get 0.14 (the smaller v, the lower the correlation)


Consider another dataset

        Gender
    City  M  F
       B 51 49
       T 24 26

For this, it would give the following


    tbl = matrix(data=c(51, 49, 24, 26), nrow=2, ncol=2, byrow=T)
    dimnames(tbl) = list(City=c(''B'', ''T''), Gender=c(''M'', ''F''))

    chi2 = chisq.test(tbl, correct=F)
    c(chi2$statistic, chi2$p.value)

    sqrt(chi2$statistic / sum(tbl))

The p-value is 0.72 which is far closer to 1, and v is 0.03 - very close to 0



## Categorical vs Numerical Variables

For this type we typically perform [One-way ANOVA test][6]: we calculate in-group variance and intra-group variance and then compare them.



### Example

We want to study the relationship between absorbed fat from donuts vs the type of fat used to produce donuts (example is taken from [here][7])

![donuts][8]

Is there any dependence between the variables?
For that we conduct ANOVA test and see that the p-value is just 0.007 - there''s no correlation between these variables.


### R

    t1 = c(164, 172, 168, 177, 156, 195)
    t2 = c(178, 191, 197, 182, 185, 177)
    t3 = c(175, 193, 178, 171, 163, 176)
    t4 = c(155, 166, 149, 164, 170, 168)

    val = c(t1, t2, t3, t4)
    fac = gl(n=4, k=6, labels=c(''type1'', ''type2'', ''type3'', ''type4''))

    aov1 = aov(val ~ fac)
    summary(aov1)

Output is


                Df Sum Sq Mean Sq F value  Pr(>F)
    fac          3   1636   545.5   5.406 0.00688 **
    Residuals   20   2018   100.9
    ---
    Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1


So we can take the p-value as the measure of correlation here as well.

## References

 - https://en.wikipedia.org/wiki/Chi-square_test
 - http://0agr.ru/wiki/index.php/Chi-square_Test_of_Independence
 - http://courses.statistics.com/software/R/R1way.htm
 - http://0agr.ru/wiki/index.php/One-Way_ANOVA_F-Test
 - http://0agr.ru/wiki/index.php/Crammer%27s_Coefficient


  [1]: https://en.wikipedia.org/wiki/Chi-square_test
  [2]: http://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V
  [3]: http://i.stack.imgur.com/zcCfV.png
  [4]: http://i.stack.imgur.com/H8bKJ.png
  [5]: http://i.stack.imgur.com/v7HY6.png
  [6]: http://en.wikipedia.org/wiki/F_test#One-way_ANOVA_example
  [7]: http://courses.statistics.com/software/R/R1way.htm
  [8]: http://i.stack.imgur.com/LMOS3.png', 816, '2014-08-05 15:55:08.573', 'd69cee38-e222-4870-9eaf-b36215621ada', 898, 'added crammer''s v', 2336, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is there a known general table of statistical techniques that explain how they scale with sample size and dimension? For example, a friend of mine told me the other day that the computation time of simply quick-sorting one dimensional data of size n goes as n*log(n).

So, for example, if we regress y against X where X is a d-dimensional variable, does it go as O(n^2*d)? How does it scale if I want to find the solution via exact Gauss-Markov solution vs numerical least squares with Newton method? Or simply getting the solution vs using significance tests?

I guess I more want a good source of answers (like a paper that summarizes the scaling of various statistical techniques) than a good answer here. Like, say, a list that includes the scaling of multiple regression, logistic regression, PCA, cox proportional hazard regression, K-means clustering, etc.', 2841, '2014-08-05 18:36:12.753', 'c9beb7d8-58ec-419f-82f1-dbb57f29e166', 915, 2338, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How do various statistical techniques (regression, PCA, etc) scale with $n$ and $d$?', 2841, '2014-08-05 18:36:12.753', 'c9beb7d8-58ec-419f-82f1-dbb57f29e166', 915, 2339, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><statistics><efficiency><scalability>', 2841, '2014-08-05 18:36:12.753', 'c9beb7d8-58ec-419f-82f1-dbb57f29e166', 915, 2340, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How do various statistical techniques (regression, PCA, etc) scale with sample size and dimension?', 2841, '2014-08-05 18:46:46.157', '46ce03b3-7ee1-4bda-95a6-6d485fa45419', 915, 'edited title', 2341, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Most of the efficient (and non trivial) statistic algorithms are iterative in nature so that the worst case analysis `O()` is irrelevant as the worst case is ''it fails to converge''.

Nevertheless, when you have a lot of data, even the linear algorithms (`O(n)`) can be slow and you then need to focus on the constant ''hidden'' behind the notation. For instance, computing the variance of a single variate is naively done scanning the data twice (once for computing an estimate of the mean, and then once to estimate the variance). But it also can be done in [one pass][1].

For iterative algorithms, what is more important is convergence rate and number of parameters as a function of the data dimensionality, an element that greatly influences convergence. Many models/algorithm grow a number of parameters that is exponential with the number of variables (e.g. splines) while some other grow linearly (e.g. support vector machines, random forests, ...)

  [1]: http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm', 172, '2014-08-05 20:24:09.200', '3e63146c-a12d-4b89-9566-57ff6b61512e', 916, 2342, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('
I am attempting to solve a set of equations which has 40 independent variables (x1, ..., x40) and one dependent variable (y). The total number of equations (number of rows) is ~300, and I want to solve for the set of 40 coefficients that minimizes the total sum-of-square error between y and the predicted value.

My problem is that the matrix is very sparse and I do not know the best way to solve the system of equations with sparse data. An example of the dataset is shown below:


       y    x1  x2 x3 x4 x5 x6 ... x40
    87169   14  0  1  0  0  2  ... 0
    46449   0   0  4  0  1  4  ... 12
    846449  0   0  0  0  0  3  ... 0
    ....

I am currently using a Genetic Algorithm to solve this and the results are coming out
with roughly a factor of two difference between observed and expected.

Can anyone suggest different methods or techniques which are capable of solving a set of equations with sparse data.

Thank you!', 802, '2014-08-05 20:45:01.383', '10b9587e-aaa1-4585-88a3-881c1eac636e', 917, 2343, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Solving a system of equations with spare data', 802, '2014-08-05 20:45:01.383', '10b9587e-aaa1-4585-88a3-881c1eac636e', 917, 2344, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><algorithms><genetic>', 802, '2014-08-05 20:45:01.383', '10b9587e-aaa1-4585-88a3-881c1eac636e', 917, 2345, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I understand you correctly, this is the case of **multiple linear regression with sparse data** (*sparse regression*). Assuming that, I hope you will find the following **resources** useful.

1) NCSU **lecture slides on sparse regression** with overview of algorithms, notes, formulas, graphics and references to literature: http://www.stat.ncsu.edu/people/zhou/courses/st810/notes/lect23sparse.pdf

2) `R` ecosystem offers many **packages**, useful for sparse regression analysis, including:

+ **Matrix** (http://cran.r-project.org/web/packages/Matrix)
+ **SparseM** (http://cran.r-project.org/web/packages/SparseM)
+ **MatrixModels** (http://cran.r-project.org/web/packages/MatrixModels)
+ **glmnet** (http://cran.r-project.org/web/packages/glmnet)
+ **flare** (http://cran.r-project.org/web/packages/flare)

3) A blog post with an **example of sparse regression solution**, based on `SparseM`: http://aleph-nought.blogspot.com/2012/03/multiple-linear-regression-with-sparse.html

4) A blog post on using **sparse matrices in R**, which includes a **primer** on using `glmnet`: http://www.johnmyleswhite.com/notebook/2011/10/31/using-sparse-matrices-in-r

5) **More examples and some discussion** on the topic can be found on **StackOverflow**: http://stackoverflow.com/questions/3169371/large-scale-regression-in-r-with-a-sparse-feature-matrix', 2452, '2014-08-05 22:34:04.550', '85bcc11d-a313-42df-9c48-1af29607e260', 918, 2346, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am attempting to solve a set of equations which has 40 independent variables (x1, ..., x40) and one dependent variable (y). The total number of equations (number of rows) is ~300, and I want to solve for the set of 40 coefficients that minimizes the total sum-of-square error between y and the predicted value.

My problem is that the matrix is very sparse and I do not know the best way to solve the system of equations with sparse data. An example of the dataset is shown below:


       y    x1  x2 x3 x4 x5 x6 ... x40
    87169   14  0  1  0  0  2  ... 0
    46449   0   0  4  0  1  4  ... 12
    846449  0   0  0  0  0  3  ... 0
    ....

I am currently using a Genetic Algorithm to solve this and the results are coming out
with roughly a factor of two difference between observed and expected.

Can anyone suggest different methods or techniques which are capable of solving a set of equations with sparse data.', 84, '2014-08-06 03:05:32.037', '10cce655-ba0c-448e-b1dc-ea379ceec6cb', 917, 'deleted 16 characters in body; edited title', 2347, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Solving a system of equations with sparse data', 84, '2014-08-06 03:05:32.037', '10cce655-ba0c-448e-b1dc-ea379ceec6cb', 917, 'deleted 16 characters in body; edited title', 2348, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data set looks like:

- 25000 observations
- up to 15 predictors of different types: numeric, multi-class categorical, binary
- target variable is binary

Which cross validation method is typical for this type of problems?

By default I''m using K-Fold. How many folds is enough in this case? (One of the models I use is random forest, which is time consuming...)', 97, '2014-08-06 08:41:44.967', 'bea47f69-9a2d-4c97-81c9-8307f5ca692f', 919, 2349, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which cross-validation type best suits to binary classification problem', 97, '2014-08-06 08:41:44.967', 'bea47f69-9a2d-4c97-81c9-8307f5ca692f', 919, 2350, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><cross-validation>', 97, '2014-08-06 08:41:44.967', 'bea47f69-9a2d-4c97-81c9-8307f5ca692f', 919, 2351, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is a text summarization project called SUMMARIST. Apparently it is able to perform abstractive text summarization. I want to give it a try but unfortunately the demo links on the website do not work. Does anybody have any information regarding this? How can I test this tool?

http://www.isi.edu/natural-language/projects/SUMMARIST.html

Regards,
PasMod', 979, '2014-08-06 09:02:01.033', '11ad1c64-63a9-44e5-8516-428c71a073b0', 920, 2352, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('SUMMARIST: Automated Text Summarization', 979, '2014-08-06 09:02:01.033', '11ad1c64-63a9-44e5-8516-428c71a073b0', 920, 2353, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining>', 979, '2014-08-06 09:02:01.033', '11ad1c64-63a9-44e5-8516-428c71a073b0', 920, 2354, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am fitting a model in R.

- use `createFolds` method to create several `k` folds from the data set
- loop through the folds, repeating the following on each iteration:
  - `train` the model on k-1 folds
  - `predict` the outcomes for the i-th fold
  - calculate prediction accuracy
- average the accuracy

Does R have a function that makes folds itself, repeats model tuning/predictions and gives the average accuracy back?', 97, '2014-08-06 09:03:20.857', '4b9e1a07-d551-41cb-b68f-af5456936955', 921, 2355, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Avoid iterations while calculating average model accuracy', 97, '2014-08-06 09:03:20.857', '4b9e1a07-d551-41cb-b68f-af5456936955', 921, 2356, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><accuracy><cross-validation><sampling><beginner>', 97, '2014-08-06 09:03:20.857', '4b9e1a07-d551-41cb-b68f-af5456936955', 921, 2357, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have  set of documents and I want classify them to true and false

My question is I have to take the whole words in the documents then I classify them depend on the similarity words in these documents or I can take only some words that I interested in then I compare it with the documents. Which one is more efficient in classify document and can work with SVM.       ', 2850, '2014-08-06 09:08:08.113', 'e9d583d5-c990-46be-b42d-106a0f0f1628', 922, 2358, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can I classify set of documents using classifying method using limited number of concepts ?', 2850, '2014-08-06 09:08:08.113', 'e9d583d5-c990-46be-b42d-106a0f0f1628', 922, 2359, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><text-mining>', 2850, '2014-08-06 09:08:08.113', 'e9d583d5-c990-46be-b42d-106a0f0f1628', 922, 2360, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think in your case a 10-fold CV will be O.K.

I think it is more important to randomize the cross validation process than selecting the ideal value for k.

So repeat the CV process several times randomly and compute the variance of your classification result to determine if the results are realiable or not.
', 979, '2014-08-06 09:08:12.117', '1bd4f458-d4f5-407b-9667-7c623fcc7ae7', 923, 2361, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes, you can do all this using the Caret (http://caret.r-forge.r-project.org/training.html) package in R. For example,

    fitControl <- trainControl(## 10-fold CV
                               method = "repeatedcv",
                               number = 10,
                               ## repeated ten times
                               repeats = 10)

    gbmFit1 <- train(Class ~ ., data = training,
                     method = "gbm",
                     trControl = fitControl,
                    ## This last option is actually one
                    ## for gbm() that passes through
                    verbose = FALSE)
    gbmFit1

which will give the output

    Stochastic Gradient Boosting

    157 samples
     60 predictors
      2 classes: ''M'', ''R''

    No pre-processing
    Resampling: Cross-Validated (10 fold, repeated 10 times)

    Summary of sample sizes: 142, 142, 140, 142, 142, 141, ...

    Resampling results across tuning parameters:

      interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD
      1                  50       0.8       0.5    0.1          0.2
      1                  100      0.8       0.6    0.1          0.2
      1                  200      0.8       0.6    0.09         0.2
      2                  50       0.8       0.6    0.1          0.2
      2                  100      0.8       0.6    0.09         0.2
      2                  200      0.8       0.6    0.1          0.2
      3                  50       0.8       0.6    0.09         0.2
      3                  100      0.8       0.6    0.09         0.2
      3                  200      0.8       0.6    0.08         0.2

    Tuning parameter ''shrinkage'' was held constant at a value of 0.1
    Accuracy was used to select the optimal model using  the largest value.
    The final values used for the model were n.trees = 150, interaction.depth = 3
    and shrinkage = 0.1.

Caret offers many other options as well so should be able to suit your needs. ', 802, '2014-08-06 12:16:22.850', '27e03d8d-a0fb-4220-8e2e-1cfdf3b8cd12', 924, 2362, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Both methods work. However, if you retain all words in documents you would essentially be working with a higher dimensional vectors (each term representing one dimension). Consequently, a classifier, e.g. SVM, would take more time to converge.

It is thus a standard practice to reduce the term-space dimensionality by pre-processing steps such as stop-word removal, stemming, Principal Component Analysis (PCA) etc.

One approach could be to analyze the document corpora by a topic modelling technique such as LDA and then retaining only those words which are representative of the topics, i.e. those which have high membership values in a single topic class.

Another approach (inspired by information retrieval) could be to retain the top K tf-idf terms from each document.', 984, '2014-08-06 12:27:31.277', '92c51892-6a0b-47cc-b534-318148b6e6bc', 925, 2363, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Both methods work. However, if you retain all words in documents you would essentially be working with high dimensional vectors (each term representing one dimension). Consequently, a classifier, e.g. SVM, would take more time to converge.

It is thus a standard practice to reduce the term-space dimensionality by pre-processing steps such as stop-word removal, stemming, Principal Component Analysis (PCA) etc.

One approach could be to analyze the document corpora by a topic modelling technique such as LDA and then retaining only those words which are representative of the topics, i.e. those which have high membership values in a single topic class.

Another approach (inspired by information retrieval) could be to retain the top K tf-idf terms from each document.', 984, '2014-08-06 13:15:51.460', '8cea6fcd-ed1d-44e9-ba89-6faabc8057a0', 925, 'deleted 4 characters in body', 2364, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Aleksandr''s answer][1] is completely correct.

However, the way the question is posed implies that this is a straightforward ordinary least squares regression question: minimizing the sum of squared residuals between a dependent variable and a linear combination of predictors.

Now, while there may be many zeros in your design matrix, your system as such is not overly large: 300 observations on 40 predictors is no more than medium-sized. You can run such a regression using R without any special efforts for sparse data. Just use the `lm()` command (for "linear model"). Use `?lm` to see the help page. And note that `lm` will by default silently add a constant column of ones to your design matrix (the intercept) - include a `-1` on the right hand side of your formula to suppress this. Overall, assuming all your data (and nothing else) is in a `data.frame` called `foo`, you can do this:

    model <- lm(y~.-1,data=foo)

And then you can look at parameter estimates etc. like this:

    summary(model)
    residuals(model)

*If* your system is *much* larger, say on the order of 10,000 observations and hundreds of predictors, looking at specialized sparse solvers as per [Aleksandr''s answer][1] may start to make sense.

Finally, in your comment to [Aleksandr''s answer][1], you mention constraints on your equation. If that is actually your key issue, there are ways to calculate constrained least squares in R. I personally like `pcls()` in the `mgcv` package. Perhaps you want to edit your question to include the type of constraints (box constraints, nonnegativity constraints, integrality constraints, linear constraints, ...) you face?

  [1]: http://datascience.stackexchange.com/a/918/2853', 2853, '2014-08-06 13:42:54.083', '25f69cf3-0003-45d8-909a-91e383dd625a', 926, 2365, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('

I''m working on the dataset with lots of NA values with sklearn and pandas.DataFrame. I implemented different imputation strategies for different columns of the dataFrame based column names. For example NAs predictor ''var1'' I impute with 0''s and for ''var2'' with mean.

When I try to cross validate my model using train_test_split it returns me a nparray which does not have column names. How can I impute missing values in this nparray?

P.S. I do not impute missing values in the original data set before splitting on purpose so I keep test and validation sets separately.
', 2854, '2014-08-06 15:07:07.457', '3571f3fb-866d-4430-9a68-711b52096512', 927, 2366, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('how to impute missing values on numpy array created by train_test_split from pandas.DataFrame?', 2854, '2014-08-06 15:07:07.457', '3571f3fb-866d-4430-9a68-711b52096512', 927, 2367, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<pandas><cross-validation><sklearn>', 2854, '2014-08-06 15:07:07.457', '3571f3fb-866d-4430-9a68-711b52096512', 927, 2368, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('K-Fold should do just fine for binary classification problem. Depending on the time it is taking to train your model and predict the outcome I would use 10-20 folds.

However sometimes a single fold takes several minutes, in this case I use 3-5 folds but not less than 3. Hope it helps.', 2854, '2014-08-06 15:10:59.600', '05c5240d-58f6-4572-8fb5-bacbc0b4c2b9', 928, 2369, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can you just cast your nparray from train_test_split back into a pandas dataFrame so you can carry out your same strategy. This is very common to what I do when dealing with pandas and scikit. For example,

     a = train_test_split
     new_df = pd.DataFrame(a)', 802, '2014-08-06 17:07:17.520', 'cde06f7b-a6cc-4001-9a33-7b39254aa575', 929, 2370, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From the link you mentioned in the comment, the train and test sets should be in the form of a  dataframe if you followed the first explanation.

In that case, you could do something like this:

    df[variable] = df[variable].fillna(df[variable].median())

You have options on what to fill the N/A values with, check out the link.
http://pandas.pydata.org/pandas-docs/stable/missing_data.html

If you followed the second explanation, using sklearn''s cross-validation, you could implement  mike1886''s suggestion of transforming the arrays into dataframes and then use the fillna option.
', 2838, '2014-08-06 18:17:29.700', 'd07e0ae4-444c-445e-b8f0-f50dd3a03172', 930, 2371, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('If I understand you correctly, this is the case of **multiple linear regression with sparse data** (*sparse regression*). Assuming that, I hope you will find the following **resources** useful.

1) NCSU **lecture slides on sparse regression** with overview of algorithms, notes, formulas, graphics and references to literature: http://www.stat.ncsu.edu/people/zhou/courses/st810/notes/lect23sparse.pdf

2) `R` ecosystem offers many **packages**, useful for sparse regression analysis, including:

+ **Matrix** (http://cran.r-project.org/web/packages/Matrix)
+ **SparseM** (http://cran.r-project.org/web/packages/SparseM)
+ **MatrixModels** (http://cran.r-project.org/web/packages/MatrixModels)
+ **glmnet** (http://cran.r-project.org/web/packages/glmnet)
+ **flare** (http://cran.r-project.org/web/packages/flare)

3) A blog post with an **example of sparse regression solution**, based on `SparseM`: http://aleph-nought.blogspot.com/2012/03/multiple-linear-regression-with-sparse.html

4) A blog post on using **sparse matrices in R**, which includes a **primer** on using `glmnet`: http://www.johnmyleswhite.com/notebook/2011/10/31/using-sparse-matrices-in-r

5) **More examples and some discussion** on the topic can be found on **StackOverflow**: http://stackoverflow.com/questions/3169371/large-scale-regression-in-r-with-a-sparse-feature-matrix

**UPDATE** (based on your comment):

If you''re trying to solve an LP problem with constraints, you may find this **theoretical paper** useful: http://web.stanford.edu/group/SOL/papers/gmsw84.pdf.

Also, check R package **limSolve**: http://cran.r-project.org/web/packages/limSolve. And, in general, check packages in *CRAN Task View* **"Optimization and Mathematical Programming"**: http://cran.r-project.org/web/views/Optimization.html.

Finally, check the book **"Using R for Numerical Analysis in Science and Engineering"** (by Victor A. Bloomfield). It has a section on solving systems of equations, represented by **sparse matrices** (section 5.7, pages 99-104), which includes examples, based on some of the above-mentioned packages: http://books.google.com/books?id=9ph_AwAAQBAJ&pg=PA99&lpg=PA99&dq=r+limsolve+sparse+matrix&source=bl&ots=PHDE8nXljQ&sig=sPi4n5Wk0M02ywkubq7R7KD_b04&hl=en&sa=X&ei=FZjiU-ioIcjmsATGkYDAAg&ved=0CDUQ6AEwAw#v=onepage&q=r%20limsolve%20sparse%20matrix&f=false. ', 2452, '2014-08-06 21:32:39.427', '00899b67-0133-4784-9f38-c90b0385489f', 918, 'Updated the answer, based on comment by the question''s author.', 2372, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To answer a simpler, but related question, namely ''How well can my model generalize on the data that I have?'' the method of learning curves might be applicable. [This][1] is a lecture given by Andrew Ng about them.

The basic idea is to plot test set error and training set error vs. the complexity of the model you are using (this can be somewhat complicated). If the model is powerful enough to fully ''understand'' your data, at some point the training set performance will be close to perfect. However, the variance of a complex model will likely cause the test set performance to increase at some point.

This analysis tells you two main things, I think. The first is an upper limit on performance. It''s pretty unlikely that you''ll do better on data that you haven''t seen than on your training data. The other thing it tells you is whether or not getting more data might help. If you can demonstrate that you fully understand your test data by driving training error to zero it might be possible, through the inclusion of more data, to drive your test error further down by getting a more complete sample and then training a powerful model on that.


  [1]: https://www.youtube.com/watch?v=g4XluwGYPaA', 2724, '2014-08-06 23:37:15.293', 'a3802df1-92d2-4e1b-ab66-d6dfc00a6a17', 931, 2373, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('To answer a simpler, but related question, namely ''How well can my model generalize on the data that I have?'' the method of learning curves might be applicable. [This][1] is a lecture given by Andrew Ng about them.

The basic idea is to plot test set error and training set error vs. the complexity of the model you are using (this can be somewhat complicated). If the model is powerful enough to fully ''understand'' your data, at some point the training set performance will be close to perfect. However, the variance of a complex model will likely cause the test set performance to increase at some point.

This analysis tells you two main things, I think. The first is an upper limit on performance. It''s pretty unlikely that you''ll do better on data that you haven''t seen than on your training data. The other thing it tells you is whether or not getting more data might help. If you can demonstrate that you fully understand your training data by driving training error to zero it might be possible, through the inclusion of more data, to drive your test error further down by getting a more complete sample and then training a powerful model on that.


  [1]: https://www.youtube.com/watch?v=g4XluwGYPaA', 2724, '2014-08-06 23:52:44.867', '3996e8df-e4cc-4763-a10c-a97c1fb349ac', 931, 'added 4 characters in body', 2374, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('To answer a simpler, but related question, namely ''How well can my model generalize on the data that I have?'' the method of learning curves might be applicable. [This][1] is a lecture given by Andrew Ng about them.

The basic idea is to plot test set error and training set error vs. the complexity of the model you are using (this can be somewhat complicated). If the model is powerful enough to fully ''understand'' your data, at some point the complexity of the model will be high enough that performance on the training set will be close to perfect. However, the variance of a complex model will likely cause the test set performance to increase at some point.

This analysis tells you two main things, I think. The first is an upper limit on performance. It''s pretty unlikely that you''ll do better on data that you haven''t seen than on your training data. The other thing it tells you is whether or not getting more data might help. If you can demonstrate that you fully understand your training data by driving training error to zero it might be possible, through the inclusion of more data, to drive your test error further down by getting a more complete sample and then training a powerful model on that.


  [1]: https://www.youtube.com/watch?v=g4XluwGYPaA', 2724, '2014-08-07 00:20:05.817', '0c5dfb0c-cece-48e2-be57-387e49b22742', 931, 'added 56 characters in body', 2375, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have managed to resolve this. There is an excellent and thorough explanation of the optimization steps in the following thesis: [Semi-Supervised Learning for Natural Language by Percy Liang](http://cs.stanford.edu/~pliang/papers/meng-thesis.pdf).

My mistake was trying to update the quality for all potential clusters pairs. Instead, you should initialize a table with the quality changes of doing each merge. Use this table to find the best merge, and the update the relevant terms that make up the table entries.', 2817, '2014-08-07 03:15:09.933', 'b39f047c-a792-4aa7-93c1-766cb6a226a8', 932, 2376, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It dates back to 1998, so most likely has been abandoned, or "acquired" by microsoft as the creator currently works there and has done since publishing that research.

see http://research.microsoft.com/en-us/people/cyl/ists97.pdf

and http://research.microsoft.com/en-us/people/cyl for the author. Maybe you could try to contact him.', 2861, '2014-08-07 09:33:50.817', '9acbf627-78eb-4aff-9939-e0062ff76949', 933, 2377, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have train and test sets of chronological data consisting of 305000 instances and 70000,appropriately. There are 15 features in each instance and only 2 possible class values ( NEW,OLD). The problem is that there are only 725 OLD instances in the train set and 95 in the test.

The only algorithm which succeeds for me to handle imbalance is NaiveBayes in Weka (0.02 precision for OLD class), others (trees) classify each instance as NEW.
What is the best approach to handle the imbalance and the appropriate algorithm in such a case?

Thank you in advance.
', 2533, '2014-08-07 10:45:38.557', '51d75735-5643-488f-9185-d48abe96e6c9', 934, 2378, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Handling huge dataset imbalance (2 class values) and appropriate ML algorithm', 2533, '2014-08-07 10:45:38.557', '51d75735-5643-488f-9185-d48abe96e6c9', 934, 2379, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><dataset>', 2533, '2014-08-07 10:45:38.557', '51d75735-5643-488f-9185-d48abe96e6c9', 934, 2380, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m not allowed to comment, but I have more a suggestion: you could try to implement some "Over-sampling Techniques" like SMOTE:
http://scholar.google.com/scholar?q=oversampling+minority+classes', 2863, '2014-08-07 12:46:25.123', 'fce33798-a605-4281-876d-a6dd7a95906d', 935, 2381, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am attempting to compile code using Knitr in R.

My code below is returning the following error, and causes errors in the rest of the document.

miss<-sample$sensor_glucose[!is.na(sample$sensor_glucose)]
Error: "## Warning: is.na() applied to non-(list or vector) of type ''NULL''"

> str(miss)
 int [1:103] 213 113 46 268 186 196 187 153 43 175 ...

Does anyone know how to remedy this problem?

Thanks in advance!', 2792, '2014-08-07 15:12:48.617', '6c3f6d41-0d59-49e7-9b42-a8bd741b9967', 936, 2382, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R error using Knitr', 2792, '2014-08-07 15:12:48.617', '6c3f6d41-0d59-49e7-9b42-a8bd741b9967', 936, 2383, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r>', 2792, '2014-08-07 15:12:48.617', '6c3f6d41-0d59-49e7-9b42-a8bd741b9967', 936, 2384, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m working on the problem with too many features and training my models takes way too long. I implemented forward selection algorithm to choose features.

However, I was wondering does scikit-learn have forward selection/stepwise regression algorithm?', 2854, '2014-08-07 15:33:43.793', '6a49a4b9-9622-4fc1-94e9-801ddaff2263', 937, 2385, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Does scikit-learn have forward selection/stepwise regression algorithm?', 2854, '2014-08-07 15:33:43.793', '6a49a4b9-9622-4fc1-94e9-801ddaff2263', 937, 2386, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<feature-selection><sklearn><scikit>', 2854, '2014-08-07 15:33:43.793', '6a49a4b9-9622-4fc1-94e9-801ddaff2263', 937, 2387, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can apply a clustering algorithm to the instances in the majority class and train a classifier with the centroids/medoids offered by the cluster algorithm. This is subsampling the majority class, the converse of oversampling the minority class. ', 172, '2014-08-07 17:49:05.640', '8cbba5d4-2664-46b1-875c-6ba49a9ad5b6', 938, 2388, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You will have best results if you care to build the folds so that each variable (and most importantly the target variable) is approximately identically distributed in each fold. This is called, when applied to the target variable, stratified k-fold. One approach is to cluster the inputs and make sure each fold contains the same number of instances from each cluster proportional to their size.', 172, '2014-08-07 17:53:06.970', 'e9227a09-3052-4089-abdc-e38019356e24', 939, 2389, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('We are storing the information about our users showing interest in our items. Based on this information, we would like to create a simple recommendation engine that will take the items I1, I2, I3 etc of the current user, search for all other users that had shown interest in those items, and then output the items I4, I5, I6 etc of the other users, sorted by their decreasing popularity. So, basically, the standard "other buyer were also interested in..." functionality.

I''m asking myself what kind of a database is suitable for a realtime recommendation engine like this. My current idea is to build a trie of item IDs, then sort the item IDs of the current user (as the order of items is irrelevant) and to go down the trie; the children of the last trie node will build the needed output.

The problem is that we have 2 million items so that according to our estimation the trie will have at least 1E12 nodes, so that we probably need a distributed sharded database to store it. Before we reinvent the wheel, are there any ready-to-use databases or generally, non-cloud solutions for recommendation engines out there?', 2873, '2014-08-07 22:30:52.913', '8b025320-54a1-4508-97e0-ba23b652abae', 940, 2390, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Database for a trie, or other appropriate structure for recommendation engine', 2873, '2014-08-07 22:30:52.913', '8b025320-54a1-4508-97e0-ba23b652abae', 940, 2391, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><recommendation><databases>', 2873, '2014-08-07 22:30:52.913', '8b025320-54a1-4508-97e0-ba23b652abae', 940, 2392, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('i want to become a **data scientist**. I studied applied **statistics** (actuarial science), so i have a great statistical background (regression, stochastic process, time series, just for mention a few). But now,  I am going to do a master degree in **Computer Science** focus in Intelligent Systems.

Here is my study plan:

 - Machine learning
 - Advanced machine learning
 - Data mining
 - Fuzzy logic
 - Recommendation Systems
 - Distributed Data Systems
 - Cloud Computing
 - Knowledge discovery
 - Business Intelligence
-Information retrieval
-Text mining

At the end, with all my statistical  and  computer science knowledge, can i call myself a data scientist? , or am i wrong?

Thanks for the answers.', 156, '2014-08-07 23:37:20.750', 'a869d554-bba0-4f90-aa21-e3103d79c851', 808, 'Probably the intended indented (sic!) formatting ', 2393, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-07 23:37:20.750', 'a869d554-bba0-4f90-aa21-e3103d79c851', 808, 'Proposed by 156 approved by 434, -1 edit id of 128', 2394, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('i want to become a **data scientist**. I studied applied **statistics** (actuarial science), so i have a great statistical background (regression, stochastic process, time series, just for mention a few). But now,  I am going to do a master degree in **Computer Science** focus in Intelligent Systems.

Here is my study plan:

 - Machine learning
 - Advanced machine learning
 - Data mining
 - Fuzzy logic
 - Recommendation Systems
 - Distributed Data Systems
 - Cloud Computing
 - Knowledge discovery
 - Business Intelligence
 - Information retrieval
 - Text mining

At the end, with all my statistical  and  computer science knowledge, can i call myself a data scientist? , or am i wrong?

Thanks for the answers.', 21, '2014-08-07 23:37:20.750', 'a15450b7-4dc4-4f0d-9095-65ec45f70f7a', 808, 'Probably the intended indented (sic!) formatting ', 2395, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<r><visualization>', 1156, '2014-08-07 23:37:26.890', '16891448-a9ad-43fa-be19-1f2b20619833', 904, 'added more relevant tags', 2396, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-07 23:37:26.890', '16891448-a9ad-43fa-be19-1f2b20619833', 904, 'Proposed by 1156 approved by 434, 21 edit id of 130', 2397, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am attempting to compile code using Knitr in R.

My code below is returning the following error, and causes errors in the rest of the document.

    miss<-sample$sensor_glucose[!is.na(sample$sensor_glucose)]
    # Error: "## Warning: is.na() applied to non-(list or vector) of type ''NULL''"

    str(miss)
    # int [1:103] 213 113 46 268 186 196 187 153 43 175 ...

Does anyone know how to remedy this problem?

Thanks in advance!
', 1333, '2014-08-07 23:37:30.707', 'af692b56-3c84-43e9-b58e-e25f7cc7c84f', 936, 'Formatted the code properly.', 2398, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-07 23:37:30.707', 'af692b56-3c84-43e9-b58e-e25f7cc7c84f', 936, 'Proposed by 1333 approved by 434, 21 edit id of 132', 2399, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<r><statistics><correlation>', 97, '2014-08-07 23:38:37.457', '79d5bbc6-0da2-475c-aa93-8fedd8dc3612', 893, 'Add relevant tag', 2400, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-07 23:38:37.457', '79d5bbc6-0da2-475c-aa93-8fedd8dc3612', 893, 'Proposed by 97 approved by 434, 21 edit id of 131', 2401, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-08-07 23:41:10.580', 'aad6cb9a-f156-4b31-a14d-ec60c3cfaa14', 266, '105', 2402, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I agree with @ssdecontrol that a minimal *reproducible example* would be the most helpful. However, looking at your code (pay attention to the sequence "Error: ... Warning: ..."), I think that the issue you''re experiencing is due to an inappropriate setting of R''s global `warn` **option**. Most likely your current setting is `2`, whereas, you, probably want treat warnings as such, without converting them to errors. If that is the case, you just need to set the option appropriately:

    options(warn=1)  # print warnings as they occur
    options(warn=2)  # treat warnings as errors

**Note for moderators/administrators**: This question seems not to be a *data science* question, but purely an *R* question. Therefore, I think it should be moved to *StackOverflow*, where it belongs.', 2452, '2014-08-08 00:15:05.527', '04ecf21d-9850-4084-9c1f-63d3b534ec15', 941, 2405, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You mentioned regression and PCA in the title, and there is a definite answer for each of those.

The asymptotic complexity of linear regression reduces to O(P^2 * N) if N > P, where P is the number of features and N is the number of observations. More detail in [Computational complexity of least square regression operation][1].

Vanilla PCA is O(P^2 * N + P ^ 3), as in [Fastest PCA algorithm for high-dimensional data][2]. However fast algorithms exist for very large matrices, explained in that answer and [Best PCA Algorithm For Huge Number of Features?][3].

However I don''t think anyone''s compiled a single lit review or reference or  book on the subject. Might not be a bad project for my free time...

  [1]: http://math.stackexchange.com/a/84503/117452
  [2]: http://scicomp.stackexchange.com/q/3220
  [3]: http://stats.stackexchange.com/q/2806/36229', 1156, '2014-08-08 00:25:04.210', '310e96c2-5db1-415a-a661-0d245f9d7091', 942, 2406, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m looking for commercial text summarization tools (APIs, Libraries,...) which are able to perform any of the following tasks:

1. Extractive Multi-Document Summarization (Generic or query-based)
2. Extractive Single-Document Summarization (Generic or query-based)
3. Generative Single-Document Summarization (Generic or query-based)
4. Generative Multi-Document Summarization (Generic or query-based)', 21, '2014-08-08 00:27:05.340', '631c50c2-9531-4352-81d8-9c6e027d502e', 710, 'deleted 47 characters in body', 2407, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I agree with @ssdecontrol that a minimal *reproducible example* would be the most helpful. However, looking at your code (pay attention to the sequence `Error: ... Warning: ...`), I believe that the issue you are experiencing is due to an inappropriate setting of R''s global `warn` **option**. It appears that your current setting is likely `2`, which refers to **converting warnings to errors**, whereas, you, most likely want the setting `1`, which is to treat warnings as such, without converting them to errors. If that is the case, you just need to set the option appropriately:

    options(warn=1)  # print warnings as they occur
    options(warn=2)  # treat warnings as errors

**Note for moderators/administrators**: This question seems not to be a *data science* question, but purely an *R* question. Therefore, I think it should be moved to *StackOverflow*, where it belongs.', 2452, '2014-08-08 03:03:11.517', '68276d65-434f-4481-9d5a-20c00a3d66be', 941, 'Improved wording.', 2409, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In addition to undersampling the majority class (i.e. taking only a few NEW), you may consider oversampling the minority class (in essence, duplicating your OLDs, but there are other smarter ways to do that)

Note that oversampling may lead to overfitting, so pay special attention to testing your classifiers


Check also this answer on CV:

 - http://stats.stackexchange.com/a/108325/49130', 816, '2014-08-08 07:12:52.680', '3a43e641-b838-4d2c-ad0a-555fc04703d7', 944, 2412, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('We have a classification algorithm to categorize Java exceptions in Production.
This algorithm is based on hierarchical human defined rules so when a bunch of text forming an exception comes up, it determines what kind of exception is (development, availability, configuration, etc.) and the responsible component (the most inner component responsible of the exception). In Java an exception can have several causing exceptions, and the whole must be analyzed.

For example, given the following example exception:

    com.myapp.CustomException: Error printing ...
    ... (stack)
    Caused by: com.foo.webservice.RemoteException: Unable to communicate ...
    ... (stack)
    Caused by: com.acme.PrintException: PrintServer002: Timeout ....
    ... (stack)

First of all, our algorithm splits the whole stack in three isolated exceptions. Afterwards it starts analyzing these exceptions starting from the most inner one. In this case, it determines that this exception (the second caused by) is of type `Availability` and that the responsible component is a "print server". This is because there is a rule that matches containing the word `Timeout` associated to the `Availability` type. There is also a rule that matches `com.acme.PrintException` and determines that the responsible component is a print server. As all the information needed is determined using only the most inner exception, the upper exceptions are ignored, but this is not always the case.

As you can see this kind of approximation is very complex (and chaotic) as a human have to create new rules as new exceptions appear. Besides, the new rules have to be compatible with the current ones because a new rule for classifying a new exception must not change the classification of any of the already classified exceptions.

We are thinking about using Machine Learning to automate this process. Obviously, I am not asking for a solution here as I know the complexity but I''d really appreciate some advice to achieve our goal.', 2878, '2014-08-08 10:01:25.400', '04debe7e-53f2-4363-aec1-c33ce6eb6552', 945, 2415, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Classifying Java exceptions', 2878, '2014-08-08 10:01:25.400', '04debe7e-53f2-4363-aec1-c33ce6eb6552', 945, 2416, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><algorithms>', 2878, '2014-08-08 10:01:25.400', '04debe7e-53f2-4363-aec1-c33ce6eb6552', 945, 2417, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-08-08 12:23:02.223', 'cd4bd7f0-35d8-4340-9eaf-82b9b5e471ce', 865, '103', 2418, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":108,"DisplayName":"rapaio"},{"Id":548,"DisplayName":"indico"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-08-08 12:24:27.027', 'bf416bd4-bda0-4744-9107-f1c8a2a7ec9b', 375, '105', 2419, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-08-08 12:26:12.553', 'b58e4bd6-a9a5-4eb4-bad3-7ace40ce50ca', 218, '102', 2420, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<r><error-handling>', 97, '2014-08-08 12:27:15.360', '4a8b34d3-6b7b-4ade-9f43-2624f6e0effa', 936, 'Adding relevant tags.', 2422, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-08 12:27:15.360', '4a8b34d3-6b7b-4ade-9f43-2624f6e0effa', 936, 'Proposed by 97 approved by 21 edit id of 133', 2423, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Cross posting this from Cross Validated:

I''ve seen this question asked before, but I have yet to come across a definitive source answering the specific questions:

* What''s the most appropriate statistical test to apply to a small A/B test?
* What''s the R code and interpretation to analyze a small A/B test?

I''m running a small test to figure out which ads perform better. I have the following results:

**Position 1:**

```variation,impressions,clicks
row-1,753,26
row-3,767 7
```

**Position 2:**

```variation,impressions,clicks
row-1,753,16
row-3,767 13
```

**Position 3:**

```variation,impressions,clicks
row-1,753,2
row-3,767 7
```

I think it''s safe to say these numbers are small and likely to be not normally distributed. Also, it''s click data so there''s a binary outcome of clicked or not and the trials are independent.

**Appropriate test**

In analyzing each position for significance, I think comparison with a binomial or Poisson distribution makes the most sense.

According to [the OpenIntro Stats](http://www.openintro.org/stat/textbook.php?stat_book=os) (and other sources) book, a variable follows a Poisson distribution "... if the event being considered is rare, the population is large, and the events occur independently of each other."

The same source classifies a binomial variable approximately the same way adding that the probability of success is the same and the number of trials is fixed.

I appreciate this is not an either/or decision and analysis can be done using both distributions.

Given A/B (split) testing is a science that has been practiced for several years, I imagine that there is a canonical test. However, looking around the internet, I mostly come across analysis that uses the standard normal distribution. That just seems wrong :)

Is there a canonical test to use for A/B tests with small #''s of clicks?

**Interpretation and R code**

I''ve used the following R code to test significance for each position:

Position 1:


    binom.test(7, 767, p=(26/753))

    Exact binomial test

    data:  7 and 767
    number of successes = 7, number of trials = 767, p-value = 1.077e-05
    alternative hypothesis: true probability of success is not equal to 0.03452855
    95 percent confidence interval:
     0.003676962 0.018713125
    sample estimates:
    probability of success
               0.009126467


I interpret this result to mean: The probability of success in the test group is indeed different than the control group with a 95% confidence interval that the success probability is between .368% and 1.87%

    ppois(((26-1)/753), lambda=(7/767), lower.tail = F)
    [1] 0.009084947

I interpret this result to mean: Given a Poisson distribution with a click rate of 7 per 767 trials, there is a 0.9% chance of having a click rate of 26 or more per 753 trials in the same distribution. Contextualized in the ad example,
there is a .1% chance that the control ad actually performs the same as the test ad.

Is the above interpretation correct? Does the test and interpretation change with the different positions (i.e. are the results of the Poisson test more appropriate for Position 3 given the small numbers)?

', 2883, '2014-08-08 13:44:52.803', 'af09d20e-ecda-445c-9464-b37302b9d6f7', 946, 2424, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Analysis of Split (A/B) tests using Poisson and/or Binomial Distribution', 2883, '2014-08-08 13:44:52.803', 'af09d20e-ecda-445c-9464-b37302b9d6f7', 946, 2425, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r>', 2883, '2014-08-08 13:44:52.803', 'af09d20e-ecda-445c-9464-b37302b9d6f7', 946, 2426, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First of all, some basics of classification (and in general any supervised ML tasks), just to make sure we have same set of concepts in mind.

Any supervised ML algorithm consists of at least 2 components:

 1. Dataset to train and test on.
 2. Algorithm(s) to handle these data.

Training dataset consists of a set of pairs `(x, y)`, where `x` is a **vector of features** and `y` is **predicted variable**. Predicted variable is just what you want to know, i.e. in your case it is exception type. Features are more tricky. You cannot just throw raw text into an algorithm, you need to extract meaningful parts of it and organize them as feature vectors first. You''ve already mentioned a couple of useful features - exception class name (e.g. `com.acme.PrintException`) and contained words ("Timeout"). All you need is to translate your row exceptions (and human-categorized exception types) into suitable dataset, e.g.:

    ex_class                  contains_timeout  ...   | ex_type
    -----------------------------------------------------------
    [com.acme.PrintException, 1                , ...] | Availability
    [java.lang.Exception    , 0                , ...] | Network
     ...

This representation is already much better for ML algorithms. But which one to take?

Taking into account nature of the task and your current approach natural choice is to use **decision trees**. This class of algorithms will compute optimal decision criteria for all your exception types and print out resulting tree. This is especially useful, because you will have possibility to manually inspect how decision is made and see how much it corresponds to your manually-crafted rules.

There''s, however, possibility that some exceptions with exactly the same features will belong to different exception types. In this case probabilistic approach may work well. Despite its name, **Naive Bayes** classifier works pretty well in most cases. There''s one issue with NB and our dataset representation, though: dataset contains *categorical* variables, and Naive Bayes can work with *numerical* attributes only*. Standard way to overcome this problem is to use [dummy variables](http://en.wikipedia.org/wiki/Dummy_variable_%28statistics%29). In short, dummy variables are binary variables that simply indicate whether specific category presents or not. For example, single variable `ex_class` with values `{com.acme.PrintException, java.lang.Exception, ...}`, etc. may be split into several variables `ex_class_printexception`, `ex_class_exception`, etc. with values `{0, 1}`:

    ex_class_printexception  ex_class_exception  contains_timeout | ex_type
    -----------------------------------------------------------------------
    [1,                    , 0                 , 1              ] | Availability
    [0,                    , 1                 , 0              ] | Network

One last algorithm to try is **Support Vector Machines (SVM)**. It neither provides helpful visualisation, nor is probabilistic, but often gives superior results.

------

*** - in fact, neither Bayes theorem, nor Naive Bayes itself state anything about variable type, but most software packages that come to mind rely on numerical features. ', 1279, '2014-08-08 13:53:36.847', '0895c081-a81f-4084-a6b5-e9970b1c789f', 947, 2427, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have found a number of libraries and tools for data science in Scala, I would like to know about which one has more adoption and which one is gaining adoption at a faster pace and to what extent this is the case. Basically, which one should I bet for (if any at this point).

Some of the libraries I''ve found are (in no particular order):

* Scalding
* Breeze
* Spark
* Saddle
* H2O
* Spire
* Mahout
* Hadoop
* MongoDB

If I need to be more specific to make the question answerable: I''m not particularly interested in clusters and Big Data at this moment, but I''m interested in sizable data (up to 100 GB) and predictive analytics.', 1281, '2014-08-08 16:49:13.850', 'ce3382f9-9efb-43b3-9c93-37abedc0a91c', 948, 2428, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Any clear winner for Data Science in Scala?', 1281, '2014-08-08 16:49:13.850', 'ce3382f9-9efb-43b3-9c93-37abedc0a91c', 948, 2429, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<tools>', 1281, '2014-08-08 16:49:13.850', 'ce3382f9-9efb-43b3-9c93-37abedc0a91c', 948, 2430, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have found a number of libraries and tools for data science in Scala, I would like to know about which one has more adoption and which one is gaining adoption at a faster pace and to what extent this is the case. Basically, which one should I bet for (if any at this point).

Some of the tools I''ve found are (in no particular order):

* Scalding
* Breeze
* Spark
* Saddle
* H2O
* Spire
* Mahout
* Hadoop
* MongoDB

If I need to be more specific to make the question answerable: I''m not particularly interested in clusters and Big Data at this moment, but I''m interested in sizable data (up to 100 GB) for information integration and predictive analytics.', 1281, '2014-08-08 17:00:50.270', '10018936-7f45-44a5-90ec-aa771a13340c', 948, 'added 24 characters in body', 2431, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am facing this bizarre issue while using `Apache Pig` **rank** utility. I am executing the following code:

    email_id_ranked = rank email_id;
    store email_id_ranked into ''/tmp/'';

So, basically I am trying to get the following result

    1,email1
    2,email2
    3,email3
    ...

Issue is sometime pig dumps the above result but sometimes it dumps only the emails without the rank. Also when I dump the data on screen using `dump` function pig returns both the columns. I don''t know where the issue is. Kindly advice.

Please let me know if you need any more information. Thanks in advance.

**Pig version: Apache Pig version 0.11.0-cdh4.6.0**', 2433, '2014-08-08 17:32:48.377', '4a411501-3693-42d5-86ac-a8042eeffd72', 949, 2432, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Pig Rank function not generating rank in output', 2433, '2014-08-08 17:32:48.377', '4a411501-3693-42d5-86ac-a8042eeffd72', 949, 2433, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop><apache-pig><pig>', 2433, '2014-08-08 17:32:48.377', '4a411501-3693-42d5-86ac-a8042eeffd72', 949, 2434, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The approximation Binomial(k,n,p) ~= Poisson(k,s) (where s = n*p) can be shown under the assumptions: <br/>
1) n >> k (to say that n!/(n-k)! ~= n^k), <br/>
2) p <<1 (to say that (1-p)^(n-k) ~= (1-p)^n). <br/>
It''s up to you whether those are sufficiently satisfied. If the exact calculation can be done quickly, in my opinion, it''s nice to stay with that.

Also since, if the probability of row 3 sample is different from the row 1 sample, it would almost certainly be on the lower side. It would probably best for you to use <br/>
binom.test(7, 767, p=(26/753), alternative=''less'')
the final option indicating that the alternative to your null hypothesis is that the probability is less than 26/753, not equal to. Of course, that''s simply just the sum of Binomial probabilities from 0 to 7 (you can check yourself), the interpretation being that *this* is the probability of having gotten at most 7 rolls from random chance, if the probability truly was 26/753.

Keep in mind the interpretation of that last sentence. These kinds of tests are generally used when we _know_ what the inherent probability is that we''re comparing to (e.g. to see if the set of coin flips has a probability significantly different from 1/2 which is what we expect from a fair coin). In this case, we don''t _know_ what the probability is that we''re comparing to, we''re just making the very crude guess that the 26/753 outcome of row 1 reflects the true probability. It''s better than a regular Normal t-test in this case, but don''t put _too_ much stock in it unless you have a much higher sample size for row 1.', 2841, '2014-08-08 19:23:33.637', '6d6879be-3505-41c8-bc0d-35a750a736c9', 950, 2435, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Not sure anybody have worked with _all_ these tools, so I''m going to share my experience with some of them and let others share their experience with the others.

**MongoDB** addresses problems that involve heterogeneous and nested objects, while data mining mostly work with simple tabular data. MongoDB is neither fast with this type of data, nor provide any advanced tools for analysis (correct me if you know any). So I can think of a very few applications for Mongo in data mining.

**Hadoop** is a large ecosystem, containing dozens of different tools. I will assume that you mean core Hadoop features - HDFS and MapReduce. HDFS provides flexible way to store large amounts of data, while MapReduce gives basis for processing them. It has its clear advantages for processing multi-terabyte datasets, but it also has significant drawbacks. In particular, because of intensive disk IO during MapReduce tasks (that slows down computations a lot) it is terrible for interactive development, iterative algorithms and working with not-that-big datasets. For more details see my [earlier answer](http://datascience.stackexchange.com/a/863/1279).

Many algorithms in Hadoop require multiple MapReduce jobs with complicated data flow. This is where **Scalding** gets shiny. Scalding (and underlying Java''s [Cascading](http://www.cascading.org/)) provides much simpler API, but at the moment uses same MapReduce as its runtime and thus holds all the same issues.

**Spark** addresses exactly these issues. It drops Hadoop''s MapReduce and offers completely new computational framework based on distributed in-memory collections and delayed evaluations. Its API is somewhat similar to Scalding''s with all MR complexity removed, so it''s really easy to get started with it. Spark is also the first in this list that comes with data mining library - [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html).

But Spark doesn''t reinvent things like basic linear algebra. For this purpose it uses **Breeze**. To my opinion, Breeze is far in quality from scientific packages like SciPy, Octave or Julia, but it is still good enough for most practical use cases.

-----

**Mahout** relies on Hadoop''s MapReduce and thus is terrible for iterative algorithms. **Spire** and **Saddle** look cute and probably have their niche, but seem to be much less well-known than Breeze. I couldn''t find much information about **H2O**, so it doesn''t look like a big player here (comments from people who used it are welcome).

-------

Some quick summary. Spark seems to be the most simple, flexible and fast-growing project for large-scale data processing. It facilitates a number of new projects (e.g. Shark or Spark SQL) and penetrates into existing (including [Cascading](http://www.concurrentinc.com/2014/05/cascading-now-supports-tez-spark-and-storm-up-next/) and [Mahout](https://mahout.apache.org/users/sparkbindings/home.html)). Spark knows how to utilize HDFS API, and thus scales to terabytes of data easily. And for data mining downshifters who don''t want to bother with a cluster setup there''s always pure Breeze.
', 1279, '2014-08-08 20:46:58.493', '814fe49e-6599-44bb-8eb4-19028d3ffffc', 951, 2436, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":62,"DisplayName":"AsheeshR"}]}', 62, '2014-08-09 01:26:26.703', 'b766d731-d62e-4120-8c15-378f9627c972', 948, '104', 2440, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Not sure anybody have worked with _all_ these tools, so I''m going to share my experience with some of them and let others share their experience with the others.

**MongoDB** addresses problems that involve heterogeneous and nested objects, while data mining mostly works with simple tabular data. MongoDB is neither fast with this type of data, nor provide any advanced tools for analysis (correct me if you know any). So I can think of a very few applications for Mongo in data mining.

**Hadoop** is a large ecosystem, containing dozens of different tools. I will assume that you mean core Hadoop features - HDFS and MapReduce. HDFS provides flexible way to store large amounts of data, while MapReduce gives basis for processing them. It has its clear advantages for processing multi-terabyte datasets, but it also has significant drawbacks. In particular, because of intensive disk IO during MapReduce tasks (that slows down computations a lot) it is terrible for interactive development, iterative algorithms and working with not-that-big datasets. For more details see my [earlier answer](http://datascience.stackexchange.com/a/863/1279).

Many algorithms in Hadoop require multiple MapReduce jobs with complicated data flow. This is where **Scalding** gets shiny. Scalding (and underlying Java''s [Cascading](http://www.cascading.org/)) provides much simpler API, but at the moment uses same MapReduce as its runtime and thus holds all the same issues.

**Spark** addresses exactly these issues. It drops Hadoop''s MapReduce and offers completely new computational framework based on distributed in-memory collections and delayed evaluations. Its API is somewhat similar to Scalding''s with all MR complexity removed, so it''s really easy to get started with it. Spark is also the first in this list that comes with data mining library - [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html).

But Spark doesn''t reinvent things like basic linear algebra. For this purpose it uses **Breeze**. To my opinion, Breeze is far in quality from scientific packages like SciPy, Octave or Julia, but it is still good enough for most practical use cases.

-----

**Mahout** relies on Hadoop''s MapReduce and thus is terrible for iterative algorithms. **Spire** and **Saddle** look cute and probably have their niche, but seem to be much less well-known than Breeze. I couldn''t find much information about **H2O**, so it doesn''t look like a big player here (comments from people who used it are welcome).

-------

Some quick summary. Spark seems to be the most simple, flexible and fast-growing project for large-scale data processing. It facilitates a number of new projects (e.g. Shark or Spark SQL) and penetrates into existing (including [Cascading](http://www.concurrentinc.com/2014/05/cascading-now-supports-tez-spark-and-storm-up-next/) and [Mahout](https://mahout.apache.org/users/sparkbindings/home.html)). Spark knows how to utilize HDFS API, and thus scales to terabytes of data easily. And for data mining downshifters who don''t want to bother with a cluster setup there''s always pure Breeze.
', 1279, '2014-08-09 21:25:58.250', '46cfb9a8-b663-4976-8424-e28e39fe1e35', 951, 'added 1 character in body', 2441, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I need to do coreference resolution for German texts and I plan to use OpenNLP to perform this task.

As far as I know OpenNLP coreference resolution does not support the German language.

Which components/data do I need to adapt the code such that it is possible to perform coreference resolution for German texts?

', 979, '2014-08-11 07:59:22.780', '0318cf90-b57d-4b98-86d4-929c289d1f03', 954, 2451, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('OpenNLP Coreference Resolution (German)', 979, '2014-08-11 07:59:22.780', '0318cf90-b57d-4b98-86d4-929c289d1f03', 954, 2452, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><text-mining>', 979, '2014-08-11 07:59:22.780', '0318cf90-b57d-4b98-86d4-929c289d1f03', 954, 2453, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('does anybofy know a libarary for performing coreference resolution on German texts?', 979, '2014-08-11 12:25:47.700', 'c482766b-cc60-471d-932b-fc876b3c87c2', 955, 2454, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Coreference Resolution for German Texts', 979, '2014-08-11 12:25:47.700', 'c482766b-cc60-471d-932b-fc876b3c87c2', 955, 2455, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp>', 979, '2014-08-11 12:25:47.700', 'c482766b-cc60-471d-932b-fc876b3c87c2', 955, 2456, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I had to create a web based dashboard. My main charting tool was d3js. But I needed to use ggplot2 to generate few charts. Through d3js''s ggplot2 extension, I could create the same. If your charts can be generated through existing extension, then web has better alternatives. Later you can export them to PDF for distribution. ', 2910, '2014-08-11 12:48:52.320', '1629b4fd-5ee1-46d1-a583-a4ffb3a715c5', 956, 2457, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I know that ARIMA can''t detect multiple seasonality, but it is possible to [use fourier functions to add a second seasonality](http://robjhyndman.com/hyndsight/dailydata/).

I need to forecast gas consumption composed by a daily, weekly (week days-weekend), yearly seasonality. Does it make sense to apply three times the [STL decomposition by LOESS](http://stat.ethz.ch/R-manual/R-devel/library/stats/html/stl.html)? The reason is that I applied the fourier method and I have bad results but I don''t know if it is only because I applied it wrong.

I''m interested in the theoretical explanation, but here you find also the code:

ARIMA + 2 STL:

    b <- ts(drop(coredata(dat.ts)), deltat=1/12/30/24, start=1)
    fit <- stl(b, s.window="periodic")
    b <- seasadj(fit)
    dat.ts <- xts(b, index(dat.ts))

    # The weekdays are extracted
    dat.weekdays <- dat.ts[.indexwday(dat.ts) %in% 1:5]
    dat.weekdaysTS <- ts(drop(coredata(dat.weekdays)), frequency=24, start=1)
    fit <- stl(dat.weekdaysTS, s.window="periodic")
    dat.weekdaysTS <- seasadj(fit)

    arima <- Arima(dat.weekdaysTS, order=c(3,0,5))


With fourier:

    dat.weekdays <- dat.ts[.indexwday(dat.ts) %in% 1:5]
    dat.weekdaysTS <- ts(drop(coredata(dat.weekdays)), frequency=24, start=1)
    z <- fourier(ts(dat.weekdaysTS, frequency=365.25), K=5)
    arima <- Arima(dat.weekdaysTS, order=c(3,0,5),xreg=z)
', 989, '2014-08-11 15:19:02.047', '62fac958-01d5-489c-8faf-16e47a941f69', 957, 2458, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Multiple seasonality with ARIMA?', 989, '2014-08-11 15:19:02.047', '62fac958-01d5-489c-8faf-16e47a941f69', 957, 2459, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<time-series>', 989, '2014-08-11 15:19:02.047', '62fac958-01d5-489c-8faf-16e47a941f69', 957, 2460, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Have a look at [Apache Mahout][1]. Last version features also user-item-based recommenders.


  [1]: https://mahout.apache.org/', 2912, '2014-08-11 16:05:02.197', '291bca1c-3e0b-46f8-91e7-f047aaad63ed', 958, 2461, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('While I am not a data scientist, I am an epidemiologist working in a clinical setting. Your research question did not specify a time period (ie odds of developing CKD in 1 year, 10 years, lifetime?).

Generally, I would go through a number of steps before even thinking about modeling (univariate analysis, bivariate analysis, colinearity checks, etc). However, the most commonly used method for trying to predict a binary event (using continuous OR binary variables) is logistic regression. If you wanted to look at CKD as a lab value (urine albumin, eGFR) you would use linear regression (continuous outcome).

While the methods used should be informed by your data and questions, clinicians are used to seeing odds ratios and risk ratios as these the most commonly reported measures of association in medical journals such as NEJM and JAMA.

If you are working on this problem from a human health perspective (as opposed to Business Intelligence) this Steyerberg''s [Clinical Prediction Models][1] is an excellent resource.


  [1]: http://www.springer.com/medicine/internal/book/978-0-387-77243-1', 2913, '2014-08-11 18:33:37.023', 'c9396a26-9548-4bc8-aca2-204c820de71e', 959, 2463, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have to agree that k-fold should do "just" fine. However, there is a nice article about the "Bootstrap .632+" method (basically a smoothened cross validation) that is supposed to be superior (however, they did the comparisons on not-binary data as far as I can tell)

Maybe you want to check out this article here: http://www.jstor.org/stable/2965703', 2556, '2014-08-11 19:21:50.137', '65c1ebcd-3dde-4f08-87ad-40afed9c1cb1', 960, 2464, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**(question unfinished: pressed Enter too early, wait for a while, please)**

![enter image description here][1]

These are 4 different weight matrices that I got after training **RBM** with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though _none_ of weights are _exactly_ the same.

I can overcome this by increasing number of weight vectors to 256 or more. In this case I get something like this:







  [1]: http://i.stack.imgur.com/lBxL5.png', 1279, '2014-08-11 21:13:36.230', '70ffe0e7-9f27-4f1d-bc67-5742491790fa', 961, 2465, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why RBM tends to learn very similar weights?', 1279, '2014-08-11 21:13:36.230', '70ffe0e7-9f27-4f1d-bc67-5742491790fa', 961, 2466, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<deep-learning>', 1279, '2014-08-11 21:13:36.230', '70ffe0e7-9f27-4f1d-bc67-5742491790fa', 961, 2467, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**(question unfinished: pressed Enter too early, wait for a while, please)**

![enter image description here][1]

These are 4 different weight matrices that I got after training **RBM** with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though _none_ of weights are _exactly_ the same.

I can overcome this by increasing number of weight vectors to 256 or more. In this case I get something like this:



But I encountered this problem several times earlier with different RBM types (binary, Gaussian, even convolutional), different number of hidden units (even pretty large), different hyper-parameters, etc.

My question is: what is the **most likely reason** for weights to get **very similar values**? Do they all just get to some local minimum? Or is it a sign of overfitting?

I currently use binary-Gaussian RBM, code may be found [here](https://github.com/faithlessfriend/Milk.jl/blob/master/src/rbm.jl).


  [1]: http://i.stack.imgur.com/lBxL5.png', 1279, '2014-08-11 21:33:34.637', '61c1e27b-1bc0-4686-b5fb-619d4433b43a', 961, 'added 512 characters in body; edited tags', 2468, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<untagged>', 1279, '2014-08-11 21:33:34.637', '61c1e27b-1bc0-4686-b5fb-619d4433b43a', 961, 'added 512 characters in body; edited tags', 2469, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('![enter image description here][1]

These are 4 different weight matrices that I got after training **RBM** with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though _none_ of weights are _exactly_ the same.

I can overcome this by increasing number of weight vectors to 512 or more. But I encountered this problem several times earlier with different RBM types (binary, Gaussian, even convolutional), different number of hidden units (including pretty large), different hyper-parameters, etc.

My question is: what is the **most likely reason** for weights to get **very similar values**? Do they all just get to some local minimum? Or is it a sign of overfitting?

I currently use binary-Gaussian RBM, code may be found [here](https://github.com/faithlessfriend/Milk.jl/blob/master/src/rbm.jl).


  [1]: http://i.stack.imgur.com/lBxL5.png', 1279, '2014-08-11 22:03:26.183', '227d1d1b-0856-4b81-af90-b310c8cf5e2a', 961, 'added 512 characters in body; edited tags', 2470, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I know the difference between cluster and classification in machine learning.
But I don''t know what is difference between text classification and topic models in documents
Also can I use topic model for the documents to identify one topic later on can I use the classification to classify the text inside this documents ?  ', 2916, '2014-08-12 03:50:52.303', '855053f7-1905-49c1-a652-e5982d196595', 962, 2471, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('what is difference between text classification and topic models?', 2916, '2014-08-12 03:50:52.303', '855053f7-1905-49c1-a652-e5982d196595', 962, 2472, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><text-mining><topic-model>', 2916, '2014-08-12 03:50:52.303', '855053f7-1905-49c1-a652-e5982d196595', 962, 2473, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For experimenting we''d like to use the [Emoji][1] embedded in many Tweets as a ground truth/training data for simple quantitative senitment analysis. Tweets are usually too unstructured for NLP to work well.

Anyway, there are 722 Emoji in Unicode 6.0, and probably another 250 will be added in Unicode 7.0.

Is there a database (like e.g. SentiWordNet) that contains sentiment annotations for them?

Also, if you have experience with using them for sentiment analysis, I''d be interested to hear.

  [1]: https://en.wikipedia.org/wiki/Emoji', 2920, '2014-08-12 07:57:03.283', '4e3682db-e672-4ad3-98cf-f4b86f55f6e4', 963, 2474, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Sentiment data for Emoji', 2920, '2014-08-12 07:57:03.283', '4e3682db-e672-4ad3-98cf-f4b86f55f6e4', 963, 2475, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><parsing>', 2920, '2014-08-12 07:57:03.283', '4e3682db-e672-4ad3-98cf-f4b86f55f6e4', 963, 2476, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Topic models are usually **unsupervised**. There are "supervised topic models", too; but even then they try to model **topics within a classes**.

E.g. you may have a class "football", but there may be topics inside this class that relate to particular matches or teams.

The challenge with topics is that they change over time; consider the matches example above. Such topics may emerge, and disappear again.', 2920, '2014-08-12 07:59:13.060', 'bdcc1aa7-dd81-4b1e-a8f6-8d07fad0993d', 964, 2477, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m looking for a product that allows us to take in a collection of datastreams, and then after some event, will find any data that changes or correlates with that event. (For example, having a headache, and identifying that I drank too much beer last night and didn''t drink enough water)', 2923, '2014-08-12 08:54:34.903', '35f6e90c-cab8-4ed6-9721-a98e7a5a8438', 965, 2478, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Tool for finding correlations between data after some event', 2923, '2014-08-12 08:54:34.903', '35f6e90c-cab8-4ed6-9721-a98e7a5a8438', 965, 2479, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<correlation>', 2923, '2014-08-12 08:54:34.903', '35f6e90c-cab8-4ed6-9721-a98e7a5a8438', 965, 2480, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Text Classification**

I give you a bunch of documents, each of which has a label attached. I ask you to learn why you think the contents of the documents have been given these labels based on their words. Then I give you new documents and ask what you think the label for each one should be. The labels have meaning to me, not to you necessarily.

**Topic Modeling**

I give you a bunch of documents, without labels. I ask you to explain why the documents have the words they do by identifying some topics that each is "about". You tell me the topics, by telling me how much of each is in each document, and I decide what the topics "mean" if anything.

You''d have to clarify what you me by "identify one topic" or "classify the text".', 21, '2014-08-12 09:52:01.153', '3e1bff8e-2fef-4893-9d93-1e3930f316ae', 966, 2481, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('does anybody know a libarary for performing coreference resolution on German texts?

As far as I know OpenNLP and Standord NLP are not able to perform coreference resolution for German Texts.

The only tool that I know is [CorZu][1] which is a python library.


  [1]: http://www.cl.uzh.ch/static/news.php?om=view&nid=163', 979, '2014-08-12 11:00:13.273', '0237e5b0-4ba8-49cb-a8b8-e1e741e03034', 955, 'added 245 characters in body', 2482, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Total of 972 emoji is not really that big not to be able to label them manually, but I doubt that they will work as a good ground truth. Sources like Twitter are full of irony, sarcasm and other tricky settings where emotional symbols (such as emoji or emoticon) mean something different from normal interpretation. For example, someone may write "xxx cheated their clients, and now they are cheated themselves! ha ha ha! :D". This is definitely negative comment, but author is glad to see xxx company in trouble and thus adds positive emoticon. These cases are not that frequent, but definitely not suitable for ground truth.

Much more common approach is to use emoticon as a **seed for collecting actual data set**. For example, in [this paper](http://www.saifmohammad.com/WebDocs/NRC-Sentiment-JAIR-2014.pdf) authors use emoticon and emotional hash tags to grab lexicon of words useful for further classification. ', 1279, '2014-08-12 12:40:44.907', '4e6c08da-f134-48fe-b648-6f87fe811c0a', 967, 2483, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For experimenting we''d like to use the [Emoji][1] embedded in many Tweets as a ground truth/training data for simple quantitative senitment analysis. Tweets are usually too unstructured for NLP to work well.

Anyway, there are 722 Emoji in Unicode 6.0, and probably another 250 will be added in Unicode 7.0.

Is there a database (like e.g. SentiWordNet) that contains sentiment annotations for them?

(Note that SentiWordNet does allow for *ambiguous* meanings, too. Consider e.g. [funny][2], which is not just positive: "this tastes funny" is probably not positive... same will hold for `;-)` for example. But I don''t think this is harder for Emoji than it is for regular words...)

Also, if you have experience with using them for sentiment analysis, I''d be interested to hear.


  [1]: https://en.wikipedia.org/wiki/Emoji
  [2]: http://sentiwordnet.isti.cnr.it/search.php?q=funny', 2920, '2014-08-12 16:06:24.710', '1f17df8c-2e75-4732-8f67-7c95c315769b', 963, 'added 346 characters in body', 2484, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Does anyone know what (from your experience) is the best open source natural language generators (NLG) out there? What are the relative merits of each?

I''m looking to do sophisticated text summarization and would like to use theme extraction/semantic modeling in conjunction with NLG tools to create accurate, context-aware, and natural-sounding text summaries.', 84, '2014-08-12 16:52:12.833', 'fc9ac128-cefe-4bd4-86be-fba2b0cb3a35', 867, 'Minor formatting changes.', 2490, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am using the `stepAIC` function in R to do a bi-directional (forward and backward) stepwise regression. I do not understand what each return value from the function means.

The output is:


              Df     Sum of Sq    RSS       AIC
    <none>                        350.71   -5406.0
    - aaa      1     0.283        350.99   -5405.9
    - bbb      1     0.339        351.05   -5405.4
    - ccc      1     0.982        351.69   -5400.5
    - ddd      1     0.989        351.70   -5400.5


Question 1) What do each of the return values `Df`, `Sum of Sq`, `RSS`, and `AIC` mean? What are their ranges? Do I want each value to be close to the min or max of its theoretical range?

Question 2) Are the values listed under `Df`, `Sum of Sq`, `RSS`, and `AIC` the values for a model where only one variable would be considered as the independent variable (i.e. y ~
aaa, y ~ bbb, etc.)?

Question 3) Why is AIC sometimes negative / positive? ', 2928, '2014-08-12 17:11:45.447', '276c1667-02a5-4db8-a07d-a399ff4c01d8', 968, 2491, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Understanding output setAIC', 2928, '2014-08-12 17:11:45.447', '276c1667-02a5-4db8-a07d-a399ff4c01d8', 968, 2492, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><r><feature-selection>', 2928, '2014-08-12 17:11:45.447', '276c1667-02a5-4db8-a07d-a399ff4c01d8', 968, 2493, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Understanding output stepAIC', 2928, '2014-08-12 18:10:24.237', 'ba8fbd96-47c2-42de-a3f7-85d98b826438', 968, 'edited title', 2494, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am using the `stepAIC` function in R to do a bi-directional (forward and backward) stepwise regression. I do not understand what each return value from the function means.

The output is:


              Df     Sum of Sq    RSS       AIC
    <none>                        350.71   -5406.0
    - aaa      1     0.283        350.99   -5405.9
    - bbb      1     0.339        351.05   -5405.4
    - ccc      1     0.982        351.69   -5400.5
    - ddd      1     0.989        351.70   -5400.5



Question Are the values listed under `Df`, `Sum of Sq`, `RSS`, and `AIC` the values for a model where only one variable would be considered as the independent variable (i.e. y ~
aaa, y ~ bbb, etc.)?

', 2928, '2014-08-12 18:36:08.880', '72d0b292-30a7-43f4-abce-610538da18b0', 968, 'deleted 248 characters in body', 2495, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('When I started with artificial neural networks (NN) I thought I''d have to fight overfitting as the main problem. But in practice I can''t even get my NN to pass the 20% error rate barrier. I can''t even beat my score on random forest!

I''m seeking some very general or not so general advice on what should one do to make a NN start capturing trends in data.

For implementing NN I use Theano Stacked Auto Encoder with [the code from tutorial][1] that works great (less than 5% error rate) for classifying the MNIST dataset. It is a multilayer perceptron, with softmax layer on top with each hidden later being pre-trained as autoencoder (fully described at [tutorial][2], chapter 8). There are ~50 input features and ~10 output classes. The NN has sigmoid neurons and all data are normalized to [0,1]. I tried lots of different configurations: number of hidden layers and neurons in them (100->100->100, 60->60->60, 60->30->15, etc.), different learning and pre-train rates, etc.

And the best thing I can get is a 20% error rate on the validation set and a 40% error rate on the test set.

On the other hand, when I try to use Random Forest (from scikit-learn) I easily get a 12% error rate on the validation set and 25%(!) on the test set.

How can it be that my deep NN with pre-training behaves so badly? What should I try?

  [1]: https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/SdA.py
  [2]: http://deeplearning.net/tutorial/deeplearning.pdf', 322, '2014-08-13 08:36:04.350', '912d42a6-d13a-40be-9ee7-c758105583ca', 731, 'polished grammar', 2496, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to fight underfitting in a deep neural net', 322, '2014-08-13 08:36:04.350', '912d42a6-d13a-40be-9ee7-c758105583ca', 731, 'polished grammar', 2497, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-13 08:36:04.350', '912d42a6-d13a-40be-9ee7-c758105583ca', 731, 'Proposed by 322 approved by 2452, 2471 edit id of 134', 2498, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('After consulting with someone I found out that the <none> corresponds to a model that would include all the variables, in other words none of the variables were removed. So consider the line in the output for the variable aaa. The listed RSS and AIC are the values for a model that would include all variables but aaa and we see an increase in the RSS and AIC. The other listed results can be considered in the same fashion. The best model is then the one where none are removed since this has the smallest AIC. ', 2928, '2014-08-13 14:18:58.917', '4943012a-36a6-4a3b-aa19-a157183d2403', 969, 2499, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('While finding frequent subgraphs in single large graph, subgraph isomorphism (test) is not considered because its not anti-monotone. How and why subgraph isomorphism is not anti-monotone ?', 2948, '2014-08-13 20:30:18.143', 'c705b315-8623-48c3-9d3f-bd8c69bdcf9e', 972, 2504, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Subgraph isomorphism and Anti-monotone property', 2948, '2014-08-13 20:30:18.143', 'c705b315-8623-48c3-9d3f-bd8c69bdcf9e', 972, 2505, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><data-mining><dataset><graphs><similarity>', 2948, '2014-08-13 20:30:18.143', 'c705b315-8623-48c3-9d3f-bd8c69bdcf9e', 972, 2506, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I like to find the feature weights in a [structured SVM][1] for ranking the features w.r.t. importance. I know that in a binary SVM the weight vector can be written as a [linear combination of examples][2]. But how do you compute the same for an SSVM?


  [1]: http://en.wikipedia.org/wiki/Structured_support_vector_machine
  [2]: http://pyml.sourceforge.net/doc/howto.pdf', 2949, '2014-08-13 20:53:15.443', '3c6af354-bd75-4f76-b64e-b3ee7d47ffb5', 973, 2507, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('feature weights in structured support vector machine', 2949, '2014-08-13 20:53:15.443', '3c6af354-bd75-4f76-b64e-b3ee7d47ffb5', 973, 2508, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<svm>', 2949, '2014-08-13 20:53:15.443', '3c6af354-bd75-4f76-b64e-b3ee7d47ffb5', 973, 2509, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a big sparse matrix of users and items they like (in the order of 1M users and 100K items, with a very low level of sparsity). I''m exploring ways in which I could perform kNN search on it. Given the size of my dataset and some initial tests I performed, my assumption is that the method I will use will need to be either parallel or distributed. So I''m considering two classes of possible solutions: one that is either available (or implementable in a reasonably easy way) on a single multicore machine, the other on a Spark cluster, i.e. as a MapReduce program. Here are three broad ideas that I considered:

* Assuming a cosine similarity metric, perform the full multiplication of the normalized matrix by its transpose (implemented as a sum of outer products)
* Using locality-sensitive hashing (LSH)
* Reducing first the dimensionality of the problem with a PCA

I''d appreciate any thoughts or advices about possible other ways in which I could tackle this problem.
', 1242, '2014-08-14 00:50:51.103', '6484f2e7-14f0-4235-8883-a701ab9e7fc9', 974, 2510, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Nearest neighbors search for very high dimensional data', 1242, '2014-08-14 00:50:51.103', '6484f2e7-14f0-4235-8883-a701ab9e7fc9', 974, 2511, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><distributed><map-reduce><dimensionality-reduction>', 1242, '2014-08-14 00:50:51.103', '6484f2e7-14f0-4235-8883-a701ab9e7fc9', 974, 2512, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m not an expert on this topic - just interested. Therefore, I can''t offer you a specific advice. However, I hope that you might get additional **ideas** toward solving your problem from the following **resources**:

1) Research paper **"Efficient K-Nearest Neighbor Join Algorithms for High Dimensional Sparse Data"**: http://arxiv.org/abs/1011.2807

2) Class project paper **"Recommendation System Based on Collaborative Filtering"** (Stanford University): http://cs229.stanford.edu/proj2008/Wen-RecommendationSystemBasedOnCollaborativeFiltering.pdf

3) Project for the **Netflix Prize Competition (*k-NN*-based)**: http://cs.carleton.edu/cs_comps/0910/netflixprize/final_results/knn/index.html

4) Research paper **"Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data"** on the *curse of dimensionality* phenomenon and its relation to *machine learning*, in general, and *k-NN algorithm*, in particular: http://jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf

5) **Software for sparse k-NN classification** (free, but appears not to be open source - might clarify with authors): http://www.autonlab.org/autonweb/10408.html

6) Several **discussion threads** on *StackOverflow*:

  + http://stackoverflow.com/questions/20333092/knn-with-big-sparse-matrices-in-python
  + http://stackoverflow.com/questions/18164348/efficient-nearest-neighbour-search-for-sparse-matrices
  + http://stackoverflow.com/questions/21085990/scipy-sparse-distance-matrix-scikit-or-scipy
  + http://stackoverflow.com/questions/10472681/handling-incomplete-data-data-sparsity-in-knn
  + http://stackoverflow.com/questions/5560218/computing-sparse-pairwise-distance-matrix-in-r (unlike all previous discussions, which refer to `Python`, this one refers to `R` ecosystem)

7) Pay attention to *GraphLab*, an open source **parallel framework for machine learning** (http://select.cs.cmu.edu/code/graphlab), which supports *parallel clustering* via `MapReduce`-like features: http://select.cs.cmu.edu/code/graphlab/clustering.html

You might also check my answer here on Data Science StackExchange on *sparse regression* for links to relevant `R` packages and `CRAN Task View` pages: http://datascience.stackexchange.com/a/918/2452.', 2452, '2014-08-14 04:28:36.933', '93839bca-fc6f-483e-9773-d4a244cd53d0', 975, 2514, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m not an expert on this topic - just interested. Therefore, I can''t offer you a specific advice. However, I hope that you might get additional **ideas** toward solving your problem from the following **resources**:

1) Research paper **"Efficient K-Nearest Neighbor Join Algorithms for High Dimensional Sparse Data"**: http://arxiv.org/abs/1011.2807

2) Class project paper **"Recommendation System Based on Collaborative Filtering"** (Stanford University): http://cs229.stanford.edu/proj2008/Wen-RecommendationSystemBasedOnCollaborativeFiltering.pdf

3) Project for the **Netflix Prize Competition (*k-NN*-based)**: http://cs.carleton.edu/cs_comps/0910/netflixprize/final_results/knn/index.html

4) Research paper **"Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data"** on the *curse of dimensionality* phenomenon and its relation to *machine learning*, in general, and *k-NN algorithm*, in particular: http://jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf

5) **Software for sparse k-NN classification** (free, but appears not to be open source - might clarify with authors): http://www.autonlab.org/autonweb/10408.html

6) Several **discussion threads** on *StackOverflow*:

  + http://stackoverflow.com/questions/20333092/knn-with-big-sparse-matrices-in-python
  + http://stackoverflow.com/questions/18164348/efficient-nearest-neighbour-search-for-sparse-matrices
  + http://stackoverflow.com/questions/21085990/scipy-sparse-distance-matrix-scikit-or-scipy
  + http://stackoverflow.com/questions/10472681/handling-incomplete-data-data-sparsity-in-knn
  + http://stackoverflow.com/questions/5560218/computing-sparse-pairwise-distance-matrix-in-r (unlike all previous discussions, which refer to `Python`, this one refers to `R` ecosystem)

7) Pay attention to *GraphLab*, an open source **parallel framework for machine learning** (http://select.cs.cmu.edu/code/graphlab), which supports *parallel clustering* via `MapReduce` model: http://select.cs.cmu.edu/code/graphlab/clustering.html

You might also check my answer here on Data Science StackExchange on *sparse regression* for links to relevant `R` packages and `CRAN Task View` pages: http://datascience.stackexchange.com/a/918/2452.', 2452, '2014-08-14 04:47:02.543', '155dbaa4-dcb1-47f1-bef6-fb0f0c0e4e11', 975, 'Improved wording.', 2515, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I hope that the following **resources** might get you additional **ideas** toward solving the problem:

1) Research paper **"Efficient K-Nearest Neighbor Join Algorithms for High Dimensional Sparse Data"**: http://arxiv.org/abs/1011.2807

2) Class project paper **"Recommendation System Based on Collaborative Filtering"** (Stanford University): http://cs229.stanford.edu/proj2008/Wen-RecommendationSystemBasedOnCollaborativeFiltering.pdf

3) Project for the **Netflix Prize Competition (*k-NN*-based)**: http://cs.carleton.edu/cs_comps/0910/netflixprize/final_results/knn/index.html

4) Research paper **"Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data"** on the *curse of dimensionality* phenomenon and its relation to *machine learning*, in general, and *k-NN algorithm*, in particular: http://jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf

5) **Software for sparse k-NN classification** (free, but appears not to be open source - might clarify with authors): http://www.autonlab.org/autonweb/10408.html

6) Several **discussion threads** on *StackOverflow*:

  + http://stackoverflow.com/questions/20333092/knn-with-big-sparse-matrices-in-python
  + http://stackoverflow.com/questions/18164348/efficient-nearest-neighbour-search-for-sparse-matrices
  + http://stackoverflow.com/questions/21085990/scipy-sparse-distance-matrix-scikit-or-scipy
  + http://stackoverflow.com/questions/10472681/handling-incomplete-data-data-sparsity-in-knn
  + http://stackoverflow.com/questions/5560218/computing-sparse-pairwise-distance-matrix-in-r (unlike all previous discussions, which refer to `Python`, this one refers to `R` ecosystem)

7) Pay attention to *GraphLab*, an open source **parallel framework for machine learning** (http://select.cs.cmu.edu/code/graphlab), which supports *parallel clustering* via `MapReduce` model: http://select.cs.cmu.edu/code/graphlab/clustering.html

You might also check my answer here on Data Science StackExchange on *sparse regression* for links to relevant `R` packages and `CRAN Task View` pages: http://datascience.stackexchange.com/a/918/2452.', 2452, '2014-08-14 11:52:35.513', '802dc5fe-a47a-428f-a9ab-5c8419b0fb50', 975, 'Improved wording.', 2516, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I like to find the weight vector for input-space features in a [structured SVM][1]. The idea is to identify the most important set of input-space features (based on the magnitude of their corresponding weights). I know that in a binary SVM the weight vector can be written as a [linear combination of examples][2], and the magnitude of those weights represents how much they were effective for the prediction problem at hand. But how do you compute the same for an SSVM?


  [1]: http://en.wikipedia.org/wiki/Structured_support_vector_machine
  [2]: http://pyml.sourceforge.net/doc/howto.pdf', 2949, '2014-08-14 14:42:08.143', '05845407-9c1f-4146-a8e4-f4b1399a90dd', 973, 'Improved clarity', 2517, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I gave a very limited partial answer for the confirmatory factor analysis package that I developed for Stata in this [Stata Journal article](http://www.stata-journal.com/article.html?article=st0169) based on timing the actual simulations. Confirmatory factor analysis was implemented as a maximum likelihood estimation technique, and I could see very easily how the computation time grew with each dimension (sample size `n`, number of variables `p`, number of factors `k`). As it is heavily dependent on how Stata thinks about the data (optimized to compute across columns/observations rather than rows), I found performance to be `O(n^{0.68} (k+p)^{2.4})` where 2.4 is the fastest matrix inversion asymptotics (and there''s hell of a lot of that in confirmatory factor analysis iterative maximization). I did not give a reference for the latter, but I think I got this from [Wikipedia](http://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra).

Note that there is also a matrix inversion step in OLS. However, for reasons of numerical accuracy, no one would really brute-force inverse the `X''X` matrix, and would rather use sweep operators and identify the dangerously collinear variables to deal with precision issues. If you add up $10^8$ numbers that originally were in [double precision](http://en.wikipedia.org/wiki/Double-precision_floating-point_format), you will likely end up with a number that only has a single precision. Numerical computing issues may become a forgotten corner of big data calculations as you start optimizing for speed.
', 1237, '2014-08-14 17:28:40.453', '9e546c45-9f1f-4060-95c6-897ea61c713b', 976, 2518, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Let me show you an example of a hypothetical online clustering application:

![enter image description here][1]

At time n points 1,2,3,4 are allocated to the blue cluster A and points b,5,6,7 are allocated to the red cluster B.

At time n+1 a new point a is introduced which is assigned to the blue cluster A but also causes the point b to be assigned to the blue cluster A as well.

In the end points 1,2,3,4,a,b belong to A and points 5,6,7 to B. To me this seems reasonable.

What seems simple at first glance is actually a bit tricky - to maintain identifiers across time steps. Let me try to make this point clear with a more borderline example:

![enter image description here][2]

The green point will cause two blue and two red points to be merged into one cluster which I arbitrarily decided to color blue - mind this is already my human heuristical thinking at work!

A computer to make this decision will have to use rules. For example when points are merged into a cluster then the identity of the cluster is determined by the majority. In this case we would face a draw - both blue and red might be valid choices for the new (here blue colored) cluster.

Imagine a fifth red point close to the green one. Then the majority would be red (3 red vs 2 blue) so red would be a good choice for the new cluster - but this would contradict the even clearer choice of red for the rightmost cluster as those have been red and probably should stay that way.

I find it fishy to think about this. At the end of the day I guess there are no perfect rules for this - rather heuristics optimizing some stability criterea.

This finally leads to my questions:

1. Does this "problem" have a name that it can be referred to?
2. Are there "standard" solutions to this and ...
3. ... is there maybe even an R package for that?


  [1]: http://i.stack.imgur.com/ZVOQN.png
  [2]: http://i.stack.imgur.com/sQj4h.jpg', 725, '2014-08-14 19:09:29.523', '8ef2711d-b827-4eb0-b6f2-e50bf776859c', 977, 2520, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Solutions for Continuous Online Cluster Identification?', 725, '2014-08-14 19:09:29.523', '8ef2711d-b827-4eb0-b6f2-e50bf776859c', 977, 2521, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><clustering>', 725, '2014-08-14 19:09:29.523', '8ef2711d-b827-4eb0-b6f2-e50bf776859c', 977, 2522, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('>  But I don''t know what is difference between text classification and topic models in documents

Text classification is a form of supervised learning -- the set of possible classes are known/defined in advance and don''t change.

Topic modeling is a form of unsupervised learning (akin to clustering) -- the set of possible topics are unknown apriori. They''re defined as part of generating the topic models. With a non-deterministic algorithm like LDA, you''ll get different topics each time you run the algorithm.

Text classification often involves mutually-exclusive classes -- think of these as buckets. But it doesn''t have to -- given the right kind of labeled input data, you can set of a series of non-mutually-exclusive binary classifiers.

Topic modeling is generally not mutually-exclusive -- the same document can have its probability distribution spread across many topics. In addition, there are also hierarchical topic modeling methods, etc.

> Also can I use topic model for the documents to identify one topic later on can I use the classification to classify the text inside this documents ?

If you''re asking whether you can take all of the documents assigned to one topic by a topic modeling algorithm and then apply a classifier to that collection, then yes, you certainly can do that. I''m not sure it makes much sense, though -- at a minimum, you''d need to pick a threshold for the topic probability distribution above which you''ll include documents in your collection (typically 0.05-0.1). Can you elaborate on your use case?

By the way, there''s a great tutorial on topic modeling using the MALLET library for Java available here: [Getting Started with Topic Modeling and MALLET][1]


  [1]: http://programminghistorian.org/lessons/topic-modeling-and-mallet', 819, '2014-08-14 21:38:57.630', 'a8f86e7c-0dfe-456d-a819-0e154bcca359', 978, 2523, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a problem of clustering huge amount of sentences into groups by their meanings. This is similar to a problem when you have lots of sentences and want to group them by their meanings.

What algorithms are suggested to do this? I don''t know number of clusters in advance (and as more data is coming clusters can change as well), what features are normally used to represent each sentence?

I''m trying now the simplest features with just list of words and distance between sentences as next   ![enter image description here][1] A and B are corresponding sets of words in sentence A and B. Does it make sense at all?

I''m trying to apply [Mean-Shift][2] algorithm from scikit library to this distance as it does not require number of clusters in advance.

If anyone will advise better methods/approaches for the problem - it will be very much appreciated as I''m still new to the topic.

Thanks.

  [1]: http://i.stack.imgur.com/dHB9X.jpg
  [2]: http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#example-cluster-plot-mean-shift-py', 2958, '2014-08-15 13:10:20.937', '4520d267-0d61-43fb-b9b2-af787e5d085b', 979, 2524, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Algorithms for text clustering', 2958, '2014-08-15 13:10:20.937', '4520d267-0d61-43fb-b9b2-af787e5d085b', 979, 2525, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining><clustering><sklearn><k-means>', 2958, '2014-08-15 13:10:20.937', '4520d267-0d61-43fb-b9b2-af787e5d085b', 979, 2526, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a problem of clustering huge amount of sentences into groups by their meanings. This is similar to a problem when you have lots of sentences and want to group them by their meanings.

What algorithms are suggested to do this? I don''t know number of clusters in advance (and as more data is coming clusters can change as well), what features are normally used to represent each sentence?

I''m trying now the simplest features with just list of words and distance between sentences as next   ![enter image description here][1] A and B are corresponding sets of words in sentence A and B. Does it make sense at all?

I''m trying to apply [Mean-Shift][2] algorithm from scikit library to this distance as it does not require number of clusters in advance.

If anyone will advise better methods/approaches for the problem - it will be very much appreciated as I''m still new to the topic.

  [1]: http://i.stack.imgur.com/dHB9X.jpg
  [2]: http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#example-cluster-plot-mean-shift-py', 84, '2014-08-15 13:35:43.170', 'a01169c0-fab0-4b5b-bde8-71874b8e4d21', 979, 'deleted 11 characters in body', 2527, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Check the **Stanford NLP Group**''s open source software (http://www-nlp.stanford.edu/software), in particular, **Stanford Classifier** (http://www-nlp.stanford.edu/software/classifier.shtml). The software is written in `Java`, which will likely delight you, but also has bindings for some other languages. Note, the *licensing* - if you plan to use their code in commercial products, you have to acquire commercial license.

Another interesting set of open source libraries, IMHO suitable for this task and much more, is **parallel framework for machine learning GraphLab** (http://select.cs.cmu.edu/code/graphlab), which includes **clustering library**, implementing various clustering algorithms (http://select.cs.cmu.edu/code/graphlab/clustering.html). It is especially suitable for **very large volume of data** (like you have), as it implements `MapReduce` model and, thus, supports *multicore* and *multiprocessor* **parallel processing**.', 2452, '2014-08-15 14:00:14.927', 'dba9f70b-da14-4dd8-9c52-b00beea5b90e', 980, 2528, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Check the **Stanford NLP Group**''s open source software (http://www-nlp.stanford.edu/software), in particular, **Stanford Classifier** (http://www-nlp.stanford.edu/software/classifier.shtml). The software is written in `Java`, which will likely delight you, but also has bindings for some other languages. Note, the *licensing* - if you plan to use their code in commercial products, you have to acquire commercial license.

Another interesting set of open source libraries, IMHO suitable for this task and much more, is **parallel framework for machine learning GraphLab** (http://select.cs.cmu.edu/code/graphlab), which includes **clustering library**, implementing various clustering algorithms (http://select.cs.cmu.edu/code/graphlab/clustering.html). It is especially suitable for **very large volume of data** (like you have), as it implements `MapReduce` model and, thus, supports *multicore* and *multiprocessor* **parallel processing**.

You most likely are aware of the following, but I will mention it just in case. **Natural Language Toolkit (NLTK)** for `Python` (http://www.nltk.org) contains modules for clustering/classifying/categorizing text. Check the relevant chapter in the `NLTK Book`: http://www.nltk.org/book/ch06.html.', 2452, '2014-08-15 14:13:35.977', '429e0a45-66ba-4bf0-92f2-3f194a59dbee', 980, 'Added info on NLTK library.', 2529, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Check the **Stanford NLP Group**''s open source software (http://www-nlp.stanford.edu/software), in particular, **Stanford Classifier** (http://www-nlp.stanford.edu/software/classifier.shtml). The software is written in `Java`, which will likely delight you, but also has bindings for some other languages. Note, the *licensing* - if you plan to use their code in commercial products, you have to acquire commercial license.

Another interesting set of open source libraries, IMHO suitable for this task and much more, is **parallel framework for machine learning GraphLab** (http://select.cs.cmu.edu/code/graphlab), which includes **clustering library**, implementing various clustering algorithms (http://select.cs.cmu.edu/code/graphlab/clustering.html). It is especially suitable for **very large volume of data** (like you have), as it implements `MapReduce` model and, thus, supports *multicore* and *multiprocessor* **parallel processing**.

You most likely are aware of the following, but I will mention it just in case. **Natural Language Toolkit (NLTK)** for `Python` (http://www.nltk.org) contains modules for clustering/classifying/categorizing text. Check the relevant chapter in the `NLTK Book`: http://www.nltk.org/book/ch06.html.

**UPDATE:**

Speaking of **algorithms**, it seems that you''ve tried most of the ones from `scikit-learn`, such as illustrated in this topic extraction example: http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html. However, you may find useful other libraries, which implement a wide variety of *clustering algorithms*, including *Non-Negative Matrix Factorization (NMF)*. One of such libraries is **Python Matrix Factorization (PyMF)** with home page at https://code.google.com/p/pymf and source code at https://github.com/nils-werner/pymf. Another, even more interesting, library, also Python-based, is **NIMFA**, which implements various *NMF algorithms*: http://nimfa.biolab.si. Here''s a research paper, describing `NIMFA`: http://jmlr.org/papers/volume13/zitnik12a/zitnik12a.pdf. Here''s an example from its documentation, which presents the solution for very similar text processing problem of *topic clustering*: http://nimfa.biolab.si/nimfa.examples.documents.html.', 2452, '2014-08-15 16:31:41.337', '2cd55170-4595-455c-bc63-d7b7b6b6419a', 980, 'Added UPDATE on algorithms.', 2530, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('R is a free, open-source programming language and software environment for statistical computing, bioinformatics, and graphics.', 2961, '2014-08-15 16:38:27.880', '64bea266-f539-4d0f-8a6b-57b3234e5dcf', 49, 'added tag summary', 2531, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-15 16:38:27.880', '64bea266-f539-4d0f-8a6b-57b3234e5dcf', 49, 'Proposed by 2961 approved by 84 edit id of 135', 2532, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Basically for this task you can efficiently use any SQL database with good support of B+tree based indexes (MySQL will suite you needs just perfect).

Create 3 tables:

 1. Documents table, columns: id/document
 2. N-grams table: n_gram_id/n_gram
 3. Mapping between n-grams and documents: document_id/n_gram_id

Create indexes on N-gram table/n_gram string and Mapping table/n_gram_id, also primary keys will be indexed by default well.

Your operations will be efficient:

 1. Insertion of document: just extract all n-grams and insert into document table and N-grams table
 2. Lookup for in_gram will be quick with support of index
 3. Querying for all n-grams that contain a sub-n-gram: in 2 steps - just query based on index all n-grams which contain sub-n-gram from 2nd table. Then - retrieve all corresponding documents for each of these n-grams.

You don''t even need to use joins to achieve all these operations so indexes will help a lot. Also if data will not suite in one machine - you can implement sharding scheme, like storing n_grams started from a-n on one server and o-z on another or other suitable scheme.

Also you can use MongoDB, but I''m not sure how exactly you need to implement indexing scheme. For MongoDB you will get sharding scheme for free as it is already built-in.', 2958, '2014-08-15 20:22:11.750', '329ca4a9-213a-422d-9822-89202664ca5f', 981, 2533, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('See [Lucene NGramTokenizer][1]

Are you sure you can''t just use lucene or similar indexing techniques?

Inverted indexes will store the n-gram only once, then just the document ids that contain the ngram; they don''t store this as highly redundant raw text.

As for finding ngrams that contain your query sub-n-gram, I would build an index on the observed ngrams, e.g. using a second lucene index, or [any other substring index][2] such as a trie or suffix tree. If your data is dynamic, probably lucene is a reasonable choice, using phrase queries to find your n-grams.

  [1]: https://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html
  [2]: https://en.wikipedia.org/wiki/Substring_index', 924, '2014-08-16 10:25:19.687', '39f1b9a3-67cc-467e-8a97-1eabb3b5f5f6', 982, 2534, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have no knowledge about the climate or soil. And I just want to find out more about these kind of dataset. I heard that Climate Corporation asked its candidates to perform statistical analysis on various climate dataset.

This is why I am asking this question. Please do not get me wrong. I am not trying to get the dataset to prepare myself for an interview, as I know they give out different dataset to people from different background.

I know that Climate Corporation only hires PHD, which I am not. I only want to play around with their dataset such that I can learn and implement **time series analysis**. That''s it.

So, if anyone does not mind sharing their dataset. Please post the link them below. Thank you very much. ', 2972, '2014-08-16 10:42:58.337', '941f2b19-e8a4-4aed-baae-67a4b3600c9a', 984, 2541, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can anyone provide the 24 hour challenge dataset from Climate Corporation?', 2972, '2014-08-16 10:42:58.337', '941f2b19-e8a4-4aed-baae-67a4b3600c9a', 984, 2542, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset>', 2972, '2014-08-16 10:42:58.337', '941f2b19-e8a4-4aed-baae-67a4b3600c9a', 984, 2543, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Let me show you an example of a hypothetical online clustering application:

![enter image description here][1]

At time n points 1,2,3,4 are allocated to the blue cluster A and points b,5,6,7 are allocated to the red cluster B.

At time n+1 a new point a is introduced which is assigned to the blue cluster A but also causes the point b to be assigned to the blue cluster A as well.

In the end points 1,2,3,4,a,b belong to A and points 5,6,7 to B. To me this seems reasonable.

What seems simple at first glance is actually a bit tricky - to maintain identifiers across time steps. Let me try to make this point clear with a more borderline example:

![enter image description here][2]

The green point will cause two blue and two red points to be merged into one cluster which I arbitrarily decided to color blue - mind this is already my human heuristical thinking at work!

A computer to make this decision will have to use rules. For example when points are merged into a cluster then the identity of the cluster is determined by the majority. In this case we would face a draw - both blue and red might be valid choices for the new (here blue colored) cluster.

Imagine a fifth red point close to the green one. Then the majority would be red (3 red vs 2 blue) so red would be a good choice for the new cluster - but this would contradict the even clearer choice of red for the rightmost cluster as those have been red and probably should stay that way.

I find it fishy to think about this. At the end of the day I guess there are no perfect rules for this - rather heuristics optimizing some stability criterea.

This finally leads to my questions:

1. Does this "problem" have a name that it can be referred to?
2. Are there "standard" solutions to this and ...
3. ... is there maybe even an R package for that?


----------

[Reasonable Inheritance of Cluster Identities in Repetitive Clustering][3]


  [1]: http://i.stack.imgur.com/ZVOQN.png
  [2]: http://i.stack.imgur.com/sQj4h.jpg
  [3]: http://www.joyofdata.de/blog/reasonable-inheritance-of-cluster-identities-in-repetitive-clustering/', 725, '2014-08-16 10:49:06.163', '812eb5dc-33b8-4540-9d85-d7144ca4b3eb', 977, 'added 202 characters in body', 2544, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a question about classifying documents using supervised learning and unsupervised learning.

For example: - I have a bunch of documents talking about football.
As we know football has different meaning in UK, USA and Australia. Therefore, it is difficult to classify these documents to three different categorizations which are soccer, American football and Australian football.
My approach tries to use cosine similarity terms which is based on unsupervised. After we use the cluster learning, we are able to create a number of clusters based on cosine similarity which each cluster will contain similar documents terms. After we create the clusters, we can use a semantic feature to identify these clusters depend on supervised model like SVM to make accurate categorizations.

My goal is to create more accurate categorizations because a new document test can be related to these categorizations or not.  ', 2916, '2014-08-16 11:05:43.353', '48304a69-f551-4cfe-a1ed-537c7aa048d3', 985, 2545, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can I use unsupervised learning then I use supervised learning?', 2916, '2014-08-16 11:05:43.353', '48304a69-f551-4cfe-a1ed-537c7aa048d3', 985, 2546, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><clustering>', 2916, '2014-08-16 11:05:43.353', '48304a69-f551-4cfe-a1ed-537c7aa048d3', 985, 2547, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a question about classifying documents using supervised learning and unsupervised learning.

For example: - I have a bunch of documents talking about football.
As we know football has different meaning in UK, USA and Australia. Therefore, it is difficult to classify these documents to three different categorizations which are soccer, American football and Australian football.
My approach tries to use cosine similarity terms which is based on unsupervised. After we use the cluster learning, we are able to create a number of clusters based on cosine similarity which each cluster will contain similar documents terms. After we create the clusters, we can use a semantic feature to identify these clusters depend on supervised model like SVM to make accurate categorizations.

My goal is to create more accurate categorizations because if I want to test a new document I want know if this document can be related to these categorizations or not.  ', 2916, '2014-08-16 16:21:39.327', '4c03e4ad-5ec0-443b-b53b-6bf3672f70ef', 985, 'added 42 characters in body', 2548, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[R][1] is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (formerly AT&T, now Lucent Technologies) by John Chambers and colleagues. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.

R provides a wide variety of statistical (linear and nonlinear modelling, classical statistical tests, time-series analysis, classification, clustering, ...) and graphical techniques, and is highly extensible. The S language is often the vehicle of choice for research in statistical methodology, and R provides an Open Source route to participation in that activity.

One of R''s strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control.

R was created by [Ross Ihaka][2] and [Robert Gentleman][3] and is now developed by the [R Development Core Team][4]. The R environment is easily extended through a packaging system on [CRAN][5].

R is available as Free Software under the terms of the Free Software Foundation''s GNU General Public License in source code form. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and Mac OS.


  [1]: http://www.r-project.org
  [2]: http://www.stat.auckland.ac.nz/~ihaka/
  [3]: http://www.gene.com/scientists/our-scientists/robert-gentleman
  [4]: http://www.r-project.org/contributors.html
  [5]: http://cran.r-project.org "CRAN The Comprehensive R Archive Network"', 2961, '2014-08-16 17:29:43.517', '820de325-f112-4cdb-8d6b-dfba5119b4cd', 48, 'added text', 2552, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-16 17:29:43.517', '820de325-f112-4cdb-8d6b-dfba5119b4cd', 48, 'Proposed by 2961 approved by 21 edit id of 136', 2553, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('OK, so here''s your data.

    dd <- data.frame(position=rep(1:3, each=2),
                     variation=rep(c(1,3), 3),
                     impressions=rep(c(753, 767), 3),
                     clicks=c(26,7,16,13,2,7))

which is

      position variation impressions clicks
    1        1         1         753     26
    2        1         3         767      7
    3        2         1         753     16
    4        2         3         767     13
    5        3         1         753      2
    6        3         3         767      7

The two model assumptions you''re thinking about are Binomial

    mod.bin <- glm(cbind(clicks, impressions-clicks) ~ variation + position,
                   family=binomial, data=dd)

where the dependent variable is constructed to have the count of the event of interest in the first column, and the Poisson

    md.pois <- glm(clicks ~ variation + position + offset(log(impressions)),
                   family=poisson, data=dd)

where the `log(impressions)` offset is necessary whenever the number of trials differs across observations.  This means coefficients are interpretable in terms of change in rate not change in count, which is what you want.

The first model generalises the `binom.test` to a setting with covariates, which is what you have.  That gets you a more direct answer to your question, and better (if not perfect) measurement of the relevant uncertainty.

**Notes**

Both models assume no interaction between variation and position (''independent effects'').  This may or may not be reasonable.  You''d want more replications to investigate that properly.  Swap the `+` for a `*` to do so.

In this data `summary` confirms that the two models give rather similar results, so concerns about Poisson vs Binomial don''t seem to matter much.

In the wild, count data is usually overdispersed, that is: more variable than you''d expect from a Poisson with a constant rate or a Binomial with constant click probability, often due to unmodeled determinants of click rate / probability.  If that''s the case then prediction intervals from these models will be too narrow.
', 2978, '2014-08-17 14:17:41.307', 'd60a5c8e-4356-4b26-a55c-a4dd1727061c', 986, 2554, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('OK, so here''s your data.

    dd <- data.frame(position=rep(1:3, each=2),
                     variation=rep(c(1,3), 3),
                     impressions=rep(c(753, 767), 3),
                     clicks=c(26,7,16,13,2,7))

which is

      position variation impressions clicks
    1        1         1         753     26
    2        1         3         767      7
    3        2         1         753     16
    4        2         3         767     13
    5        3         1         753      2
    6        3         3         767      7

The two model assumptions you''re thinking about are Binomial

    mod.bin <- glm(cbind(clicks, impressions-clicks) ~ variation + position,
                   family=binomial, data=dd)

where the dependent variable is constructed to have the count of the event of interest in the first column, and the Poisson

    md.pois <- glm(clicks ~ variation + position + offset(log(impressions)),
                   family=poisson, data=dd)

where the `log(impressions)` offset is necessary whenever the number of trials differs across observations.  This means coefficients are interpretable in terms of change in rate not change in count, which is what you want.

The first model generalises the `binom.test` to a setting with covariates, which is what you have.  That gets you a more direct answer to your question, and better (if not perfect) measurement of the relevant uncertainty.

**Notes**

Both models assume no interaction between variation and position (''independent effects'').  This may or may not be reasonable.  You''d want more replications to investigate that properly.  Swap the `+` for a `*` to do so.

In this data `summary` confirms that the two models give rather similar results, so concerns about Poisson vs Binomial don''t seem to matter much.

In the wild, count data is usually overdispersed, that is: more variable than you''d expect from a Poisson with a constant rate or a Binomial with constant click probability, often due to unmodeled determinants of click rate / probability.  If that''s the case then prediction intervals from these models will be too narrow.
', 2978, '2014-08-17 15:04:13.880', '4682c521-3eea-43dd-b5c9-625a6af3e00f', 986, '[Edit removed during grace period]', 2555, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The problem I am tackling is categorizing short texts into multiple classes. My current approach is to use tf-idf weighted term frequencies and learn a simple linear classifier (logistic regression). This works reasonably well (around 90% macro F-1 on test set, nearly 100% on training set). A big problem are unseen words/n-grams.

I am trying to improve the classifier by adding other features, e.g. a fixed sized vector computed using distributional similarities (as computed by word2vec) or other categorical features of the examples. My idea was to just add the features to the sparse input features from the bag of words. However, this results in worse performance on the test and training set. The additional features by themselves give about 80% F-1 on the test set, so they aren''t garbage. Scaling the features didn''t help as well. My current thinking is that these kind of features don''t mix well with the (sparse) bag of words features.

So the question is: assuming the additional features provide additional information, what is the best way to incorporate them? Could training separate classifiers and combining them in some kind of ensemble work (this would probably have the drawback that no interaction between the features of the different classifiers could be captured)? Are there other more complex models I should consider?', 2979, '2014-08-17 17:29:44.123', 'd5f0645f-9684-4d92-9d6a-ea99f04edc8d', 987, 2556, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Text categorization: combining different kind of features', 2979, '2014-08-17 17:29:44.123', 'd5f0645f-9684-4d92-9d6a-ea99f04edc8d', 987, 2557, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><feature-selection><logistic-regression><information-retrieval>', 2979, '2014-08-17 17:29:44.123', 'd5f0645f-9684-4d92-9d6a-ea99f04edc8d', 987, 2558, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('![enter image description here][1]

These are 4 different weight matrices that I got after training **RBM** with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though _none_ of weights are _exactly_ the same.

I can overcome this by increasing number of weight vectors to 512 or more. But I encountered this problem several times earlier with different RBM types (binary, Gaussian, even convolutional), different number of hidden units (including pretty large), different hyper-parameters, etc.

My question is: what is the **most likely reason** for weights to get **very similar values**? Do they all just get to some local minimum? Or is it a sign of overfitting?

I currently use binary-Gaussian RBM, code may be found [here](https://github.com/faithlessfriend/Milk.jl/blob/master/src/rbm.jl).

**UPD.** My dataset is based on [CK+](http://www.pitt.edu/~emotion/ck-spread.htm), which contains > 10k images of 327 individuals. However I do pretty heavy preprocessing. First, I clip only pixels inside of outer contour of a face. Second, I transform each face (using piecewise affine wrapping) to the same grid (e.g. eyebrows, nose, lips etc. are in the same (x,y) position on all images). After preprocessing images look like this:

![enter image description here][2] ![enter image description here][3]

When training RBM, I take only non-zero pixels, so outer black region is ignored.


  [1]: http://i.stack.imgur.com/lBxL5.png
  [2]: http://i.stack.imgur.com/FQMSp.png
  [3]: http://i.stack.imgur.com/Hjcaw.png', 1279, '2014-08-17 19:47:22.887', 'fc05cec7-f50e-4f17-8b85-fc3df5eaa496', 961, 'added 685 characters in body', 2560, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here is a couple of tools that may be worth a look:

 - Bart, an open source tool that have been used for several languages, including German. [Available from the website][1]
 - Sucre is a tool developed at the University of Stuttgart. I don''t know if it''s available easily. [You can see this paper about it][2].


  [1]: http://www.bart-coref.org/
  [2]: http://hnk.ffzg.hr/bibl/acl2011/CoNLL-ST/pdf/CoNLL-ST3.pdf', 2985, '2014-08-18 08:49:11.933', '8310b575-14d8-4752-812a-11589f8439b7', 988, 2561, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to run SVR using scikit learn ( python ) on a training dataset having 595605 rows and 5 columns(features) and test dataset having 397070 rows. The data has been pre-processed and regularized.

I am able to successfully run the test examples but on executing using my dataset and letting it run for over an hour, I could still not see any output or termination of program. I have tried executing using a different IDE and even from terminal but that doesn''t seem to be the issue.
I have also tried changing the ''C'' parameter value from 1 to 1e3.

I am facing similar issues with all svm implementations using scikit.

Am I not waiting enough for it to complete ?
How much time should this execution take ?

From my experience it shouldn''t require over a few minutes.

Here is my system configuration:
Ubuntu 14.04, 8GB RAM, lots of free memory, 4th gen i7 processor', 793, '2014-08-18 10:46:57.360', 'c7dd32e8-9dfc-41c0-8c01-3fc3ccd96871', 989, 2562, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('SVM using scikit learn runs endlessly and never completes execution', 793, '2014-08-18 10:46:57.360', 'c7dd32e8-9dfc-41c0-8c01-3fc3ccd96871', 989, 2563, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><svm><scikit>', 793, '2014-08-18 10:46:57.360', 'c7dd32e8-9dfc-41c0-8c01-3fc3ccd96871', 989, 2564, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This makes sense. IIUC, the speed of execution of support vector operations is bound by number of samples, not dimensionality. In other words, it is capped by CPU time and not RAM. I''m not sure exactly how much time this should take, but I''m running some benchmarks to find out.', 2987, '2014-08-18 11:33:32.617', '279ae776-6a8f-446d-969d-6115ecd5ec68', 990, 2565, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('![enter image description here][1]

These are 4 different weight matrices that I got after training **RBM** with ~4k visible units and only 96 hidden units/weight vectors. As you can see, weights are extremely similar - even black pixels on the face are reproduced. The other 92 vectors are very similar too, though _none_ of weights are _exactly_ the same.

I can overcome this by increasing number of weight vectors to 512 or more. But I encountered this problem several times earlier with different RBM types (binary, Gaussian, even convolutional), different number of hidden units (including pretty large), different hyper-parameters, etc.

My question is: what is the **most likely reason** for weights to get **very similar values**? Do they all just get to some local minimum? Or is it a sign of overfitting?

I currently use a kind of Gaussian-Bernoulli RBM, code may be found [here](https://github.com/faithlessfriend/Milk.jl/blob/master/src/rbm.jl).

**UPD.** My dataset is based on [CK+](http://www.pitt.edu/~emotion/ck-spread.htm), which contains > 10k images of 327 individuals. However I do pretty heavy preprocessing. First, I clip only pixels inside of outer contour of a face. Second, I transform each face (using piecewise affine wrapping) to the same grid (e.g. eyebrows, nose, lips etc. are in the same (x,y) position on all images). After preprocessing images look like this:

![enter image description here][2] ![enter image description here][3]

When training RBM, I take only non-zero pixels, so outer black region is ignored.


  [1]: http://i.stack.imgur.com/lBxL5.png
  [2]: http://i.stack.imgur.com/FQMSp.png
  [3]: http://i.stack.imgur.com/Hjcaw.png', 1279, '2014-08-18 11:59:06.607', 'c0b07bd1-6715-4391-ad4d-1909ad6dadae', 961, 'added 13 characters in body', 2566, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Linear models simply add their features multiplied by corresponding weights. If, for example, you have 1000 sparse features only 3 or 4 of which are active in each instance (and the others are zeros) and 20 dense features that are all non-zeros, then it''s pretty likely that dense features will make most of the impact while sparse features will add only a little value. You can check this by looking at feature weights for a few instances and how they influence resulting sum.

One way to fix it is to go away from additive model. Here''s a couple of candidate models.

**SVM** is based on separating hyperplanes. Though hyperplane is linear model itself, SVM doesn''t sum up its parameters, but instead tries to split feature space in an optimal way. Given the number of features, I''d say that linear SVM should work fine while more complicated kernels may tend to overfit the data.

Despite its name, **Naive Bayes** is pretty powerful statistical model that showed good results for text classification. It''s also flexible enough to capture imbalance in frequency of sparse and dense features, so you should definitely give it a try.

Finally, **random forests** may work as a good ensemble method in this case. Randomization will ensure that different kinds of features (sparse/dense) will be used as primary decision nodes in different trees. RF/decision trees are also good for inspecting features themselves, so it''s worth to note their structure anyway.

Note that all of these methods have their drawbacks that may turn them into a garbage in your case. Combing sparse and dense features isn''t really well-studied task, so let us know what of these approaches works best for your case. ', 1279, '2014-08-18 13:35:28.000', '67dd8bfc-d153-4485-a2ae-743470c719db', 991, 2567, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve been analyzing a data set of ~400k records and 9 variables. I''ve fitted a logistic regression, a regression tree, a random forest, and a gradient boosted tree. All of them give virtual identical goodness of fit numbers when I validate them on another data set.

Why is this so? I''m guessing that it''s because my observations to variable ratio is so high. If this is correct, at what observation to variable ratio will different models start to give different results? ', 1241, '2014-08-18 14:56:13.800', 'b00586ea-8d49-42d0-9f14-a68bb7cb93e0', 992, 2568, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why might several types of models give almost identical results?', 1241, '2014-08-18 14:56:13.800', 'b00586ea-8d49-42d0-9f14-a68bb7cb93e0', 992, 2569, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 1241, '2014-08-18 14:56:13.800', 'b00586ea-8d49-42d0-9f14-a68bb7cb93e0', 992, 2570, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('> I''m guessing that it''s because my observations to variable ratio is so high.

I think this explanation makes perfect sense.

> If this is correct, at what observation to variable ratio will different models start to give different results?

This will probably depend very much on your specific data (for instance, even whether your nine variables are continuous, factors, ordinary or binary), as well as any tuning decisions you made while fitting your model.

But you can play around with the observation-to-variable ratio - not by increasing the number of variables, but by decreasing the number of observations. Randomly draw 100 observations, fit models and see whether different models yield different results. (I guess they will.) Do this multiple times with different samples drawn from your total number of observations. Then look at subsamples of 1,000 observations... 10,000 observations... and so forth.', 2853, '2014-08-18 16:28:02.147', '21b988fa-aca6-46ef-8198-e8ed0a72d4c9', 993, 2571, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('its worth also looking at the training errors.

basically I disagree with your  analysis. if logistic regression etc are all giving the same results it would suggest that the ''best model'' is a very simple one (that all models can fit equally well - eg basically linear).

So then the question might be why is the best model a simple model?:
It might suggest that your variables are not very predictive. Its of course hard to analyse without knowing the data. ', 1256, '2014-08-18 17:05:19.957', '241eb3d9-21a7-49ad-83e1-7f6bc6ac2228', 994, 2572, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As @seanv507 suggested, the similar performance may simply be due to the data being best separated by a linear model. But in general, the statement that it is because the "observations to variable ratio is so high" is incorrect. Even as your ratio of sample size to number of variables goes to infinity, you should not expect different models to perform nearly identically, unless they all provide the same predictive bias.', 964, '2014-08-18 17:25:27.007', '82212744-0b00-4237-a706-c2d750f47b19', 995, 2573, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve been analyzing a data set of ~400k records and 9 variables The dependent variable is binary. I''ve fitted a logistic regression, a regression tree, a random forest, and a gradient boosted tree. All of them give virtual identical goodness of fit numbers when I validate them on another data set.

Why is this so? I''m guessing that it''s because my observations to variable ratio is so high. If this is correct, at what observation to variable ratio will different models start to give different results? ', 1241, '2014-08-18 17:31:27.327', 'ee45f103-6ff1-4ab7-a6f5-47d0d36ac2ce', 992, 'added 33 characters in body', 2574, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Kernelized SVMs require the computation of a distance function between each point in the dataset, which is the dominating cost of O(n_features x n_observations^2). The storage of the distances is a burden on memory, so they''re recomputed on the fly. Thankfully, only the points nearest the decision boundary are needed most of the time. Frequently computed distances are stored in a cache. If the cache is getting thrashed then the running time blows up to O(n_features x n_observations^3). (Seriously, no LaTeX?)

You can increase this cache by invoking SVR as

    model = SVR(cache_size=7000)

In general, this is not going to work. But all is not lost. You can subsample the data and use the rest as a validation set, or you can pick a different model. Above the 200,000 observation range, it''s wise to choose linear learners.

Kernel SVM can be approximated, by approximating the kernel matrix and feeding it to a linear SVM. This allows you to trade off between accuracy and performance in linear time.

A popular means of achieving this is to use 100 or so cluster centers found by kmeans/kmeans++ as the basis of your kernel function. The new derived features are then fed into a linear model. This works very well in practice. Tools like [sophia-ml][1] and [vowpal wabbit][2] are how Google, Yahoo and Microsoft do this. Input/output becomes the dominating cost for simple linear learners.

In the abundance of data, nonparametric models perform roughly the same for most problems. The exceptions being structured inputs, like text, images, time series, audio.

Further reading:

[How to implement this.][3]

[How to train an ngram neural network with dropout that scales linearly][4]

[Kernel Approximations][5]

[A formal paper on using kmeans to approximate kernel machines][6]


  [1]: https://code.google.com/p/sofia-ml/
  [2]: https://github.com/JohnLangford/vowpal_wabbit/wiki
  [3]: http://fastml.com/the-secret-of-the-big-guys/
  [4]: http://fastml.com/go-non-linear-with-vowpal-wabbit/
  [5]: http://peekaboo-vision.blogspot.co.uk/2012/12/kernel-approximations-for-efficient.html
  [6]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.9009&rep=rep1&type=pdf', 2997, '2014-08-19 00:56:40.890', 'f4df1883-d6d3-4a05-93ed-2abe3725002c', 996, 2575, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where can I find free spatio-temporal dataset for download so that I can play with it in R ?

Thanks', 2972, '2014-08-19 03:41:24.207', '8be17476-d086-4339-8bf5-aed4dd7c98e9', 997, 2576, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where can I find free spatio-temporal dataset for download ?', 2972, '2014-08-19 03:41:24.207', '8be17476-d086-4339-8bf5-aed4dd7c98e9', 997, 2577, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset>', 2972, '2014-08-19 03:41:24.207', '8be17476-d086-4339-8bf5-aed4dd7c98e9', 997, 2578, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('No, sklearn doesn''t seem to have a forward selection algorithm. However, it does provide recursive feature elimination, which is a greedy feature elimination algorithm similar to sequential backward selection. See the documentation here:

http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html



', 2969, '2014-08-19 13:33:32.453', 'c6c30270-1fc6-4803-b34a-b31aee75e6cb', 998, 2579, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This results means that whatever method you use, you are able to get reasonably close to the optimal decision rule (aka [Bayes rule](http://en.wikipedia.org/wiki/Admissible_decision_rule#Bayes_rules_and_generalized_Bayes_rules)). The underlying reasons have been explained in Hastie, Tibshirani and Friedman''s ["Elements of Statistical Learning"](http://statweb.stanford.edu/~tibs/ElemStatLearn/). They demonstrated how the different methods perform by comparing Figs. 2.1, 2.2, 2.3, 5.11 (in my first edition -- in section on multidimensional splines), 12.2, 12.3 (support vector machines), and probably some others. If you have not read that book, you need to drop everything **RIGHT NOW** and read it up. (I mean, it isn''t worth losing your job, but it is worth missing a homework or two if you are a student.)

I don''t think that observations to variable ratio is the explanation. In light of my rationale offered above, it is the relatively simple form of the boundary separating your classes in the multidimensional space that all of the methods you tried have been able to identify.', 1237, '2014-08-19 14:13:07.147', 'e168fe05-382a-4abf-b4e7-1c9cc749a097', 999, 2580, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can get some documented, publicly available EEG data from the HeadIT database at UCSD.
http://headit-beta.ucsd.edu/studies

The data itself appears to be in Biosemi Data Format (.bdf) files, described here: http://www.biosemi.com/faq/file_format.htm

Biosemi provides links to several open-source methods to access and import .bdf files on their website, including several functions for importing into Matlab, as well as into Python (BioSig) and C/C++ libraries:
http://www.biosemi.com/download.htm

Just as a forewarning, EEG data can be a bit of a bear to work with, due to it''s inherently low signal/noise ratio.', 2969, '2014-08-19 15:21:57.833', '74a14eb1-6b2d-4c2f-a14f-4998d6927ca8', 1000, 2581, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First thing that came to mind would be one''s personal workout data from running or biking apps.

Otherwise there is a dataset around NYC''s taxi trip data. Quick Googling brought me this: http://www.andresmh.com/nyctaxitrips/. Variables include time and location for both pickups and dropoffs.

Another dataset comes from Chicago''s bikesharing service. It can be found here: https://www.divvybikes.com/datachallenge.', 525, '2014-08-19 15:47:29.413', 'd458ff0d-ef85-45a0-a96d-1724495a0428', 1001, 2582, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Caveat: I am a complete beginner when it comes to machine learning, but eager to learn.

I have a large dataset and I''m trying to find pattern in it. There may / may not be correlation across the data, either with known variables, or variables that are contained in the data but which I haven''t yet realised are actually variables / relevant.

I''m guessing this would be a familiar problem in the world of data analysis, so I have a few questions:

1. The ''silver bullet'' would be to throw this all this data into a stats / data analysis program and for it to crunch the data looking for known / unknown patterns trying to find relations. Is SPSS suitable, or are there other applications which may be better suited.

2. Should I learn a language like R, and figure out how to manually process the data. Wouldn''t this comprimise finding relations as I would have to manually specify what and how to analyse the data?

3. How would a professional data miner approach this problem and what steps would s/he take?', 2861, '2014-08-19 17:50:52.583', '0debddc2-9400-4fa2-a9cd-2aaa88e8a011', 1002, 2583, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Making sense of large data sets', 2861, '2014-08-19 17:50:52.583', '0debddc2-9400-4fa2-a9cd-2aaa88e8a011', 1002, 2584, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 2861, '2014-08-19 17:50:52.583', '0debddc2-9400-4fa2-a9cd-2aaa88e8a011', 1002, 2585, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I recently read [Similarity Measures for Short Segments of Text](http://research.microsoft.com/en-us/um/people/sdumais/ecir07-metzlerdumaismeek-final.pdf) (Metzler et al.).  It describes basic methods for measuring query similarity, and in the paper, the data consists of queries and their top results. Results are lists of page urls, page titles, and short page snippets.  In the paper, the authors collect 200 results per query.

When using the public Google APIs to retrieve results, I was only able to collect 4-10 results per query.  There''s a substantial difference between 10 and 200.  Hence, how much data is commonly used in practice to measure query similarity (e.g., how many results per query)?

References are a plus!', 1097, '2014-08-19 18:59:03.013', '9bad51a6-9339-4ece-a86b-44783e8ef692', 1003, 2586, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Query similarity: how much data is used in practice?', 1097, '2014-08-19 18:59:03.013', '9bad51a6-9339-4ece-a86b-44783e8ef692', 1003, 2587, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><dataset><search>', 1097, '2014-08-19 18:59:03.013', '9bad51a6-9339-4ece-a86b-44783e8ef692', 1003, 2588, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Another idea is to combine OpenStreetMap project map data, for example, using corresponding nice R package (http://www.r-bloggers.com/the-openstreetmap-package-opens-up), with census data (population census data, such as the US data: http://www.census.gov/data/data-tools.html, as well as census data in other categories: http://national.census.okfn.org) to analyze temporal patterns of geosocial trends. HTH.', 2452, '2014-08-20 03:06:01.753', '8d2f2492-595a-4f45-8bd5-210098995854', 1005, 2593, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I will try to answer your questions, but before I''d like to note that using term "large dataset" is misleading, as "large" is a *relative* concept. You have to provide more details. If you''re dealing with **bid data**, then this fact will most likely affect selection of preferred *tools*, *approaches* and *algorithms* for your **data analysis**. I hope that the following thoughts of mine on data analysis address your sub-questions. Please note that the numbering of my points does not match the numbering of your sub-questions. However, I believe that it better reflects general **data analysis workflow**, at least, how I understand it.

1) Firstly, I think that you need to have at least some kind of **conceptual model** in mind (or, better, on paper). This model should guide you in your *exploratory data analysis (EDA)*. A presence of a *dependent variable (DV)* in the model means that in your *machine learning (ML)* phase later in the analysis you will deal with so called supervised ML, as opposed to unsupervised ML in the absence of an identified DV.

2) Secondly, **EDA** is a crucial part. IMHO, EDA should include **multiple iterations** of producing *descriptive statistics* and *data visualization*, as you refine your understanding about the data. Not only this phase will give you valuable insights about your datasets, but it will feed your next important phase - **data cleaning and transformation**. Just throwing your raw data into a statistical software package won''t give much - for any **valid** statistical analysis, data should be *clean, correct and consistent*. This is often the most time- and effort-consuming, but absolutely necessary part. For more details on this topic, read these nice papers: http://vita.had.co.nz/papers/tidy-data.pdf (by Hadley Wickham) and http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf (by Edwin de Jonge and Mark van der Loo).

3) Now, as you''re hopefully done with *EDA* as well as data cleaning and transformation, your ready to start some more statistically-involved phases. One of such phases is *exploratory factor analysis (EFA)*, which will allow you to extract the underlying **structure** of your data. For datasets with large number of variables, the positive side effect of EFA is *dimensionality reduction*. And, while in that sense EFA is similar to *principal components analysis (PCA)* and other dimensionality reduction approaches, I think that EFA is more important as it allows to refine your conceptual model of the phenomena that your data "describe", thus making sense of your datasets. Of course, in addition to EFA, you can/should perform **regression analysis** as well as apply **machine learning techniques**, based on your findings in previous phases.

Finally, a note on **software tools**. In my opinion, current state of statistical software packages is at such point that practically any major software packages have comparable offerings feature-wise. If you study or work in an organization that have certain policies and preferences in term of software tools, then you are *constrained* by them. However, if that is not the case, I would heartily recommend **open source** statistical software, based on your comfort with its specific *programming language*, *learning curve* and your *career perspectives*. My current platform of choice is **R Project**, which offers mature, powerful, flexible, extensive and open statistical software, along with amazing ecosystem of packages, experts and enthusiasts. Other nice choices include *Python*, *Julia* and specific open source software for processing **big data**, such as *Hadoop*, *Spark*, *NoSQL* databases, *WEKA*. For more examples of open source software for **data mining**, which include general and specific statistical and ML software, see this section of a Wikipedia page: http://en.wikipedia.org/wiki/Data_mining#Free_open-source_data_mining_software_and_applications.', 2452, '2014-08-20 05:43:08.610', 'f24f0d86-665c-4ac8-a533-a4a37a9e714a', 1006, 2594, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I will try to answer your questions, but before I''d like to note that using term "large dataset" is misleading, as "large" is a *relative* concept. You have to provide more details. If you''re dealing with **bid data**, then this fact will most likely affect selection of preferred *tools*, *approaches* and *algorithms* for your **data analysis**. I hope that the following thoughts of mine on data analysis address your sub-questions. Please note that the numbering of my points does not match the numbering of your sub-questions. However, I believe that it better reflects general **data analysis workflow**, at least, how I understand it.

1) Firstly, I think that you need to have at least some kind of **conceptual model** in mind (or, better, on paper). This model should guide you in your *exploratory data analysis (EDA)*. A presence of a *dependent variable (DV)* in the model means that in your *machine learning (ML)* phase later in the analysis you will deal with so called supervised ML, as opposed to unsupervised ML in the absence of an identified DV.

2) Secondly, **EDA** is a crucial part. IMHO, EDA should include **multiple iterations** of producing *descriptive statistics* and *data visualization*, as you refine your understanding about the data. Not only this phase will give you valuable insights about your datasets, but it will feed your next important phase - **data cleaning and transformation**. Just throwing your raw data into a statistical software package won''t give much - for any **valid** statistical analysis, data should be *clean, correct and consistent*. This is often the most time- and effort-consuming, but absolutely necessary part. For more details on this topic, read these nice papers: http://vita.had.co.nz/papers/tidy-data.pdf (by Hadley Wickham) and http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf (by Edwin de Jonge and Mark van der Loo).

3) Now, as you''re hopefully done with *EDA* as well as data cleaning and transformation, your ready to start some more statistically-involved phases. One of such phases is *exploratory factor analysis (EFA)*, which will allow you to extract the underlying **structure** of your data. For datasets with large number of variables, the positive side effect of EFA is *dimensionality reduction*. And, while in that sense EFA is similar to *principal components analysis (PCA)* and other dimensionality reduction approaches, I think that EFA is more important as it allows to refine your conceptual model of the phenomena that your data "describe", thus making sense of your datasets. Of course, in addition to EFA, you can/should perform **regression analysis** as well as apply **machine learning techniques**, based on your findings in previous phases.

Finally, a note on **software tools**. In my opinion, current state of statistical software packages is at such point that practically any major software packages have comparable offerings feature-wise. If you study or work in an organization that have certain policies and preferences in term of software tools, then you are *constrained* by them. However, if that is not the case, I would heartily recommend **open source** statistical software, based on your comfort with its specific *programming language*, *learning curve* and your *career perspectives*. My current platform of choice is **R Project**, which offers mature, powerful, flexible, extensive and open statistical software, along with amazing ecosystem of packages, experts and enthusiasts. Other nice choices include *Python*, *Julia* and specific open source software for processing **big data**, such as *Hadoop*, *Spark*, *NoSQL* databases, *WEKA*. For more examples of open source software for **data mining**, which include general and specific statistical and ML software, see this section of a Wikipedia page: http://en.wikipedia.org/wiki/Data_mining#Free_open-source_data_mining_software_and_applications.

UPDATE: Forgot to mention *Rattle* (http://rattle.togaware.com), which is also a very popular open source R-oriented GUI software for data mining.', 2452, '2014-08-20 06:28:06.767', '7c798888-6f30-4aa4-b617-0500a6515b9e', 1006, 'Added info on Rattle software.', 2595, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><tools><beginner>', 97, '2014-08-20 09:50:18.473', 'c4111f36-1fd8-4aeb-bfb8-5c2b98f62cb4', 1002, 'Added relevant tags.', 2596, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-20 09:50:18.473', 'c4111f36-1fd8-4aeb-bfb8-5c2b98f62cb4', 1002, 'Proposed by 97 approved by 2452, 21 edit id of 138', 2597, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-mining><classification><binary>', 97, '2014-08-20 11:06:54.447', '8bf693ce-e8ac-4969-95dc-d001ce903041', 992, 'More relevant tags.', 2598, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-20 11:06:54.447', '8bf693ce-e8ac-4969-95dc-d001ce903041', 992, 'Proposed by 97 approved by 1241 edit id of 140', 2599, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><dataset><text-mining><search>', 97, '2014-08-20 12:14:01.593', 'c8aaba30-05f0-420b-9bd8-dc228ee5428d', 1003, 'Added more relevant tags.', 2600, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-20 12:14:01.593', 'c8aaba30-05f0-420b-9bd8-dc228ee5428d', 1003, 'Proposed by 97 approved by 2452, 1097 edit id of 139', 2601, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to scrap some data from a website.
I have used import.io but still not much satisfied.. can any of you suggest about it.. whats the best tool to get the unstructured data from web', 867, '2014-08-20 14:12:03.870', '17f2b70e-c8c1-4629-bec3-3be4f9b44879', 1007, 2602, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Looking for Web scrapping tool for unstructured data', 867, '2014-08-20 14:12:03.870', '17f2b70e-c8c1-4629-bec3-3be4f9b44879', 1007, 2603, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<tools><crawling>', 867, '2014-08-20 14:12:03.870', '17f2b70e-c8c1-4629-bec3-3be4f9b44879', 1007, 2604, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try BeautifulSoup - http://www.crummy.com/software/BeautifulSoup/

From the website "Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping."
I have no personally used it, but it often comes up in regards to a nice library for scraping. Here''s a blog post on using it to scrape Craigslist http://www.gregreda.com/2014/07/27/scraping-craigslist-for-tickets/', 403, '2014-08-20 15:34:00.830', '6adcf555-c3d1-4515-89be-c3a929fcfc7f', 1008, 2605, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you have R and the `spacetime` package then you are only `data(package="spacetime")` away from a list of space-time data sets bundled with the package:


    Data sets in package spacetime:

    DE_NUTS1 (air)          Air quality data, rural background PM10 in
                            Germany, daily averages 1998-2009
    fires                   Northern Los Angeles County Fires
    rural (air)             Air quality data, rural background PM10 in
                            Germany, daily averages 1998-2009
then for example:

    > data(fires)
    > str(fires)
    ''data.frame'': 313 obs. of  3 variables:
     $ Time: int  5863 5870 6017 6018 6034 6060 6176 6364 6366 6372 ...
     $ X   : num  63.9 64.3 64.1 64 64.4 ...
     $ Y   : num  19.4 20.1 19.7 19.8 20.3 ...

', 471, '2014-08-20 15:53:29.403', '69567b12-e9c7-4e97-8159-b9f4f94a45da', 1009, 2606, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You don''t mention what language you''re programming in (please consider adding it as a tag), so general help would be to seek out a HTML parser and use that to pull the data. Some web sites can have simply awful HTML code and can be very difficult to scrape, and just when you think you have it...

A HTML parser will parse all the html and allow you to access it in a structured sort of way, whether that''s from an array, an object etc.', 2861, '2014-08-20 19:08:39.807', 'b10522a6-83c5-4c81-8c92-76f83b05a779', 1010, 2607, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve just started reading about AB testing, as it pertains to optimizing website design.  I find it interesting that most of the methods assume that changes to the layout and appearance are independent of each other.  I understand that the most common method of optimization is the [''multi-armed bandit''][1] procedure.  While I grasp the concept of it, it seems to ignore the fact that changes (changes to the website in this case) are not independent to each other.

For example, if company is testing the placement and color of the logo on the website, they find the optimal color first then the optimal placement.  Not that I''m some expert on human psychology, but shouldn''t these be related? Can the multi-armed bandit method be efficiently used in this case or more complicated cases?

My first instinct is to say no.  On that note, why haven''t people used heuristic algorithms to optimize over complicated AB testing sample spaces?  For an example, I thought someone might have used a genetic algorithm to optimize a website layout, but I can find no examples of something like this out there. This leads me to believe that I''m missing something important in my understanding of AB testing as it applies to website optimization.

Why isn''t heuristic optimization used on more complicated websites?


  [1]: http://en.wikipedia.org/wiki/Multi-armed_bandit', 375, '2014-08-20 21:12:49.927', 'e72474ef-cfb1-418d-96d9-6dca6d4cea9b', 1013, 2614, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using Heuristic Methods for AB Testing', 375, '2014-08-20 21:12:49.927', 'e72474ef-cfb1-418d-96d9-6dca6d4cea9b', 1013, 2615, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<optimization><consumerweb>', 375, '2014-08-20 21:12:49.927', 'e72474ef-cfb1-418d-96d9-6dca6d4cea9b', 1013, 2616, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have installed [Drake][1] on Windows 7 64-bit.

I am using JDK 1.7.0_51.

I tried both using the pre-compiled jar file and
compiling from the Clojure source using [leiningen][2].
The resulting Drake version is 0.1.6, the current development version.

When running Drake, I get the current version number.

Next, I tried to go through [the tutorial][3]. The command:


    java -jar drake.jar  -w .\workflow.d

results in the following Exception:

    java.lang.Exception: no input data found in locations: D:\tools\drake\in.c
    sv

Even though the file exists and has text inside it.
The same scenario works in a similar installation on Ubuntu 12.04.
Am I doing something wrong, or is this a Windows-specific bug?

  [1]: https://github.com/Factual/drake
  [2]: https://github.com/technomancy/leiningen
  [3]: https://github.com/Factual/drake/wiki/Tutorial', 895, '2014-08-21 06:31:50.197', '0a0454df-f6df-423a-be9c-133105f16d7d', 1015, 2618, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Making Factual drake work on Windows 7 64-bit', 895, '2014-08-21 06:31:50.197', '0a0454df-f6df-423a-be9c-133105f16d7d', 1015, 2619, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<tools>', 895, '2014-08-21 06:31:50.197', '0a0454df-f6df-423a-be9c-133105f16d7d', 1015, 2620, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m studying RL in order to implement a kind of time series pattern analyzer such as market.

The most examples I have seen are based on the maze environment.

But in real market environment, the signal changes endlessly as time passes and I can not guess how can I model environment and states.


Another question is about buy sell modeling.

Let''s assume that the agent randomly buy at time t and sell at time t + alpha.

It''s so simple to calculate reward.
The problem is how can I model Q matrix and how can I model signals between buy ans sell actions.

Can you share some source code or guidance for similar situation?

Thanks in advance,
', 3030, '2014-08-21 10:13:54.130', 'c3d129a6-ba96-45cb-b8e2-df55ec1a1631', 1017, 2624, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How can I model open environment in reinforcement learning?', 3030, '2014-08-21 10:13:54.130', 'c3d129a6-ba96-45cb-b8e2-df55ec1a1631', 1017, 2625, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 3030, '2014-08-21 10:13:54.130', 'c3d129a6-ba96-45cb-b8e2-df55ec1a1631', 1017, 2626, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Where can I find free spatio-temporal dataset for download?', 97, '2014-08-21 11:52:31.890', '54809862-f276-46ed-964e-fc708ea34cfe', 997, 'More relevant tags.', 2627, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<dataset><open-source><freebase>', 97, '2014-08-21 11:52:31.890', '54809862-f276-46ed-964e-fc708ea34cfe', 997, 'More relevant tags.', 2628, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-21 11:52:31.890', '54809862-f276-46ed-964e-fc708ea34cfe', 997, 'Proposed by 97 approved by 21 edit id of 141', 2629, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I want to scrape some data from a website.
I have used import.io but still not much satisfied.. can any of you suggest about it.. whats the best tool to get the unstructured data from web', 471, '2014-08-21 11:52:35.660', '6b3a8a15-e730-4999-a589-6cc2ac949426', 1007, 'scraping not scrapping.', 2630, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Looking for Web scraping tool for unstructured data', 471, '2014-08-21 11:52:35.660', '6b3a8a15-e730-4999-a589-6cc2ac949426', 1007, 'scraping not scrapping.', 2631, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-21 11:52:35.660', '6b3a8a15-e730-4999-a589-6cc2ac949426', 1007, 'Proposed by 471 approved by 2452, 21 edit id of 142', 2632, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think it always depends on the scenario. Using a representative data set is not always the solution. Assume that your training set with 1000 negative examples and 20 positive examples. Without any modeification of the classifier, your algorithm will tend to classify all new examples as negative. In some scenarios this is O.K. But in many cases the costs of missing postive examples is high so you have to find a solution for it.

In such cases you can use a cost sensitive machine learning algorithm. For example in the case of medical diagnosis data analysis.

In summary: Classification erros do not have the same cost! ', 979, '2014-08-22 09:03:13.333', '9203de8c-e10b-40b5-bda7-5ec6f9fff9b2', 1019, 2634, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am hoping to model the characteristics of the users of a specific page on Facebook, which has roughly 2 million likes. I have been looking at the Facebook SDK/API, but I can''t really see if what I would like to do is possible. It seems that the users share quite different amounts of data so I probably discard a lot of users and only use the ones with a quite open public profile. I would like to have the following data:

1) See the individuals that have ''liked'' the page.

2) See the list of friends for each person that have ''liked'' the page.

3) See gender for each person (optional)

4) See other pages that each person has liked (optional)

Could anyone tell me if it is possible to get this data? As mentioned earlier it is okay if I discard data for users that don''t like to share this data.', 3044, '2014-08-22 14:01:23.640', '56642b23-61e8-44b0-b4db-6c88c45e0e33', 1020, 2644, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Available data about ''likers'' as a page on Facebook', 3044, '2014-08-22 14:01:23.640', '56642b23-61e8-44b0-b4db-6c88c45e0e33', 1020, 2645, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<social-network-analysis>', 3044, '2014-08-22 14:01:23.640', '56642b23-61e8-44b0-b4db-6c88c45e0e33', 1020, 2646, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have thousands of lists of strings, and each list has about 10 strings. Most strings in a given list are very similar, though some strings are (rarely) completely unrelated to the others and some strings contain irrelevant words. I am looking for an algorithm or a library that will convert each list into a "most probable", "most consensual" string.

Here is one such list.

<ul>
<li>Star Wars: Episode IV A New Hope | StarWars.com</li>
<li>Star Wars Episode IV - A New Hope (1977)</li>
<li>Star Wars: Episode IV - A New Hope - Rotten Tomatoes</li>
<li>Watch Star Wars: Episode IV - A New Hope Online Free</li>
<li>Star Wars (1977) - Greatest Films</li>
<li>[REC] 4 poster promises death by outboard motor - SciFiNow</li>
</ul>

For this list, any string matching the regular expression `^Star Wars:? Episode IV (- )?A New Hope$` would be acceptable.

I have looked at Andrew Ng''s course on Machine Learning on Coursera, but I was not able to find a similar problem.', 3047, '2014-08-22 15:59:07.097', '0a36001e-e244-4286-9611-e2f7be21d52d', 1021, 2647, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Most representative string among list', 3047, '2014-08-22 15:59:07.097', '0a36001e-e244-4286-9611-e2f7be21d52d', 1021, 2648, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><similarity><information-retrieval>', 3047, '2014-08-22 15:59:07.097', '0a36001e-e244-4286-9611-e2f7be21d52d', 1021, 2649, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Want to wish you good luck. Some time ago faced with the same problem, but didn''t find any satisfying solution.
First of all, there is no way to get list of users, who "liked" a particular page. Even, if you are an administrator of this page (I was). One only can get list of last 3 or 5 hundred users.

Friendships data for most of the users is also inaccessible. Looks like gender is the only thing from your list, that you can get.

Data about pages, that exact user "likes", should be available (as it''s written in docs), but in reality, through API you can collect something only for friends and FoF. Even though this data is available through web interface. So the only way is to try dirty trick with parsing and scraping (but remember, that I didn''t advise it ;) ). ', 941, '2014-08-22 18:27:04.377', '2522fe43-42d4-49fe-89fc-99736814abee', 1022, 2650, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As a naive solution I would suggest to first select the strings which contain the most frequent tokens inside the list. In this way you can get rid of irrelevant string.

In the second phrase I would do a majority voting. Assuming the 3 sentences:

 - Star Wars: Episode IV A New Hope | StarWars.com
 - Star Wars Episode IV - A New Hope (1977)
 - Star Wars: Episode IV - A New Hope - Rotten Tomatoes

I would go through the tokens one by one. We start by "Star". It wins as all the string start with it. "Wars" will also win. The next one is ":". It will also win.

All the tokens will ein in majority voting till "Hope". The next token after "Hope" will be either "|", or "(" or "-". None of the will win in majority voting so I will stop here!

Another solution would be probably to use [Longest common subsequence][1].

As I said I have not though about it much. So there might be much more better solutions to your problem :-)


  [1]: http://en.wikipedia.org/wiki/Longest_common_subsequence_problem', 979, '2014-08-23 09:19:08.577', 'e72189e8-ee4f-4c69-914d-229fcf6bfc38', 1023, 2651, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<social-network-analysis>', 21, '2014-08-23 09:22:19.073', '44d49a63-828d-4d5a-84d3-c048c65725cc', 1020, 'edited tags', 2652, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve fit a GLM (Poisson) to a data set where one of the variables is categorical for the year a customer bought a product from my company, ranging from 1999 to 2012. There''s a linear trend of the coefficients for the values of the variable as the year of sale increases.

Is there any problem with trying to improve predictions for 2013 and maybe 2014 by extrapolating to get the coefficients for those years?', 1241, '2014-08-23 13:47:01.907', '7c0f8fbe-2297-4fae-bcfb-39d0b3e452e3', 1024, 2653, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Extrapolating GLM coefficients for year a product was sold for future years?', 1241, '2014-08-23 13:47:01.907', '7c0f8fbe-2297-4fae-bcfb-39d0b3e452e3', 1024, 2654, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><statistics>', 1241, '2014-08-23 13:47:01.907', '7c0f8fbe-2297-4fae-bcfb-39d0b3e452e3', 1024, 2655, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Extrapolating GLM coefficients for year a product was sold into future years?', 1241, '2014-08-23 13:55:05.587', 'ebafe88f-eca5-44c6-841f-f715739f6b50', 1024, 'edited title', 2656, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have been developing a chess program which makes use of alpha-beta pruning algorithm and an evaluation function that evaluates positions using the following features namely material, kingsafety, mobility, pawn-structure and trapped pieces etc..... My evaluation function is derived from the f(p) = w1 * material + w2 * kingsafety + w3 * mobility + w4 * pawn-structure + w5 * trapped pieces , where "w" is the weight assigned to each feature. At this point i want to tune the weights of my evaluation function using temporal difference, where the agent plays against itself and in the process gather training data from its environment (which is a form of reinforcement learning). i have read some books and articles in order to have an insight on how to implement this in java but they seems to be theoretical rather than practical. please i need a detailed explanation and pseudo codes on how to automatically tune the weights of my evaluation function based on previous games. thanks in advances.', 3052, '2014-08-23 13:56:43.813', '93bd42d1-7f77-451d-b83b-9450aedc5ad9', 1025, 2657, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('implementing temporal difference in chess', 3052, '2014-08-23 13:56:43.813', '93bd42d1-7f77-451d-b83b-9450aedc5ad9', 1025, 2658, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><algorithms>', 3052, '2014-08-23 13:56:43.813', '93bd42d1-7f77-451d-b83b-9450aedc5ad9', 1025, 2659, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think there are two separate issues to consider: Training time, and prediction accuracy.

Take a simple example : consider you have two classes, that have a multivariate normal distribution. Basically you need to estimate the respective class means and class covariances.  Now the first thing you care about is your estimate of the difference in the class means: but your performance is limited by the accuracy of the worst estimated mean: its no good estimating one mean to the 100th decimal place - if the other mean is only estimated to 1 decimal place.  So its a waste of computing resources to use all the data - you can instead undersample the  more common class AND reweight the classes appropriately. ( those computing resources can then be used exploring different input variables etc)

Now the second issue is predictive accuracy: different algorithms use different error metrics, which may or may not agree with your own objectives. eg logistic regression will penalise overall probability error,  so if  most of your data is from one class, then it will tend to try to improve accurate probability estimates ( eg 90 vs 95% probability) of that one class rather than trying to identify the rare class. in that case you would definitely want to try to reweight to emphasize the rare class ( and subsequently adjust the estimate [by adjusting the bias term] to get the probability estimates realigned)
', 1256, '2014-08-23 14:15:58.660', 'ef60fcbf-9006-49d8-ba91-ebe8b2b66338', 1026, 2660, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A first remark, you should watch ''Wargames'' to know what you''re getting yourself into.

What you want is f(p) such that f(p) is as close as possible to strength of position.

A very simple solution using genetic algo would be to setup 10000 players with different weights and see which wins. Then keep the top 1000 winners'' weight, copy them 10 times, alter them slightly to explore weight space, and run the simulation again. That''s standard GA, given a functional form, what are the best coefficients for it.

Another solution is to extract the positions, so you have a table ''(material, kingsafety, mobility, pawn-structure, trappedpieces) -> goodness of position'' where goodness of position is some objective factor (outcome win/lose computed using simulations above or known matches, depth of available tree, number of moves under the tree where one of the 5 factors gets better. You can then try different functional forms for your f(p), regression, svm.', 3053, '2014-08-23 15:25:44.903', 'aad5350d-8e69-41a8-96df-7391f77cbfc5', 1027, 2661, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have been reading around about Random Forests but I cannot really find a definitive answer about the problem of overfitting. According to the original paper of Breiman, they should not overfit when increasing the number of trees in the forest, but it seems that there is not consensus about this. This is creating me quite some confusion about the issue.

Maybe someone more expert than me can give me a more concrete answer or point me in the right direction to better understand the problem.', 3054, '2014-08-23 16:54:06.380', '4b5a061b-2637-4a98-8069-0f7b8e8166ab', 1028, 2662, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Do Random Forest overfit?', 3054, '2014-08-23 16:54:06.380', '4b5a061b-2637-4a98-8069-0f7b8e8166ab', 1028, 2663, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><random-forest>', 3054, '2014-08-23 16:54:06.380', '4b5a061b-2637-4a98-8069-0f7b8e8166ab', 1028, 2664, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which one will be the dominating programming language for next 5 years for analytics , machine learning . R verses python verses SAS. Advantage and disadvantage.', 3057, '2014-08-23 19:34:09.417', '2303e9a9-4fee-4acf-ab87-44508d1a20fe', 1029, 2665, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which one will be the dominating programming language for next 5 years for analytics , machine learning . R or python or SAS', 3057, '2014-08-23 19:34:09.417', '2303e9a9-4fee-4acf-ab87-44508d1a20fe', 1029, 2666, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><python>', 3057, '2014-08-23 19:34:09.417', '2303e9a9-4fee-4acf-ab87-44508d1a20fe', 1029, 2667, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I believe that this is a case for applying *time series analysis*, in particular *time series forecasting* (http://en.wikipedia.org/wiki/Time_series).', 2452, '2014-08-23 20:05:38.700', '6d85fbc5-f967-4df2-a848-ab1c3287afca', 1030, 2668, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I believe that this is a case for applying *time series analysis*, in particular *time series forecasting* (http://en.wikipedia.org/wiki/Time_series). Consider the following resources:

 - http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471363553.html
 - http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf (especially section
   4.6)
 - http://arxiv.org/abs/0802.0219 (Bayesian approach)', 2452, '2014-08-23 20:32:06.300', 'c3feccfa-de96-4fd1-b1de-fc82d3cddae8', 1030, 'Added resources on the topic.', 2669, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I believe that this is a case for applying *time series analysis*, in particular *time series forecasting* (http://en.wikipedia.org/wiki/Time_series). Consider the following resources on **time series regression**:

 - http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471363553.html
 - http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf (especially section
   4.6)
 - http://arxiv.org/abs/0802.0219 (Bayesian approach)', 2452, '2014-08-23 20:37:43.057', 'b73da1df-c791-4de1-822d-5b169566c0ba', 1030, 'Added resources on the topic.', 2670, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is a great [survey](http://blog.revolutionanalytics.com/2014/01/in-data-scientist-survey-r-is-the-most-used-tool-other-than-databases.html) published by O''Reilly collected at Strata.

You can see that SAS is not widely popular, and there is no reason why that should change at this point. One can rule that out.

R is barely ahead of Python, 43% vs 41%. You can find many blogs expressing the rise of Python in data science. I would go with Python in the near future.

But 5 years is a very long time. I think Golang will steal a lot of developers from Python in general. This might spill over to data science usage as well. Code can be written to execute in parallel very easily, which makes it a perfect vehicle for Big Data processing. [Julia''s](http://julialang.org/) benchmarks for technical computing are even more impressive, and you can have iPython like stuff with iJulia. Hence Python is likely to lose some steam to both. But there are ways to call Julia functions from R and Python, so you can experiment using best sides of each.', 3051, '2014-08-24 00:46:03.813', '153192f3-74b1-4e48-bf85-b96a2b1f1650', 1031, 2671, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><algorithms>', 97, '2014-08-24 03:31:06.623', 'e1dea512-eb36-46a7-8ac5-c7f4a57c1e1c', 1025, 'Additional tags.', 2672, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-24 03:31:06.623', 'e1dea512-eb36-46a7-8ac5-c7f4a57c1e1c', 1025, 'Proposed by 97 approved by 84 edit id of 146', 2673, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You may want to check [cross-validated][1] - a stachexchange website for many things, including machine learning.

In particular, this question (with exactly same title) has already been answered multiple times. Check these links: http://stats.stackexchange.com/search?q=random+forest+overfit

But I may give you the short answer to it: yes, it does overfit, and sometimes you need to control the complexity of the trees in your forest, or even prune when they grow too much - but this depends on the library you use for building the forest. E.g. in `randomForest` in R you can only control the complexity


  [1]: http://stats.stackexchange.com/', 816, '2014-08-24 08:22:23.497', '788bd2ec-3973-4fb3-8ecc-36c48dfa1312', 1032, 2674, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Due to the very Big increase in **Big Data** (pun intended) and the desire for robust stable scalable applications I actually believe it to be **Scala**.  **Spark** will inevitably become the main Big Data Machine Learning tool, and it''s main API is in Scala.  Furthermore you simply cannot build a product with scripting languages like Python and R, one can only experiment with these languages.  What Scala brings is a way to BOTH experiment and produce a product.  More reasons

1. Think functionally - write faster code and more readable code
2. Scala means the end of the two team development cycle. So better product ownership, more agile cross functional teams, and half as many employees required to make a product as we will no longer need both a "research" team and an engineering team, Data Scientists will be able to do both.  This is because Scala is;

 - A production quality language - static typing, but with the flexibility of dynamic typing due to implicits
 - Interoperable with rest of Java world (so Apache Commons Math, Databases, Cassandra, HBase, HDFS, Akka, Storm, many many databases, more spark components (e.g. graphx, SparkStreaming)

3. Step into Spark code easily and understand it, also helps with debugging

4. Scala is awesome:
 - Amazing IDE support due to static typing
 - Property based tests with ScalaCheck - insane unit testing
 - Very concise language
 - Suits mathematicians perfectly (especially Pure Mathematicians)

5. A little more efficient as compiled not interpreted

6. Python Spark API sits on Scala API and therefore will always be behind Scala API

7. Much easier to do Mathematics in Scala as it''s a Scalable Language where one can easily define DSLs and due to being so functional

8. Akka - another way other than storm to do High Velocity

9. Pimp my library pattern makes adding methods to Spark RDDs really easy', 2668, '2014-08-24 11:03:08.773', '97758086-a6f3-400f-8d83-f93497844b15', 1033, 2675, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am exploring different types of parse tree structures. The two widely known parse tree structures are
a) Constituency based parse tree and
b) Dependency based parse tree structures.

I am able to use generate both types of parse tree structures using Stanford NLP package. However, I am not sure how to use these tree structures for my classification task.

For e.g If I want to do sentiment analysis and want to categorize text into positive and negative classes, what features can I derive from parse tree structures for my classification task?', 3064, '2014-08-24 17:09:40.510', '02ea46a5-5b2c-48f2-ad93-aac5e1136f59', 1034, 2676, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What features are generally used from Parse trees in classification process in NLP?', 3064, '2014-08-24 17:09:40.510', '02ea46a5-5b2c-48f2-ad93-aac5e1136f59', 1034, 2677, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><nlp><feature-selection><feature-extraction>', 3064, '2014-08-24 17:09:40.510', '02ea46a5-5b2c-48f2-ad93-aac5e1136f59', 1034, 2678, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First compute the edit distance between all pairs of strings.  See http://en.wikipedia.org/wiki/Edit_distance and http://web.stanford.edu/class/cs124/lec/med.pdf.  Then exclude any outliers strings based on some distance threshold.

With remaining strings, you can use the distance matrix to identify the most central string.  Depending on the method you use, you might get ambiguous results for some data. No method is perfect for all possibilities.  For your purposes, all you need is some heuristic rules to resolve ambiguities -- i.e. pick two or more candidates.

Maybe you don''t want to pick "most central" from your list of strings, but instead want to generate a regular expression that captures the pattern common to all the non-outlier strings.  One way to do this is to synthesize a string that is equidistant from all the non-outlier strings.  You can work out the required edit distance from the matrix, and then you''d randomly generate regular using those distances as constraints.  Then you''d test candidate regular expressions and accept the first one that fits the constraints and also accepts all the strings in your non-outlier list.  (Start building regular expressions from longest common substring lists, because those are non-wildcard characters.)', 609, '2014-08-24 22:02:01.050', 'cfdda994-b00a-43a9-a69f-f45696beda13', 1035, 2679, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am exploring how to model a data set using normal distributions with both mean and variance defined as linear functions of independent variables.

Something like N ~ (f(x), g(x)).

I generate a random sample like this:

    def draw(x):
        return norm(5 * x + 2, 3 *x + 4).rvs(1)[0]

So I want to retrieve 5, 2 and 4 as the parameters for my distribution.

I generate my sample:

smp = np.zeros((100,2))

    for i in range(0, len(smp)):
        smp[i][0] = i
        smp[i][1] = draw(i)

The likelihood function is:

    def lh(p):
        p_loc_b0 = p[0]
        p_loc_b1 = p[1]
        p_scl_b0 = p[2]
        p_scl_b1 = p[3]

        l = 1
        for i in range(0, len(smp)):
            x = smp[i][0]
            y = smp[i][1]
            l = l * norm(p_loc_b0 + p_loc_b1 * x, p_scl_b0 + p_scl_b1 * x).pdf(y)

        return -l

So the parameters for the linear functions used in the model are given in the p 4-variable vector.

Using scipy.optimize, I can solve for the MLE parameters using an extremely low xtol, and already giving the solution as the starting point:

    fmin(lh, x0=[2,5,3,4], xtol=1e-35)

Which does not work to well:

    Warning: Maximum number of function evaluations has been exceeded.
    array([ 3.27491346,  4.69237042,  5.70317719,  3.30395462])

Raising the xtol to higher values does no good.

So i try using a starting solution far from the real solution:

    >>> fmin(lh, x0=[1,1,1,1], xtol=1e-8)
    Optimization terminated successfully.
             Current function value: -0.000000
             Iterations: 24
             Function evaluations: 143
    array([ 1.,  1.,  1.,  1.])

Which makes me think:

PDF are largely clustered around the mean, and have very low gradients only a few standard deviations away from the mean, which must be not too good for numerical methods.

So how does one go about doing these kind of numerical estimation in functions where gradient is very near to zero away from the solution?





', 3068, '2014-08-25 00:28:09.003', 'a88573cb-460e-4dc9-a3e0-05130f55018e', 1036, 2680, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to numerically estimate MLE estimators in python when gradients are very small far from the optimal solution?', 3068, '2014-08-25 00:28:09.003', 'a88573cb-460e-4dc9-a3e0-05130f55018e', 1036, 2681, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><statistics>', 3068, '2014-08-25 00:28:09.003', 'a88573cb-460e-4dc9-a3e0-05130f55018e', 1036, 2682, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I understand you question correctly, there are two reasons why genetic algorithm might not a good idea for optimizing website features:

1) Feedback data is coming it too slow, say once a day, genetic algorithm might take a while to converge.

2) In the process of testing genetic algorithm will probably come up with combinations that are ''strange'' and that might not be the risk the company wants to take.

', 3070, '2014-08-25 03:01:09.837', '298def1a-1df8-4ed3-a87c-26875b7801f3', 1037, 2683, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have thousands of lists of strings, and each list has about 10 strings. Most strings in a given list are very similar, though some strings are (rarely) completely unrelated to the others and some strings contain irrelevant words. They can be considered to be noisy variations of a canonical string. I am looking for an algorithm or a library that will convert each list into this canonical string.

Here is one such list.

<ul>
<li>Star Wars: Episode IV A New Hope | StarWars.com</li>
<li>Star Wars Episode IV - A New Hope (1977)</li>
<li>Star Wars: Episode IV - A New Hope - Rotten Tomatoes</li>
<li>Watch Star Wars: Episode IV - A New Hope Online Free</li>
<li>Star Wars (1977) - Greatest Films</li>
<li>[REC] 4 poster promises death by outboard motor - SciFiNow</li>
</ul>

For this list, any string matching the regular expression `^Star Wars:? Episode IV (- )?A New Hope$` would be acceptable.

I have looked at Andrew Ng''s course on Machine Learning on Coursera, but I was not able to find a similar problem.', 3047, '2014-08-25 08:11:49.307', '684ff727-ba8a-4186-93f2-bf67376b18a9', 1021, 'Introduced the word ''canonical'' as suggested by Sean Owen', 2684, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Extract canonical string from a list of noisy strings', 3047, '2014-08-25 08:11:49.307', '684ff727-ba8a-4186-93f2-bf67376b18a9', 1021, 'Introduced the word ''canonical'' as suggested by Sean Owen', 2685, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have created external table in Hive in the hdfs path ''hdfs://localhost.localdomain:8020/user/hive/training''. If I apply describe command I can find the table path as shown below. But when I browse through the namenode web page, the table name does not showing up in the path.

    hive> describe extended testtable4;
    OK
    firstname   string
    lastname    string
    address string
    city    string
    state   string
    country string

        ***Detailed Table Information  Table(tableName:testtable4, dbName:default, owner:cloudera, createTime:1408765301, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:firstname, type:string, comment:null), FieldSchema(name:lastname, type:string, comment:null), FieldSchema(name:address, type:string, comment:null), FieldSchema(name:city, type:string, comment:null), FieldSchema(name:state, type:string, comment:null), FieldSchema(name:country, type:string, comment:null)], location:hdfs://localhost.localdomain:8020/user/hive/training, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=,, field.delim=,, line.delim=
        }), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{EXTERNAL=TRUE, transient_lastDdlTime=1408765301}, viewOriginalText:null, viewExpandedText:null, tableType:EXTERNAL_TABLE)
        Time taken: 0.7 seconds***', 1314, '2014-08-25 09:53:13.823', 'fa73acad-d874-4896-b391-945f6ae09ebf', 1038, 2686, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hive External table does not showing in Namenode (Cloudera-QuickstartVm)', 1314, '2014-08-25 09:53:13.823', 'fa73acad-d874-4896-b391-945f6ae09ebf', 1038, 2687, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 1314, '2014-08-25 09:53:13.823', 'fa73acad-d874-4896-b391-945f6ae09ebf', 1038, 2688, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think dependencies can be used to improve the accurary of your sentiment classifier. Consider the following examples:

E1: Bill is not a scientist

and assume that the token "scientist" has a positive sentiment in a specific domain.

Knowing the dependency neg(scientist, not) we can see that the example above has a negative sentiment. Without knowing this dependency we would probably classify the sentence as positive.

Another types of dependencies can be used probably in the same way to improve the accuracy of the classifiers.', 979, '2014-08-25 11:29:35.047', 'e16007b0-f0e5-43dd-8c32-333b6f3c8c40', 1039, 2689, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('If I understand you question correctly, there are two reasons why genetic algorithm might not a good idea for optimizing website features:

1) Feedback data is coming in too slow, say once a day, genetic algorithm might take a while to converge.

2) In the process of testing genetic algorithm will probably come up with combinations that are ''strange'' and that might not be the risk the company wants to take.

', 3070, '2014-08-25 16:52:44.817', 'e7ac6d32-18f9-4df4-929c-071827338e9b', 1037, 'edited body', 2693, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you suspect your response is linear with year, then put year in as a numeric term in your model rather than a categorical.

Extrapolation is then perfectly valid based on the usual assumptions of the GLM family. Make sure you correctly get the errors on your extrapolated estimates.', 471, '2014-08-26 07:14:54.767', '03cd336b-ebaf-4b11-a980-fbdd38fa3ab8', 1041, 2695, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('If you suspect your response is linear with year, then put year in as a numeric term in your model rather than a categorical.

Extrapolation is then perfectly valid based on the usual assumptions of the GLM family. Make sure you correctly get the errors on your extrapolated estimates.

Just extrapolating the parameters from a categorical variable is wrong for a number of reasons. The first one I can think of is that there may be more observations in some years than others, so any linear extrapolation needs to weight those year''s estimates more. Just eyeballing a line - or even fitting a line to the coefficients - won''t do this. ', 471, '2014-08-26 07:20:52.097', '076d707a-ebd6-45c4-aedb-02853cdc6d5e', 1041, 'added 353 characters in body', 2696, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am seeking a basic list of key data analysis methods used for studying social media platforms online. Are there such key methods, or does this process generally vary according to topic? And is there a standard order in which these methods are applied?(The particular context I''m interested in is how the news is impacting on social media)', 3058, '2014-08-26 07:33:40.080', 'deff398a-46c6-48b5-bce4-e64ab2bfd40d', 1042, 2697, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Key data analysis methods used for studying social media platforms online?', 3058, '2014-08-26 07:33:40.080', 'deff398a-46c6-48b5-bce4-e64ab2bfd40d', 1042, 2698, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<social-network-analysis>', 3058, '2014-08-26 07:33:40.080', 'deff398a-46c6-48b5-bce4-e64ab2bfd40d', 1042, 2699, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<statistics>', 471, '2014-08-26 11:36:01.420', '857b9c13-b8c2-45df-91f0-535a19dc6d46', 1024, 'note data mining', 2700, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-26 11:36:01.420', '857b9c13-b8c2-45df-91f0-535a19dc6d46', 1024, 'Proposed by 471 approved by -1 edit id of 147', 2701, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<statistics><glm><regression>', 21, '2014-08-26 11:36:01.420', '86fc47e3-adbd-4f2d-8602-277fc0e67dca', 1024, 'note data mining', 2702, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-08-26 11:36:17.830', '2e3f126f-c1c5-436f-8e9c-4fcefc62e9e9', 1029, '105', 2703, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As what I described in the title, we are especially interested in those for dealing with big data----ts efficiency and stability, and used in industry not in experiment or university. Thanks!', 3097, '2014-08-26 14:53:40.647', 'd6b915dd-3de0-4f46-927e-cbb9bcfb9f2d', 1044, 2707, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('which programming language has a large library that can do machine learning algorithm, R, matlab or python', 3097, '2014-08-26 14:53:40.647', 'd6b915dd-3de0-4f46-927e-cbb9bcfb9f2d', 1044, 2708, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><data-mining><python><r>', 3097, '2014-08-26 14:53:40.647', 'd6b915dd-3de0-4f46-927e-cbb9bcfb9f2d', 1044, 2709, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Studying social media platforms - key data analysis methods?', 3058, '2014-08-26 17:25:29.930', 'c680def1-a502-4614-9cfd-9617970038ee', 1042, 'Clarifying title - to make it easier to find in searches', 2711, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Have anyone used Shark as repository from resulting datasets from Apache Spark?

I''m starting some tests with Spark and read about this database tecnology. Have anyone been using it?', 3050, '2014-08-26 21:37:12.107', 'af29426e-c8e4-4932-8ce2-0b694194389d', 1045, 2712, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using Shark with Apache Spark', 3050, '2014-08-26 21:37:12.107', 'af29426e-c8e4-4932-8ce2-0b694194389d', 1045, 2713, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop>', 3050, '2014-08-26 21:37:12.107', 'af29426e-c8e4-4932-8ce2-0b694194389d', 1045, 2714, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try <http://deeplearning4j.org/word2vec.html>. This has an implementation of Word2Vec used instead of Bag of Words for NER and other NLP tasks.', 3100, '2014-08-26 21:51:07.247', 'b21210e3-442a-4e79-929d-fd5221e48a01', 1046, 2715, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('1. SPSS is a great tool, but you can accomplish a great deal with resources that you already have on your computer, like Excel, or that are free, like the R-project. Although these tools are powerful, and can help you identify patterns, you''ll need to have a firm grasp of your data before running analyses (I''d recommend running descriptive statistics on your data, and exploring the data with graphs to make sure everything is looking normal). In other words, the tool that you use won''t offer a "silver bullet", because the output will only be as valuable as the input (you know the saying... "garbage in, garbage out"). Much of what I''m saying has already been stated in the reply by Aleksandr - spot on.

2. R can be challenging for those of us who aren''t savvy with coding, but the free resources associated with R and its packages are abundant. If you practice learning the program, you''ll quickly gain traction. Again, you''ll need to be familiar with your data and the analyses you want to run anyway, and that fact remains regardless of the statistical tools you utilize.

3. I''d begin by getting super familiar with my data (follow the steps outlined in the reply from Aleksandr, for starters). You might consider picking up John Foreman''s book called Data Smart. It''s a hands-on book, as John provides datasets and you follow along with his examples (using Excel) to learn various ways of navigating and exploring data. For beginners, it''s a great resource. ', 3101, '2014-08-26 22:48:16.617', '0e002b24-0d6f-4253-8367-d506fdb5a1d3', 1047, 2716, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve making some researches last months and I could find more libraries, contente and active community with Python. Actually I''m using it to ETL processes, some minning jobs and to make map/reduce.', 3050, '2014-08-27 03:41:20.317', '5095a275-69f6-4130-937d-a318578a087b', 1048, 2717, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a linearly increasing time series dataset of a sensor, with value ranges between 50 and 150. I''ve implemented a [Simple Linear Regression][1] algorithm to fit a regression line on such data, and I''m predicting the date when the series would reach 120.

All works fine when the series move upwards. But, there are cases in which the sensor reaches around 110 or 115, and it is reset; in such cases the values would start over again at, say, 50 or 60.

This is where I start facing issues with the regression line, as it starts moving downwards, and it starts predicting old date. I think I should be considering only the subset of data from where it was previously reset. However, I''m trying to understand if there are any algorithms available that consider this case.

I''m new to data science, would appreciate any pointers to move further.

**Edit: nfmcclure''s suggestions applied**

Before applying the suggestions

![enter image description here][2]

Below is the snapshot of what I''ve got after splitting the dataset where the reset occurs, and the slope of two set.

![enter image description here][3]

finding the mean of the two slopes and drawing the line from the mean.

![enter image description here][4]

Is this OK?


  [1]: http://en.wikipedia.org/wiki/Simple_linear_regression
  [2]: http://i.stack.imgur.com/ZsyyQ.png
  [3]: http://i.stack.imgur.com/OEQCw.png
  [4]: http://i.stack.imgur.com/i2qv5.png', 870, '2014-08-27 07:15:52.587', '0b98857f-6701-4bdb-b2e7-35f96eb23eb5', 671, 'added 534 characters in body', 2718, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**General description of the problem**

There is a graph where some of the nodes have a certain type(There are about 3-4 types). For other nodes, type is not known.
I want to predict, based on my graph, for the nodes with unknown type their "most probable" type.

**Possible framework**

I guess the general framework for such tasks is called `label propagation algorithm`.
according to literature on the topic.
Here are some examples: [one][1], [two][2]

Another often mentioned topic is `Frequent Subgraph Mining`, which includes algorithms like `SUBDUE`,`SLEUTH`, and `gSpan`.

**Found in R**

The only label propagation algorithm I managed to find in `R` is `label.propagation.community()` from `igraph` library.
However as the name suggests it is mostly for finding communities, not classification of nodes.

There also seems to be several references to `subgraphMining` library, (here for example)
but looks like it is missing from CRAN.

**Question**


If someone could suggest libraries/frameworks for the task described, I would be very grateful.
Made the question as specific as I could, hope that shall do)


  [1]: http://lvk.cs.msu.su/~bruzz/articles/classification/zhu02learning.pdf
  [2]: http://www.csc.ncsu.edu/faculty/samatova/practical-graph-mining-with-R/slides/pdf/Frequent_Subgraph_Mining.pdf', 3108, '2014-08-27 13:01:14.643', '5ab1dbc4-579d-41fd-bf10-0f237ac659d1', 1050, 2720, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Libraries for (label propagation algorithms/frequent subgraph mining) for graphs in R', 3108, '2014-08-27 13:01:14.643', '5ab1dbc4-579d-41fd-bf10-0f237ac659d1', 1050, 2721, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><r><graphs>', 3108, '2014-08-27 13:01:14.643', '5ab1dbc4-579d-41fd-bf10-0f237ac659d1', 1050, 2722, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('> When using the public Google APIs to retrieve results, I was only able to collect 4-10 results per query.

Here''s how to get more than 10 results per query: https://support.google.com/customsearch/answer/1361951?hl=en

> Google Custom Search and Google Site Search return up to 10 results per query. If you want to display more than 10 results to the user, you can issue multiple requests (using the start=0, start=11 ... parameters) and display the results on a single page. In this case, Google will consider each request as a separate query, and if you are using Google Site Search, each query will count towards your limit.

There are other search engine APIs as well (e.g., [Bing](http://datamarket.azure.com/dataset/bing/search))', 819, '2014-08-27 18:33:23.110', '1e74981f-fb4d-438f-b93f-db73ce68d490', 1051, 2723, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to [summarize](http://stat.ethz.ch/R-manual/R-devel/library/base/html/summary.html) (as in R) the contents of a CSV (possibly after [loading](http://www.endmemo.com/program/R/readcsv.php) it, or storing it somewhere, that''s not a problem). The summary should contain the quartiles, mean, median, min and max of the data in a CSV file for each numeric (integer or real numbers) dimension. The standard deviation would be cool as well.

I would also like to generate some plots to visualize the data, for example 3 plots for the 3 pairs of variables that are more correlated ([correlation coefficient](http://www.r-tutor.com/elementary-statistics/numerical-measures/correlation-coefficient)) and 3 plots for the 3 pairs of variables that are least correlated.

R requires only a few lines to implement this. Are there any libraries (or tools) that would allow a similarly simple (and efficient if possible) implementation in Java or Scala?

PD: This is a specific use case for a [previous (too broad) question](http://datascience.stackexchange.com/questions/948/any-clear-winner-for-data-science-in-scala).
', 1281, '2014-08-28 01:36:40.540', '6e7e69d5-c437-49c0-b43d-57f16720c2e1', 1053, 2725, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Summarize and visualize a CSV in Java/Scala?', 1281, '2014-08-28 01:36:40.540', '6e7e69d5-c437-49c0-b43d-57f16720c2e1', 1053, 2726, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<tools>', 1281, '2014-08-28 01:36:40.540', '6e7e69d5-c437-49c0-b43d-57f16720c2e1', 1053, 2727, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are several reasons why you are getting erroneous results.
First, you should consider using log likelihood instead of likelihood. There are numerical issues with multiplying many small numbers(imagine if you had millions of samples you had to multiply millions of small numbers for the lhd). Also taking gradients for optimization methods that require gradients is often easier when you are dealing with the log likelihood. In general, it is good to have an objective which is a sum rather than a product of variables when dealing with optimization problems.

Second, fmin is using Nelder-Mead simplex algorithm which has no convergence guarantees according to [scipy documentation][2]. This means the convergence is totally random and you should not expect to find parameters close to the originals. To get around this, I would suggest you to use a gradient based method like stochastic gradient descent or BFGS. Since you know the generative model (rvs are Gaussian distributed) you can write the likelihood and log likelihood as:
![equations][1]

Where a,b,c and d are your model parameters 5,2,3 and 4 respectively.
Then take the [gradient][3] with respect to [a,b,c,d] and feed that into prime input of fmin_bfgs. Note that due to varying variance what could be solved by just linear regression is now a nastier problem.

Finally, you may also want to check Generalized least squares on http://en.wikipedia.org/wiki/Linear_regression#Least-squares_estimation_and_related_techniques and http://en.wikipedia.org/wiki/Heteroscedasticity, which talk about your problem and offer several available solutions.

Good luck!

[1]: http://i.stack.imgur.com/bfsvr.png
[2]: http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html#scipy.optimize.fmin
[3]: http://en.wikipedia.org/wiki/Gradient', 1350, '2014-08-28 06:49:05.210', 'de794cbb-535a-4b5f-a2aa-a75fb99591e8', 1054, 2728, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Scala is the only real language that has Big Data at it''s core. You have MLLib that sits on Spark, and as Scala is Functional it makes parallel computing really natural.  R, Python and Matlab are not suitable for industry productization, well some would say Python''s horrible dynamic typing can be handled a little using special build tools, but really its not type safe and there is no way to solve that problem.', 2668, '2014-08-28 11:24:38.743', '5d1aec13-6927-476f-9783-bfaf269e8174', 1055, 2730, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<tools><visualization><scala><csv>', 97, '2014-08-28 16:28:30.453', '87a35c96-2f73-4be5-b717-aa9c8c913e37', 1053, 'Additional related tags.', 2731, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-28 16:28:30.453', '87a35c96-2f73-4be5-b717-aa9c8c913e37', 1053, 'Proposed by 97 approved by 1281 edit id of 148', 2732, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It sounds as if you want to use unsupervized learning to create a training set. Am I right? You use your cluster analysis to determine which docs come from UK, US or Oz -- or which docs are talking about Soccer, Football or Australian football respectively? Then feed those tagged docs into a supervized learning algorithm of some sort?

How well this works will depend entirely on how well you can distinguish UK, US and OZ. I would have thought it would be fairly straightforward to find documents where national origin was known, so that you could build a supervized algorithm for detecting language variant. You wouldn''t even need a corpus that talked about football, since dialectical differences show up in other ways that are subject matter independent. (For example, I am clearly from North America, since I just wrote "in ways that are subject matter independent" rather than "Since dialectical differences do not depend on subject matter").

However, the answer to your question, "can I use unsupervized learning and then supervized learning" is No. If the results of an unsupervized learning algorithm are fed to a supervized learning algorithm, the net result is unsupervized --- there are still no grown-ups in the room. And the classification errors of the resulting process will contain error terms from both stages. This doesn''t mean you shouldn''t use the method you propose ... it might still work well ... but it won''t be a supervized learning algorithm.', 3077, '2014-08-28 16:56:45.577', 'a473ee5b-d514-46d3-93c9-29e579202f79', 1056, 2733, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You might want to try this book [Mining the Social Web][1] for an overview of different techniques. Obviously, the methods you need will depend on the use case. A lot of people do interesting things with graphs, displaying relationships between users, with respect to certain topics. Or you might simply to a timeline showing how a news topic builds in interest and wanes.


  [1]: http://shop.oreilly.com/product/0636920030195.do', 3077, '2014-08-28 18:53:57.297', '77e213eb-f2aa-4724-bf3d-9647303ec910', 1057, 2734, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is there any article or any research that made this calculus? How much data space is used by all scientific articles? in pdf, txt, compressed, or something. Is there any way to have a measure of this?

What would be the better way to realize this study?

Regards and thanks in advance', 3128, '2014-08-29 01:36:36.320', 'e4a95989-8fb4-467d-985e-3caa32df4696', 1058, 2735, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How much data space is used by all scientific articles?', 3128, '2014-08-29 01:36:36.320', 'e4a95989-8fb4-467d-985e-3caa32df4696', 1058, 2736, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><research>', 3128, '2014-08-29 01:36:36.320', 'e4a95989-8fb4-467d-985e-3caa32df4696', 1058, 2737, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a question regarding the use of neural network. I am currently working with R (neural net package) and I am facing the following issue.
My testing and validation set are always late with respect to the historical data. Is there a way of correcting the result?
Maybe something is wrong in my analysis

1- I use the daily log return r(t) = ln(s(t)/s(t-1))
2- I normalise my data with the sigmoid function (sigma and mu computed on my whole set)
3- I train my neural networks with 10 dates and the output is the normalised value that follows these 10 dates.

I tried to add the trend but there is no improvement, I observed 1-2 days late. My process seems ok, what do you think about it?

Thanking you in advance for your help

', 3055, '2014-08-29 06:00:53.420', 'f4c46038-6135-4633-b90c-24b2878616a7', 1059, 2738, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Neural Network Prediction Foreign Exchange and Lateness', 3055, '2014-08-29 06:00:53.420', 'f4c46038-6135-4633-b90c-24b2878616a7', 1059, 2739, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><neuralnetwork>', 3055, '2014-08-29 06:00:53.420', 'f4c46038-6135-4633-b90c-24b2878616a7', 1059, 2740, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Checkout Breeze and apache commons math for the maths, and ScalaLab for some nice examples of how to plot things in Scala.

I''ve managed to get an environment setup where this would just be a couple of lines. I dont actually use ScalaLab, rather borrow some of its code, I use Intellij worksheets instead.', 2668, '2014-08-29 10:42:14.957', '8d82f984-37ac-44a9-991e-d75275f18053', 1060, 2741, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a question regarding the use of neural network. I am currently working with R ([neuralnet package][1]) and I am facing the following issue.
My testing and validation set are always late with respect to the historical data. Is there a way of correcting the result?
Maybe something is wrong in my analysis

 1. I use the daily log return r(t) = ln(s(t)/s(t-1))
 2. I normalise my data with the sigmoid function (sigma and mu computed on my whole set)
 3. I train my neural networks with 10 dates and the output is the normalised value that follows these 10 dates.

I tried to add the trend but there is no improvement, I observed 1-2 days late. My process seems ok, what do you think about it?


  [1]: http://cran.r-project.org/web/packages/neuralnet/index.html', 2961, '2014-08-29 10:42:29.200', 'd22184f8-356b-447d-a438-c8eba18e9db9', 1059, 'improved formatting; edited title', 2742, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Forecasting Foreign Exchange with Neural Network - Lag in Prediction', 2961, '2014-08-29 10:42:29.200', 'd22184f8-356b-447d-a438-c8eba18e9db9', 1059, 'improved formatting; edited title', 2743, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-08-29 10:42:29.200', 'd22184f8-356b-447d-a438-c8eba18e9db9', 1059, 'Proposed by 2961 approved by 3055 edit id of 149', 2744, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m looking for the best solution to manage and host datasets for journalistic pursuits. I am assessing https://www.documentcloud.org and http://datahub.io/.

Can anyone explain the differences between them, or recommend a superior solution?', 3133, '2014-08-29 11:40:28.657', 'ace41ebf-5ae7-429e-8f17-8f1f00ef358d', 1061, 2745, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Apps to manage/host data sets', 3133, '2014-08-29 11:40:28.657', 'ace41ebf-5ae7-429e-8f17-8f1f00ef358d', 1061, 2746, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><optimization>', 3133, '2014-08-29 11:40:28.657', 'ace41ebf-5ae7-429e-8f17-8f1f00ef358d', 1061, 2747, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can definitely try to first cluster your data, and then try to see if the cluster information helps your classification task.

For example if your data looked like this (in 1D):

    AA A AA A A      BBB B B B BB BB BB      AA AA A A AAA

then it may be reasonable to run a clustering algorithm on each class, to obtain *two different kinds of A*, and learn two separate classifiers for A1 and A2, and just drop the cluster distinction for the final output.

Other common unsupervised techniques used include PCA.

As for your football example, the problem is that the unsupervised algorithm does not know what it should be looking for. Instead of learning to separate american football and soccer, it may just as well decide to cluster on international vs. national games. Or Europe vs. U.S.; which may look like it learned about american football and soccer at first, but it put american soccer into the same cluster as american football, and american football teams in Europe into the Europe cluster... because it does not have *guidance* on what structure you are interested in; and the continents *are* a valid structure, too!

So usually, I would not blindly assume that unsupervised techniques yield a distrinction that matches your desired result. They can yield *any* kind of structure, and you will want to carefully inspect what they found before using it. If you use it blindly, make sure you spend enough time on evaluation (e.g. if the clustering improves your classifier performance, then it probably worked as intended ...)', 924, '2014-08-29 16:19:06.903', '76053f2b-fc26-45b4-914c-47609c387c0d', 1062, 2748, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('It sounds as if you want to use unsupervized learning to create a training set. Am I right? You use your cluster analysis to determine which docs come from UK, US or Oz -- or which docs are talking about Soccer, Football or Australian football respectively? Then feed those tagged docs into a supervized learning algorithm of some sort?

How well this works will depend entirely on how well you can distinguish UK, US and OZ. I would have thought it would be fairly straightforward to find documents where national origin was known, so that you could build a supervized algorithm for detecting language variant. You wouldn''t even need a corpus that talked about football, since dialectical differences show up in other ways that are subject matter independent. (For example, I am clearly from North America, since I just wrote "in ways that are subject matter independent" rather than "Since dialectical differences do not depend on subject matter").

However, the answer to your question, "can I use unsupervized learning and then supervized learning" is No, if you are looking for supervized learning. If the results of an unsupervized learning algorithm are fed to a supervized learning algorithm, the net result is unsupervized --- there are still no grown-ups in the room. And the classification errors of the resulting process will contain error terms from both stages. You won''t get the same performance as you would if you did a SVM with properly tagged training data. This doesn''t mean you shouldn''t use the method you propose ... it might still work well ... but it won''t be a supervized learning algorithm.', 3077, '2014-08-29 19:26:39.963', '7ef9c4c6-9955-47c5-a27a-535824b91602', 1056, 'added 145 characters in body', 2749, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If your data is numeric, try loading it into ELKI (Java). With the `NullAlgorithm` it will give you scatterplots, histograms and parallel coordinate plots. It''s fast in reading the data; only the current Apache Batik-based visualization is slooow because it''s using SVG. :-( I''m mostly using it "headless".

It also has classes for various statistics (including higher order moments on data streams), but I havn''t seen them in the default UI yet.', 924, '2014-08-30 17:05:23.197', '69e2277b-d98e-433e-9875-b53436508461', 1063, 2751, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('MapReduce is not used in searching. It was used a long time ago to build the index; but it is a batch processing framework, and most of the web does not change all the time, so the newer architectures are all *incremental* instead of batch oriented.

Search in Google will largely work the same it works in Lucene and Elastic Search, except for a lot of fine tuned extra weighting and optimizations. But at the very heart, they will use some form of an **inverted index**. In other words, they do *not* search several terabytes when you enter a search query (even when it is not cached). They likely don''t look at the actual documents at all. But they use a lookup table that lists which documents match your query term (with stemming, misspellings, synonyms etc. all preprocessed). They probably retrieve the *list* of the top 10000 documents for each word (10k integers - just a few kb!) and compute the best matches from that. Only if there aren''t good matches in these lists, they expand to the next such blocks etc.

Queries for common words can be easily cached; and via preprocessing you can build a list of the top 10k results and then rerank them according to the user profile.

Rare terms on the other hand aren''t much of a challenge either - one of the lists only contains a few matching documents, and you can immediately discard all others.

I recommend reading this article:

> **The Anatomy of a Large-Scale Hypertextual Web Search Engine**<br />
> Sergey Brin and Lawrence Page<br />
> Computer Science Department, Stanford University, Stanford, CA 94305<br />
> http://infolab.stanford.edu/~backrub/google.html

And yes, that''s the Google founders who wrote this. It''s not the latest state, but it will already work at a pretty large scale.', 924, '2014-08-30 18:14:05.577', '2ed46644-e79d-4c38-b8da-2576453a5082', 1064, 2752, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('State of the art as in: used in practise or worked on in theory?

APRIORI is used everywhere, except in developing new frequent itemset algorithms. It''s easy to implement, and easy to reuse in very different domains. You''ll find hundreds of APRIORI implementations of varying quality. And it''s easy to get APRIORI wrong, actually.

FPgrowth is much harder to implement, but also much more interesting. So from an academic point of view, everybody tries to improve FPgrowth - getting work based on APRIORI accepted will be very hard by now.

If you have a good implementation, every algorithm has it''s good and it''s bad situations in my opinion. A good APRIORI implementation will *only* need to scan the database *k* times to find all frequent itemsets of length *k*. In particular if your data fits into main memory this is cheap. What can kill APRIORI is too many frequent 2-itemsets (in particular when you don''t use a Trie and similar acceleration techniques etc.). It works best on large data with a low number of frequent itemsets.

Eclat works on columns; but it needs to read each column much more often. There is some work on diffsets to reduce this work. If your data does not fit into main memory, Eclat suffers probably more than Apriori. By going depth first, it will also be able to return a first interesting result much earlier than Apriori, and you can use these results to adjust parameters; so you need less iterations to find good parameters. But by design, it cannot exploit pruning as neatly as Apriori did.

FPGrowth compresses the data set into the tree. This works best when you have lots of duplicate records. You could probably reap of quite some gains for Apriori and Eclat too if you can presort your data and merge duplicates into weighted vectors. FPGrowth does this at an extreme level. The drawback is that the implementation is much harder; and once this tree does not fit into memory anymore it gets a mess to implement.

As for performance results and benchmarks - don''t trust them. There are so many things to implement incorrectly. Try 10 different implementations, and you get 10 very different performance results. In particular for APRIORI, I have the impression that most implementations are broken in the sense of missing some of the main contributions of APRIORI... and of those that have these parts right, the quality of optimizations varies a lot.

There are actually even papers on how to implement these algorithms efficiently:

> Efficient Implementations of Apriori and Eclat.<br /> Christian Borgelt<br />Workshop of Frequent Item Set Mining Implementations (FIMI 2003, Melbourne, FL, USA).

You may also want to read these surveys on this domain:

* > Goethals, Bart. "Survey on frequent pattern mining." Univ. of Helsinki (2003).

* > Ferenc Bodon, A Survey on Frequent Itemset Mining, Technical Report, Budapest University of Technology and Economic, 2006,

* > Frequent Item Set Mining<br />Christian Borgelt<br />Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 2(6):437-456. 2012
', 924, '2014-08-30 18:36:07.490', '6b0a47d4-dd83-4b62-8bd4-00142af38530', 1065, 2753, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('MapReduce is not used in searching. It was used a long time ago to build the index; but it is a batch processing framework, and most of the web does not change all the time, so the newer architectures are all *incremental* instead of batch oriented.

Search in Google will largely work the same it works in Lucene and Elastic Search, except for a lot of fine tuned extra weighting and optimizations. But at the very heart, they will use some form of an **inverted index**. In other words, they do *not* search several terabytes when you enter a search query (even when it is not cached). They likely don''t look at the actual documents at all. But they use a lookup table that lists which documents match your query term (with stemming, misspellings, synonyms etc. all preprocessed). They probably retrieve the *list* of the top 10000 documents for each word (10k integers - just a few kb!) and compute the best matches from that. Only if there aren''t good matches in these lists, they expand to the next such blocks etc.

Queries for common words can be easily cached; and via preprocessing you can build a list of the top 10k results and then rerank them according to the user profile. There is nothing to be gained by computing an "exact" answer, too. Looking at the top 10k results is likely enough; there is no correct answer; and if a better result somewhere at position 10001 is missed, nobody will know or notice (or care). It likely was already ranked down in preprocessing and would not have made it into the top 10 that is presented to the user at the end (or the top 3, the user actually looks at)

Rare terms on the other hand aren''t much of a challenge either - one of the lists only contains a few matching documents, and you can immediately discard all others.

I recommend reading this article:

> **The Anatomy of a Large-Scale Hypertextual Web Search Engine**<br />
> Sergey Brin and Lawrence Page<br />
> Computer Science Department, Stanford University, Stanford, CA 94305<br />
> http://infolab.stanford.edu/~backrub/google.html

And yes, that''s the Google founders who wrote this. It''s not the latest state, but it will already work at a pretty large scale.', 924, '2014-08-30 18:40:02.403', 'bdf8db39-5e85-474e-b8a2-145312034e1d', 1064, 'added 422 characters in body', 2754, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Perhaps you are looking to quantify the amount of filespace used by a specific subset of data that we will label as "academic publications."

Well, to estimate, you could find stats on how many publications are housed at all the leading libraries (JSTOR, EBSCO, AcademicHost, etc) and then get the mean average size of each. Multiply that by the number of articles and whamo, you''ve got yourself an estimate.

Here''s the problem, though: PDF files store the text from string `s` differently (in size) than, say, a text document stores that same string. Likewise, a compressed JPEG will store an amount of information `i` differently than a non-compressed JPEG. So you see we could have two of the same articles containing the same information `i` but taking up different amounts of memory `m`.

Are you looking to get a wordcount on the amount of scientific literature?

Are you looking to get an approximation of file system space used to store all academically published content in the world? ', 3152, '2014-09-01 00:01:58.917', '6fdae315-8e4f-4584-bbe5-b90baf947085', 1066, 2755, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('
It is likely to be very hard to draw any conclusion if you are training with only 10 input samples.  With more data, your diagnosis that the model is predicting lagged values would have more plausibility.  As it stands, it seems pretty likely that your model is just saying that the last observed value is pretty close to correct.  This isn''t the same as a real lag model, but it is a very reasonable thing to guess if you haven''t seen enough data.

', 3153, '2014-09-01 00:17:56.070', 'dfcbac7f-32ab-49ef-974f-b006c4bdb98f', 1067, 2756, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''d have a closer look at one of Apache Spark''s modules: [MLlib][1].


  [1]: https://spark.apache.org/mllib/ "MLlib"', 3150, '2014-09-01 08:10:56.967', '5c506027-97cf-4af7-aa90-bd8b60283d08', 1068, 2758, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to evaluate and compare several different machine learning models built with different parameters (i.e. downsampling, outlier removal) and different classifiers (i.e. Bayes Net, SVM, Decision Tree).

I am performing a type of cross validation where I randomly select 67% of the data for use in the training set and 33% of the data for use in the testing set. I perform this for several iterations, say, 20.

Now, from each iteration I am able to generate a confusion matrix and compute a kappa. My question is, what are some ways to aggregate these across the iterations? I am also interested in aggregating accuracy and expected accuracy, among other things.

For the kappa, accuracy, and expected accuracy, I have just been taking the average up to this point. One of the problems is that when I recompute kappa with the aggregated average and expected average, it is not the same with the aggregated kappa.

For the confusion matrix, I have been first normalizing the confusion matrix from each iteration and then averaging them, in an attempt to avoid an issue of confusion matrices with different numbers of total cases (which is possible with my cross validation scheme).

When I recompute the kappa from this aggregated confusion matrix, it is also different from the previous two.

Which one is most correct? Is there another way of computing an average kappa that is more correct?

Thanks, and if more concrete examples are in order to illustrate my question please let me know.', 3169, '2014-09-02 03:24:12.793', '499fd9ac-ea9d-425c-bd06-e48df968385e', 1069, 2759, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Kappa From Combined Confusion Matrices', 3169, '2014-09-02 03:24:12.793', '499fd9ac-ea9d-425c-bd06-e48df968385e', 1069, 2760, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><confusion-matrix>', 3169, '2014-09-02 03:24:12.793', '499fd9ac-ea9d-425c-bd06-e48df968385e', 1069, 2761, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am interested in knowing the differences in **functionality** between SAP HANA and Exasol. Since this is a bit of an open ended question let me be clear. I am not interested in people debating which is "better" or faster. I am only interested in what each was designed to do so please keep your opinions out of it. I suspect it is a bit like comparing HANA to Oracle Exalytics where there is some overlap but the functionality goals are different. ', 2511, '2014-09-02 08:47:38.737', '1ba0f838-8e37-4bbb-9a49-a7cd874b9058', 1070, 2762, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('SAP HANA vs Exasol', 2511, '2014-09-02 08:47:38.737', '1ba0f838-8e37-4bbb-9a49-a7cd874b9058', 1070, 2763, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 2511, '2014-09-02 08:47:38.737', '1ba0f838-8e37-4bbb-9a49-a7cd874b9058', 1070, 2764, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How is the concept of data different for different disciplines? Obviously, for physicists and sociologists, "data" is something different.', 3178, '2014-09-02 09:48:08.150', '76f25cb6-17bd-466b-b7fc-1a2b7a4cea97', 1071, 2765, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How is the concept of data different for different disciplines?', 3178, '2014-09-02 09:48:08.150', '76f25cb6-17bd-466b-b7fc-1a2b7a4cea97', 1071, 2766, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<definitions>', 3178, '2014-09-02 09:48:08.150', '76f25cb6-17bd-466b-b7fc-1a2b7a4cea97', 1071, 2767, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''s not an enormous difference between what you can do with the two databases, it''s more a question of the focus and the way the functionality is implemented and that''s where it becomes difficult to explain without using words like "better" and "faster" (and for sure words like "cheaper")

EXASOL was designed for speed and ease of use with Analytical processing and is designed to run on clusters of commodity hardware. SAP is a more complex, aims to do more than "just" Analytical processing and runs only on a range of "approved" hardware.

What type of differences did you have in mind ?', 3181, '2014-09-02 14:01:07.407', 'fead75fd-a6f6-489a-9b00-a17b946fdf9c', 1072, 2769, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am looking for packages (either in python, R, or a standalone package) to perform online learning to predict stock data.

I have found and read about Vowpal Wabbit (https://github.com/JohnLangford/vowpal_wabbit/wiki),
which seems to be quite promising but I am wondering if there are any other packages out there.

Thanks in advance.', 802, '2014-09-02 19:17:43.210', 'da6c98d6-fae0-4fb6-a2d9-0095624bccf5', 1073, 2770, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Libraries for Online Machine Learning', 802, '2014-09-02 19:17:43.210', 'da6c98d6-fae0-4fb6-a2d9-0095624bccf5', 1073, 2771, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><online-learning>', 802, '2014-09-02 19:17:43.210', 'da6c98d6-fae0-4fb6-a2d9-0095624bccf5', 1073, 2772, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In R''s ksvm the polynomial kernel is defined as:

(scale * crossprod(x, y) + offset)^degree

How do the scale and offset parameters affect the model and what range should they be in? (intuitively please)

Are the scale and offset for numeric stability only (that''s what it looks like to me), or do they influence prediction accuracy as well?

Can good values for scale and offset be calculated/estimated when the data is known or is a grid search required? The caret package always sets the offset to 1, but it does a grid search for scale. (Why) is an offset of 1 a good value?

Thanks

PS.: Wikipedia didn''t really help my understanding:

> For degree-d polynomials, the polynomial kernel is defined as
>
> ![][1]
>
> where x and y are vectors in the input space, i.e. vectors of features
> computed from training or test samples, ![][2] **is a constant trading**
> **off the influence of higher-order versus lower-order terms** in the
> polynomial. When ![][3], the kernel is called homogeneous.(**A further**
> **generalized polykernel divides ![][4] by a user-specified scalar**
> **parameter ![][5].**)

Neither did ?polydot''s explanation in R''s help system:

> **scale**: The scaling parameter of the polynomial and tangent kernel is a
> convenient way of normalizing patterns (<-!?) without the need to modify the
> data itself
>
> **offset**: The offset used in a polynomial or hyperbolic tangent kernel (<- lol thanks)

  [1]: http://i.stack.imgur.com/3stCR.png
  [2]: http://i.stack.imgur.com/34dwi.png
  [3]: http://i.stack.imgur.com/79cDG.png
  [4]: http://i.stack.imgur.com/Ps0qt.png
  [5]: http://i.stack.imgur.com/sjfS1.png
', 676, '2014-09-02 19:29:07.490', 'a50bb5b2-6eb4-4bc0-8a4a-0f2de27a1ae2', 1074, 2773, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Polynomial Kernel Parameters in SVMs', 676, '2014-09-02 19:29:07.490', 'a50bb5b2-6eb4-4bc0-8a4a-0f2de27a1ae2', 1074, 2774, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><r><svm>', 676, '2014-09-02 19:29:07.490', 'a50bb5b2-6eb4-4bc0-8a4a-0f2de27a1ae2', 1074, 2775, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Background:
I run a product that compares sets of data (data matching and data reconciliation).
To get the result we need to compare each row in a data set with every N rows on the opposing data set
Now however we get sets of up to 300 000 rows of data in each set to compare and are getting 90 Billion computations to handle.


So my question is this:
Even though we dont have the data volumes to use Hadoop, we have the computational need for something distributed. Is Hadoop a good choice for us?', 3203, '2014-09-04 18:13:57.343', 'bde86dd6-cf78-4216-940c-a3f95e062926', 1075, 2778, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hadoop for grid computing', 3203, '2014-09-04 18:13:57.343', 'bde86dd6-cf78-4216-940c-a3f95e062926', 1075, 2779, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop>', 3203, '2014-09-04 18:13:57.343', 'bde86dd6-cf78-4216-940c-a3f95e062926', 1075, 2780, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You could look at scikit-learn and orange module in python.
Scikit-learn has a SGD classifier and regressor that could do a partial fit data in case of online learning.
In R take a look at caret package
', 3211, '2014-09-05 06:17:28.533', '4d502329-0d0c-43bd-b4a6-e2d63dde961b', 1076, 2781, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Your job seems like a map-reduce job and hence might be good for Hadoop. Hadoop has a zoo of an ecosystem though.

Hadoop is a distributed file system. It distributes data on a cluster and because this data is split up it can be analysed in parallel. Out of the box, Hadoop allows you to write map reduce jobs on the platform and this is why it might help with your problem.

The following technologies work on Hadoop:

- If the data can be represented in a table format you might want to check out technologies like [hive](https://hive.apache.org/) and [impala](http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html). Impala uses all the distributed memory across a cluster and is very performant while it allows you to still work with a table structure.
- A more new, but promising alternative might also be [spark](https://spark.apache.org/) which allows for more iterative procedures to be run on the cluster.

Don''t underestimate the amount of time setting up and the amount of time needed to understand Hadoop.', 3213, '2014-09-05 11:00:22.053', 'f1865d44-dcdf-496f-b346-e0d91114f0ef', 1077, 2782, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m going to classify unstructured text documents, namely web sites of unknown structure. The number of categories is highly limited (at this point I believe, that there are not going to be more than three categories). Any suggestions where to start?

I''m not sure if the "bag of words" approach would be feasible. Later, I can add another classification stage based on document structure (so finally decision trees are going to be utilised also).

I can say, that I am somehow familiar with Mahout and Hadoop, therefore Java is preferred. If needed, I can switch to Scala and/or Spark engine (the ML library).', 3215, '2014-09-05 12:08:11.347', 'ea4c7fb0-c9ee-4a33-8d4d-35404d8d2895', 1078, 2783, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Unstructured text classification', 3215, '2014-09-05 12:08:11.347', 'ea4c7fb0-c9ee-4a33-8d4d-35404d8d2895', 1078, 2784, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><text-mining>', 3215, '2014-09-05 12:08:11.347', 'ea4c7fb0-c9ee-4a33-8d4d-35404d8d2895', 1078, 2785, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m searching for data sets for evaluating text retrieval quality.

TF-IDF is a popular similarity measure, but is it the best choice? And which *variant* is the best choice? [Lucenes Scoring][1] for example uses IDF^2, and IDF defined as 1+log(numdocs/(docFreq+1)). TF in lucene is defined as sqrt(frequency)...

Many more variants exist, including [Okapi BM25][2], which is used by the [Xapian search engine][3]...

I''d like to study the different variants, and I''m looking for **evaluation data sets**. Thanks!

  [1]: https://lucene.apache.org/core/3_6_1/api/all/org/apache/lucene/search/Similarity.html
  [2]: https://en.wikipedia.org/wiki/Okapi_BM25
  [3]: http://xapian.org/docs/bm25.html', 2920, '2014-09-05 14:47:52.127', '3f3e8b30-558f-4783-bbdd-aa7a355f3b6f', 1079, 2786, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data sets for evaluating text retrieval quality', 2920, '2014-09-05 14:47:52.127', '3f3e8b30-558f-4783-bbdd-aa7a355f3b6f', 1079, 2787, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><text-mining><similarity><information-retrieval>', 2920, '2014-09-05 14:47:52.127', '3f3e8b30-558f-4783-bbdd-aa7a355f3b6f', 1079, 2788, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was curious about the ANOVA RBF kernel provided by kernlab package available in R.

I tested it with a numeric dataSet of 34 input variables and one output variable. For each variable I have 700 different values. Comparing with other kernels, I got very bad results with this kernel.
For example using the simple RBF kernel I could predict with 0,88 R2 however with the anova RBF I could only get 0,33 R2.
I thought that ANOVA RBF would be a very good kernel. Any thoughts? Thanks

The code is as follows:

    set.seed(100) #use the same seed to train different models
    svrFitanovaacv <- train(R ~ .,
                           data = trainSet,
                           method = SVManova,
                           preProc = c("center", "scale"),
                           trControl = ctrl, tuneLength = 10) #By default, RMSE and R2 are computed for regression (in all cases, selects the tunning and cross-val model with best value) , metric = "ROC"

**define custom model in caret package:**

    library(caret)
    #RBF ANOVA KERNEL
    SVManova <- list(type = "Regression", library = "kernlab", loop = NULL)
    prmanova <- data.frame(parameter = c("C", "sigma", "degree", "epsilon"),
                         class = rep("numeric", 4),
                         label = c("Cost", "Sigma", "Degree", "Epsilon"))
    SVManova$parameters <- prmanova
    svmGridanova <- function(x, y, len = NULL) {
    library(kernlab)
    sigmas <- sigest(as.matrix(x), na.action = na.omit, scaled = TRUE, frac = 1)
    expand.grid(sigma = mean(sigmas[-2]), epsilon = 0.000001,
                C = 2^(-40:len), degree = 1:2) # len = tuneLength in train
    }
    SVManova$grid <- svmGridanova
    svmFitanova <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
      ksvm(x = as.matrix(x), y = y,
           kernel = "anovadot",
           kpar = list(sigma = param$sigma, degree = param$degree),
           C = param$C, epsilon = param$epsilon,
           prob.model = classProbs,
           ...) #default type = "eps-svr"
    }
    SVManova$fit <- svmFitanova
    svmPredanova <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
      predict(modelFit, newdata)
    SVManova$predict <- svmPredanova
    svmProb <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
      predict(modelFit, newdata, type="probabilities")
    SVManova$prob <- svmProb
    svmSortanova <- function(x) x[order(x$C), ]
    SVManova$sort <- svmSortanova

**load data:**

    dataA2<-read.csv("C:/results/A2.txt",header = TRUE,
                                 blank.lines.skip = TRUE,sep = ",")
    set.seed(1)
    inTrainSet <- createDataPartition(dataA2$R, p = 0.75, list = FALSE) #[[1]]
    trainSet <- dataA2[inTrainSet,]
    testSet <- dataA2[-inTrainSet,]
    #-----------------------------------------------------------------------------
    #K-folds resampling method for fitting svr
    ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10,
                         allowParallel = TRUE) #10 separate 10-fold cross-validations

**link to data:**

    wuala.com/jpcgandre/Documents/Data%20SVR/?key=BOD9NTINzRHG', 3216, '2014-09-05 16:34:55.170', 'e689cc79-8f9e-41ec-a808-418584b88302', 1080, 2789, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('ANOVA RBF kernel returns very poor results', 3216, '2014-09-05 16:34:55.170', 'e689cc79-8f9e-41ec-a808-418584b88302', 1080, 2790, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r>', 3216, '2014-09-05 16:34:55.170', 'e689cc79-8f9e-41ec-a808-418584b88302', 1080, 2791, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data is, at it''s most basic reduction, a raw element of something. Data is a raw "thing" that exists in any form from which we can analyze it and construct intelligence. When I was an Intelligence Analyst, we used to define data as "anything and everything that could be used to construct a hypothesis."

Thus, data for any discipline is interchangeable; as a sociologist, I have a vector of discrete variables indicating ethnicity, as an economist I have a vector with housing prices, and as an anthropologist I have a vector of tablet names used in some long-gone civilization.

Data is data.', 3152, '2014-09-05 16:36:03.227', '34f9d35a-434a-4770-9f0f-a58a9e6fa616', 1081, 2792, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here are a couple of really great open source software packages for text classification that should help get you started:

 - [MALLET](http://mallet.cs.umass.edu/) is a CPL-licensed Java-based machine learning toolkit built by UMass for working with text data. It includes implementations of several [classification](http://mallet.cs.umass.edu/classification.php) algorithms (e.g., naïve Bayes, maximum entropy, decision trees).
 - The [Stanford Classifier](http://nlp.stanford.edu/software/classifier.shtml) from the Stanford NLP Group is a GPL-licensed Java implementation of a maximum entropy classifier designed to work with text data.', 819, '2014-09-05 19:13:39.993', '84e8c21e-115f-4ffd-9c2b-9e5bc58152b3', 1082, 2793, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Let''s work it out from the ground up. Classification (also known as categorization) is an example of **supervised learning**. In supervised learning you have:

 * **model** - something that approximates internal structure in your data, enabling you to reason about it and make useful predictions (e.g. predict class of an object); normally model has parameters that you want to "learn"
 * **training** and **testing datasets** - sets of objects that you use for training your model (finding good values for parameters) and further evaluating
 * **training** and **classification algorithms** - first describes how to learn model from training dataset, second shows how to derive class of a new object given trained model

Now let''s take a simple case of spam classification. Your training dataset is a corpus of emails + corresponding labels - "spam" or "not spam". Testing dataset has the same structure, but made from some independent emails (normally one just splits his dataset and makes, say, 9/10 of it to be used for training and 1/10 - for testing). One way to model emails is to represent each of them as a set (bag) of words. If we assume that words are independent of each other, we can use **Naive Bayes classifier**, that is, calculate prior probabilities for each word and each class (training algorithm) and then apply Bayes theorem to find posterior probability of a new document to belong to particular class.

So, basically we have:

    raw model + training set + training algorithm -> trained model
    trained model + classification algorithm + new object -> object label

Now note that we represented our objects (documents) as a bag of words. But is the only way? In fact, we can extract much more from raw text. For example, instead of words as is we can use their [stems or lemmas](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), throw out noisy [stop words](http://en.wikipedia.org/wiki/Stop_words), add [POS tags](http://nlp.stanford.edu/software/tagger.shtml) of words, extract [named entities](http://nlp.stanford.edu/software/CRF-NER.shtml) or even explore HTML structure of the document. In fact, more general representation of a document (and, in general, any object) is a **feature vector**. E.g. for text:

    actor, analogue, bad, burn, ..., NOUN, VERB, ADJ, ..., in_bold, ... | label
        0,        0,   1,    1, ...,    5,    7,   2, ...,       2, ... | not spam
        0,        1,   0,    0, ...,    3,   12,  10, ...,       0, ... | spam

Here first row is a list of possible features and subsequent rows show how many times that feature occurs in a document. E.g. in first document there''s no occurrences of word "actor", 1 occurrence of word "burn", 5 nouns, 2 adjectives and 2 pieces of text in bold. Last column corresponds to a resulting class label.

Using feature vector you can incorporate any properties of your texts. Though finding good set of features may take some time.

And what about model and algorithms? Are we bound to Naive Bayes. Not at all. **logistic regression**, **SVM**, **decision trees** - just to mention few popular classifiers. (Note, that we say "classifier" in most cases we mean model + corresponding algorithms for training and classification).

As for implementation, you can divide task into 2 parts:

 1. Features extraction - transforming raw texts into feature vectors.
 2. Object classification - building and applying model.

First point is well worked out in many [NLP libraries](http://stackoverflow.com/questions/4115526/natural-language-processing). Second is about machine learning, so, depending on your dataset, you can use either [Weka](http://www.cs.waikato.ac.nz/ml/weka/), or [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html). ', 1279, '2014-09-07 01:08:04.330', '936e16a5-1155-4728-923d-d38c4cac8477', 1084, 2799, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I understand your description correctly, hadoop seems a huge overhead, for the wrong problem. basically you just need a standard distributed architecture: don''t you just have to pass pairs of rows - eg mpi.... ipython.parallel, ...', 1256, '2014-09-07 08:02:39.670', 'b4e04688-9fa2-455c-b0d6-a33c93707e1b', 1085, 2800, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('We currently have some ~500 bio-medical  documents each of some 1-2 MB . We want to use a non query based method to rank the documents in order of their unique content score. I''m calling it as "unique content" bcos our researchers want to know from which document to start reading . All the documents are of the same topic ,in bio medical world we know that there is always a lot of content overlap . So all we want to do is to arrange the documents in the order of their unique content.

Most Information Retrieval literature suggest query based ranking which dose not fit our need.  ', 3232, '2014-09-07 22:51:27.490', '386dea09-ffd8-4225-9029-d240d6dcbe97', 1086, 2801, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Document Ranking - Non Query based', 3232, '2014-09-07 22:51:27.490', '386dea09-ffd8-4225-9029-d240d6dcbe97', 1086, 2802, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><text-mining><information-retrieval>', 3232, '2014-09-07 22:51:27.490', '386dea09-ffd8-4225-9029-d240d6dcbe97', 1086, 2803, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here''s a simple initial approach to try:

 1. Calculate the [TF-IDF](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) score of each word in each document.
 2. Sort the documents by the average TF-IDF score of their words.
 3. The higher the average TF-IDF score, the more unique a document is with respect to the rest of the collection.

You might also try a clustering-based approach where you look for outliers, or perhaps something with the [Jaccard index](http://en.wikipedia.org/wiki/Jaccard_index) using a bag-of-words model.', 819, '2014-09-08 00:47:21.590', 'e735cd70-e599-4433-97a6-ad29d8b83d65', 1087, 2804, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Topic Modeling would be a very appropriate method for your problem.  Topic Models are a form of unsupervised learning/discovery, where a specified (or discovered) number of topics are defined by a list of words that have a high probability of appearing together. In a separate step, you can label each topic using subject matter experts, but for your purposes this isn''t necessary since you are only interested in getting to three clusters.

You treat each document as a bag of words, and pre-process to remove stop words, etc.  With the simplest methods, you pre-specify the number of topics.  In your case, you could either specify "3", which is your fixed limit on categories, or pick a larger number of topics (between 10 and 100), and then in a separate step, form three clusters for documents with common emphasis on topics. K-means or other clustering methods could be used.  (I''d recommend the latter approach)

You don''t need to code topic modeling software from scratch. Here''s a web page with many resources, including software libraries/packages: http://www.cs.princeton.edu/~blei/topicmodeling.html

None are in Java, but there are ways to run C++ and Python under Java.', 609, '2014-09-08 02:45:42.653', '872e5103-0d50-41d7-8a04-fb983204aa5c', 1088, 2805, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You could use Topic Modeling as described in this paper:
http://faculty.chicagobooth.edu/workshops/orgs-markets/pdf/KaplanSwordWin2014.pdf

They performed Topic Modeling on abstracts of patents (limited to 150 words).  They identified papers as "novel" if they were the first to introduce a topic, and measured degree of novelty by how many papers in the following year used the same topic. (Read the paper for details).

I suggest that you follow their lead and only process paper abstracts.  Processing the body of each paper might reveal some novelty that the abstract does not, but you also run the risk of having much more noise in your topic model (i.e. extraneous topics, extraneous words).

While you say that all 500 papers are on the same "topic", it''s probably safer to say that they are all on the same "theme" or in the same "sub-category" of Bio-medicine. Topic modeling permits decomposition of the "theme" into "topics".

The good news is that there are plenty of good packages/libraries for Topic Modeling.  You still have to do preprocessing, but you don''t have to code the algorithms yourself.  See this page for many resources:
http://www.cs.princeton.edu/~blei/topicmodeling.html
', 609, '2014-09-08 03:07:07.413', '553f1fe8-a71d-47ff-b119-049257fbfedd', 1089, 2806, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It may be unlikely that anyone knows this but I have a specific question about Freebase.  Here is the Freebase page from the [Ford Taurus automotive model][1] .  It has a property called "Related Models".  Does anyone know how this list of related models was compiled.  What is the similarity measure that they use?  I don''t think it is only about other wikipedia pages that link to or from this page.  Alternatively, it may be that this is user generated.  Does anyone know for sure?


  [1]: http://www.freebase.com/m/014_d3', 387, '2014-09-08 14:57:11.223', '05ce02e9-1ad4-454c-885e-ff69e9ec5f49', 1090, 2808, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Freebase Related Models', 387, '2014-09-08 14:57:11.223', '05ce02e9-1ad4-454c-885e-ff69e9ec5f49', 1090, 2809, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset>', 387, '2014-09-08 14:57:11.223', '05ce02e9-1ad4-454c-885e-ff69e9ec5f49', 1090, 2810, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the best technology to be used to create my custom bag of words with N-grams to apply to. I want to know a functionality that can be achieved over GUI. I cannot use spot fire as it is not available in the organization. Though i can get SAP Hana or R-hadoop. But R-hadoop is bit challenging, any suggessions.', 3244, '2014-09-08 19:33:00.253', 'dbaab81d-72b5-40e5-a1c5-5f4f1e88eeb2', 1091, 2811, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Creating Bag of words', 3244, '2014-09-08 19:33:00.253', 'dbaab81d-72b5-40e5-a1c5-5f4f1e88eeb2', 1091, 2812, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><text-mining>', 3244, '2014-09-08 19:33:00.253', 'dbaab81d-72b5-40e5-a1c5-5f4f1e88eeb2', 1091, 2813, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are there any machine learning libraries for Ruby that are relatively complete (including a wide variety of learners for supervised and unsupervised learning), robustly tested, and well-documented? I love Python''s [scikit-learn][1] for its incredible documentation, but a client would prefer to write the code in Ruby since that''s what they''re familiar with.

Ideally I am looking for a library or set of libraries which, like scikit and numpy, can implement a wide variety of data structures like sparse matrices, as well as learners.

Some examples of things we''ll need to do are binary classification using SVMs, and  implementing bag of words models which we hope to concatenate with arbitrary numeric data, as described in [this][2] Stackoverflow post.

Thanks in advance!


  [1]: http://scikit-learn.org/
  [2]: http://stackoverflow.com/q/20106940/1435804', 2487, '2014-09-08 21:25:26.183', '1bea11d2-a90f-455e-8d84-a2ba0ff85026', 1092, 2814, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine learning libraries for Ruby', 2487, '2014-09-08 21:25:26.183', '1bea11d2-a90f-455e-8d84-a2ba0ff85026', 1092, 2815, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 2487, '2014-09-08 21:25:26.183', '1bea11d2-a90f-455e-8d84-a2ba0ff85026', 1092, 2816, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Problem**

For my machine learning task, I create a set of predictors.
Predictors come in "bundles" - multi-dimensional measurements (3 or 4 - dimensional in my case).

The hole "bundle" makes sense only if it has been measured, and taken all together.

The problem is, different ''bundles'' of predictors can be measured only for small part of the sample, and those parts don''t necessary intersect for different ''bundles''.

As parts are small, imputing leads to considerable decrease in accuracy(catastrophical to be more accurate)


**Possible solutions**

I could create dummy variables that would mark whether the measurement has taken place for each variable. The problem is, when random forests draws random variables, it does so individually.

So there are two basic ways to solve this problem:
1) Combine each "bundle" into one predictor. That is possible, but it seems information will be lost.
2) Make random forest draw variables not individually, but by obligatory "bundles".

**Problem for random forest**

As random forest draws variables randomly, it takes features that are useless (or much less useful) without other from their "bundle". I have a feeling that leads to a loss of accuracy.

**Example**

For example I have variables `a`,`a_measure`, `b`,`b_measure`.
The problem is, variables `a_measure` make sense only if variable `a` is present, same for `b`. So I either have to combine `a`and `a_measure` into one variable, or make random forest draw both, in case at least one of them is drawn.

**Question**

What are the best practice solutions for problems when different sets of predictors are measured for small parts of overall population, and these sets of predictors come in obligatory "bundles"?

Thank you!', 3108, '2014-09-09 06:33:00.730', '25902b9b-c963-46df-a5ad-5fd6f0ad0d2b', 1094, 2820, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Create obligatory combinations of variables for drawing by random forest', 3108, '2014-09-09 06:33:00.730', '25902b9b-c963-46df-a5ad-5fd6f0ad0d2b', 1094, 2821, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><random-forest>', 3108, '2014-09-09 06:33:00.730', '25902b9b-c963-46df-a5ad-5fd6f0ad0d2b', 1094, 2822, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Creating obligatory combinations of variables for drawing by random forest', 3108, '2014-09-09 06:45:25.510', '5b943376-2090-4e6d-8c71-aab4834da891', 1094, 'edited title', 2823, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The problem refers to decision trees building. According to Wikipedia ''[Gini coefficient][1]'' should not be confused with ''[Gini impurity][2]''. However both measures can be used when building a decision tree - these can support our choices when splitting the set of items.

1) ''Gini impurity'' - it is a standard decision-tree splitting metric (see in the link above);

2) ''Gini coefficient'' - each splitting can be assessed based on the AUC criterion. For each splitting scenario we can build a ROC curve and compute AUC metric. According to Wikipedia AUC=(GiniCoeff+1)/2;

Question is: are both these measures equivalent? On the one hand, I am informed that Gini coefficient should not be confused with Gini impurity. On the other hand, both these measures can be used in doing the same thing - assessing the quality of a decision tree split.


  [1]: http://en.wikipedia.org/wiki/Gini_coefficient
  [2]: http://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity', 3250, '2014-09-09 12:44:16.967', '307ec241-59ea-4da8-9ca1-0a10222ff7db', 1095, 2825, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Gini coefficient vs Gini impurity - decision trees', 3250, '2014-09-09 12:44:16.967', '307ec241-59ea-4da8-9ca1-0a10222ff7db', 1095, 2826, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 3250, '2014-09-09 12:44:16.967', '307ec241-59ea-4da8-9ca1-0a10222ff7db', 1095, 2827, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('> create my custom bag of words with N-grams to apply to

My initial recommendation would be to use the [NLTK library for Python](http://www.nltk.org/). NLTK offers methods for [easily extracting bigrams from text](http://www.nltk.org/book/ch01.html#bigrams_index_term) or [ngrams of arbitrary length](https://nltk.googlecode.com/svn/trunk/doc/api/nltk.util-module.html#ngrams), as well as methods for analyzing the [frequency distribution of those items](http://www.nltk.org/book/ch01.html#frequency_distribution_index_term). However, all of this requires a bit of programming.

> a functionality that can be achieved over GUI

That''s tricky. Have you looked into [GATE](https://gate.ac.uk/)? I''m not exactly sure if/how GATE does what you want, but it does offer a GUI.', 819, '2014-09-09 15:05:46.093', '76fa47d4-6b7c-40c6-92e8-cea79bce8a7e', 1096, 2828, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ll go ahead and post an answer for now; if someone has something better I''ll accept theirs.

At this point the most powerful option appears to be accessing WEKA using jRuby. We spent yesterday scouring the ''net, and this combination was even used by a [talk at RailsConf 2012][1], so I would guess if there were a comparable pure ruby package, they would have used it.

Note that if you know exactly what you need, there are plenty of individual libraries that either [wrap standalone packages like libsvm][2] or [re-implement some individual algorithms like Naive Bayes in pure Ruby][3] and will spare you from using jRuby.

But for a general-purpose library, WEKA and jRuby seem to be the best bet at this time.


  [1]: http://www.confreaks.com/videos/867-railsconf2012-practical-machine-learning-and-rails
  [2]: https://github.com/febeling/rb-libsvm
  [3]: https://github.com/alexandru/stuff-classifier', 2487, '2014-09-09 16:58:10.537', '2e83dbfb-9bfa-4b96-8463-b4f36c8f8380', 1097, 2829, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Are there any machine learning libraries for Ruby that are relatively complete (including a wide variety of algorithms for supervised and unsupervised learning), robustly tested, and well-documented? I love Python''s [scikit-learn][1] for its incredible documentation, but a client would prefer to write the code in Ruby since that''s what they''re familiar with.

Ideally I am looking for a library or set of libraries which, like scikit and numpy, can implement a wide variety of data structures like sparse matrices, as well as learners.

Some examples of things we''ll need to do are binary classification using SVMs, and  implementing bag of words models which we hope to concatenate with arbitrary numeric data, as described in [this][2] Stackoverflow post.

Thanks in advance!


  [1]: http://scikit-learn.org/
  [2]: http://stackoverflow.com/q/20106940/1435804', 2487, '2014-09-09 17:00:05.697', 'c39dafd4-4e73-4bc1-93eb-5d59aa0e2ef3', 1092, 'added 2 characters in body', 2830, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can use SKlearn, It is a python library. It is simplest method which i like with minimal code. You can follow this link http://scikit-learn.org/stable/modules/feature_extraction.html ', 3259, '2014-09-10 07:33:06.470', '97bad0a7-1478-4abc-9710-da5ec0eeaa02', 1098, 2831, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('No, despite their names they are not equivalent or even that similar. Gini impurity is a measure of misclassification, which applies in a multiclass classifier context. Gini coefficient applies to binary classification, and requires a classifier that can in some way rank examples according to likelihood of being in the positive class. Both could be applied in some cases, but they are different measures for different things. Impurity is what is commonly used in decision trees.', 21, '2014-09-10 08:15:17.343', '1cd3ba1b-124a-4e7d-8d6e-590e348c07d9', 1099, 2832, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m looking at pybrain for taking server monitor alarms and determining the root cause of a problem. I''m happy with training it using supervised learning and curating the training data sets. The data is structured something like this:

 * Server Type **A** #1
  * Alarm type 1
  * Alarm type 2
 * Server Type **A** #2
  * Alarm type 1
  * Alarm type 2
 * Server Type **B** #1
  * Alarm type **99**
  * Alarm type 2

So there are *n* servers, with *x* alarms that can be `UP` or `DOWN`. Both `n` and `x` are variable.

If Server A1 has *alarm 1 & 2* as `DOWN`, then we can say that *service a* is down on that server and is the cause of the problem.

If *alarm 1* is down on all servers, then we can say that *service a* is the cause.

There can potentially be multiple options for the cause, so straight classification doesn''t seem appropriate.

I would also like to tie later sources of data to the net. Such as just scripts that ping some external service.

All the appropriate alarms may not be triggered at once, due to serial service checks, so it can start with one server down and then another server down 5 minutes later.

I''m trying to do some basic stuff at first:

    from pybrain.tools.shortcuts import buildNetwork
    from pybrain.datasets import SupervisedDataSet
    from pybrain.supervised.trainers import BackpropTrainer


    INPUTS = 2
    OUTPUTS = 1

    # Build network

    # 2 inputs, 3 hidden, 1 output neurons
    net = buildNetwork(INPUTS, 3, OUTPUTS)


    # Build dataset

    # Dataset with 2 inputs and 1 output
    ds = SupervisedDataSet(INPUTS, OUTPUTS)


    # Add one sample, iterable of inputs and iterable of outputs
    ds.addSample((0, 0), (0,))



    # Train the network with the dataset
    trainer = BackpropTrainer(net, ds)

    # Train 1000 epochs
    for x in xrange(10):
        trainer.train()

    # Train infinite epochs until the error rate is low
    trainer.trainUntilConvergence()


    # Run an input over the network
    result = net.activate([2, 1])

But I[m having a hard time mapping variable numbers of alarms to static numbers of inputs. For example, if we add an alarm to a server, or add a server, the whole net needs to be rebuilt. If that is something that needs to be done, I can do it, but want to know if there''s a better way.

Another option I''m trying to think of, is have a different net for each type of server, but I don''t see how I can draw an environment-wide conclusion, since it will just make evaluations on a single host, instead of all hosts at once.

Which type of algorithm should I use and how do I map the dataset to draw environment-wide conclusions as a whole with variable inputs?', 3263, '2014-09-10 14:50:13.720', 'df440c8f-1823-4ae3-b2e5-47b7e33ac1bd', 1100, 2834, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Neural net for server monitoring', 3263, '2014-09-10 14:50:13.720', 'df440c8f-1823-4ae3-b2e5-47b7e33ac1bd', 1100, 2835, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork>', 3263, '2014-09-10 14:50:13.720', 'df440c8f-1823-4ae3-b2e5-47b7e33ac1bd', 1100, 2836, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think they both represent the same concept.

In classification trees, the Gini Index is used to compute the impurity of a data partition. So Assume the data partition D consisiting of 4 classes each with equal probability. Then the Gini Index (Gini Impurity) will be:
Gini(D) = 1 - (0.25^2 + 0.25^2 + 0.25^2 + 0.25^2)

In CART we perform binary splits. So The gini index will be computed as the weighted sum of the resulting partitions and we select the split with the smallest gini index.

So the use of Gini Impurity (Gini Index) is not limited to binary situations.

Another term for Gini Impurity is Gini Coefficient which is used normally as a measure of income distribution.
', 979, '2014-09-10 15:52:04.743', '6bc15ac4-6032-4377-9eec-9cb3ac3013a1', 1101, 2837, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><ruby>', 2487, '2014-09-10 20:58:00.777', '7c03bc4d-6a66-4fca-84f9-780088d920d1', 1092, 'edited tags', 2838, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The CoreNLP parts of speech tagger and name entity recognition tagger are pretty good out of the box, but I''d like to improve the accuracy further so that the overall program runs better. To explain more about accuracy -- there are situations in which the POS/NER is wrongly tagged. For instance:

- "Oversaw car manufacturing" gets tagged as NNP-NN-NN

Rather than VB* or something similar, since it''s a verb-like phrase (I''m not a linguist, so take this with a grain of salt).

So what''s the best way to accomplish accuracy improvement?

 - Are there better models out there for POS/NER that can be incorporated into CoreNLP?
 - Should I switch to other NLP tools?
 - Or create training models with exception rules?', 2785, '2014-09-11 17:09:52.313', '3159f3f0-114a-4243-ad5b-252b74f5b7f3', 1102, 2839, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Improve CoreNLP POS tagger and NER tagger?', 2785, '2014-09-11 17:09:52.313', '3159f3f0-114a-4243-ad5b-252b74f5b7f3', 1102, 2840, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><language-model>', 2785, '2014-09-11 17:09:52.313', '3159f3f0-114a-4243-ad5b-252b74f5b7f3', 1102, 2841, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Your best best is to train your own models on the kind of data you''re going to be working with.', 819, '2014-09-12 00:40:07.877', 'df8f5cbb-26be-4982-99d6-44c2b42aa426', 1103, 2842, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have been tasked with creating a pipeline chart with the live data and the budgeted numbers.

I know what probability of each phase of reaching the next.  The problem is I have no Idea what to do about the pipeline budgeting with regards to time.  For instance what period of time should I have closed sales in the chart.

I have honestly been working on trying to figure it out.  Each successive revision gets me farther from the answer.', 3279, '2014-09-12 04:22:38.823', 'f032c4e8-5e18-4853-b87a-660658209cc0', 1104, 2843, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Modeling Pipeline Budget', 3279, '2014-09-12 04:22:38.823', 'f032c4e8-5e18-4853-b87a-660658209cc0', 1104, 2844, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation><time-series>', 3279, '2014-09-12 04:22:38.823', 'f032c4e8-5e18-4853-b87a-660658209cc0', 1104, 2845, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am currently trying to implement logistic regression with iteratively reweightes LS, according to "Pattern Recognition and Machine Learning" by C. Bishop. In a first approach I tried to implement it in C#, where I used Gauss'' algorithm to solve eq. 4.99. For a single feature it gave very promising (nearly exact) results, but whenever I tried to run it with more than one feature my system matrix became singular, and the weights did not converge. I first thought that it was my implementation, but when I implemented it in SciLab the results sustained. The SciLab (more concise due to matrix operators) code I used is

    phi = [1; 0; 1; 1];
    t = [1; 0; 0; 0];
    w= [1];

    w'' * phi(1,:)''

    for in=1:100
        y = [];
        R = zeros(size(phi,1));
        R_inv = zeros(size(phi,1));

        for i=1:size(phi,1)
            y(i) = 1/(1+ exp(-(w'' * phi(i,:)'')));
            R(i,i) = y(i)*(1 - y(i));
            R_inv(i,i) = 1/R(i,i);
        end

        z = phi * w - R_inv*(y - t)
        w = inv(phi''*R*phi)*phi''*R*z
    end

With the values for phi (input/features) and t (output/classes), it yields a weight of  -0.6931472, which is pretty much 1/3, which seems fine to me, for there is 1/3 probability of beeing assigned to class 1, if feature 1 is present (please forgive me, if my terms do not comply with ML-language completely, for I am an software developer). If I now added an intercept feature, which would accord to

    phi = [1, 1; 1, 0; 1, 1; 1, 1];
    w = [1; 1];

my R-matrix becomes singular and the last weights value is

    w  =
      - 5.8151677
      1.290D+30

which - to my reading - would mean, that the probability of belonging to class 1 would be close to 1 if feature 1 is present about 3% for the rest. There has got to be any error I made, but I do not get which one. For both implementations yield the same results I suspect that there is some point I''ve been missing or gotten wrong, but I do not understand which one.', 3283, '2014-09-12 11:06:48.203', 'd19c1b64-2f4c-432f-9275-d7a894c2c3fe', 1105, 2846, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Logistic Regression implementation does not converge', 3283, '2014-09-12 11:06:48.203', 'd19c1b64-2f4c-432f-9275-d7a894c2c3fe', 1105, 2847, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<logistic-regression>', 3283, '2014-09-12 11:06:48.203', 'd19c1b64-2f4c-432f-9275-d7a894c2c3fe', 1105, 2848, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am having an HTML string and want to find out if a word I supply is relevant in that string.

Relevancy could be measured based on frequency in the text.

An example to illustrate my problem:

    this is an awesome bike store
    bikes can be purchased online.
    the bikes we own rock.
    check out our bike store now


Now I want to test a few other words:

    bike repairs
    dog poo

`bike repairs` should be marked as relevant whereas `dog poo` should not be marked as relevant.

Questions:

 * How could this be done?
 * How to I filter out ambiguous words like `in` or `or`

Thanks for your ideas!

I guess it''s something Google does to figure out what keywords are relevant to a website. I am basically trying to reproduce their on-page rankings.', 3284, '2014-09-12 11:48:21.617', '56894379-2f75-484e-adc0-15a500500383', 1106, 2849, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Find out if a word is relevant to a string', 3284, '2014-09-12 11:48:21.617', '56894379-2f75-484e-adc0-15a500500383', 1106, 2850, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining>', 3284, '2014-09-12 11:48:21.617', '56894379-2f75-484e-adc0-15a500500383', 1106, 2851, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a classification problem with approximately 1000 positive and 10000 negative samples in training set. So this data set is quite unbalanced. Plain random forest is just trying to mark all test samples as a majority class.

Some good answers about sub-sampling and weighted random forest are given here: http://datascience.stackexchange.com/questions/454/what-are-the-implications-for-training-a-tree-ensemble-with-highly-biased-datase

Which classification methods besides RF can handle the problem in the best way?', 97, '2014-09-12 15:20:51.767', '35a9ce98-7cc6-4803-9fe2-3cad4780f8d9', 1107, 2852, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Quick guide into training highly imbalanced data sets', 97, '2014-09-12 15:20:51.767', '35a9ce98-7cc6-4803-9fe2-3cad4780f8d9', 1107, 2853, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><dataset><unbalanced-classes>', 97, '2014-09-12 15:20:51.767', '35a9ce98-7cc6-4803-9fe2-3cad4780f8d9', 1107, 2854, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As mentioned [before][1], I have a classification problem and unbalanced data set. Which contains 9-10 times more negative samples than positive.
I have trained `"gbm"` Generalized Boosted Regression model from `CARET` package in `R` and get the following output:

      interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD
      1                  50       0.906     0.523  0.00978      0.0512
      1                  100      0.91      0.561  0.0108       0.0517
      1                  150      0.91      0.572  0.0104       0.0492
      2                  50       0.908     0.569  0.0106       0.0484
      2                  100      0.91      0.582  0.00965      0.0443
      2                  150      0.91      0.584  0.00976      0.0437
      3                  50       0.909     0.578  0.00996      0.0469
      3                  100      0.91      0.583  0.00975      0.0447
      3                  150      0.911     0.586  0.00962      0.0443


Looking at the 90% accuracy I assume that model has labeled all the samples as majority class. That''s clear.
And what is not transparent: how Kappa is calculated.

- What does this Kappa values (near to 60%) really mean? Is it enough to say that the model is not classifying them just by chance?
- What do `Accuracy SD` and `Kappa SD` stand for?

  [1]: http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets', 97, '2014-09-12 16:26:15.827', '80256954-6698-4110-a4f5-e6ea4df86169', 1108, 2855, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Kappa near to 60% in unbalanced (1:10) data set', 97, '2014-09-12 16:26:15.827', '80256954-6698-4110-a4f5-e6ea4df86169', 1108, 2856, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><unbalanced-classes><caret><gbm><kappa>', 97, '2014-09-12 16:26:15.827', '80256954-6698-4110-a4f5-e6ea4df86169', 1108, 2857, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Undersampling the majority class is usually the way to go in such situations.

If you think that you have too few instances of the positive class, you may perform oversampling, for example, sample 5n instances with replacement from the dataset of size n.

Caveats:

 - Some methods may be sensitive to changes in the class distribution, e.g. for Naive Bayes - it affects the prior probabilities.
 - Oversampling may lead to overfitting  ', 816, '2014-09-12 20:30:51.740', '44d5239d-3c01-4421-985d-6c5bbbd34225', 1109, 2858, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to cluster a set of long-tailed /pareto-alike data into several bins (Actually the bin number is not determined yet).
Is there any algorithms or models I can use?', 3289, '2014-09-13 06:33:17.360', '75e074d1-641e-43d0-a9db-1b2851bb45ce', 1110, 2860, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to cluster a set of long-tailed / pareto data', 3289, '2014-09-13 06:33:17.360', '75e074d1-641e-43d0-a9db-1b2851bb45ce', 1110, 2861, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering><k-means>', 3289, '2014-09-13 06:33:17.360', '75e074d1-641e-43d0-a9db-1b2851bb45ce', 1110, 2862, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('- Max Kuhn covers this well in Ch16 of *Applied Predictive Modeling*.
- As mentioned in the linked thread, imbalanced data is essentially a cost sensitive training problem. Thus any cost sensitive approach is applicable to imbalanced data.
- There are a large number of such approaches. Not all implemented in R: C50, weighted SVMs are options. Jous-boost. Rusboost I think is only available as Matlab code.
- I don''t use Weka, but believe it has a large number of cost sensitive classifiers.
-  *Handling imbalanced datasets: A review*: Sotiris Kotsiantis, Dimitris Kanellopoulos, Panayiotis Pintelas''
-  *On the Class Imbalance Problem*: Xinjian Guo, Yilong Yin, Cailing Dong, Gongping Yang, Guangtong Zhou
       ', 3294, '2014-09-13 15:36:19.867', 'b78e5cab-dc65-407d-9482-88e8acbf8f2f', 1112, 2866, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a general methodological question. I have two columns of data, with one a column a numeric variable for age and another column a short character variable for text responses to a question.

My goal is to group the age variable (that is, create cut points for the age variable), based on the text responses. I''m unfamiliar with any general approaches for doing this sort of analysis. What general approaches would you recommend? Ideally I''d like to categorize the age variable based on linguistic similarity of the text responses.', 36, '2014-09-13 17:13:23.373', 'b310df29-2ee1-40dc-b59a-89d1d0565a01', 1113, 2867, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('General approahces for grouping a continuous variable based on text data?', 36, '2014-09-13 17:13:23.373', 'b310df29-2ee1-40dc-b59a-89d1d0565a01', 1113, 2868, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><text-mining>', 36, '2014-09-13 17:13:23.373', 'b310df29-2ee1-40dc-b59a-89d1d0565a01', 1113, 2869, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Gradient boosting is also a good choice here.  You can use the gradient boosting classifier in sci-kit learn for example.  Gradient boosting is a principled method of dealing with class imbalance by constructing successive training sets based on incorrectly classified examples.', 92, '2014-09-13 18:17:33.253', 'de7976f6-99d2-4f8f-acec-7259342a6c23', 1114, 2870, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to build a textual search engine?', 381, '2014-09-15 01:19:20.410', '8d9817e1-f259-4c66-9bac-d75eed281eed', 1106, 'copy edit title', 2877, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-09-15 01:19:20.410', '8d9817e1-f259-4c66-9bac-d75eed281eed', 1106, 'Proposed by 381 approved by 84 edit id of 150', 2878, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As mentioned [before][1], I have a classification problem and unbalanced data set. The majority class contains 88% of all samples.
I have trained `"gbm"` Generalized Boosted Regression model from `CARET` package in `R` and get the following output:

      interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD
      1                  50       0.906     0.523  0.00978      0.0512
      1                  100      0.91      0.561  0.0108       0.0517
      1                  150      0.91      0.572  0.0104       0.0492
      2                  50       0.908     0.569  0.0106       0.0484
      2                  100      0.91      0.582  0.00965      0.0443
      2                  150      0.91      0.584  0.00976      0.0437
      3                  50       0.909     0.578  0.00996      0.0469
      3                  100      0.91      0.583  0.00975      0.0447
      3                  150      0.911     0.586  0.00962      0.0443


Looking at the 90% accuracy I assume that model has labeled all the samples as majority class. That''s clear.
And what is not transparent: how Kappa is calculated.

- What does this Kappa values (near to 60%) really mean? Is it enough to say that the model is not classifying them just by chance?
- What do `Accuracy SD` and `Kappa SD` mean?

  [1]: http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets', 97, '2014-09-15 07:31:57.053', '6324f8a0-6ec1-4803-b70e-47621f148486', 1108, 'deleted 15 characters in body', 2879, '5');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (97, '2014-09-15 11:28:09.737', 'a14ea099-5846-4145-994a-8baf462bd614', 1108, '6', 2883, '33');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are several approaches. You can start from the second one.

** Equal-width (distance) partitioning:**

- It divides the range into N intervals of equal size: uniform grid

- if A and B are the lowest and highest values of the attribute, the width of intervals will be: `W = (B-A)/N`.

- The most straightforward
- Outliers may dominate presentation
- Skewed data is not handled well.

** Equal-depth (frequency) partitioning:**

- It divides the range into
N intervals, each containing approximately
same number of samples
- Good data scaling
- Managing categorical attributes can be tricky.


**Other Methods**

- `Rank`: The rank of a number is its size relative to other values of a numerical variable. First, we sort the list of values, then we assign the position of a value as its rank. Same values receive the same rank but the presence of duplicate values affects the ranks of subsequent values (e.g., 1,2,3,3,5). Rank is a solid binning method with one major drawback, values can have different ranks in different lists.
- `Quantiles (median, quartiles, percentiles, ...)`: Quantiles are also very useful binning methods but like Rank, one value can have different quantile if the list of values changes.
- `Math functions`: For example, logarithmic binning is an effective method for the numerical variables with highly skewed distribution (e.g., income).

**Entropy-based Binning**

[Entropy based method][1] uses a split approach. The entropy (or the information content) is calculated based on the class label. Intuitively, it finds the best split so that the bins are as pure as possible that is the majority of the values in a bin correspond to have the same class label. Formally, it is characterized by finding the split with the maximal information gain.


  [1]: http://www.saedsayad.com/supervised_binning.htm', 97, '2014-09-15 13:03:44.083', '0e7e00dd-8416-4968-9570-7ca0bb8f8d0d', 1118, 2885, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[RFM][1] - is a ranking model when all customers are ranked according to their purchasing **F** requency, **R** recency and **M** monetary value. This indicator is highly used by marketing departments of various organizations to segment customers into groups according to customer value.

The question is following: are there any substantial models based on RFM scoring (or related to) which have solid predictive power?


  [1]: http://en.wikipedia.org/wiki/RFM_(customer_value)', 97, '2014-09-15 13:14:40.797', 'cb72c3ae-20b6-445a-bad0-3b1a947688ac', 1119, 2886, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Predictive modeling based on RFM scoring indicators', 97, '2014-09-15 13:14:40.797', 'cb72c3ae-20b6-445a-bad0-3b1a947688ac', 1119, 2887, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<rmf-scoring><marketing><predictive-modeling><e-commerce>', 97, '2014-09-15 13:14:40.797', 'cb72c3ae-20b6-445a-bad0-3b1a947688ac', 1119, 2888, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This may provide some answer: http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf

You may also check out Max Kuhn''s "Applied Predictive Modeling" book. He talks about the caret package at length in this book, including the kappa statistics and how to use it. This may be of some help to you.', 3314, '2014-09-15 18:14:16.180', '350393e8-1dab-47de-9f65-26b51e2c354a', 1120, 2889, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[RFM][1] - is a ranking model when all customers are ranked according to their purchasing **F** requency, **R** recency and **M** monetary value. This indicator is highly used by marketing departments of various organizations to segment customers into groups according to customer value.

The question is following: are there any substantial models based on RFM scoring (or related to) which have solid predictive power?

Edit:
- predicting which customer will most likely spend more
- who is going to upgrade/renew subscribtion/refund etc

  [1]: http://en.wikipedia.org/wiki/RFM_(customer_value)', 97, '2014-09-15 20:03:21.770', '33c9d010-fe35-4e0b-90f0-c048922cec12', 1119, 'Make edits based on feedback in comments', 2890, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The Kappa is Cohen''s Kappa score for inter-rater agreement. It''s a commonly-used metric for evaluating the performance of machine learning algorithms and human annotaters, particularly when dealing with text/linguistics.

What it does is compare the level of agreement between the output of the (human or algorithmic) annotater and the ground truth labels, to the level of agreement that would occur through random chance. There''s a very good overview of how to calculate Kappa and use it to evaluate a classifier in this stats.stackexchange.com answer [here][1], and a more in-depth explanation of Kappa and how to interpret it in [this paper][2], entitled "Understanding Interobserver Agreement: The Kappa Statistic" by Viera & Garrett (2005).

The benefit of using Kappa, particularly in an unbalanced data set like yours, is that with a 90-10% imbalance between the classes, you can achieve 90% accuracy by simply labeling all of the data points with the label of the more commonly occurring class. The Kappa statistic is describing how well the classifier performs above that baseline level of performance.

Kappa ranges from -1 to 1, with 0 indicating no agreement between the raters, 1 indicating a perfect agreement, and negative numbers indicating systematic disagreement. While interpretation is somewhat arbitrary (and very task-dependent), Landis & Koch (1977) defined the following interpretation system which can work as a general rule of thumb:

    Kappa Agreement
    < 0 Less than chance agreement
    0.010.20 Slight agreement
    0.21 0.40 Fair agreement
    0.410.60 Moderate agreement
    0.610.80 Substantial agreement
    0.810.99 Almost perfect agreement

Which would indicate that your algorithm is performing moderately well. Accuracy SD and Kappa SD are the respective Standard Deviations of the Accuracy and Kappa scores. I hope this is helpful!

  [1]: http://stats.stackexchange.com/questions/82162/kappa-statistic-in-plain-english
  [2]: http://www1.cs.columbia.edu/~julia/courses/CS6998/Interrater_agreement.Kappa_statistic.pdf', 2969, '2014-09-15 20:29:16.620', '55ec9883-99ba-4d99-981f-2ea251bff672', 1121, 2891, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A Google search leads to many relevant resources that answer your question:

 -  USING RFM DATA TO OPTIMIZE DIRECT MARKETING CAMPAIGNS: A LINEAR PROGRAMMING APPROACH http://www.thefreelibrary.com/Using+RFM+data+to+optimize+direct+marketing+campaigns%3A+a+linear...-a0272246211
 - Data Mining using RFM Analysis http://cdn.intechweb.org/pdfs/13162.pdf
 - *Libby on Recency, Frequency, Monetary Value* (book) http://www.amazon.com/Recency-Frequency-Monetary-Century-Library/dp/1882222067

From a data science point of view, there is nothing very special or unique about this problem. You have three independent variables and one dependent variable. Regression, clustering, and classification methods can be applied.', 609, '2014-09-15 22:43:37.583', 'a00d30ff-f513-4e4c-bbc5-836971cbf426', 1122, 2892, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Suppose I am interested in classifying a set of instances composed by different content types, e.g.:

 - a piece of **text**
 - an **image**

as `relevant` or `non-relevant` for a specific class `C`.

In my classification process I perform the following steps:

 1. Given a sample, I subdivide it in text and image
 2. A first SVM binary classifier (`SVM-text`), trained only on text, classifies the text as `relevant`/`non-relevant` for the class `C`
 3. A second SVM binary classifier (`SVM-image`), trained only on images, classifies the image as `relevant`/`non-relevant` for the class `C`

Both `SVM-text` and `SVM-image` produce an estimate of the probability of the analyzed content (text or image) of being relevant for the class `C`. Given this, I am able to state whether the text is relevant for `C` and the image is relevant for `C`.

However, these estimates are valid for segments of the original sample (either the text or the image), while it is not clear how to obtain a general opinion on the whole original sample (text+image). How can I combine conveniently the opinions of the two classifiers, so as to obtain a classification for the whole original sample?', 3321, '2014-09-16 08:01:35.997', 'd5d65697-ade6-4b4d-9ab1-d3cc4e2f485a', 1123, 2893, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Combine multiple classifiers to build a multi-modal classifier', 3321, '2014-09-16 08:01:35.997', 'd5d65697-ade6-4b4d-9ab1-d3cc4e2f485a', 1123, 2894, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><svm>', 3321, '2014-09-16 08:01:35.997', 'd5d65697-ade6-4b4d-9ab1-d3cc4e2f485a', 1123, 2895, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am working on a text classification problem using Random Forest as classifiers, and a bag-of-words approach.
I am using the basic implementation of Random Forests (the one present in scikit), that creates a binary condition on a single variable at each split. Given this, is there a difference between using simple tf (term frequency) features. where each word has an associated weight that represents the number of occurrences in the document, or tf-idf (term frequency * inverse document frequency), where the term frequency is also multiplied by a value that represents the ratio between the total number of documents and the number of documents containing the word)?

In my opinion, there should not be any difference between these two approaches, because the only difference is a scaling factor on each feature, but since the split is done at the level of single features this should not make a difference.

Am I right in my reasoning? ', 3054, '2014-09-16 08:14:06.307', 'b0205c4f-314a-4a36-b97c-3d0a9de93f9e', 1124, 2896, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Difference between tf-idf and tf with Random Forests', 3054, '2014-09-16 08:14:06.307', 'b0205c4f-314a-4a36-b97c-3d0a9de93f9e', 1124, 2897, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><text-mining><random-forest>', 3054, '2014-09-16 08:14:06.307', 'b0205c4f-314a-4a36-b97c-3d0a9de93f9e', 1124, 2898, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As mentioned [before][1], I have a classification problem and unbalanced data set. The majority class contains 88% of all samples.
I have trained a Generalized Boosted Regression model using `gbm()` from the `gbm` package in `R` and get the following output:

      interaction.depth  n.trees  Accuracy  Kappa  Accuracy SD  Kappa SD
      1                  50       0.906     0.523  0.00978      0.0512
      1                  100      0.91      0.561  0.0108       0.0517
      1                  150      0.91      0.572  0.0104       0.0492
      2                  50       0.908     0.569  0.0106       0.0484
      2                  100      0.91      0.582  0.00965      0.0443
      2                  150      0.91      0.584  0.00976      0.0437
      3                  50       0.909     0.578  0.00996      0.0469
      3                  100      0.91      0.583  0.00975      0.0447
      3                  150      0.911     0.586  0.00962      0.0443


Looking at the 90% accuracy I assume that model has labeled all the samples as majority class. That''s clear.
And what is not transparent: how Kappa is calculated.

- What does this Kappa values (near to 60%) really mean? Is it enough to say that the model is not classifying them just by chance?
- What do `Accuracy SD` and `Kappa SD` mean?

  [1]: http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets', 2853, '2014-09-16 08:20:44.820', '0d301572-0ec6-47ba-8049-2a9ca049addf', 1108, 'gbm() is not in the caret (not CARET) package, but in gbm. Corrected this.', 2899, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-09-16 08:20:44.820', '0d301572-0ec6-47ba-8049-2a9ca049addf', 1108, 'Proposed by 2853 approved by 97 edit id of 153', 2900, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am interested in the field of **named entity disambiguation** and want to learn more about it. I have heard that there are contests organised by various associations on these kind of research topics. These contests are very helpful as they give a practical experience in these fields.

I found one such contest organised by Microsoft research [here][1] though the dates have already passed. Can anyone point me to any other such contests ? Also, is there a site which catalogues these contests so that one can just go there and know about all upcoming contests ?

Thanks in advance.


  [1]: http://web-ngram.research.microsoft.com/erd2014/', 3325, '2014-09-16 11:41:48.140', '2d4e9cc7-2791-4ff1-874e-50b5b8dd6d62', 1125, 2901, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Named entity disambiguation contests', 3325, '2014-09-16 11:41:48.140', '2d4e9cc7-2791-4ff1-874e-50b5b8dd6d62', 1125, 2902, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><recommendation>', 3325, '2014-09-16 11:41:48.140', '2d4e9cc7-2791-4ff1-874e-50b5b8dd6d62', 1125, 2903, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Basically, you can do one of two things:

 1. **Combine features** from both classifiers. I.e., instead of `SVM-text` and `SVM-image` you may train single `SVM` that uses both - textual and visual features.
 2. Use [**ensemble learning**](http://en.wikipedia.org/wiki/Ensemble_learning). If you already have probabilities from separate classifiers, you can simply use them as weights and compute weighted average. For more sophisticated cases there are Bayesian combiners (each classifier has its prior), boosting algorithms (e.g. see [AdaBoost](http://en.wikipedia.org/wiki/AdaBoost)) and others.

Note, that ensembles where initially created for combining different learners, not different sets if features. In this later case ensembles have advantage mostly in cases when different kinds of features just can''t be combined in a single vector efficiently. But in general, combing features is simpler and more straightforward. ', 1279, '2014-09-16 12:32:10.130', 'e34cf838-97f8-4a8d-9ffe-ba830d5c14b1', 1126, 2904, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**One class learning**

I wouldn''t be too quick to throw out one-class classification methods (option 2) - the key is to model the positive (minority) class with the one-class model.

There has been research demonstrating cases where one-class classification out-performed other approaches like sampling for highly imbalanced data as often seen with protein classification tasks.

I couldn''t find the research I recalled, but I did find some other comparisons, showing using one-class classifiers (typically modeling the minority class) achieved as good or better performance than binary classification typically with sampled "negatives" from the large set of proteins not known to be positive.

Additionally this approach also gives the advantage of much improved run-time - since you only need to train the classifier on the smaller, positive set.  A couple papers:

["Prediction of protein-protein interactions using one-class classification methods and integrating diverse biological data"][1]

["A One-Class Classification Approach for Protein Sequences and Structures"][2]

At the very least I would try some one-class methods and compare the performance using validation with your binary/multi-class classification approaches.  There are also open source implementations for many of these so it shouldn''t be too costly to try them out, for example LibSVM has a one-class SVM implementation.  Additionally, it might prove valuable for use in an ensemble with binary classifiers, since there may be more disagreement in their predictions.


**Higher level representation embedding / clustering**

Along the lines of what you were thinking with (1) and the other post suggesting PCA, approaches like clustering, sparse coding, or even topic modeling - treating each protein as a document string and different protein families as different topics - could yield a representation that might make classifying the proteins straightforward.

I.e., you could identify which group/cluster a protein belongs to or classify the cluster memberships / embedded representations.

E.g., such embedding approaches as sparse coding can yield representations that reveal which cluster a protein belongs too - so that some sets of features are only active (non-zero) for proteins in the same cluster - which can make classifying them much easier.

Additionally class labels or known cluster membership can be incorporated in the embedding process for most methods.


**Ensemble**

Ensembles of multiple classifiers tend to work best - especially when the classifiers are very diverse and can achieve comparable performance individually.

There are at least two ways use ensembles for this problem.

 1. You can build an ensemble of binary classifiers by sampling multiple different same-size negative sets and training a classifier on each.
 2. You can build an ensemble from different approaches, such as binary classifiers with different negative samples, combined with a one-class classification approach, combined with classification models trained on the embedded data.



  [1]: http://journal.imbio.de/articles/pdf/jib-77.pdf
  [2]: http://www.inf.u-szeged.hu/~busarobi/PDFs/BaBuKe09.pdf', 3326, '2014-09-16 14:29:41.520', '99399230-add1-4098-8c66-a55afa801663', 1127, 2905, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am looking for information on (formal) algebraic systems that can be used to transform time-series - in either a practical or academic context.

I hope that there exists (at least one) small, expressive, set of operators - ranging over (finite) time-series.  I want to compare and contrast different systems  with respect to algebraic completeness and brevity of representation of common time-series transformations in various domains.

I realise this question is broad - but hope it is not too vague for datascience.stackexchange.  I welcome any pointers to relevant literature for specific scenarios, or the general problem.
', 3328, '2014-09-16 15:47:17.430', '0fb0605b-c299-4b5c-b6da-36c18691b3eb', 1128, 2907, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Looking for algebras designed to transform time series', 3328, '2014-09-16 15:47:17.430', '0fb0605b-c299-4b5c-b6da-36c18691b3eb', 1128, 2908, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><algorithms><time-series>', 3328, '2014-09-16 15:47:17.430', '0fb0605b-c299-4b5c-b6da-36c18691b3eb', 1128, 2909, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<clustering><k-means><data-binning><data-partition><long-tail>', 97, '2014-09-16 15:54:44.060', 'ad1ead95-e404-4a16-9b3a-df1c23120b29', 1110, 'Add more relevant tags', 2910, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-09-16 15:54:44.060', 'ad1ead95-e404-4a16-9b3a-df1c23120b29', 1110, 'Proposed by 97 approved by 21 edit id of 152', 2911, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am looking for information on (formal) algebraic systems that can be used to transform time-series - in either a practical or academic context.

I hope that there exists (at least one) small, expressive, set of operators - ranging over (finite) time-series.  I want to compare and contrast different systems  with respect to algebraic completeness and brevity of representation of common time-series transformations in various domains.

I realise this question is broad - but hope it is not too vague for datascience.stackexchange.  I welcome any pointers to relevant literature for specific scenarios, or the general subject.
', 3328, '2014-09-16 15:57:24.737', 'b2b92f6b-e36c-4332-a1a4-991e39adb66f', 1128, 'Clarification.', 2916, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It''s probably due to the [effect coding][1] of categorical predictors. Eg, the regression coefficient for CPR = 7 is not zero but -(sum of regression coefficients for the other 6 levels). I guess EM should have an option to switch it to reference coding, then your way of computing the predicted probability should work.


  [1]: http://www.ats.ucla.edu/stat/sas/seminars/sas_logistic/logistic1.htm', 3315, '2014-09-16 16:37:33.357', 'dc642ca3-797a-4716-9883-bd904539fdf7', 1129, 2917, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":471,"DisplayName":"Spacedman"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-09-16 18:35:02.173', '297072fd-5dd6-4420-b1f4-1a9a93ed66f3', 1044, '103', 2918, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am looking for information on (formal) algebraic systems that can be used to transform time-series - in either a practical or academic context.

I hope that there exists (at least one) small, expressive, set of operators - ranging over (finite) time-series.  I want to compare and contrast different systems  with respect to algebraic completeness, and brevity of representation, of common time-series transformations in various domains.

I realise this question is broad - but hope it is not too vague for datascience.stackexchange.  I welcome any pointers to relevant literature for specific scenarios, or the general subject.
', 3328, '2014-09-16 22:46:24.743', 'dbab70dc-2c36-4165-b66e-d73783cd696e', 1128, 'Commas to breathe. :)', 2923, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The most direct and obvious transformation is from time domain to [frequency domain][1]. Possible methods include [Fourier transform][2] and [wavelet transform][3].  After the transform the signal is represented by a function of frequency-domain elements which can be operated on using ordinary algebra.

It''s also possible to model a time series as a trajectory of a dynamical system in a state space (see: http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9892.1980.tb00300.x/abstract, and http://www3.stat.sinica.edu.tw/statistica/oldpdf/A2n16.pdf). Dynamical systems can be modeled symbolically at a course-grain level (see: http://en.wikipedia.org/wiki/Symbolic_dynamics and http://www.math.washington.edu/SymbolicDynamics/)  Symbolic dynamics draws on linear algebra.


  [1]: http://en.wikipedia.org/wiki/Frequency_domain
  [2]: http://en.wikipedia.org/wiki/Fourier_transform
  [3]: http://en.wikipedia.org/wiki/Wavelet_transform', 609, '2014-09-17 00:44:19.967', '3060f89c-4c6a-4aaf-82ba-997b8901587a', 1131, 2924, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('  I am a research scholar in data mining i want c# coding for deploying k means clustering for mixed numeric and catetgorical data. please help me', 3336, '2014-09-17 04:49:39.770', '35b0f019-b0b1-418f-8dda-16abdd601ff6', 1132, 2925, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('k means clustering for mixed numeric and categorical data using c#', 3336, '2014-09-17 04:49:39.770', '35b0f019-b0b1-418f-8dda-16abdd601ff6', 1132, 2926, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<k-means>', 3336, '2014-09-17 04:49:39.770', '35b0f019-b0b1-418f-8dda-16abdd601ff6', 1132, 2927, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The solution is described here: http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data

C# implementation can be found in ALGLIB library, which I strongly recommend: http://www.alglib.net/translator/man/manual.csharp.html#gs_packages', 97, '2014-09-17 10:16:07.100', 'b7bf7a35-f73c-401e-a8e2-e331ffd24923', 1133, 2928, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a rather large commute every day - it ranges between about an hour and about an hour and half of driving.

I have been tracking my driving times, and want to continue to do so. I am capturing the date, my time of departure, my time of arrival, the route I took (there are two or three possible ones), weather conditions (wet/dry and clear/hazy/foggy), and whether I stopped (and if so, for what reason - fuel/toilet break/food break, and for how long) for every journey to and from work.

I would like to create a system to analyse this data and suggest an optimal departure time (for the next journey) based on day of the week, weather conditions, and whether i need to stop.

Anecdotally, I can see that Tuesday mornings are worse than other mornings, the earlier I leave the more likely I am to take a toilet break or a food break, and obviously that the journey takes longer on rainy or foggy days than on clear and dry days - but I would like the system to empirically tell me that!

I assume this is a machine-learning and statistical analysis problem.

However, I have absolutely no knowledge of machine-learning, or statistical methods.

What statistical methods should I use to do this kind of analysis to the point where the data will lead to suggestions like "tomorrow is Tuesday and it is going to rain, so you must leave home between 7.50 and 8.00, and take route XYZ, to get the optimal driving time. Oh and chances are you will need a toilet break - and I have factored that in"? (assume that I manually enter tomorrows weather forecast - Ill look into integrating with a weather service later)

Note that this is life-hacking for me, trying to optimise the hell out of a tedious process, and it is very personal - specific to me and my habits, specific to this route, and specific to the morning/evening commute times. Google Maps with Traffic, TomTom with IQ, and Waze do very well in the more open-ended situations of ad-hoc driving-time prediction. Even Apple is happy to tell me on my iPhone notification screen how long it will take me to get home if I leave right now.

Also note, it appears to me that traffic is not a consideration - that is to say, I do not think I need to know the actual traffic conditions - traffic is a function of day of the week and weather. For example, there are more people on the roads on Monday and Tuesday mornings, and people drive more slowly, and more people are in cars (opting to drive instead of cycle or take public transport) when it rains.

To what extent can I let the data do all the talking? I have a somewhat ambiguous hidden agenda which may not be apparent from the data;

- I should be at work at 9.30 (i.,e. 9.15 +/- 15 minutes) every day, but the occasional 10am arrival is OK
- I want to leave home as late as possible, and yet arrive at work as early as possible
- I want to leave work as early as possible, and yet have done at least 8 hours work
- it is OK for me to, say, leave half an hour early on one day but stay late on another to compensate

I think I can come up with a procedural formula that can encompass all of these rules, but my gut feeling is that statistical analysis can make it a lot smarter.

Apart from the methods of analysis, the technology stack is not an issue. Java is my language of choice - I am quite familiar with programming in it, and in creating web applications.

Assuming that it is possible, are there Java libraries that can provide the requisite methods?

What limitations are there? I want to keep capturing more and more data every day, making the data set bigger, hopefully, making the prediction more accurate.

What other ways are there to do it? Can I push this data into, say, Wolfram Programming Cloud, or maybe something Google provides to get the desired results?', 3343, '2014-09-17 13:13:59.147', '2d9fbdcc-eccd-421d-b6d8-176584217903', 1135, 2930, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Statistical Commute Analysis in Java', 3343, '2014-09-17 13:13:59.147', '2d9fbdcc-eccd-421d-b6d8-176584217903', 1135, 2931, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><statistics>', 3343, '2014-09-17 13:13:59.147', '2d9fbdcc-eccd-421d-b6d8-176584217903', 1135, 2932, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am looking for information on (formal) algebraic systems that can be used to transform time-series - in either a practical or academic context.

I hope that there exists (at least one) small, expressive, set of operators - ranging over (finite) time-series.  I want to compare and contrast different systems  with respect to algebraic completeness, and brevity of representation, of common time-series transformations in various domains.

I realise this question is broad - but hope it is not too vague for datascience.stackexchange.  I welcome any pointers to relevant literature for specific scenarios, or the general subject.

**Edit... (Attempt to better explain what I meant by an algebraic system...)**

I was thinking about "abstract algebras" as discussed in Wikipedia:

http://en.wikipedia.org/wiki/Algebra#Abstract_algebra
http://en.wikipedia.org/wiki/Abstract_algebra#Basic_concepts

Boolean Algebras are (very simple) algebras that range over Boolean values.  A simple example of such an algebra would consist the values True and False and the operations AND, OR and NOT. One might argue this algebra is ''complete'' as, from these two constants (free-variables) and three basic operations, arbitrary boolean functions can be constructed/described.

I am interested to discover algebras where the values are (time-domain) time-series.  I''d like it to be possible to construct "arbitrary" functions, that map time-series to time-series, from a few operations which, individually, map time-series to time-series.  I am open to liberal interpretations of "arbitrary". I would be especially interested in examples of these algebras where the operations consist ''higher-order functions'' - which may have been developed for a specific domain.
', 3328, '2014-09-17 13:19:41.883', 'd03c8143-8cfd-460f-82bf-3ad1d1909572', 1128, 'More info on what I mean by ''an algebra''', 2933, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am looking for information on (formal) algebraic systems that can be used to transform time-series - in either a practical or academic context.

I hope that there exists (at least one) small, expressive, set of operators - ranging over (finite) time-series.  I want to compare and contrast different systems  with respect to algebraic completeness, and brevity of representation, of common time-series transformations in various domains.

I realise this question is broad - but hope it is not too vague for datascience.stackexchange.  I welcome any pointers to relevant literature for specific scenarios, or the general subject.

**Edit... (Attempt to better explain what I meant by an algebraic system...)**

I was thinking about "abstract algebras" as discussed in Wikipedia:

http://en.wikipedia.org/wiki/Algebra#Abstract_algebra
http://en.wikipedia.org/wiki/Abstract_algebra#Basic_concepts

Boolean Algebras are (very simple) algebras that range over Boolean values.  A simple example of such an algebra would consist the values True and False and the operations AND, OR and NOT. One might argue this algebra is ''complete'' as, from these two constants (free-variables) and three basic operations, arbitrary boolean functions can be constructed/described.

I am interested to discover algebras where the values are (time-domain) time-series.  I''d like it to be possible to construct "arbitrary" functions, that map time-series to time-series, from a few operations which, individually, map time-series to time-series.  I am open to liberal interpretations of "arbitrary". I would be especially interested in examples of these algebras where the operations consist ''higher-order functions'' - where such operations have been developed for a specific domain.
', 3328, '2014-09-17 13:34:51.830', '53ea3277-44b4-4e57-a739-879b99b3971e', 1128, 'added 12 characters in body', 2934, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m just thinking out loud....

I think you *do* want to model the traffic, at least over a work day, otherwise it wouldn''t matter what time you traveled! Absent any data, I''d assume there isn''t much variance over the working week, but that''s one thing the data will quickly confirm or refute. If it is varying, you can use a different model for each day.

You have two variables; the departure times from home and work, respectively. Let''s call them t_h and t_w. Let''s call the commute time T_c(t), where t is the time of day. You can estimate this function from the data, so I''ll assume it is given.

You want to **maximize** c t_w + (1-c) t_h subject to the constraints t_h + T_c(t_h) < 9.5 and t_w > t_h + T_c(t_h) + 8

where c is a constant you can set to adjust the relative importance of leaving home early relative to leaving work early. You should be able to solve this numerical optimization problem with Mathematica, MATLAB, or something similar. I would not recommend Java; it''s not meant for this. The only tricky part is estimating T_c. You know that it''s a non-negative function, so you could use the standard trick of estimating it''s logarithm (say, with kernels) and exponentiating. For implementation with Mathematica see [Smoothing Data, Filling Missing Data, and Nonparametric Fitting](http://reference.wolfram.com/language/ref/InterpolatingFunction.html) and [Constrained Optimization](http://reference.wolfram.com/language/tutorial/ConstrainedOptimizationOverview.html).', 381, '2014-09-18 01:39:30.063', 'bc145164-017e-45eb-93a7-ef4a41647871', 1136, 2935, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m just thinking out loud....

I think you *do* want to model the traffic, at least over a work day, otherwise it wouldn''t matter what time you traveled! Absent any data, I''d assume there isn''t much variance over the working week, but that''s one thing the data will quickly confirm or refute. If it is varying, you can use a different model for each day.

You have two variables; the departure times from home and work, respectively. Let''s call them t_h and t_w. Let''s call the commute time T_c(t), where t is the time of day. You can estimate this function from the data, so I''ll assume it is given.

You want to maximize c t_h - (1-c) t_w subject to the constraints t_h + T_c(t_h) < 9.5 and t_w > t_h + T_c(t_h) + 8

where c is a constant you can set to adjust the relative importance of leaving home early relative to leaving work early. You should be able to solve this numerical optimization problem with Mathematica, MATLAB, or something similar. I would not recommend Java; it''s not meant for this. The only tricky part is estimating T_c. You know that it''s a non-negative function, so you could use the standard trick of estimating it''s logarithm (say, with kernels) and exponentiating. For implementation with Mathematica see [Smoothing Data, Filling Missing Data, and Nonparametric Fitting](http://reference.wolfram.com/language/ref/InterpolatingFunction.html) and [Constrained Optimization](http://reference.wolfram.com/language/tutorial/ConstrainedOptimizationOverview.html).', 381, '2014-09-18 01:57:47.243', 'eee120c7-16e3-4a0e-b42e-ca525d14cd13', 1136, 'edited body', 2936, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have time series data from mobile sensors for different motions such as walking, pushups, dumbellifts, rowing and so on. All these motions have different length of time series. For classifying them using DTW, how do I choose an appropriate window size that will give good results?', 2475, '2014-09-18 06:22:13.940', '97c45a73-1bcd-4795-84af-748b09ed2296', 1137, 2938, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Choosing a window size for DTW', 2475, '2014-09-18 06:22:13.940', '97c45a73-1bcd-4795-84af-748b09ed2296', 1137, 2939, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><time-series>', 2475, '2014-09-18 06:22:13.940', '97c45a73-1bcd-4795-84af-748b09ed2296', 1137, 2940, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Decision trees (and hence Random Forests) are insensitive to monotone transformations of input features.

Since multiplying by the same factor is a monotone transformation, I''d assume that for Random Forests there indeed is no difference.

However, you eventually may consider using other classifiers that do not have this property, so it may still make sense to use the entire TF * IDF. ', 816, '2014-09-18 13:14:37.940', '1015f31c-d296-408d-b6a2-a0f45e2be0f5', 1138, 2941, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Some of the GREC shared task challenges included a named entity recognition & coreference resolution component (i.e., disambiguation), but I don''t think they''ve run GREC since 2010...?

https://sites.google.com/site/genchalrepository/reg-in-context/grec-ner', 819, '2014-09-18 13:51:58.473', 'd21e602f-e5f7-4999-b121-b698308e7aef', 1139, 2942, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES (' - pre-process your documents (some of the steps may be skipped)
  - [tokenize][1]
  - [remove stop words][2]
  - [stem][3] or [lemmatize][4]
  - do normalization (e.g. U.S.A. -> USA, météo -> meteo, etc) and orthographic correction
  - perform phonetic normalization (e.g. with [Soundex][5])
  - find equivalence classes (using thesauri, e.g. [WordNet][6])
 - use a [Vector Space model][7] to represent documents (you may use TF, aforementioned [TF-IDF][8] or other models)
 - do the same with the query: preprocess and represent it in the vector space
 - find the most similar documents by computing the vector similarity (e.g. using the [cosine similarity][9])

That''s an outline of the [Information Retrieval][10] process


[Introduction to Information Retrieval][11] by Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze is a very good book to get started in IR.


----

Or just use [Apache Solr][12] to get everything you need out of the box (or [Apache Lucene][13], that is used by Solr, to build your own application)


  [1]: http://en.wikipedia.org/wiki/Tokenization
  [2]: http://en.wikipedia.org/wiki/Stop_words
  [3]: http://en.wikipedia.org/wiki/Stemming
  [4]: http://en.wikipedia.org/wiki/Lemmatisation
  [5]: http://en.wikipedia.org/wiki/Soundex
  [6]: http://en.wikipedia.org/wiki/WordNet
  [7]: http://en.wikipedia.org/wiki/Vector_space_model
  [8]: http://en.wikipedia.org/wiki/Tf%E2%80%93idf
  [9]: http://en.wikipedia.org/wiki/Cosine_similarity
  [10]: http://en.wikipedia.org/wiki/Information_retrieval
  [11]: http://nlp.stanford.edu/IR-book/
  [12]: http://lucene.apache.org/solr/
  [13]: http://lucene.apache.org/core/', 816, '2014-09-18 19:56:25.827', '8381f088-e789-472a-949f-7abbc44be003', 1140, 2943, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am looking to choose my career in the area of decision science or predictive modeling and I am aware that this is kind of opinion based but I would like to have some suggestion from experts that I can use it to build my career in correct path. What are the tools should I know like R, SAS or any other. What are the thinks I should know to work in a data science or machine learning or predictive modeling. For me I am having problem in identifying steps that I should follow. Please suggest me some steps to follow.', 3057, '2014-09-19 02:34:04.093', 'c716ed21-f752-4205-9c65-ea0aec138ac2', 1141, 2944, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Some suggestion for career in decisions science or predictive modeling', 3057, '2014-09-19 02:34:04.093', 'c716ed21-f752-4205-9c65-ea0aec138ac2', 1141, 2945, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><r><statistics><predictive-modeling>', 3057, '2014-09-19 02:34:04.093', 'c716ed21-f752-4205-9c65-ea0aec138ac2', 1141, 2946, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First of all I should say you question probably is an off-topic and will be closed soon.

Anyway I can target you to similar questions discussed at this SE site already:

- http://datascience.stackexchange.com/questions/808/statistics-computer-science-data-science

- http://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require

A set of relevant questions at Cross Validated Stack Exchange:

- http://stats.stackexchange.com/questions/tagged/careers

This is good infographics of data science knowledge you might need to start a career:

![enter image description here][1]


  [1]: http://i.stack.imgur.com/tJJ2x.png', 97, '2014-09-19 07:15:13.803', '46145d2d-6804-45d5-b521-254aad8a3c44', 1142, 2947, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('First of all I should say you question probably is an off-topic and will be closed soon.

**Discussed at this SE site**

Anyway I can target you to similar questions discussed at this SE site already:

- http://datascience.stackexchange.com/questions/808/statistics-computer-science-data-science

- http://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require

**Cross Validated SE**

A set of relevant questions at Cross Validated Stack Exchange:

- http://stats.stackexchange.com/questions/tagged/careers

**Data scientist map**

This is good infographics of data science knowledge you might need to start a career:

![This is good infographics of data science knowledge you might need to start a career][1]


**Careers SE**

Also, simple "data scientist" querying of Careers SE site
http://careers.stackoverflow.com/jobs?searchTerm=data+scientist&location= will lead you to the following knowlege "tags":

- [R][2]
- [bigdata][3]
- [data-visualization][4]
- [hadoop][5]
- [mapreduce][6]
- [scala][7]
- [python][8]
- [matlab][9]

etc.


  [1]: http://i.stack.imgur.com/tJJ2x.png
  [2]: http://careers.stackoverflow.com/jobs/tag/r
  [3]: http://careers.stackoverflow.com/jobs/tag/bigdata
  [4]: http://careers.stackoverflow.com/jobs/tag/data-visualization
  [5]: http://careers.stackoverflow.com/jobs/tag/hadoop
  [6]: http://careers.stackoverflow.com/jobs/tag/mapreduce
  [7]: http://careers.stackoverflow.com/jobs/tag/scala
  [8]: http://careers.stackoverflow.com/jobs/tag/python
  [9]: http://careers.stackoverflow.com/jobs/tag/matlab', 97, '2014-09-19 07:23:47.883', '17ede6c8-7fea-4f83-bc52-97998c358c4d', 1142, 'More inforamation', 2948, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('First of all I should say you question probably is an off-topic and will be closed soon.

**Discussed at this SE site**

Anyway I can target you to similar questions discussed at this SE site already:

- http://datascience.stackexchange.com/questions/808/statistics-computer-science-data-science

- http://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require

**Cross Validated SE**

A set of relevant questions at Cross Validated Stack Exchange:

- http://stats.stackexchange.com/questions/tagged/careers

**Data scientist map**

This is good infographics of data science knowledge you might need to start a career ([Link to image][1]):

![This is good infographics of data science knowledge you might need to start a career][2]


**Careers SE**

Also, simple "data scientist" querying of Careers SE site
http://careers.stackoverflow.com/jobs?searchTerm=data+scientist&location= will lead you to the following knowlege "tags":

- [R][3]
- [bigdata][4]
- [data-visualization][5]
- [hadoop][6]
- [mapreduce][7]
- [scala][8]
- [python][9]
- [matlab][10]

etc.


  [1]: http://pennlio.files.wordpress.com/2013/09/roadtodatascientist1.png
  [2]: http://i.stack.imgur.com/tJJ2x.png
  [3]: http://careers.stackoverflow.com/jobs/tag/r
  [4]: http://careers.stackoverflow.com/jobs/tag/bigdata
  [5]: http://careers.stackoverflow.com/jobs/tag/data-visualization
  [6]: http://careers.stackoverflow.com/jobs/tag/hadoop
  [7]: http://careers.stackoverflow.com/jobs/tag/mapreduce
  [8]: http://careers.stackoverflow.com/jobs/tag/scala
  [9]: http://careers.stackoverflow.com/jobs/tag/python
  [10]: http://careers.stackoverflow.com/jobs/tag/matlab', 97, '2014-09-19 09:04:24.007', '9ec20645-4e1b-4d48-ac15-c75a77498b28', 1142, 'add link to higher resolution picture', 2949, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am a research scholar in data mining. I''m interested in C# implementation of K-Means clustering algorithm for mixed numeric and categorical data.', 97, '2014-09-19 12:15:42.527', '97e05d34-1be7-4b65-8dfd-705dae268134', 1132, 'Retag, reformulate.', 2950, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('K-Means clustering for mixed numeric and categorical data implementation in C#', 97, '2014-09-19 12:15:42.527', '97e05d34-1be7-4b65-8dfd-705dae268134', 1132, 'Retag, reformulate.', 2951, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<clustering><k-means><beginner><c#><library>', 97, '2014-09-19 12:15:42.527', '97e05d34-1be7-4b65-8dfd-705dae268134', 1132, 'Retag, reformulate.', 2952, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-09-19 12:15:42.527', '97e05d34-1be7-4b65-8dfd-705dae268134', 1132, 'Proposed by 97 approved by 84 edit id of 154', 2953, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Some suggestion for career in data science or predictive modeling', 97, '2014-09-19 12:19:25.697', 'f7f47f2e-b958-4a2c-88f9-066f9f9d9b7d', 1141, 'Add relevant tags.', 2956, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<education><beginner><career>', 97, '2014-09-19 12:19:25.697', 'f7f47f2e-b958-4a2c-88f9-066f9f9d9b7d', 1141, 'Add relevant tags.', 2957, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-09-19 12:19:25.697', 'f7f47f2e-b958-4a2c-88f9-066f9f9d9b7d', 1141, 'Proposed by 97 approved by 84 edit id of 155', 2958, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have time series data from mobile sensors for different motions such as walking, pushups, dumbellifts, rowing and so on. All these motions have different length of time series. For classifying them using [Dynamic Time Warping (DTW)](http://en.wikipedia.org/wiki/Dynamic_time_warping), how do I choose an appropriate window size that will give good results?', 84, '2014-09-19 12:22:10.130', '5d7fbb0a-c632-4ede-b399-35afbf905613', 1137, 'added 76 characters in body; edited tags', 2959, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<classification><time-series><parameter>', 84, '2014-09-19 12:22:10.130', '5d7fbb0a-c632-4ede-b399-35afbf905613', 1137, 'added 76 characters in body; edited tags', 2960, '6');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-09-19 14:27:04.823', '6d3931cd-189a-48e0-8541-2306cedb2375', 1143, 'from http://stats.stackexchange.com/questions/116034/machine-learning-algorithms-depth-of-understanding-vs-number-of-algorithms', 2961, '36');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-09-19 14:27:04.823', '272f235e-737c-46f3-ac5d-a5d3eab8a547', 1144, 'from http://stats.stackexchange.com/questions/116034/machine-learning-algorithms-depth-of-understanding-vs-number-of-algorithms/116044#116044', 2962, '36');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-09-19 14:27:04.823', '3aa36824-8c24-4832-ae88-d0b5aa16cf5e', 1145, 'from http://stats.stackexchange.com/questions/116034/machine-learning-algorithms-depth-of-understanding-vs-number-of-algorithms/116049#116049', 2963, '36');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Arguably someone calling himself a data scientist ought to know more about the intricacies of the algorithms he uses&mdash;e.g. what affects the convergence rate of the Fisher scoring algorithm in GLM&mdash;than a common or garden statistician&mdash;who might be content just to know that the maximum-likelihood solution will be found (perhaps after he makes a cup of coffee). In any case understanding the general concepts of statistics & machine learning is important in addition to familiarity with the methods you do use&mdash;the theory behind them, the assumptions they make, what diagnostic checks you should perform, how to interpret the results. Avoid being [this parody](http://stats.stackexchange.com/questions/104500/skills-hard-to-find-in-machine-learners/104507#104507).

You''d probably enjoy reading [Hastie et al. (2009), *The Elements of Statistical Learning*](http://statweb.stanford.edu/~tibs/ElemStatLearn/).', 'Scortchi', 3361, '2014-09-19 11:38:36.150', '3a6a80d5-eaea-4ee0-9422-067265500974', 1144, 2964, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would recommend limiting yourself to a few tried and trusted algorithms. I would not recommend Elements of statistical learning ( as a first book).  It is too theoretical, aimed  at graduate students, with exercises asking how to prove X or Y...  I think [ISL][1] is more appropriate, with more practical advice ( in any case both books are free as pdf downloads).

Besides statistics, I would make sure you are comfortable with experimental design/AB tests, and with Business Intelligence/Visualisation.

  [1]: http://www-bcf.usc.edu/~gareth/ISL/', 1256, '2014-09-19 12:39:23.103', '7bd46b4c-3bc7-465a-b812-e72872a81115', 1145, 2965, '2');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Recently i was introduced to the field of Data Science (its been 6 months approx),and i started the journey with Machine Learning Course by Andrew Ng and post that started working on the Data Science Specialization by JHU.

On practical Application front i have been working on building a Predictive Model that would Predict attrition so far i have used glm,bayesglm,rf in an effort to learn and apply these methods,but i find a lot of gap in my understanding of these Algorithms.

Dilemma:Whether "i should focus more on learning the intricacies of a few algorithms or should i use the approach of knowing a lot of them as and when and as much as required"

**Please guide me in the right direction,may be by suggesting books or articles or anything that you think would help.

*** i would be grateful if you would reply with an idea of guiding someone who has just started his career in the field of Data Science, and wants to be a person who solves practical issues for the business world.

***I would read(as many as possible) resources(books,articles) suggested in this post and would provide a personal feed back on the pros and cons of the same so as to make this a helpful post for people who come across a similar question in future,and i think it would be great if people suggesting these books can do the same.
 ', 'Vinay Tiwari', 3360, '2014-09-19 09:08:55.180', 'ca3951a0-9f91-4a01-a272-c03fad3ceb7c', 1143, 2966, '2');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine Learning Algorithms, Depth of Understanding Vs Number of algorithms,', 'Vinay Tiwari', 3360, '2014-09-19 09:08:55.180', 'ca3951a0-9f91-4a01-a272-c03fad3ceb7c', 1143, 2967, '1');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r>', 'Vinay Tiwari', 3360, '2014-09-19 09:08:55.180', 'ca3951a0-9f91-4a01-a272-c03fad3ceb7c', 1143, 2968, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You *do* want to model the traffic, at least over a work day, otherwise it wouldn''t matter what time you traveled! Absent any data, I''d assume there isn''t much variance over the working week, but that''s one thing the data will quickly confirm or refute. If it is varying, you can use a different model for each day.

You have two variables; the departure times from home and work, respectively. Let''s call them t_h and t_w. Let''s call the commute time T_c(t), where t is the time of day. You can estimate this function from the data, so I''ll assume it is given.

You want to maximize c t_h - (1-c) t_w subject to the constraints t_h + T_c(t_h) < 9.5 and t_w > t_h + T_c(t_h) + 8

where c is a constant you can set to adjust the relative importance of leaving home early relative to leaving work early. You should be able to solve this numerical optimization problem with Mathematica, MATLAB, or something similar. I would not recommend Java; it''s not meant for this. The only tricky part is estimating T_c. You know that it''s a non-negative function, so you could use the standard trick of estimating it''s logarithm (say, with kernels) and exponentiating. For implementation with Mathematica see [Smoothing Data, Filling Missing Data, and Nonparametric Fitting](http://reference.wolfram.com/language/ref/InterpolatingFunction.html) and [Constrained Optimization](http://reference.wolfram.com/language/tutorial/ConstrainedOptimizationOverview.html).', 381, '2014-09-19 20:17:35.690', 'bd14eab3-e309-4c56-ba92-7af7dd1ef61f', 1136, 'deleted 42 characters in body', 2969, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Since it is general methodological question, let''s assume we have only one text-based variable - total number of words in a sentence. First of all, it''s worth to **visualize** your data. I will pretend I have following data:

![number of words vs. age][1]

Here we see slight dependency between age and number of words in responses. We may assume that young people (approx. between 12 and 25) tend to use 1-4 words, while people of age 25-35 try to give longer answers. But how do we split these points? I would do it something like this:

![enter image description here][2]

In 2D plot it looks pretty straightforward, and this is how it works most of the time in practise. However, you asked for splitting data by a single variable - age. That is, something like this:

![enter image description here][3]

Is it a good split? I don''t know. In fact, it depends on your actual needs and interpretation of the "cut points". That''s why I asked about *concrete* task. Anyway, this interpretation is up to you.

In practise, you will have much more text-based variables. E.g. you can use every word as a feature (don''t forget to [stem or lemmatize](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) it first) with values from zero to a number of occurrences in the response. Visualizing high-dimensional data is not an easy task, so you need a way to discover groups of data without plotting them. [**Clustering**](http://en.wikipedia.org/wiki/Cluster_analysis) is a general approach for this. Though clustering algorithms may work with data of arbitrary dimensionality, we still have only 2D to plot it, so let''s come back to our example.

With algorithm like [**k-means**](http://en.wikipedia.org/wiki/K-means_clustering) you can obtain 2 groups like this:

![enter image description here][4]

Two dots - red and blue - show cluster centres, calculated by k-means. You can use coordinates of these points to split your data by any subset of axes, even if you have 10k dimensions. But again, the most important question here is: *what linguistic features will provide reasonable grouping of ages*.


  [1]: http://i.stack.imgur.com/KciVJ.png
  [2]: http://i.stack.imgur.com/uect9.png
  [3]: http://i.stack.imgur.com/G10GI.png
  [4]: http://i.stack.imgur.com/Tp4aK.png', 1279, '2014-09-19 22:51:43.883', '235ecbb4-2047-4775-b395-5184d06cd51a', 1146, 2970, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One of the discussed nice aspects of the procedure that Vowpal Wabbit uses for updates to sgd
[pdf][1] is so-called weight invariance, described in the linked as:

"Among these updates we mainly focus on a novel
set of updates that satisfies an additional invariance
property: for all importance weights of h, the update
is equivalent to two updates with importance weight
h=2. We call these updates importance invariant."

What does this mean and why is it useful?


  [1]: http://lowrank.net/nikos/pubs/liw.pdf', 1138, '2014-09-20 02:22:07.510', 'd749348a-88d1-4fb6-912e-965e432c26ab', 1147, 2972, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Invariance Property of Vowpal Wabbit Updates - Explaination', 1138, '2014-09-20 02:22:07.510', 'd749348a-88d1-4fb6-912e-965e432c26ab', 1147, 2973, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><gradient-descent>', 1138, '2014-09-20 02:22:07.510', 'd749348a-88d1-4fb6-912e-965e432c26ab', 1147, 2974, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you have enough data, use cross validation.

If you don''t have a lot of data, use cross validation on a similar dataset, and transfer the window size (the UCR archive has a bunch of similar dataset)


Don''t forget, that the best warping  window size depends on the amount of training data. As you get more data, you can have a smaller warping window, see fig 6 of http://www.cs.ucr.edu/~eamonn/DTW_myths.pdf

eamonn

 ', 3376, '2014-09-21 06:01:41.097', '976db5f7-47f3-490a-846f-bb5ae9b57e59', 1148, 2975, '2');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (97, '2014-09-21 18:25:47.173', 'd33e6f10-9261-40d2-b006-56237df70c72', 1108, '6', 2976, '34');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am working on a text classification problem on tweets. At the moment I was only considering the content of the tweets as a source of information, and I was using a simple bag of words approach using term frequencies as features, using Random Forests (this is something I cannot change).

Now my idea is to try to incorporate information present in the URLs used in tweets. Now, not all the tweets have URLs, and if I decide to use the same term frequency representation also for URLs I will have a huge number of features only from URLs. For this reason, I suppose that having a single set of features containing both the tweet term frequencies and the URL term frequencies could be bad. Besides I''ll have to fill some impossible values (like -1) for the URL features for tweets that do not have URLs, and I will probably worsen the classification for this tweets, as I will have a huge number of uninformative features.

Do you have any suggestions regarding this issue? ', 3054, '2014-09-22 15:48:32.697', '8bfea748-0d61-4c9b-931d-dbd550708ea6', 1149, 2977, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Text Classification with mixed features in Random Forests', 3054, '2014-09-22 15:48:32.697', '8bfea748-0d61-4c9b-931d-dbd550708ea6', 1149, 2978, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining><random-forest>', 3054, '2014-09-22 15:48:32.697', '8bfea748-0d61-4c9b-931d-dbd550708ea6', 1149, 2979, '3');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (1138, '2014-09-23 00:44:43.947', '2ea7fbee-2412-49b1-a6ca-267590650a60', 1147, '7', 2980, '33');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This has been asked, and answered, on CrossValidated. See http://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression and http://stats.stackexchange.com/questions/45803/logistic-regression-in-r-resulted-in-hauck-donner-phenomenon-now-what for two related answers (and there are other related questions that you can explore there). That logistic regression can blow up is a known effect in computational statistics.

Also, in situations where exp(-30) gives you roughly the relative accuracy of the double type, you would want to be extremely careful with accumulation of the round-off errors, as 1+exp(-30)=1: summing the likelihood contributions may run into numerical problems, especially when you start computing numerical derivatives and gradients. For a brief introduction into the issue in application to the typical problems in statistical computing and the specific problems it tends to encounter, see http://www.stata.com/meeting/nordic-and-baltic14/abstracts/materials/dk14_gould.pdf.', 1237, '2014-09-23 02:20:24.670', '9f51de23-e4f6-4705-92ae-1fc9792a1b70', 1150, 2981, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m a java developer and I want to perceive career in Data Science and machine learning. Please advice me where and how to begin with? What subjects like mathematical/statistical skills required and so on.


Thanks!', 3390, '2014-09-23 03:34:35.750', '32119643-bcaf-4ff0-8001-1da306adc05e', 1151, 2982, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Prerequisites for Data Science', 3390, '2014-09-23 03:34:35.750', '32119643-bcaf-4ff0-8001-1da306adc05e', 1151, 2983, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata>', 3390, '2014-09-23 03:34:35.750', '32119643-bcaf-4ff0-8001-1da306adc05e', 1151, 2984, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[RFM][1] - is a ranking model when all customers are ranked according to their purchasing **F** requency, **R** recency and **M** monetary value. This indicator is highly used by marketing departments of various organizations to segment customers into groups according to customer value.

The question is following: are there any substantial models based on RFM scoring (or related to) which have solid predictive power?

**Update**:

- predicting which customer will most likely spend more
- who is going to upgrade/renew subscribtion/refund etc

**Update2**:

- I understand, this simple problem with three independent variable and one classifier. My guess and experience say these pure three factors do not predict future customer value. But they can be used together with another data or can be an additional input into some model.
- Please share which methodologies worked for you personally and are likely to have high predictive ability. What kind of data you used together with RFM indicators and it worked well?

  [1]: http://en.wikipedia.org/wiki/RFM_(customer_value)', 97, '2014-09-23 08:57:41.193', 'ab747210-9454-46be-9f90-0fc9089d0d92', 1119, 'Add more information, corrected spelling error in tag.', 2985, '5');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (97, '2014-09-23 08:58:09.600', '3743307a-43c4-4952-984f-6d2a393e4de4', 1119, '8', 2986, '33');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<marketing><predictive-modeling><e-commerce><rfm-scoring>', 97, '2014-09-23 08:57:41.193', 'ab747210-9454-46be-9f90-0fc9089d0d92', 1119, 'Add more information, corrected spelling error in tag.', 2987, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('[RFM][1] - is a ranking model when all customers are ranked according to their purchasing **F** requency, **R** recency and **M** monetary value. This indicator is highly used by marketing departments of various organizations to segment customers into groups according to customer value.

The question is following: are there any substantial models based on RFM scoring (or related to) which have solid predictive power?

**Update**:

- predicting which customer will most likely spend more
- who is going to upgrade/renew subscribtion/refund etc

**Update2**:

- I understand, this is simple problem with three independent variable and one classifier. My guess and experience say these pure three factors do not predict future customer value. But they can be used together with another data or can be an additional input into some model.
- Please share which methodologies worked for you personally and are likely to have high predictive ability. What kind of data you used together with RFM indicators and it worked well?

  [1]: http://en.wikipedia.org/wiki/RFM_(customer_value)', 97, '2014-09-23 09:11:21.413', '6b4fe253-ae05-49e7-9ba7-a17b4053c8db', 1119, 'minor changes in spelling', 2988, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are you using raw term frequencies, or TF-IDF?

Perhaps you could simply combine the terms in the tweet with the terms in the URL-linked pages (if any) into a single bag of words, calculate TF-IDF, and normalize to avoid bias towards longer documents (i.e., those tweets containing URL links).

> if I decide to use the same term frequency representation also for URLs I will have a huge number of features only from URLs

I don''t understand what you mean here. Aren''t your features the terms in your bag of words? So the number of features will be the size of your vocabulary, which I imagine won''t change much whether you include URLs or not.

> Besides I''ll have to fill some impossible values (like -1) for the URL features for tweets that do not have URLs, and I will probably worsen the classification for this tweets, as I will have a huge number of uninformative features.

I don''t understand this either. Term-document matrices are virtually always a sparse matrix, since most of the terms in your vocabulary won''t appear in most of your documents. So, the vast majority of values in your TDM will be 0. I don''t know where you''re getting -1 from.', 819, '2014-09-23 13:51:24.650', '8dd3de35-d107-4bae-aa99-1010d3aac6cf', 1152, 2990, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I remember a long time ago playing with Elastic Search (the website is very different now from what I remember). There is some stuff about dealing with human language here : http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/languages.html

Be warned that Elastic search is like a big bazooka to your problem. If your problem is very simple, maybe you want to go from scratch. There is some docs in the web about it.', 3400, '2014-09-23 20:39:29.810', 'd19294f6-7cae-425e-8520-45b6e0300f89', 1153, 2991, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a problem and I''m having trouble representing it - first I thought I should use graph theory (nodes and edges) and now I''m not sure.

My data is some tanks names and it''s volumes, those tanks are connected by pipelines which I have the names and length.

    ------(pipeline 1)------.-----(pipeline 2)------.----(pipeline 3)---
      |                     |    |                           |         |
    [R tank 1]        [S tank 1] [S tank 2]            (pipeline 4) [S tank 3]
                                                             |
                                                         [S tank 4]

    R tank is sink (receiver) and S tank is source (sender)

Problem is the pipe names change doesn''t occur where there is a tank - they change name because historical reasons, size or connections...

So if I want to graphically show that S tank 2 is connected to pipeline 2 at point X and pipeline 2 connects to pipeline and the content goes to R tank 1, how should I do this? (I think the point X may not be relevant but if I had some way to get the distance travelled would be great).  ', 3400, '2014-09-23 20:58:01.027', '090da214-733c-4707-9da8-5b9ee773d541', 1154, 2992, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Trouble representing a problem', 3400, '2014-09-23 20:58:01.027', '090da214-733c-4707-9da8-5b9ee773d541', 1154, 2993, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><recommendation><graphs>', 3400, '2014-09-23 20:58:01.027', '090da214-733c-4707-9da8-5b9ee773d541', 1154, 2994, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I couldn''t quite think of how best to title this, so recommendations are welcome. Same goes for the tags--I don''t have the reputation to use the tags that I thought were appropriate. The question is this:

"Suppose you have a N pairs of observations, (x,y), and you have a model with some unknown parameters, B, that estimates the relationship between x and y, F(x,B) -> y. Now suppose you determine B using the method of least-squares (and, implicitly, that all the assumptions of least-squares are satisfied). The parameters, B, are themselves random variables, each with its own variance. Is there any way to estimate the reduction (or increase) in the variance of B that would result from applying the same method of least-squares to N+1 pairs of observations?"

The question is asked in the context of experimentation. If each data point costs $X, an affirmative answer to the question would go a long way in determining whether or not to continue testing.

Many thanks.', 3411, '2014-09-24 16:25:48.803', '96f9e453-8772-4a9d-9c15-380beec0f46d', 1155, 2995, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Estimating Variance Reduction Resultant from Additional Data', 3411, '2014-09-24 16:25:48.803', '96f9e453-8772-4a9d-9c15-380beec0f46d', 1155, 2996, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics><algorithms>', 3411, '2014-09-24 16:25:48.803', '96f9e453-8772-4a9d-9c15-380beec0f46d', 1155, 2997, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m a java developer and I want to pursue career in Data Science and machine learning. Please advise me where and how to begin? What subjects like mathematical/statistical skills are required and so on.', 21, '2014-09-24 18:52:22.853', '2564ab31-82ba-499a-aee7-21a5d0398a7e', 1151, 'deleted 16 characters in body', 2998, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Suppose I have trained a SOM. What are the common techniques to relate a region on the map with the original data?', 3167, '2014-09-25 08:33:13.583', 'a793d0e4-f3bb-49cc-9ff0-a98650c2dcf8', 1158, 3005, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to associate SOM region with data?', 3167, '2014-09-25 08:33:13.583', 'a793d0e4-f3bb-49cc-9ff0-a98650c2dcf8', 1158, 3006, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork>', 3167, '2014-09-25 08:33:13.583', 'a793d0e4-f3bb-49cc-9ff0-a98650c2dcf8', 1158, 3007, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a large set of data, about 8GB. I want to use machine learning to analyze it. So I think I should do SVD->PCA to reduce the data dimension for efficiency. But MATLAB and Octabe cannot load such large data. What are the other tools I can use to do SVD with such large amount of data? ', 3167, '2014-09-25 08:40:59.467', 'd3bc44db-fee9-4fa6-8f8d-df847695c399', 1159, 3008, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to do SVD with big data?', 3167, '2014-09-25 08:40:59.467', 'd3bc44db-fee9-4fa6-8f8d-df847695c399', 1159, 3009, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><data-mining><dimensionality-reduction>', 3167, '2014-09-25 08:40:59.467', 'd3bc44db-fee9-4fa6-8f8d-df847695c399', 1159, 3010, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I understand you correctly, I would try a few featurization methods to transform the text column to a numeric value. Then you can proceed with analysis as usual. There is a great book on NLP called [Taming Text][1] that would give numerous ways to think about your text variables.


  [1]: http://tamingtext.com/', 3430, '2014-09-25 15:02:45.533', 'e9c81db1-3461-4f21-b76e-b74286269541', 1160, 3011, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would say that trying out the resources online such as the [Explore Data Science course][1] and Andrew Ng''s ML Course on Coursera (mentioned in the notes above) are great. However, nothing beats true data that you understand inside and out. After picking up some skills from Explore Data Science, I propose you collect your fuel usage data continuously. Collect the basics: city, state, gallons, total cost, and your vehicles mileage. When you forget to enter it, then you have to learn how to work with missing data. As you feel more comfortable, enter in kaggle contests. The data science community is growing so quickly that the online resources are vast and will help you identify any areas where a text book is what you need. Good Luck!! Have fun!


  [1]: https://exploredatascience.boozallen.com', 3430, '2014-09-25 15:20:18.660', '9226ab28-62de-4eac-a17e-9ce96b1f0a63', 1161, 3012, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think ethics in Data Science is important. There is a fundamental difference in using user data to better their experience and show relevant ads and using user data to trick people into clicking on ads for the sake of monetary profit. Personally I like ads that give me relevant information like deals on things I would buy anyway. However, showing me weight loss ads because I got dumped is creepy and unethical. As my friend Peter always says, "don''t be creepy with data". ', 3430, '2014-09-25 15:29:00.877', '75aea98a-2e3a-48c0-a123-c5a1ba36e3db', 1162, 3013, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Don''t bother.

First rule of programming- which also applies to data science: get everything working on a small test problem.

so take a random sample of your data of say 100,000 rows. try different algorithms etc.  once you have got everything working to your satisfaction, you can try larger (and larger) data sets - and see how the test error reduces as you add more data.

furthermore you do not want to apply svd to only 8 columns: you apply it when you have a lot of columns.

', 1256, '2014-09-25 17:33:09.617', 'bcfaa60c-0420-48d1-b2bb-db1dc7853b70', 1163, 3014, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Although you can probably find some tools that will let you do it on a single machine, you''re getting into the range where it make sense to consider "big data" tools like Spark, especially if you think your data set might grow. Spark has a component called MLlib which supports PCA and SVD. [The documentation has examples](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html).', 381, '2014-09-25 18:48:12.303', '4f5e5915-6557-40d9-8db1-b039fc1b9137', 1164, 3015, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m going to start a Computer Science phd this year and for that I need a research topic. I am interested in Predictive Analytics in the context of Big Data. In that field, what are the unexplored areas that can help me choose a strong topic? Thanks.', 3433, '2014-09-25 20:18:46.880', 'e733f512-11f3-4995-a166-5a86d0abd51d', 1165, 3016, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Looking for a strong Phd Topic in Predictive Analytics in the context of Big Data', 3433, '2014-09-25 20:18:46.880', 'e733f512-11f3-4995-a166-5a86d0abd51d', 1165, 3017, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><data-mining><statistics><predictive-modeling>', 3433, '2014-09-25 20:18:46.880', 'e733f512-11f3-4995-a166-5a86d0abd51d', 1165, 3018, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a large set of data, about 8GB. I want to use machine learning to analyze it. So I think I should do SVD/PCA to reduce the data dimension for efficiency. But MATLAB and Octave cannot load such a large dataset.

What tools I can use to do SVD with such a large amount of data? ', 2853, '2014-09-25 21:10:47.007', '8719a645-6787-4aad-8993-2de2de5ebc10', 1159, 'mainly: changed "Octabe" to "Octave" so search engines will find this', 3019, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-09-25 21:10:47.007', '8719a645-6787-4aad-8993-2de2de5ebc10', 1159, 'Proposed by 2853 approved by 21 edit id of 157', 3020, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m studying reinforcement learning in order to implement a kind of time series pattern analyzer such as market.

The most examples I have seen are based on the maze environment.

But in real market environment, the signal changes endlessly as time passes and I can not guess how can I model environment and states.


Another question is about buy-sell modeling.

Let''s assume that the agent randomly buy at time t and sell at time t + alpha.

It''s simple to calculate reward.
The problem is how can I model Q matrix and how can I model signals between buy and sell actions.

Can you share some source code or guidance for similar situation?

Thanks in advance,
', 384, '2014-09-25 21:10:55.473', 'd6134bcf-50ac-4752-a9f8-0823d73d9283', 1017, 'corrected some words', 3021, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-09-25 21:10:55.473', 'd6134bcf-50ac-4752-a9f8-0823d73d9283', 1017, 'Proposed by 384 approved by 21 edit id of 156', 3022, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Matlab is a great tool for some mathematical experiments, Neural Networks, Image Processing ... I would like to know is there such a comprehensive and strong tool for data manipulation and NLP tasks?', 3436, '2014-09-26 06:56:26.913', '938c93e6-369b-45a9-9c14-41eed0f0385c', 1166, 3024, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the best tool for NLP?', 3436, '2014-09-26 06:56:26.913', '938c93e6-369b-45a9-9c14-41eed0f0385c', 1166, 3025, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><nlp>', 3436, '2014-09-26 06:56:26.913', '938c93e6-369b-45a9-9c14-41eed0f0385c', 1166, 3026, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The correct model is binomial, both poisson and normal are just approximations. The binomial pdf is defined on the integers between zero and number of trials. The poisson is defined on the integers between 0 and infinity, and normal is on all real variables between zero and infinity.

In other words: for a poisson there is a (possibly small) but non zero probability of having more clicks than impressions. For gaussian you can have even negative clicks. Of course, the particular parameters determine how big an impact this has... probably worth plotting the respective pdfs', 1256, '2014-09-26 07:47:45.600', '68e9014f-20d9-43c8-b8e1-752354b29dfb', 1167, 3027, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Matlab is a great tool for some mathematical experiments, Neural Networks, Image Processing ... I would like to know is there such a comprehensive and strong tool for data manipulation and NLP tasks? such as tokenizing, POS tagging, parsing, training, testing .... However I am new to NLP and I would like the tool let me experiment, get familiar and progress', 3436, '2014-09-26 07:48:50.967', 'fcb05ff0-0c46-40b9-808a-76b0ee4d7d64', 1166, 'added 65 characters in body; edited title', 3028, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A tool like Matlab for NLP?', 3436, '2014-09-26 07:48:50.967', 'fcb05ff0-0c46-40b9-808a-76b0ee4d7d64', 1166, 'added 65 characters in body; edited title', 3029, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Please take a look at this paper:

**Survey on Various Natural Language Processing Toolkits**

In this work several NLP tools are introduced. Examples are:

 - Carabao Language Toolkit
 - GATE
 - Stanford NLP toolkit
 - NLTK

Just google the title of the paper and you will find the .pdf file.

', 979, '2014-09-26 08:51:46.727', '14e5ba07-ae85-40aa-af08-9fc34fb7480c', 1168, 3030, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First of all, **dimensionality reduction** is used when you have **many dimensions** with **low variance** and you want to reduce problem size by projecting into a smaller dimension space. With 8 variables (columns) your space is already low-dimensional, reducing number of variables further is unlikely to solve technical issues with memory size, but may affect dataset quality a lot. In you concrete case it''s more promising to take a look at [**online learning**](http://en.wikipedia.org/wiki/Online_machine_learning) methods. Roughly speaking, instead of working with the whole dataset, these methods take a little part of them (often referred to as "mini-batches") at a time and build a model incrementally. (I personally like to interpret word "online" as a reference to some infinitely long source of data from Internet like Twitter feed, where you just can''t load the whole dataset at once).

But what if you really wanted to apply dimensionality reduction technique like PCA to a dataset that doesn''t fit into a memory? Normally dataset is represented as a data matrix *X* of size *n* x *m*, where *n* is number of observations (rows) and *m* is a number of variables (columns). Typically problem with memory comes from only one of these two numbers.

Too many observations (n >> m)
------------------------------

When you have **too many observations**, but number of variables is from small to moderate, you can **build covariance matrix incrementally**. Indeed, typical PCA consists of constructing covariance matrix of size *m* x *m* and applying singular value decomposition to it. With *m*=1000 variables of type float64 covariance matrix has size 1000*1000*8 ~ 8Mb, which easily fits into memory and may be used with SVD. So you need only to build covariance matrix without loading entire dataset into memory - [pretty tractable task](http://rebcabin.github.io/blog/2013/01/22/covariance-matrices/).

Alternatively, you can select small representative sample from your dataset and **approximate covariance matrix**. This matrix will have all the same properties as normal, just a little bit less accurate.

Too many variables (n << m)
---------------------------

On another hand, sometimes, when you have **too many variables**, covariance matrix itself will not fit into memory. E.g. if you work with 640x480 images, every observation has 640*480=307200 variables, which results in 703Gb covariance matrix! That''s definitely not what you would like to keep in memory of your computer. Or even in memory of your cluster. So we need to reduce dimensions without building covariance matrix at all.

My favourite method for doing it is [**Random Projection**](http://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf). In short, if you have dataset *X* of size *n* x *m*, you can multiply it by some sparse random matrix *R* of size *m* x *k* (with *k* << *m*) and obtain new matrix *X''* of a much smaller size *n* x *k* with *approximately same properties* as original one. Why it works? Well, you should know that PCA aims to find set of orthogonal axes (principal components) and project your data onto first *k* of them. It turns out, that sparse random vectors are *nearly orthogonal* and thus may also be used as a new basis.

And, of course, you don''t have to multiply the whole dataset *X* by *R* - you can translate every observation *x* into new basis separately or in mini-batches.

There''s also somewhat similar algorithm called **Random SVD**. I don''t have any real experience with it, but you can find example code with explanations [here](http://stats.stackexchange.com/a/11934/3305).

--------

As a bottom line, here''s short check list for dimensionality reduction of big datasets:

1. If you have not that many dimensions (variables), simply use online learning algorithms.
2. If there are many observations, but moderate number of variables (covariance matrix fits into memory), construct matrix incrementally and use normal SVD.
3. If number of variables is too high, use incremental algorithms. ', 1279, '2014-09-26 11:39:07.313', 'c9549a48-39cb-44a9-ac51-63e067bc3a73', 1169, 3031, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a large set of data, about 8GB. I want to use machine learning to analyze it. So I think I should do SVD then PCA to reduce the data dimension for efficiency. But MATLAB and Octave cannot load such a large dataset.

What tools I can use to do SVD with such a large amount of data? ', 3167, '2014-09-26 12:31:23.367', 'f5ceac15-f996-4d55-a91e-9775ced2f071', 1159, 'edit again...', 3032, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to do SVD and PCA with big data?', 3167, '2014-09-26 12:31:23.367', 'f5ceac15-f996-4d55-a91e-9775ced2f071', 1159, 'edit again...', 3033, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Matlab is a great tool for some mathematical experiments, Neural Networks, Image Processing ...

I would like to know if there is such a comprehensive and strong tool for data manipulation and NLP tasks? such as tokenization, POS tagging, parsing, training, testing ....

However I am new to NLP and I need a tool which let me experiment, get familiar and progress', 3436, '2014-09-26 14:18:10.033', 'c79fedfc-df51-4821-b576-354d215f386e', 1166, 'added 6 characters in body', 3034, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''s a NLP toolbox for MATLAB called [**MatlabNLP**][1]. It includes modules for tokenization, preprocessing (stop word removal, text cleaning, stemming), and learning algorithms (linear regression, decision trees, support vector machines and a Naïve Bayes). A module for POS tagging is coming soon.

  [1]: https://github.com/faridani/MatlabNLP', 3412, '2014-09-26 16:15:05.580', '79ac4113-7219-4f4c-956d-f5806831af05', 1171, 3038, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('First of all, **dimensionality reduction** is used when you have **many covariated dimensions** and want to reduce problem size by rotating data points into new orthogonal basis and taking only axes with largest variance. With 8 variables (columns) your space is already low-dimensional, reducing number of variables further is unlikely to solve technical issues with memory size, but may affect dataset quality a lot. In you concrete case it''s more promising to take a look at [**online learning**](http://en.wikipedia.org/wiki/Online_machine_learning) methods. Roughly speaking, instead of working with the whole dataset, these methods take a little part of them (often referred to as "mini-batches") at a time and build a model incrementally. (I personally like to interpret word "online" as a reference to some infinitely long source of data from Internet like Twitter feed, where you just can''t load the whole dataset at once).

But what if you really wanted to apply dimensionality reduction technique like PCA to a dataset that doesn''t fit into a memory? Normally dataset is represented as a data matrix *X* of size *n* x *m*, where *n* is number of observations (rows) and *m* is a number of variables (columns). Typically problem with memory comes from only one of these two numbers.

Too many observations (n >> m)
------------------------------

When you have **too many observations**, but number of variables is from small to moderate, you can **build covariance matrix incrementally**. Indeed, typical PCA consists of constructing covariance matrix of size *m* x *m* and applying singular value decomposition to it. With *m*=1000 variables of type float64 covariance matrix has size 1000*1000*8 ~ 8Mb, which easily fits into memory and may be used with SVD. So you need only to build covariance matrix without loading entire dataset into memory - [pretty tractable task](http://rebcabin.github.io/blog/2013/01/22/covariance-matrices/).

Alternatively, you can select small representative sample from your dataset and **approximate covariance matrix**. This matrix will have all the same properties as normal, just a little bit less accurate.

Too many variables (n << m)
---------------------------

On another hand, sometimes, when you have **too many variables**, covariance matrix itself will not fit into memory. E.g. if you work with 640x480 images, every observation has 640*480=307200 variables, which results in 703Gb covariance matrix! That''s definitely not what you would like to keep in memory of your computer. Or even in memory of your cluster. So we need to reduce dimensions without building covariance matrix at all.

My favourite method for doing it is [**Random Projection**](http://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf). In short, if you have dataset *X* of size *n* x *m*, you can multiply it by some sparse random matrix *R* of size *m* x *k* (with *k* << *m*) and obtain new matrix *X''* of a much smaller size *n* x *k* with *approximately same properties* as original one. Why it works? Well, you should know that PCA aims to find set of orthogonal axes (principal components) and project your data onto first *k* of them. It turns out, that sparse random vectors are *nearly orthogonal* and thus may also be used as a new basis.

And, of course, you don''t have to multiply the whole dataset *X* by *R* - you can translate every observation *x* into new basis separately or in mini-batches.

There''s also somewhat similar algorithm called **Random SVD**. I don''t have any real experience with it, but you can find example code with explanations [here](http://stats.stackexchange.com/a/11934/3305).

--------

As a bottom line, here''s short check list for dimensionality reduction of big datasets:

1. If you have not that many dimensions (variables), simply use online learning algorithms.
2. If there are many observations, but moderate number of variables (covariance matrix fits into memory), construct matrix incrementally and use normal SVD.
3. If number of variables is too high, use incremental algorithms. ', 1279, '2014-09-26 18:17:00.380', '1110c2a5-7349-4d71-8cca-14d9d32e8eb0', 1169, 'added 33 characters in body', 3039, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The correct model is binomial, both poisson and normal are just approximations. The binomial pdf is defined on the integers between zero and number of trials. The poisson is defined on the integers between 0 and infinity, and normal is on all real variables between +/- infinity.

In other words: for a poisson there is a (possibly small) but non zero probability of having more clicks than impressions. For gaussian you can have even negative clicks. Of course, the particular parameters determine how big an impact this has... probably worth plotting the respective pdfs', 1256, '2014-09-27 08:14:16.690', 'add96ebd-db52-4d79-b2e5-0c0da25c31d3', 1167, 'correction', 3042, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are there any general open-source programs or libraries (e.g., a Python library) for analyzing user search behavior?  By "search behavior", I mean a user''s interaction with a search engine, such as querying, clicking relevant results, and spending time on those results.  I''d like something with the following properties - it doesn''t have to be all of them, but the more the merrier:

 - Models individual user behavior (aggregate and time-based)
 - Models group user behavior
 - Simulates individual user behavior, given a model
 - Is easily extensible (to accept data input formats, user models, document models, etc., that end-users define)

Links are a plus!', 1097, '2014-09-27 13:14:16.710', '6f16a386-d752-4160-8b35-525ffc8fba3c', 1172, 3043, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('General programs/libraries for studying user search behavior?', 1097, '2014-09-27 13:14:16.710', '6f16a386-d752-4160-8b35-525ffc8fba3c', 1172, 3044, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><search><library>', 1097, '2014-09-27 13:14:16.710', '6f16a386-d752-4160-8b35-525ffc8fba3c', 1172, 3045, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m going to start a Computer Science phd this year and for that I need a research topic. I am interested in Predictive Analytics in the context of Big Data. I am interested by the area of Education (MOOCs, Online courses...). In that field, what are the unexplored areas that can help me choose a strong topic? Thanks.', 3433, '2014-09-27 16:56:14.523', '94179df2-9c43-4038-b803-d34c25b17576', 1165, 'Precising the question', 3046, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Not sure if this is Math, Stats or Data Science, but I figured I would post it here to get the site used.

As a programmer, when you have a system/component implemented, you might want to allow some performance monitoring. For example to query how often a function call was used, how long it took and so on. So typically you care about count, means/percentile, max/min and similiar statistics. This could be measurements since startup, but also a rolling average or window.

I wonder if there is a good data structure which can be updated efficiently concurrently which can be used as the source for most of those queries. For example having a ringbuffer of rollup-metrics (count, sum, min, max) over increasing periods of time and a background aggregate process triggered regularly.

The focus here (for me) is on in-memory data structures with limited memory consumption. (For other things I would use a RRD type of library).', 3445, '2014-09-27 21:58:42.477', '172b7ec8-4570-44ed-8d87-698248ca2988', 1173, 3047, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Versatile data structure for combined statistics', 3445, '2014-09-27 21:58:42.477', '172b7ec8-4570-44ed-8d87-698248ca2988', 1173, 3048, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><statistics><algorithms>', 3445, '2014-09-27 21:58:42.477', '172b7ec8-4570-44ed-8d87-698248ca2988', 1173, 3049, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have post already the question few months ago about my project that I''m starting to work on. This post can be see here:
http://datascience.stackexchange.com/questions/211/human-activity-recognition-using-smartphone-data-set-problem

Now, I know this is based around multivariate time series analysis and tasks are to classify and cluster the data. I have gathered some materials (e-books, tutorials etc.) on this but still can''t see a more detailed picture of how even I should start. Here''s the tutorial that looks like it might be helpful but the thing is my data looks differently and I''m not really sure if this can be applied to my work.

http://little-book-of-r-for-multivariate-analysis.readthedocs.org/en/latest/src/multivariateanalysis.html#scatterplots-of-the-principal-components

So basically, my questions are:

How I can start on some very basic analysis? How to read data so it any meaning for me.
Any tips and advises will be much appreciated!
Note: I''m just the beginner in data science.  ', 295, '2014-09-28 12:51:43.823', '848ee655-c9b3-4d37-9400-dbcacf7cfb8d', 1174, 3050, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to classify and cluster this time series data', 295, '2014-09-28 12:51:43.823', '848ee655-c9b3-4d37-9400-dbcacf7cfb8d', 1174, 3051, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><classification><dataset><clustering><time-series>', 295, '2014-09-28 12:51:43.823', '848ee655-c9b3-4d37-9400-dbcacf7cfb8d', 1174, 3052, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Often different data samples have different weighting ( eg the costs of misclassification error  for one group of data  is higher than for other classes).
Most error metrics are of the form $\sum_i e_i$ where e_i is the loss ( eg squared error) on data point $i$. Therefore weightings of the form $\sum_i w_i e_i$ are equivalent to duplicating the data w_i times (eg for w_i integer).

One simple case is if you have repeated data - rather than keeping all the duplicated data points, you just "weight" your one repeated sample by the number of instances.

Now whilst this is easy to do in a batch setting, it is hard in vowpal wabbits online big data setting: given that you have a large data set, you do not just want to represent the data n times to deal with the weighting ( because it increases your computational load). Similarly, just multiplying the gradient vector by the weighting - which is correct in batch gradient descent - will cause big problems for stochastic/online gradient descent: essentially you shoot off in one direction ( think of large integer weights) then you shoot off in the other - causing significant instability. SGD essentially relies on all the errors to be of roughly the same order ( so that the learning rate can be set appropriately). So what they propose is to ensure that the update for training sample x_i with weight n is equivalent to presenting training sample x_i n times consecutively.

The idea being that presenting it consecutively reduces the problem because the error gradient (for that single example $x_i$) reduces for each consecutive presentation and update (as you get closer & closer to the minimum for that specific example). In other words the consecutive updates provides a kind of feedback control.

To me it sounds like you would still have instabilities (you get to zero error on x_i, then you get to zero error on x_i+1,...). the learning rate will need to be adjusted to take into account the size of the weights.
', 1256, '2014-09-29 00:20:24.093', '63980a62-add3-4f31-85c7-3c44670823f0', 1175, 3053, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('PCA is usually implemented by computing SVD on the covariance matrix.

Computing the covariance matrix is an **embarrassingly parallel** task, so it scales *linear* with the number of records, and is trivial to distribute on multiple machines!

Just do one pass over your data to compute the means. Then a second pass to compute the covariance matrix. This can be done with map-reduce easily - essentially it''s the same as computing the means again. Sum terms as in covariance are trivial to parallelize! You may only need to pay attention to numerics when summing a lot of values of similar magnitude.

Things get different when you have a huge number of *variables*. But on an 8 GB system, you should be able to run PCA on up to 20.000 dimensions in-memory with the BLAS libraries. But then you may run into the problem that PCA isn''t all that reliable anymore, because it has too many degrees of freedom. In other words: it overfits easily. I''ve seen the recommendation of having at least 10*d*d records (or was it d^3). So for 10000 dimensions, you should have at least a billion records (of 10000 dimensions... that is a lot!) for the result to be statistically reliable.', 924, '2014-09-29 00:28:26.413', 'f27ec6e9-1c38-4ea7-8dfc-a12b30494d21', 1176, 3054, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to implement item based collaborative filtering. Do any distance calculations allow for weighting of certain ranges of values within each vector?

For example, I would like to be able to say values 10..22 within each vector are more significant than values within the range 0..10.

I''ve been experimenting with Pearson, Tanimoto and Euclidean algorithms, but they all seem to assume equal weighting for each value within the vector.

Am I approaching this problem in the right way, and if not, how do others deal with this problem? ', 3459, '2014-09-29 14:52:48.130', 'f3ed955a-b694-4605-9861-eae11bf1cd80', 1177, 3055, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Distance calculation/vector range significance', 3459, '2014-09-29 14:52:48.130', 'f3ed955a-b694-4605-9861-eae11bf1cd80', 1177, 3056, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><algorithms><beginner>', 3459, '2014-09-29 14:52:48.130', 'f3ed955a-b694-4605-9861-eae11bf1cd80', 1177, 3057, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The kinds of tools you will use will vary based on the problem you are trying to solve. Social media data is rich and therefore many questions can be asked - and many tools can be used.

However, there is a general pattern you might keep in mind. Typically, you will have to use the platform''s API to gather data. You will then have to normalized and store the data in a data warehouse. Finally, you will access and analyze the data with the tools you desire. Keeping the end goal in mind, you will have to strategically choose the best technologies for the job you are doing.

For example, let''s say you wanted to study the relationships between users on a social network - a question like "Who are the mutual friends of the most popular individuals in this social network?" In this case, you would gather data using the social media platform''s API, normalize it into a CSV, import it into a Neo4j database, and then use Cypher to make queries. ', 3466, '2014-09-29 17:46:58.617', '2dec957f-047d-4171-bd96-a00becd75eb0', 1178, 3058, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I created a scoring system ("Thomas Scoring System") to deal with this problem.  If you treat "distance" as a similarity score, this system should work for you. http://exploringpossibilityspace.blogspot.com/2014/02/thomas-scoring-system.html

> Here''s Thomas Scoring System (TSS) in a nutshell: Treat each metric and their values as evidence that weigh for or against specific values in the performance index. This is an inference process and not an arithmetic calculation, as in the Usual Method. The output of TSS is an estimation of the Weight of Evidence for all index values.  The Weight of Evidence can be expressed as a probability distribution, where all the evidence weights sum to one.

To use your example, each range is a condition: A) 10 <= *x* <= 22; and B) *x* < 10. If A is true, then it weighs in favor of certain similarity (distance) scores.  If B is true, then it weights in favor of other similarity (distance) scores, but would have less weight compared to other evidence.', 609, '2014-09-29 20:31:49.387', 'fa362528-8b27-490c-a456-b4fd10881417', 1179, 3059, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You mention distance metrics but Pearson and Tanimoto are not. For Euclidean distance, simply scale the dimensions in question by some factor. For example doubling a dimension''s values makes its contribution to distance larger and so makes the feature more important to the distance metric.

On a related note you may wish to look at the Mahalanobis distance which kind of accounts for covariance or ''redundancy'' across dimensions. ', 21, '2014-09-29 22:45:38.623', 'f5bc63f9-a0a6-424e-8aac-00051ae6fcd0', 1180, 3060, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want the text-based semantic clustering EMD do.
Is there a better way of using LDA to detect topics in text, there are so provide better results?
I''m going to do my EMD on discovery topics.
Thanks', 3473, '2014-09-30 09:43:24.153', '5a382b27-0389-4092-9b49-2389b5c08082', 1181, 3061, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('document clustering by semantic similarity based EMD(earth mover distance)', 3473, '2014-09-30 09:43:24.153', '5a382b27-0389-4092-9b49-2389b5c08082', 1181, 3062, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining><clustering>', 3473, '2014-09-30 09:43:24.153', '5a382b27-0389-4092-9b49-2389b5c08082', 1181, 3063, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It sounds like you would like the [Boost Accumulators](http://www.boost.org/doc/libs/1_56_0/doc/html/accumulators.html) library:

> Boost.Accumulators is both a library for incremental statistical
> computation as well as an extensible framework for incremental
> calculation in general. The library deals primarily with the concept
> of an accumulator, which is a primitive computational entity that
> accepts data one sample at a time and maintains some internal state.
> These accumulators may offload some of their computations on other
> accumulators, on which they depend. Accumulators are grouped within an
> accumulator set. Boost.Accumulators resolves the inter-dependencies
> between accumulators in a set and ensures that accumulators are
> processed in the proper order.', 515, '2014-09-30 14:29:06.307', '6a1e9e3e-9a5b-41fb-a506-8e0f0a705403', 1182, 3064, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m interested in discovering some kind of dis-associations between the periods of a time series based on its data e.g. find some (unknown number of) periods where the data is not similar with the data from another period.

Also I would like to compare the same data but over 2 years (something like DTW?).

I get my data Excel as a two-column list: c1=date (one per each day of the year), c2=Data To Analyze

So, what algorithms could I use and in what software?

Thank you, Catalin
', 3482, '2014-09-30 19:53:20.163', 'b10ed5f7-52df-4f67-808f-6d4ab5501d04', 1183, 3065, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('discovering dis-associations between periods of time-series', 3482, '2014-09-30 19:53:20.163', 'b10ed5f7-52df-4f67-808f-6d4ab5501d04', 1183, 3066, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering><time-series>', 3482, '2014-09-30 19:53:20.163', 'b10ed5f7-52df-4f67-808f-6d4ab5501d04', 1183, 3067, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would reccomend python if you lazily evaluate the file you will have a miniscule memory footprint, and numpy/scipy give you access to all of the tools Octave/Matlab would. ', 890, '2014-09-30 20:12:32.870', '92ff7bbe-e5ea-45f5-b76d-296abeba3cd2', 1184, 3068, '2');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-10-01 01:44:47.377', '76217eaf-3218-43b1-8bde-d63304bc9df6', 1147, '7', 3069, '34');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to implement GD for standard task of NN training :) The best papers for practioneer I''ve founded so far are:

1) "Efficient BackProp" by Yann LeCun et al.

2) "Stochastic Gradient Descent Tricks" by Leon Bottou

Are there some other must read papers on this topic?

Thank you!', 2471, '2014-10-01 10:32:32.960', '99b9e231-8dfb-4e00-b439-1d4ba646d05e', 1185, 3070, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are some best papers on gradient descent for NN implementation?', 2471, '2014-10-01 10:32:32.960', '99b9e231-8dfb-4e00-b439-1d4ba646d05e', 1185, 3071, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork><gradient-descent>', 2471, '2014-10-01 10:32:32.960', '99b9e231-8dfb-4e00-b439-1d4ba646d05e', 1185, 3072, '3');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-10-01 10:52:58.303', 'c4d53ee5-222a-42f1-ac62-3bf2273ad1ee', 1119, '8', 3073, '34');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('

I am planning to use scikit linear svc classifier for text classification.

I have 1 million classified data. What I am planning to do is when user enters keyword ... first classifier will classify it in one catagory and search will happen in that catagory.

Now how do I confirm that classification will not take much time ... as I don''t want search to suffer for better catagory result.

Is using scikit for website / web application is suitable ?

Do anyone know how amazon or flipkart do classification on user query or they have completely different logic ?
', 3498, '2014-10-01 13:26:52.037', 'cfde0e8c-db9a-4b7a-baea-daa065d6413b', 1186, 3074, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How much time scikit classifier will take to classify?', 3498, '2014-10-01 13:26:52.037', 'cfde0e8c-db9a-4b7a-baea-daa065d6413b', 1186, 3075, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><python><scikit>', 3498, '2014-10-01 13:26:52.037', 'cfde0e8c-db9a-4b7a-baea-daa065d6413b', 1186, 3076, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a client that is managing several campaigns. However I''m not clear what percentage should be applied to each channel that bring traffic to my website, when assessing their participation in the objectives.

Thank you very much.

for those interested here I leave the link to my profile on linkedin. [Specialist online markegin in Bogotá][1].

thank you very much


  [1]: https://www.linkedin.com/pub/mario-mu%C3%B1oz-ahumada/52/287/28b', 3499, '2014-10-01 14:03:05.870', '26b1ccee-55c5-482a-96a1-a772e2fd0a2b', 1187, 3077, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which method works best for attribution models for the objectives?', 3499, '2014-10-01 14:03:05.870', '26b1ccee-55c5-482a-96a1-a772e2fd0a2b', 1187, 3078, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics>', 3499, '2014-10-01 14:03:05.870', '26b1ccee-55c5-482a-96a1-a772e2fd0a2b', 1187, 3079, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('> How I can start on some very basic analysis?

Take your labeled data and compute histograms of the values for each of the sets. Plot these and visually see if there''s any differences. Also compute the mean and variance of each of the different labeled sets and see if there are differences.

If it''s timeseries data, take small (overlapping) windows of time and compute various metrics - min, max, variance, mean, for instance - and use that as input to a classifier.
', 403, '2014-10-01 14:44:35.467', '88163fbd-78a5-40fa-a37d-cb50b8bd5e7d', 1188, 3080, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m interested in discovering some kind of dis-associations between the periods of a time series based on its data e.g. find some (unknown number of) periods where the data is not similar with the data from another period.

Also I would like to compare the same data but over 2 years (something like DTW?).

I get my data Excel as a two-column list: c1=date (one per each day of the year), c2=Data To Analyze

So, what algorithms could I use and in what software?

Thank you, Catalin

**Update/Later edit:**
I''m looking for dates as cut-off points from which the DataToAnalyze could be part of another cluster of consecutive dates. e.g. 2014-1-1 --> 2014-3-10 are part of Cluster_1 based on DataToAnalyze, 2014-3-11 --> 2014-5-2 are part of Cluster_2 based on DataToAnalyze and so on, where the Clusters of consecutive dates should be automatically determined based on some algorithms (which ones and which software)
', 3482, '2014-10-01 15:30:00.230', 'ea06d968-cd5d-4bf2-9726-39b24bd378fc', 1183, 'added 432 characters in body', 3081, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The database and the classification rule, how to define confusion matrix of classification rules?

MinSupp=3% và MinConf=30%

    No. outlook temperature humidity    windy   play
    1   sunny       hot     high        FALSE   no
    2   sunny       hot     high        TRUE    no
    3   overcast    hot     high        FALSE   yes
    4   rainy       mild    high        FALSE   yes
    5   rainy       cool    normal      FALSE   yes
    6   rainy       cool    normal      TRUE    no
    7   overcast    cool    normal      TRUE    yes
    8   sunny       mild    high        FALSE   no
    9   sunny       cool    normal      FALSE   yes
    10  rainy       mild    normal      FALSE   yes
    11  sunny       mild    normal      TRUE    yes
    12  overcast    mild    high        TRUE    yes
    13  overcast    hot     normal      FALSE   yes
    14  rainy       mild    high        TRUE    no


Rule found:

    1: (outlook,overcast) -> (play,yes)
    [Support=0.29 , Confidence=1.00 , Correctly Classify= 3, 7, 12, 13]

    2: (humidity,normal), (windy,FALSE) -> (play,yes)
    [Support=0.29 , Confidence=1.00 , Correctly Classify= 5, 9, 10]

    3: (outlook,sunny), (humidity,high) -> (play,no)
    [Support=0.21 , Confidence=1.00 , Correctly Classify= 1, 2, 8]

    4: (outlook,rainy), (windy,FALSE) -> (play,yes)
    [Support=0.21 , Confidence=1.00 , Correctly Classify= 4]

    5: (outlook,sunny), (humidity,normal) -> (play,yes)
    [Support=0.14 , Confidence=1.00 , Correctly Classify= 11]

    6: (outlook,rainy), (windy,TRUE) -> (play,no)
    [Support=0.14 , Confidence=1.00 , Correctly Classify= 6, 14]


Thanks. ', 3503, '2014-10-01 18:11:26.220', '3b610c8d-cf13-499d-80c1-cdda69eb719e', 1189, 3082, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to define confusion matrix of classification rules', 3503, '2014-10-01 18:11:26.220', '3b610c8d-cf13-499d-80c1-cdda69eb719e', 1189, 3083, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<confusion-matrix>', 3503, '2014-10-01 18:11:26.220', '3b610c8d-cf13-499d-80c1-cdda69eb719e', 1189, 3084, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('One of the discussed nice aspects of the procedure that Vowpal Wabbit uses for updates to sgd
[pdf][1] is so-called weight invariance, described in the linked as:

"Among these updates we mainly focus on a novel
set of updates that satisfies an additional invariance
property: for all importance weights of h, the update
is equivalent to two updates with importance weight
h/2. We call these updates importance invariant."

What does this mean and why is it useful?


  [1]: http://lowrank.net/nikos/pubs/liw.pdf', 1138, '2014-10-01 18:15:01.893', '09ebebda-8b09-4d54-b349-3fea0678127a', 1147, 'edited body', 3085, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Statement of problem: An ambulance is at the hospital dropping off a patient. The goal of the paramedic is to get released from the hospital as soon as possible. I am curious, what are the factors in how long an ambulance off loads a patient at the hospital? Can I predict how long an offload will take given certain variables. And how confident can I be in this model? The Dependent Variable is HospitalTime, it is a ratio type of data and is measured in seconds. The Independent Variables are:

 - Hospital, a nominal type of data recoded into integers, 1 would stand
   for Lee Memorial.
 - Ambulance, a nominal type of data recoded into integers, 9 would
   stand for ambulance #9
 - PatientPriority is an ordinal type of data recoded into integers. A 1
   is a high priority, 2 is a medium priority and 3 is low acuity.
 - MonthOfCall is an interval type of data recoded into integers. A 6
   would be June and 12 is December. A 12 (December) is not twice as
   much as a 6 (June) in this case.
 - HourOfCall is an interval type of data recoded into integers. Once
   again, an offload happening at 10:00 pm is not more than something
   happening at 10:00 am.
 - Officer1 and Officer2 are nominal data and are integers representing
   an EMT and a paramedic.

My question is this: Given this type of data and my goal to predict the off loading time at the hospital, what kind of regression model should I look into?

I have looked at my statistics books from university days and they are all using ratio data. My data is mixed with nominal, ordinal, interval and ratio.

I have as much data as you could ask for. I have at least 100,000 observations.

Can you please push me in the right direction? What kind of model should I use with this type of data?

Shown below are observations to give you a tiny peak at my data IncidentID,HospitalTime,Hospital,Ambulance,PatientPriority,MonthOfCall,HourOfCall,Officer1,Officer2 757620,1849,7,11,2,10,10,234,771,chr(10) 802611,2625,7,11,3,1,18,234,777,chr(10) 765597,1149,7,12,3,11,2,234,777,chr(10) 770926,1785,7,12,3,11,15,234,777,chr(10) 771689,3557,7,12,2,11,14,234,777,chr(10) 822758,1073,7,20,3,3,13,777,307,chr(10) 767249,2570,7,22,2,11,11,560,778,chr(10) 767326,1998,7,22,1,11,18,560,777,chr(10) 785903,1660,7,22,3,12,12,234,777,chr(10) 787644,2852,7,22,3,12,17,234,777,chr(10) 760294,1327,7,23,2,10,14,498,735,chr(10) 994677,3653,7,32,2,2,15,181,159,chr(10) 994677,3653,7,32,2,2,15,181,159,chr(10) 788471,2053,5,9,2,1,3,498,777,chr(10) 788471,2053,5,9,2,1,3,498,777,chr(10) 759983,1342,5,11,2,10,8,474,777,chr(10) 791243,1635,5,11,2,1,18,234,777,chr(10) 800796,1381,5,11,3,1,11,234,777,chr(10)

P.S. This question is cross-posted in Stack-Overflow under the same title and author.', 3504, '2014-10-01 19:51:21.220', 'ac9d90c1-266e-48b1-8569-525e3b3dc5a1', 1190, 3086, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Type of regression with nominal, ordinal, interval and ratio data', 3504, '2014-10-01 19:51:21.220', 'ac9d90c1-266e-48b1-8569-525e3b3dc5a1', 1190, 3087, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<regression>', 3504, '2014-10-01 19:51:21.220', 'ac9d90c1-266e-48b1-8569-525e3b3dc5a1', 1190, 3088, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The dataset that I am experimenting with is in the form of a table with columns userid and itemid. If there is a row for a given user and a given item, that means the user accessed the item (like in an online store). I am trying to cluster similar items based on this data. If a pair of items is accessed together often, then the items are similar.

Because this is a case of a high dimensionality (# of users and items will be in 10,000''s) I think I am justified in trying to use SVD as a pre-clustering step and then do some classical clustering. When I tried doing this I got poor clustering results when compared with simple hierarchical clustering. Items that weren''t very similar were being bucketed together in one dimension, while there were available dimensions that weren''t used. The results weren''t completely random, but they were definitely worse than the output from the hierarchical clustering. I attempted the SVD step with Mahaut and Octave and the results were similar. For the hierarchical clustering I used the Jaccard measure.

At this point I am starting to doubt the notion of SVD as a way to reduce dimensionality. Do you think that SVD cannot be used effectively in this case (and why?) or do you think that I made some mistake along the way? Thank you.', 3506, '2014-10-02 03:30:37.930', 'bebec4ea-f06f-4297-820d-e86573c2de92', 1191, 3089, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using SVD for clustering', 3506, '2014-10-02 03:30:37.930', 'bebec4ea-f06f-4297-820d-e86573c2de92', 1191, 3090, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering>', 3506, '2014-10-02 03:30:37.930', 'bebec4ea-f06f-4297-820d-e86573c2de92', 1191, 3091, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have several datasets with thousands of variables. This different datasets have different variables for the same thing.



Is is there a way to automatically/semi-automatically check compatible variables and to make them consistent?



if there is such thing, that would save me months of tedious work.

The data is stored in SPSS format.





Many thanks




', 3507, '2014-10-02 06:18:11.397', '8734523d-f3a1-4c41-82e7-0ed1ccda7d30', 1192, 3092, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Amalgamating multiple datasets with different variables coding', 3507, '2014-10-02 06:18:11.397', '8734523d-f3a1-4c41-82e7-0ed1ccda7d30', 1192, 3093, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><data-cleaning>', 3507, '2014-10-02 06:18:11.397', '8734523d-f3a1-4c41-82e7-0ed1ccda7d30', 1192, 3094, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The only reliable way to see how long it takes is to code it up and give it a shot.  Training will take more time, then you can save your model (pickle) to use later.', 3457, '2014-10-02 08:17:03.253', 'bd9b87a0-e3bb-4969-b23e-6cdab8d37c9b', 1193, 3095, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('We are using Singular Value Decomposition in the much same manner as you, except rather than clustering similar items, we are using a reduced-rank matrix to power a recommendation engine based on a term-document matrix in Latent Semantic Indexing.

From your brief description, your approach seems sound enough. However, I highly recommend reading [Berry, Dumais & O''Brien''s Using Linear Algebra for Intelligent Information Retrieval][1].

Key to using SVD is selecting an acceptable rank-k approximation to the original sparse matrix. You should carry out some exploratory analysis to see how much variance can be explained, using the singular values in the diagonal matrix Sigma. This question was brought up in [this question on CrossValidated][2].

A lot of the papers I''ve read suggest anywhere a rank k from 200 to 300 singular values. In a proof-of-concept implementation, we had original sparse matrix of about 10000 rows (unique terms) to about 1000 columns (unique documents), and we were capturing just under 85% of the variance with only 300 singular values.

However, that really hinges upon the nature of your data, so your mileage may vary.



  [1]: http://lsirwww.epfl.ch/courses/dis/2003ws/papers/ut-cs-94-270.pdf
  [2]: http://stats.stackexchange.com/questions/49646/for-a-random-matrix-shouldnt-a-svd-explain-nothing-at-all-what-am-i-doing-wro', 24, '2014-10-02 08:58:18.340', '511739bd-8281-4d43-9ccc-0ad9a798724b', 1194, 3096, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The dataset that I am experimenting with is in the form of a table with columns userid and itemid. If there is a row for a given user and a given item, that means the user accessed the item (like in an online store). I am trying to cluster similar items based on this data. If a pair of items is accessed together often, then the items are similar.

Because this is a case of a high dimensionality (# of users and items will be in 10,000''s) I think I am justified in trying to use SVD as a pre-clustering step and then do some classical clustering. When I tried doing this I got poor clustering results when compared with simple hierarchical clustering. Items that weren''t very similar were being bucketed together in one dimension, while there were available dimensions that weren''t used. The results weren''t completely random, but they were definitely worse than the output from the hierarchical clustering. I attempted the SVD step with Mahaut and Octave and the results were similar. For the hierarchical clustering I used the Jaccard measure.

At this point I am starting to doubt the notion of SVD as a way to reduce dimensionality. Do you think that SVD cannot be used effectively in this case (and why?) or do you think that I made some mistake along the way?', 84, '2014-10-02 12:36:46.010', '4f5e3dd7-1da9-4397-b99e-70c08830d06d', 1191, 'deleted 11 characters in body', 3097, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a client that is managing several campaigns. However I''m not clear what percentage should be applied to each channel that bring traffic to my website, when assessing their participation in the objectives.

For those interested, here I leave the link to my profile on linkedin. [Specialist online markegin in Bogotá][1].

  [1]: https://www.linkedin.com/pub/mario-mu%C3%B1oz-ahumada/52/287/28b', 84, '2014-10-02 12:39:45.060', '747a6bac-a669-4820-a053-990f51b00ff5', 1187, 'deleted 49 characters in body', 3098, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m coding a program that tests several classifiers over a database weather.arff, I found rules below, I want classify test objects.

I do not understand how the classification, it is described:
"In classification, let R be the set of generated rules and T the training data. The basic idea of the proposed method is to choose a set of high confidence rules in R to cover T. In classifying a test object, the first rule in the set of rules that matches the test object condition classifies it. This process ensures that only the highest ranked rules classify test objects. "

How to classify test objects?

    No. outlook temperature humidity    windy   play
    1   sunny       hot     high        FALSE   no
    2   sunny       hot     high        TRUE    no
    3   overcast    hot     high        FALSE   yes
    4   rainy       mild    high        FALSE   yes
    5   rainy       cool    normal      FALSE   yes
    6   rainy       cool    normal      TRUE    no
    7   overcast    cool    normal      TRUE    yes
    8   sunny       mild    high        FALSE   no
    9   sunny       cool    normal      FALSE   yes
    10  rainy       mild    normal      FALSE   yes
    11  sunny       mild    normal      TRUE    yes
    12  overcast    mild    high        TRUE    yes
    13  overcast    hot     normal      FALSE   yes
    14  rainy       mild    high        TRUE    no

Rule found:

    1: (outlook,overcast) -> (play,yes)
    [Support=0.29 , Confidence=1.00 , Correctly Classify= 3, 7, 12, 13]

    2: (humidity,normal), (windy,FALSE) -> (play,yes)
    [Support=0.29 , Confidence=1.00 , Correctly Classify= 5, 9, 10]

    3: (outlook,sunny), (humidity,high) -> (play,no)
    [Support=0.21 , Confidence=1.00 , Correctly Classify= 1, 2, 8]

    4: (outlook,rainy), (windy,FALSE) -> (play,yes)
    [Support=0.21 , Confidence=1.00 , Correctly Classify= 4]

    5: (outlook,sunny), (humidity,normal) -> (play,yes)
    [Support=0.14 , Confidence=1.00 , Correctly Classify= 11]

    6: (outlook,rainy), (windy,TRUE) -> (play,no)
    [Support=0.14 , Confidence=1.00 , Correctly Classify= 6, 14]

Thanks,
Dung
', 3503, '2014-10-02 13:06:22.433', '0508967e-d8b6-4b6e-9f64-0185b512a148', 1195, 3099, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to classify test objects?', 3503, '2014-10-02 13:06:22.433', '0508967e-d8b6-4b6e-9f64-0185b512a148', 1195, 3100, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 3503, '2014-10-02 13:06:22.433', '0508967e-d8b6-4b6e-9f64-0185b512a148', 1195, 3101, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have several datasets with thousands of variables. This different datasets have different variables for the same thing. Is there a way to automatically/semi-automatically check compatible variables and make them consistent?

If there is such thing, that would save me months of tedious work. The data is stored in SPSS format.
', 84, '2014-10-02 13:25:16.317', 'bc089f0d-3767-4eb8-bc9b-ef1aee733f34', 1192, 'deleted 52 characters in body', 3102, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m interested in discovering some kind of dis-associations between the periods of a time series based on its data, e.g., find some (unknown number of) periods where the data is not similar with the data from another period.

Also I would like to compare the same data but over 2 years (something like DTW?).

I get my data Excel as a two-column list:

    c1=date (one per each day of the year), c2=Data To Analyze

So, what algorithms could I use and in what software?

**Update/Later edit:**
I''m looking for dates as cut-off points from which the DataToAnalyze could be part of another cluster of consecutive dates. For example:

    2014-1-1 --> 2014-3-10
are part of *Cluster_1* based on DataToAnalyze. And:

    2014-3-11 --> 2014-5-2
are part of *Cluster_2* based on DataToAnalyze, and so on. So, clusters of consecutive dates should be automatically determined based on some algorithms, which is what I''m looking for. Which ones (or which software) would be applicable to this problem?', 84, '2014-10-02 13:36:58.920', 'b3d60015-37dc-4e94-98c7-032960acca6a', 1183, 'Improving question.', 3103, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Discovering dis-associations between periods of time-series', 84, '2014-10-02 13:36:58.920', 'b3d60015-37dc-4e94-98c7-032960acca6a', 1183, 'Improving question.', 3104, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a question regarding the use of neural network. I am currently working with R ([neuralnet package][1]) and I am facing the following issue.
My testing and validation set are always late with respect to the historical data. Is there a way of correcting the result?
Maybe something is wrong in my analysis

 1. I use the daily log return $$r(t) = \ln[s(t)/s(t-1)]$$
 2. I normalise my data with the sigmoid function (sigma and mu computed on my whole set)
 3. I train my neural networks with 10 dates and the output is the normalised value that follows these 10 dates.

I tried to add the trend but there is no improvement, I observed 1-2 days late. My process seems ok, what do you think about it?


  [1]: http://cran.r-project.org/web/packages/neuralnet/index.html', 37, '2014-10-02 13:40:00.890', '1021a740-042f-49fc-97c5-e0fd5215571d', 1059, 'Using mathml', 3105, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-10-02 13:40:00.890', '1021a740-042f-49fc-97c5-e0fd5215571d', 1059, 'Proposed by 37 approved by 84 edit id of 158', 3106, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Many discussions of missing data in supervised (and unsupervised) learning deal with various methods of imputation, like mean values or EM. But in some cases the data will be missing as a necessary consequence of the data generation process.

For instance, let''s say I''m trying to predict students'' grades, and one of the inputs I want to analyze is the average grades of the student''s siblings. If a particular student is an only child, then that value will be missing, not because we failed to collect the data, but because logically there is no data to collect. This is distinct from cases where the student has siblings, but we can''t find their grades.

Other examples abound: say we''re in college admissions and we want to include students'' AP exam results, but not all students took AP exams. Or we''re looking at social network data, but not all subjects have facebook and/or twitter accounts.

These data are missing, but they''re certainly not missing at random. And many algorithms, such as all supervised learning packages in scikit-learn, simply demand that there be no missing values at all in the data set.

How have people dealt with this in the past, and what off-the-shelf solutions are there? For instance, I believe the gradient boosting algorithm in R uses trees with three possible branches: left, right, and missing. Any other alternatives out there?
', 3510, '2014-10-02 17:32:42.003', '4f635e89-e07c-4ad9-bba9-f0b88bc67fd6', 1196, 3107, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Supervised Learning with Necessarily Missing Data', 3510, '2014-10-02 17:32:42.003', '4f635e89-e07c-4ad9-bba9-f0b88bc67fd6', 1196, 3108, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 3510, '2014-10-02 17:32:42.003', '4f635e89-e07c-4ad9-bba9-f0b88bc67fd6', 1196, 3109, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m sorry if this is a basic or repeated question. My searches are yielding little fruit.

I''ve made a Naive Bayes classifier that uses the bag-of-words technique to classify spam posts on a messageboard. It works, but I think I could get much better results if my models considered the word orderings and phrases. (ex: ''girls'' and ''live'' may not trigger a high spam score, even though ''live girls'' is most likely junk). How can I build a model that takes word ordering into account?

I''ve considered storing n-grams (check-out-these, out-these-live, these-live-girls), but this seems to radically increase the size of the dictionary I keep score in and causes inconsistency as phrases with very similar wording but different order will slip through.

I''m not tied to Bayesian classification, but I''d like something that someone without a strong background in statistics could grok and implement. ', 3513, '2014-10-02 23:15:27.050', '9ee9ae96-1875-4ffe-937c-6d063128fd58', 1197, 3111, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How can I classify text considering word order, instead of just using a bag-of-words approach?', 3513, '2014-10-02 23:15:27.050', '9ee9ae96-1875-4ffe-937c-6d063128fd58', 1197, 3112, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 3513, '2014-10-02 23:15:27.050', '9ee9ae96-1875-4ffe-937c-6d063128fd58', 1197, 3113, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As you mentioned, the API is the hard part, not the data.  [Quandl][1] seems to solve this problem by providing over 10 million publicly available data sets under one easy, RESTful API.  If programming isn''t your strong suit, there is a free tool to make loading data into Excel very easy.  Additionally, if you *do* enjoy programming, there are several native libraries in [R, Python, Java and more][2].


  [1]: http://quandl.com
  [2]: https://www.quandl.com/help/libraries', 3514, '2014-10-03 00:25:48.490', 'de5d7d36-8adf-4ae4-8ee9-21c6b67970bc', 1198, 3114, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a question regarding the use of neural network. I am currently working with R ([neuralnet package][1]) and I am facing the following issue.
My testing and validation set are always late with respect to the historical data. Is there a way of correcting the result?
Maybe something is wrong in my analysis

 1. I use the daily log return
![r(t) = ln(s(t)/s(t-1))][2]
 2. I normalise my data with the sigmoid function (sigma and mu computed on my whole set)
 3. I train my neural networks with 10 dates and the output is the normalised value that follows these 10 dates.

I tried to add the trend but there is no improvement, I observed 1-2 days late. My process seems ok, what do you think about it?


  [1]: http://cran.r-project.org/web/packages/neuralnet/index.html
  [2]: http://i.stack.imgur.com/A3r16.gif', 37, '2014-10-03 02:42:11.010', 'cca832d1-bdf8-42d0-a8b6-28d3dee974f3', 1059, 'removed mathjax since it was not working, added image', 3115, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-10-03 02:42:11.010', 'cca832d1-bdf8-42d0-a8b6-28d3dee974f3', 1059, 'Proposed by 37 approved by 84 edit id of 159', 3116, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I solved a similar problem not so long ago. Let X be a numerical variable that has missing values. First, we assign the value 0 to those instances in which the value is missing. Then, we add a new categorical variable to our model, X_missing, which domain is {True,False}. We obtain a data model with mixed numerical/categorical variables. You can then apply gradient descent to train a regression model from these variables. ', 2576, '2014-10-03 11:22:55.273', '57850470-ec53-4173-a0ea-319a0ea08302', 1199, 3117, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think I''d try to model the _rate_ at which the ambulances are released instead of the time it takes. That would let me use Poisson regression, which is the canonical type of GLM for rates (in R''s GLM, set family = "poisson".) However, in order to use PR, the data needs to have its variance equal to its mean, or at least close to it. ', 1241, '2014-10-03 11:39:11.240', '03a767ac-39c2-42d8-ae53-969e32bb374c', 1200, 3118, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try out some generative models like HMM.
Just check the following link:
http://stats.stackexchange.com/questions/91290/how-do-i-train-hmms-for-classification
', 11, '2014-10-03 16:07:30.327', 'ae14e3c4-6ca9-4671-aaab-78313c74d093', 1202, 3122, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is a very simple hack to incorporate word order in an existing bag-of-words model implementation. Treat some of the phrases, such as the frequently occurring bi-grams (e.g. New York) as a unit, i.e. a single word instead of treating them as separate entities. This will ensure that "New York" is different from "York New". You could also define higher order word shingles such as for n=3,4 etc.

You could use the Lucene [ShingleFilter][1] to decompose your document text into shingles as a pre-processing step and then apply the classifier on this decomposed text.

    import java.io.*;
    import org.apache.lucene.analysis.core.*;
    import org.apache.lucene.analysis.*;
    import org.apache.lucene.analysis.shingle.ShingleFilter;
    import org.apache.lucene.analysis.standard.*;
    import org.apache.lucene.util.*;
    import org.apache.lucene.analysis.util.*;
    import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
    import org.apache.lucene.analysis.charfilter.*;
    import org.apache.lucene.analysis.core.WhitespaceTokenizer;

    class TestAnalyzer extends Analyzer {

        TestAnalyzer() {
            super();
        }

        protected TokenStreamComponents createComponents( String fieldName, Reader reader ) {
            String token;
            TokenStream result = null;

            Tokenizer source = new WhitespaceTokenizer( Version.LUCENE_CURRENT, reader );
            result = new ShingleFilter(source, 2, 2);

            return new TokenStreamComponents( source, result );

        }
    }

    public class LuceneTest {

        public static void main(String[] args) throws Exception {

            TestAnalyzer analyzer = new TestAnalyzer();

            try {
                TokenStream stream = analyzer.tokenStream("field", new StringReader("This is a sample sentence."));
                CharTermAttribute termAtt = stream.addAttribute(CharTermAttribute.class);

                stream.reset();

                // print all tokens until stream is exhausted
                while (stream.incrementToken()) {
                    System.out.println(termAtt.toString());
                }

                stream.end();
                stream.close();
             }
             catch (Exception ex) {
                 ex.printStackTrace();
             }

        }
    }




  [1]: http://lucene.apache.org/core/4_6_0/analyzers-common/org/apache/lucene/analysis/shingle/ShingleFilter.html', 984, '2014-10-03 18:05:13.163', 'e4540237-b7e7-449c-a88a-c75ccfdec5fb', 1203, 3123, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to do some data mining and NLP experiments to do some research

I have decided to use NLTK or related tools and software

Which environment or operating system do you suggest for my purpose? I mean doing research on NLP

Windows or Linux?

I am a user of Windows but I thought if Linux has better shell and related software for NLP tasks then I switch to Linux

What is your experience and your preferred OS?
', 3436, '2014-10-03 20:12:24.427', 'c8dca015-9103-4802-8ee4-9541fb708179', 1204, 3124, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('NLP lab, linux or windows?', 3436, '2014-10-03 20:12:24.427', 'c8dca015-9103-4802-8ee4-9541fb708179', 1204, 3125, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><tools>', 3436, '2014-10-03 20:12:24.427', 'c8dca015-9103-4802-8ee4-9541fb708179', 1204, 3126, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Next week I''m going to begin prototyping a recommendation engine for work. I''ve implemented/completed the Netflix Challenge in Java before (for college) but have no real idea what to use for a production/enterprise level recommendation engine. Taking into consideration everything from a standalone programming language to things like Apache Mahout and Neo4j, does anyone have any advice on how to proceed?', 3526, '2014-10-04 07:50:11.633', '9813725d-b609-430f-a3b1-e456b7cefb2c', 1205, 3127, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Recommended Language/Framework for Building a New Recommendation Engine', 3526, '2014-10-04 07:50:11.633', '9813725d-b609-430f-a3b1-e456b7cefb2c', 1205, 3128, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation>', 3526, '2014-10-04 07:50:11.633', '9813725d-b609-430f-a3b1-e456b7cefb2c', 1205, 3129, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Suppose your test object is `(sunny, hot, normal, TRUE)`. Look through the rules top to bottom and see if any of the conditions are matched. The first rule for example tests the `outlook` feature. The value doesn''t match, so the rule isn''t matched. Move on to the next rule. And so on. In this case, rule 5 matches the test case and the classification for the p lay variable is "yes".

More generally, for any test case, look at the values its features take and find the first rule that those values satisfy. The implication of that rule will be its classification.', 51, '2014-10-04 15:52:44.370', '5719091c-ce57-4ac8-9d88-a17456c619c2', 1206, 3130, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are a bunch of techniques. You have already mentioned n-gram, then there is word combination and others. But the main problem (at least from your point of view) is that as the feature becomes more complex (like n-gram) the feature count increases dramatically. This is manageable. Basically before classification you must score your features and then threshold at a certain score. this way the features (or in your case n-grams) that are scored below a certain level are omitted and the feature count becomes manageable.
as for the scoring. There are numerous ways (which to select is dependent on your application) to score the features. You can begin with "BiNormal separation", "chi square", "Information Gain" and etc.
I don''t know if this answer helps you but if you are interested i can elaborate...

I forgot, in word combination you put a window of size m on the text and extract each combination of n words. of course n<m.', 3530, '2014-10-04 19:37:51.523', 'afd28301-a231-4f53-b4da-e1b149128470', 1207, 3131, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Okay, here is the background:
I am doing text mining, and my basic flow is like this:
extract feature (n-gram), reduce feature count, score (tf-idf) and classify. for my own sake i am doing comparison between SVM and neural network classifiers. here is the weird part (or am i wrong and this is reasonable?), if i use 2gram the classifiers'' result (accuracy/precision) is different and the SVM is the better one; but when i use 3-gram the results are exactly the same. what causes this? is there any explanation? is it the case of very separable classes?', 3530, '2014-10-04 19:49:53.543', 'be632437-38e4-4e57-af0c-9e613dac1045', 1208, 3132, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What circumstances causes two different classifiers to classify data exactly like one another', 3530, '2014-10-04 19:49:53.543', 'be632437-38e4-4e57-af0c-9e613dac1045', 1208, 3133, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining><neuralnetwork><svm>', 3530, '2014-10-04 19:49:53.543', 'be632437-38e4-4e57-af0c-9e613dac1045', 1208, 3134, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('coursera should be a good start. i have seen machine learning on their course list if im not mistaken. once you get the hang of it, a machine learning classic text like "machine learning and pattern recognition" by bishop or other texts can familiarize you with different classes of algorithms.
if you want to go in depth, mathematics is a must, but if you just want to get familiar and use the algorithms there''s absolutely no need for it', 3530, '2014-10-04 19:55:31.480', '5b18f9da-5e32-4a71-88b3-24052116af71', 1209, 3135, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('i have seen many papers that are basically what you just described. there is nothing wrong with what you are doing, but there are severe limitations in how this can predict the market.
let me give you an example: suppose that u have some data and you begin to predict. with each set of data, you predict the next datapoint. and then you feed this datapoint back to the system as input and do this on and on .... in most of the times the system would just continue the last trend and the timeseries won''t **break**. this is not prediction, this is line continuation... only when the system sees the break in real data, will the prediction break, and this is the lag that you are talking about (if i understand your question right).
The first thing that you can do to enhance this is to extract some market indicators from the price. this would really help', 3530, '2014-10-04 20:09:17.547', 'df667c46-1da1-49be-9862-ef3ca1442121', 1210, 3136, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('AAAAh. this was one of my course assignments. this is a simple dataset. almost all simple classifiers work well with this. try to reduce the features with PCA and then use a classifier like K-Nearest Neighbors or Neural Networks. you can skip the feature reduction part but that would probably hurt a little', 3530, '2014-10-04 20:25:30.527', 'e045cf40-1a75-4cc3-ace2-0eefbf4f57db', 1211, 3137, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you merely want to scale up a simple collaborative filter (low rank matrix factorization), I''d suggest looking at [graphlab](http://docs.graphlab.org/collaborative_filtering.html). Spark''s [MLLib](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html) is another option ([details](https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html)), and it also supports implicit feedback out of the box. Mahout''s behind the curve today; I wouldn''t bother with it until it is [migrated to Spark](https://www.mapr.com/blog/mahout-spark-what%E2%80%99s-new-recommenders).

If you want to do something that the libraries don''t do, say with regularization, you''ll have to roll your own solution in your general purpose programming language of choice. It''s not hard to get a prototype running, but you might run into scale problems in production; that''s why I recommended solutions that scale easily.

There are also black box recommender systems from commercial vendors but I have no experience with those.', 381, '2014-10-05 05:01:03.437', '8401f31a-9a74-417f-999f-2b5547b219d6', 1212, 3138, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m looking for a supervised learning algorithm that can take 2d data for input and output. As an example of something similar to my data, image a black image with some sparse white dots. Blur that image using a full range of grayscale. Then create a machine that can take the blurred image as input and produce the original sharp image as output.

I could make some sample 1D data by taking a region/radius around the original sharp point, but I don''t know the exact radius. It would be significant data duplication and a lot of guessing.

Any good algorithm suggestions for this problem? Thanks for your time.', 3539, '2014-10-05 05:12:13.633', '9a882b27-53bf-4ce6-a9ee-237de02610ca', 1213, 3139, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('machine learning algorithms for 2d data?', 3539, '2014-10-05 05:12:13.633', '9a882b27-53bf-4ce6-a9ee-237de02610ca', 1213, 3140, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><dimensionality-reduction>', 3539, '2014-10-05 05:12:13.633', '9a882b27-53bf-4ce6-a9ee-237de02610ca', 1213, 3141, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am a hands on researcher and I like testing out viable solutions, so I tend to run a lot of experiments. For example, if I am calculating a similarity score between documents, I might want to try out many measures. In fact, for each measure I might need to make several runs to test the effect of some parameters.

So far, I''ve been tracking the runs inputs and their results by writing out the results into  files with as much info about the inputs. The problem is that retrieving a specific result becomes a challenge sometimes, even if I try to add the input info to th filename. I tried using a spreadsheet with links to results but this isn''t making a huge difference.

What tools/process do you use for the book keeping of your experiments?', 3540, '2014-10-05 06:25:27.303', '94226d09-fadf-4775-b119-21e58a9d6ec8', 1214, 3142, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Book keeping of experiment runs and results', 3540, '2014-10-05 06:25:27.303', '94226d09-fadf-4775-b119-21e58a9d6ec8', 1214, 3143, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<tools><experiments>', 3540, '2014-10-05 06:25:27.303', '94226d09-fadf-4775-b119-21e58a9d6ec8', 1214, 3144, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I vastly prefer to use windows.

Build your lab in Linux. Installation of these types of tools is often much easier under Linux. The anaconda distribution (http://docs.continuum.io/anaconda/pkg-docs.html) contains nltk, and is available under Linux (http://docs.continuum.io/anaconda/install.html).

As one example of an issue we''ve run into with windows, installing theano with cuda drivers is very difficult under windows, and quite simple under Linux.', 2969, '2014-10-05 12:25:15.910', 'f5311d8e-d273-4e44-bd94-59a355c49aba', 1215, 3145, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I vastly prefer to use windows.

Despite that preference, i would recommend that you build your lab in Linux. Installation of these types of tools is often much easier under Linux. The anaconda distribution (http://docs.continuum.io/anaconda/pkg-docs.html) contains nltk, and is available under Linux (http://docs.continuum.io/anaconda/install.html).

As one example of an issue we''ve run into with windows, installing theano with cuda drivers is very difficult under windows, and quite simple under Linux.', 2969, '2014-10-05 14:53:33.433', 'c72ce0de-a743-4663-a942-759d77fe577a', 1215, 'clarification', 3146, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would like to do some data mining and NLP experiments to do some research

I have decided to use NLTK or related tools and software

Which environment or operating system do you suggest for my purpose? I mean doing research on NLP

Windows or Linux?

I am a user of Windows but I thought if Linux has better shell and related software for NLP tasks then I switch to Linux

What is your experience and your preferred OS?
As NLTK is in Python I thought Python is a good language for my purpose, do you suggest Python too?
', 3436, '2014-10-05 19:10:30.700', 'ebc60e26-61e1-4a90-834e-5cc740cbd70b', 1204, 'added 101 characters in body; edited title', 3147, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('NLP lab, linux or windows and which programming languages?', 3436, '2014-10-05 19:10:30.700', 'ebc60e26-61e1-4a90-834e-5cc740cbd70b', 1204, 'added 101 characters in body; edited title', 3148, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would like to do some data mining and NLP experiments to do some research

I have decided to use NLTK or related tools and software

Which environment or operating system do you suggest for my purpose? I mean doing research on NLP

Windows or Linux?

I am a user of Windows but I thought if Linux has better shell and related software for NLP tasks then I switch to Linux

What is your experience and your preferred OS?

As NLTK is in Python I thought Python is a good language for my purpose, do you suggest Python too?
', 3436, '2014-10-05 19:19:37.300', '33f032ed-f511-4204-9f14-faf75bffd465', 1204, 'added 2 characters in body', 3149, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am a 35 year old IT professional who is purely technical. I am good at programming, learning new technologies, understanding them and implementing. I did not like mathematics at school, so I didn''t score well in mathematics. I am very much interested in pursuing a career in Big Data analytics. I am more interested in Analytics rather than Big Data technologies (Hadoop etc.), though I do not dislike it.  However, when I look around in the internet, I see that, people who are good in analytics (Data Scientists) are mainly Mathematics graduates  who have done their PHds and sound like intelligent creatures, who are far far ahead of  me. I get scared sometimes to think whether my decision is correct, because learning advance statistics on your own is very tough and requires a of hard work and time investment.

I would like to know whether my decision is correct, or should I leave this piece of work to only intellectuals who have spend their life in studying in prestigious colleges and earned their degrees and PHDs.', 3550, '2014-10-06 12:00:55.303', '1b86f08e-f278-4607-8e22-195d6933daa3', 1216, 3150, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Career switch to Big Data Analytics', 3550, '2014-10-06 12:00:55.303', '1b86f08e-f278-4607-8e22-195d6933daa3', 1216, 3151, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 3550, '2014-10-06 12:00:55.303', '1b86f08e-f278-4607-8e22-195d6933daa3', 1216, 3152, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Due to high demand, it is possible to start a career in data science without a formal degree. My experience is that having a degree is often a ''requirement'' in job descriptions, but if the employer is desperate enough, then that won''t matter. In general, it''s harder to get into large corporations with formalized job application processes than smaller companies without them. "Knowing people" can get you a long way, in either case.

Regardless of your education, no matter how high demand is, you must have the skills to do the job.

You are correct in noting that advanced statistics and other mathematics are very hard to learn independently. It is a matter of how badly you want to make the career change. While some people do have ''natural talent'' in mathematics, everybody does have to do the work to learn. Some may learn more quickly, but everybody has to take the time to learn.

What it comes down to is your ability to show potential employers that you have a genuine interest in the field, and that you will be able to learn quickly on the job. The more knowledge you have, the more projects you can share in a portfolio, and the more work experience under your belt, the higher level jobs that will be available to you. You may have to start  in an entry level position first.

I could suggest ways to study mathematics independently, but that isn''t part of your question. For now, just know that it''s hard, but possible if you are determined to make a career change. Strike while the iron is hot (while demand is high).
', 3466, '2014-10-06 19:38:00.207', 'cf0fd6e9-1c88-4d4a-8fc2-e39afea93e8f', 1220, 3162, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Python is easy to use and manage in Linux. The Python package manager PIP and the Python environment manager Virtualenv are both fully supported in Linux. See: http://docs.python-guide.org/en/latest/starting/install/linux/

The PyCharm IDE from JetBrains is also available for free in Linux (community edition), and it runs well. This is a great tool for a Python developer.

In addition, if you need to integrate other tools, they will most likely be best supported in Linux. Need a MapReduce in your app? Hadoop will run in Linux. Need to store data in MySQL? Linux. Most data science tools are open source, and most open source technologies are supported best in Linux.

There is also no cost for running Linux.

If you need to keep Windows for your primary work computer (like your laptop), then you can install Linux on a server and use Putty to SSH into the Linux machine. You would then perform your work on the Linux machine, though you would still be running Windows locally.', 3466, '2014-10-06 19:54:57.197', 'e89626da-0dd8-4d53-b3ae-957c9d54947e', 1221, 3163, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You should look more into the infrastructure side of things if you don''t like maths. The lower you go in the software stack, the further away you get from maths (of the data science sort). In other words, you could build the foundation that others will use to create the tools that will serve analysts. Think of companies like Cloudera, MapR, Databricks, etc. Skills that will come in handy are distributed systems and database design. You are not going to be become a data scientist without maths; that''s a ridiculous notion!', 381, '2014-10-06 23:38:17.523', '5e06b143-6be9-4a4d-9e3e-5bd420df227b', 1222, 3164, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('If you merely want to scale up a simple collaborative filter (low rank matrix factorization), I''d suggest looking at [graphlab](http://docs.graphlab.org/collaborative_filtering.html). Another graph-based (or should I say [Giraph](http://giraph.apache.org/)?) solution is [Okapi](http://grafos.ml/okapi.html#collaborative). Spark''s [MLLib](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html) is another option ([details](https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html)), and it also supports implicit feedback out of the box. Mahout''s behind the curve today; I wouldn''t bother with it until it is [migrated to Spark](https://www.mapr.com/blog/mahout-spark-what%E2%80%99s-new-recommenders).

If you want to do something that the libraries don''t do, say with regularization, you''ll have to roll your own solution in your general purpose programming language of choice. It''s not hard to get a prototype running, but you might run into scale problems in production; that''s why I recommended solutions that scale easily.

There are also black box recommender systems from commercial vendors but I have no experience with those.', 381, '2014-10-07 07:45:19.727', '3b2092b4-749f-453b-89ba-8f4eac690d15', 1212, 'added 139 characters in body', 3165, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m curious if anyone else has run into this. I have a data set with about 350k samples, each with 4k sparse features. The sparse fill rate is about 0.5%. The data is stored in a ```scipy.sparse.csr.csr_matrix``` object, with ```dtype=''numpy.float64''```.

I''m using this as an input to sklearn''s Logistic Regression classifier. The [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) indicates that sparse CSR matrices are acceptable inputs to this classifier. However, when I train the classifier, I get extremely bad memory performance; the memory usage of my process explodes from ~150 MB to fill all the available memory and then everything grinds to a halt as memory swapping to disk takes over.

Does anyone know why this classifier might expand the sparse matrix to a dense matrix? I''m using the default parameters for the classifier at the moment, within an updated anacoda distribution. Thanks!

    scipy.__version__ = ''0.14.0''
    sklearn.__version__ = ''0.15.2''', 3568, '2014-10-07 17:27:22.063', '37ebf0fc-f4a4-4180-b6dd-66a2c64ff922', 1223, 3171, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Scikit Learn Logistic Regression Memory Leak', 3568, '2014-10-07 17:27:22.063', '37ebf0fc-f4a4-4180-b6dd-66a2c64ff922', 1223, 3172, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<efficiency><performance><sklearn>', 3568, '2014-10-07 17:27:22.063', '37ebf0fc-f4a4-4180-b6dd-66a2c64ff922', 1223, 3173, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Ok, this ended up being an RTFM situation, although in this case it was RTF error message.

While running this, I kept getting the following error:

    DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().

I assumed that, since this had to do with the target vector, and since it was a warning only, that it would just silently change my target vector to 1-D.

However, when I explicitly converted my target vector to 1-D, my memory problems went away. Apparently having the target vector in an incorrect form caused it to convert my input vectors into dense vectors from sparse vectors.

Lesson learned: __follow the recommendations__ when sklearn ''suggests'' you do something.', 3568, '2014-10-07 18:09:19.433', '8c32a744-0aec-44c8-9965-9abdc25bdb63', 1224, 3174, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have product purchase count data which looks likes this:

    user item1 item2
       a     2     4
       b     1     3
       c     5     6
       ...   ...   ...

These data are imported into `python` using `numpy.genfromtxt`. Now I want to process it to get the correlation between `item1` purchase amount and `item2` purchase amount -- basically for each value `x` of `item1` I want to find all the users who bought `item1` in `x` quantity then average the `item2` over the same users. What is the best way to do this? I can do this by using `for` loops but I thought there might be something more efficient than that. Thanks!', 3580, '2014-10-08 10:42:41.833', 'c38add41-a9ff-4e10-a668-50f3236b1f36', 1225, 3175, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('data processing, correlation calculation', 3580, '2014-10-08 10:42:41.833', 'c38add41-a9ff-4e10-a668-50f3236b1f36', 1225, 3176, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><correlation>', 3580, '2014-10-08 10:42:41.833', 'c38add41-a9ff-4e10-a668-50f3236b1f36', 1225, 3177, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you want to do NLP stay away from Windows! ( No Offense) Python NLTK stands out in terms of resource and community support for NLP.

Moreover in Linux you can always leverage the benefit of  awk,sed, grep which are pretty useful tools when it comes to pre-processing. And no worries of funny characters getting introduced when you use a file.', 3232, '2014-10-08 13:37:27.357', 'baaff80b-3291-4179-941e-143cc708bf47', 1226, 3178, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to figure out a good (and fast) solution to the following problem:

I have two models I''m working with, let''s call them players and teams. A player can be on multiple teams and a team can have multiple players). I''m working on creating a UI element on a form that allows a user to select multiple teams (checkboxes). As the user is selecting (or deselecting) teams, I''d like to display the teams grouped by the players.

So for examples:

1. If the selected teams have no players that intersect, each team would have its own section.

1. If the user selects two teams and they have the same players, there would be one section containing the names of the two teams and all the players.

1. If TEAM_A has players [1, 2, 4, 5] and TEAM_B has players [1, 3, 5, 6]. There would be the following sections: SECTION_X = [TEAM_A, TEAM_B, 1, 5], SECTION_Y = [TEAM_A, 2, 3], SECTION _Z = [TEAM_B, 3, 5]

I hope that''s clear. Essentially, I want to find the teams that players have in common and group by that. I was thinking maybe there is a way to do this by navigating a bipartite graph? Not exactly sure how though and I might be overthinking it. I was hoping to do this by creating some type of data structure on the server and using it on the client. I would love to hear your suggestions and I appreciate any help you can give!', 3583, '2014-10-08 16:17:11.663', 'af80619c-345d-4528-bb3b-51138c25173d', 1227, 3180, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Union grouping in bipartite graphs?', 3583, '2014-10-08 16:17:11.663', 'af80619c-345d-4528-bb3b-51138c25173d', 1227, 3181, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<graphs>', 3583, '2014-10-08 16:17:11.663', 'af80619c-345d-4528-bb3b-51138c25173d', 1227, 3182, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Use one of Pandas'' built in functions:
http://pandas.pydata.org/pandas-docs/stable/computation.html#correlation', 3586, '2014-10-08 22:12:31.823', '861d7a50-95ba-4c56-8a50-6696e4a83dd0', 1228, 3183, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Lets say I have a database of users who rate different products on a scale of 1-5. Our recommendation engine recommends products to users based on the preferences of other users who are highly similar. My first approach to finding similar users was to use Cosine Similarity, and just treat user ratings as vector components. The main problem with this approach is that it just measures vector angles and doesn''t take rating scale or magnitude into consideration.

**My question is this:**

**Are there any drawbacks to** **just using the percentage difference between the vector components of two vectors as a measure of similarity**? What disadvantages, if any, would I encounter if I used that method, instead of Cosine Similarity or Euclidean Distance?

**For Example, why not just do this:**

    n = 5 stars
    a = (1,4,4)
    b = (2,3,4)

    similarity(a,b) = 1 - ( (|1-2|/5) + (|4-3|/5) + (|4-4|/5) ) / 3 = .86667

**Instead of Cosine Similarity :**

    a = (1,4,4)
    b = (2,3,4)

    CosSimilarity(a,b) =
    (1*2)+(4*3)+(4*4) / sqrt( (1^2)+(4^2)+(4^2) ) * sqrt( (2^2)+(3^2)+(4^2) ) = .9697', 3587, '2014-10-09 01:41:11.797', '1d6a2e56-77d7-41b6-9739-70e614e70225', 1229, 3184, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Cosine Similarity for Ratings Recommendations? Why use it?', 3587, '2014-10-09 01:41:11.797', '1d6a2e56-77d7-41b6-9739-70e614e70225', 1229, 3185, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><recommendation>', 3587, '2014-10-09 01:41:11.797', '1d6a2e56-77d7-41b6-9739-70e614e70225', 1229, 3186, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For ratings, I think you would need to use [Spearman''s rank correlation][1] for your similarity metric.

Cosine similarity is often used when comparing documents, and perhaps would not be a good fit for rank variables. Euclidean distance is fine for lower dimensions, but comparison of rank variables normally call for Spearman.

Here''s a [question on CrossValidated regarding Spearman (vs Pearson)][2], which might shed more light for you.


  [1]: http://mathworld.wolfram.com/SpearmanRankCorrelationCoefficient.html
  [2]: http://stats.stackexchange.com/questions/8071/how-to-choose-between-pearson-and-spearman-correlation', 24, '2014-10-09 01:57:41.010', 'd1085fe1-e386-4e34-9249-ca3c8d061b5d', 1230, 3187, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Rating bias and scale can easily be accounted for by standardization. The point of using  Euclidean similarity metrics in vector space co-embeddings is that it reduces the recommendation problem to one of finding the nearest neighbors, which can be done efficiently both exactly and approximately. What you don''t want to do in real-life settings is to have to compare every item/user pair and sort them according to some expensive metric. That just doesn''t scale.

One trick is to use an approximation to cull the herd to a managable size of tentative recommendations, then to run your expensive ranking on top of that.', 381, '2014-10-09 02:24:07.640', '2cd93f9f-9b0b-49f9-842b-cdd2566d2ee7', 1231, 3188, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('nDCG is a ranking metric and RMSE is not. In the context of recommender systems, you would use a ranking metric when your ratings are implicit (e.g., item skipped vs. item consumed) rather than explicit (the user provides an actual number, a la Netflix).', 381, '2014-10-09 02:35:24.533', '313b1f2f-5791-4924-9675-e0811063e8d3', 1232, 3189, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Not online learning, but take a look at this posts, this may help you to get a start

http://francescopochetti.com/stock-market-prediction-part-introduction/', 3596, '2014-10-09 10:22:47.857', '4def9a3a-b0c8-49b7-a92b-f54c8eac559a', 1233, 3190, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Not online learning, but take a look at this post, this may help you to get a start

http://francescopochetti.com/stock-market-prediction-part-introduction/', 3596, '2014-10-09 10:45:23.670', '7b993024-8e5a-42b8-9815-96ff0c4518db', 1233, 'deleted 1 character in body', 3191, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am developing a recommendation engine for stack overflow (personal project). Check it on http://recommender.im .

It is still a working in progress, but I have a quite functional website working. I am putting there most of the code I used through python notebooks.

Basically I used:<br>
- Frontend: angularJS<br>
- Website backend: flask + scikit-learn<br>
- machine learning and data preparation: python, pandas, scikit-learn

I really like python for data science as the community and libraries are really good.', 3596, '2014-10-09 10:50:39.197', '8442a94e-226b-46f5-878b-09138296b5d8', 1234, 3192, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Pandas is the best thing since sliced bread (for data science, at least).

an example:

    import pd
    In [22]: df = pd.read_csv(''yourexample.csv'')

    In [23]: df
    Out[23]:
       user   item1   item2
    0     a        2      4
    1     b        1      3
    2     c        5      6

    In [24]: df.columns
    Out[24]: Index([u''user '', u''item1 '', u''item2''], dtype=''object'')

    In [25]: df.corr()
    Out[25]:
              item1      item2
    item1   1.000000  0.995871
    item2   0.995871  1.000000

    In [26]: df.cov()
    Out[26]:
              item1      item2
    item1   4.333333  3.166667
    item2   3.166667  2.333333

Bingo!', 3596, '2014-10-09 10:58:02.557', '3e0c317a-d0a3-44a1-8c18-25941ab32aa0', 1235, 3193, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('how to get the Polysemes of a word in wordnet or any other api. I am looking for any api. with java any idea is appreciated?', 3598, '2014-10-09 12:26:01.643', '76aaaa27-d735-48bb-82b1-98113034df00', 1236, 3194, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('how to get the Polysemes of a word in wordnet or any other api?', 3598, '2014-10-09 12:26:01.643', '76aaaa27-d735-48bb-82b1-98113034df00', 1236, 3195, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp>', 3598, '2014-10-09 12:26:01.643', '76aaaa27-d735-48bb-82b1-98113034df00', 1236, 3196, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Rating bias and scale can easily be accounted for by standardization. The point of using  Euclidean similarity metrics in vector space co-embeddings is that it reduces the recommendation problem to one of finding the nearest neighbors, which can be done efficiently both exactly and approximately. What you don''t want to do in real-life settings is to have to compare every item/user pair and sort them according to some expensive metric. That just doesn''t scale.

One trick is to use an approximation to cull the herd to a managable size of tentative recommendations, then to run your expensive ranking on top of that.

edit: Microsoft Research is presenting a paper that covers this very topic at RecSys right now: [Speeding Up the Xbox Recommender System Using a
Euclidean Transformation for Inner-Product Spaces](http://www.ulrichpaquet.com/Papers/SpeedUp.pdf)', 381, '2014-10-10 00:03:00.447', '44ceb20f-7bec-4371-8949-672fb2cea567', 1231, 'added 248 characters in body', 3202, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I know there is the normal subtract the mean and divide by the standard deviation for standardizing your data, but I am interested to know if there are more appropriate methods for this kind of discrete data.

I have 5 items that have been ranked by customers. First 2 items were ranked on a 1-10 scale. Others are 1-100 and 1-5.

To transform everything to a 1 to 10 scale, is there another method better suited for this case? If the data has a central tendency then the standard would work fine, but what about when you have more of a halo effect or some more exponential distribution?', 3430, '2014-10-10 00:59:46.703', '19ab429a-361b-42ed-b9c2-8ac87a39d6e6', 1240, 3203, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Given different scales of items being ranked. What are the scaling options for standardizing to a common scale?', 3430, '2014-10-10 00:59:46.703', '19ab429a-361b-42ed-b9c2-8ac87a39d6e6', 1240, 3204, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics>', 3430, '2014-10-10 00:59:46.703', '19ab429a-361b-42ed-b9c2-8ac87a39d6e6', 1240, 3205, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I do movement building work for Effective Altruism (http://en.m.wikipedia.org/wiki/Effective_altruism), and would like to level up our growth strategy. It occurred to me that a social network visualization tool which allowed us to strategically find and recruit new influencers/donors would be mega useful. I''d love to find something (preferably free), similar to InMaps, which would allow us to:
- Combine all of our social media connections into a single map
- Easily see who the superconnectors are
- Weight each person by their degree of social influence (perhaps some function of things like Klout score * amount of social media connections * number of Google mentions, etc)

Does such a thing exist? If not, is anyone interested in pro bono work for an amazing cause? =)

Disclaimer: I am a data science noob, so preferably the solution would be one with a nice GUI and minimal involvement of R or Python.

Cheers,
Tyler', 3605, '2014-10-10 01:12:19.163', '60285777-7da5-4f75-80c0-d0524f861bdd', 1241, 3206, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Collaborative Social Network Visualization', 3605, '2014-10-10 01:12:19.163', '60285777-7da5-4f75-80c0-d0524f861bdd', 1241, 3207, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<social-network-analysis>', 3605, '2014-10-10 01:12:19.163', '60285777-7da5-4f75-80c0-d0524f861bdd', 1241, 3208, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think [Gephi][1], an open-source visualization tool, would help you a lot. Actually, as I know, the InMaps and its community detection algorithm are same as the Gephi''s.




  [1]: https://gephi.github.io/', 1048, '2014-10-10 03:33:45.353', '2b6ed6d2-bbe3-4d73-9ddd-a518da6b695f', 1242, 3209, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Suppose, for example, that the first search result on a page of Google search results is swapped with the second result. How much would this change the click-through probabilities of the two results? How much would its click-through probability drop if the fifth search result was swapped with the sixth?

Can we say something, with some level of assurance, about how expected click-through probabilities change if we do these types of pairwise swaps within pages of search results?

What we seek is a measure of the contribution to click-through rates made specifically by position bias.

Likely, how position ranking would affect the sales in Amazon or other online shopping website? If we cast the sales into two parts, the product quality and its ranking effect.

```
sales = alpha*quality + beta*position + epsilon
```

How can we quantify the beta?


', 1048, '2014-10-10 03:45:09.343', '27661241-571e-42f6-8fa3-1f0f272e7a28', 1243, 3210, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can we quantify how position within search results is related to click-through probability?', 1048, '2014-10-10 03:45:09.343', '27661241-571e-42f6-8fa3-1f0f272e7a28', 1243, 3211, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation><search><information-retrieval><regression>', 1048, '2014-10-10 03:45:09.343', '27661241-571e-42f6-8fa3-1f0f272e7a28', 1243, 3212, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When ML algorithms, e.g. Vowpal Wabbit or some of the factorization machines winning click through rate competitions ([Kaggle][1]), mention that features are ''hashed'', what does that actually mean for the model? Lets say there is a variable that represents the ID of an internet add, which takes on values such as ''236BG231''. Then I understand that this feature is hashed to a random integer. But, my question is:

 - Is the integer now used in the model, as an integer (numeric) OR
 - is the hashed value actually still treated like a categorical variable and one-hot-encoded? Thus the hashing trick is just to save space somehow with large data?

  [1]: https://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10555/3-idiots-solution/55862#post55862', 1138, '2014-10-10 03:48:54.660', '859bfa76-07e4-4eec-a9df-1b0b7e311a0f', 1244, 3213, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hashing Trick - what actually happens', 1138, '2014-10-10 03:48:54.660', '859bfa76-07e4-4eec-a9df-1b0b7e311a0f', 1244, 3214, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><predictive-modeling><kaggle>', 1138, '2014-10-10 03:48:54.660', '859bfa76-07e4-4eec-a9df-1b0b7e311a0f', 1244, 3215, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('let''s assume that I want to train a stochastic gradient descent regression algorithm using a dataset that has N samples. Since the size of the dataset is fixed, I will reuse the data T times. At each iteration or "epoch", I use each training sample exactly once after randomly reordering the whole training set.

My implementation is based on Python and Numpy. Therefore, using vector operations can remarkably decrease computation time. Coming up with a vectorized implementation of batch gradient descent is quite straightforward. However, in the case of stochastic gradient descent I can not figure out how to avoid the outer loop that iterates through all the samples at each epoch.

Does anybody know any vectorized implementation of stochastic gradient descent?

', 2576, '2014-10-10 13:34:11.543', 'fba425a9-bb7d-4bc5-b34e-8f60c2c72fc6', 1246, 3222, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Stochastic gradient descent based on vector operations?', 2576, '2014-10-10 13:34:11.543', 'fba425a9-bb7d-4bc5-b34e-8f60c2c72fc6', 1246, 3223, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><gradient-descent><regression>', 2576, '2014-10-10 13:34:11.543', 'fba425a9-bb7d-4bc5-b34e-8f60c2c72fc6', 1246, 3224, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am new to this forum. Chiming in late on this question. I have been maintaining (I am a co-founder of) a catalog of publicly available data portals. There is over 1000 now listed and cover portals at international, federal, state, municipal and academic levels across the globe.

http://www.opengeocode.org/opendata/', 3609, '2014-10-10 14:35:11.323', '95c3c07c-5735-4295-b48e-8879c076cd86', 1247, 3225, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am new to this forum. Data cleansing of address data is an area I work in. I agree with the other posters that you should not modify the original data, but add fields for corrected values. I developed a technique in our systems (opengeocode.org) we call ''reduced to common form''. In this method, addresses and geographic names are analyzed for reduction into an unambiguous short form, which is then used for record matching (vs. the original values). For example, the method I use for matching US postal addresses is based on the US Post Office''s published method for matching addresses.

For geographic names, the method will reduce to short gazetteer form in Romanized script.

The link below is an article I wrote a couple of years ago that explains how the street address reduction works:

http://www.nwstartups.com/api/doc/middleware.php#streetR

', 3609, '2014-10-10 14:53:22.500', '100ae9f3-7a1e-4a01-94f7-ae592022772e', 1248, 3226, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are several third-party Java APIs for WordNet listed here: http://wordnet.princeton.edu/wordnet/related-projects/#Java

In the past, I''ve used JWNL the most: http://sourceforge.net/projects/jwordnet/

The documentation for JWNL isn''t great, but it should provide the functionality you need.', 819, '2014-10-10 16:54:45.047', '597237cc-a858-4b77-8c87-094a9af5a03e', 1249, 3227, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I recently ran into a similar problem: How to manage extracting a variety of features from a large dataset, without knowing up front what all of them would be. (Even calculating mean values repeatedly would be computationally expensive.) Further, how would I manage predictions based on different feature sets? Meaning, if I added a new feature, how would I know which models to train on new features? It could quickly snowball into a huge mess.

My current solution is to track it all in a local NoSQL database (MongoDB). For example, I might have a collection `features`, each entry of which has a name, a description of how the feature was was calculated, the python file that ran the extraction, etc.

Likewise, a collection `models` includes models run on the data. Each entry might have a name, a list of features that were used to train the model, its eventual parameters, predicted values on a held-out test set, metrics for how the model performed, etc.

From my vantage point, this has a number of benefits:

 - By saving predictions, I can use them later in ensemble predictions.
 - Because I keep track of which features were used, I know which ones need retraining as I extract more features.
 - By saving model descriptions, I ensure that I always know what I''ve tried. I never have to wonder, "Have I tried LASSO with regularization parameters set by grid-search CV?" I can always look it up, and see how successful it was.

From your question, it sounds like you could adapt this approach to your problem''s workflow. Install Mongo or another database of choice, and then save each experimental run, its inputs, its results, and anything else you might wish to track over the course of the project. This should be much easier to query than a spreadsheet, at the least.', 1154, '2014-10-10 18:10:14.507', '2773921e-175f-4d35-8884-bea941103eaa', 1250, 3233, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I do movement building work for Effective Altruism (http://en.m.wikipedia.org/wiki/Effective_altruism), and would like to level up our growth strategy. It occurred to me that a social network visualization tool which allowed us to strategically find and recruit new influencers/donors would be mega useful. I''d love to find something (preferably free), similar to InMaps, which would allow us to:

 * Combine all of our social media connections into a single map
 * Easily see who the superconnectors are
 * Weight each person by their degree of social influence (perhaps some function of things like Klout score * amount of social media connections * number of Google mentions, etc)

Does such a thing exist? If not, is anyone interested in pro bono work for an amazing cause? =)

Disclaimer: I am a data science noob, so preferably the solution would be one with a nice GUI and minimal involvement of R or Python.', 21, '2014-10-11 09:45:11.417', 'a3d57271-2ab5-4590-9d72-22f4ecd25916', 1241, 'deleted 12 characters in body', 3238, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I hope you can help me ?! I have some questions on this topic. I m new in the field of deep learning and I did some tutorials but I cant relate or distinguish concepts from eachother. Hence I formulated some questions and I hope that some specialist of this community can answer them and help me to understand whats deep learning really is. Neural networks? special methods? word vector representation? Information extraction? I need a clear picture of the field. THanks a lot in advance, here are my questions:

a) Why is there a lot of interest in the NLP and ML community for deep learning architectures? What are deep learning architectures? *I think Multi-Layer Neural (Tensor) Networks, so called deep networks, like Socher use it?*

b) Why do NLP and ML community need approaches to learn complex non-linear relationships? *What''s an good example of compley nonlinear relationships? I think Single-Layer Neural networks can only distinguish linear input data according to their activation function. And because they have input data that can''t easily separateby a straight line. they need deep networks with ML for processing complex data. Maybe they need them because they using multi dimensional word vector spaces?*

c) What are deep learning methods: for example backpropagation or is it a neural tensor network or is it the word vector representation? *I cant distinguish what are deep learning architectures and methods? Or are the the same?*

e) And how can I relate deep learning to open information extraction? And what are good examples for the application of deep learning methods for open information extraction? *My approach is, that open information extraction is extraction of knowlegede on any domain with any relations from unstructured data (like webpages) with the target to aquiring knowlege. If you do this for a specific domain you have to optimize your NLP tools manually to this domain and relations, but in other domain that doesnt work well. Therefore "deep learning"(clearer: deep neural networks, recursive or tensor NN) architectures are interessting, because they unsupervised learn to extract knowlegde and need no manually optimizating relating to a topic.*
*A method are word vectors for representing words and not specific programmed model. The "deep learning" in the NLP field is, that the NN learn the before manual models automatically and mapping the words in a vector space. An exmaple is Socher''s Recursive Deep Models for Semantic Compositionality, where deep learning is used to extract the positive or negative statement of a sentance.*

*Is this approach true or i m on a false way?*

For any comments i m very thankful. Thanks in advance!
', 3615, '2014-10-11 10:24:01.393', '196c0d28-5fc1-4e11-8a4d-ede67ca04233', 1253, 3240, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why is there a lot of interest in the NLP and Machine Learning community for deep learning?', 3615, '2014-10-11 10:24:01.393', '196c0d28-5fc1-4e11-8a4d-ede67ca04233', 1253, 3241, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><nlp><neuralnetwork>', 3615, '2014-10-11 10:24:01.393', '196c0d28-5fc1-4e11-8a4d-ede67ca04233', 1253, 3242, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><nlp><neuralnetwork><information-retrieval>', 3615, '2014-10-11 10:39:04.313', '151e62e4-be54-4b85-a789-3d4caa1ffc3a', 1253, 'edited tags', 3243, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The a second bullet is the value in feature hashing.  Hashing and one hot encoding to sparse data saves space.  Depending on the hash algo you can have varying degrees of collisions which acts as a kind of dimensionality reduction.

Also, in the specific case of Kaggle feature hashing and one hot encoding help with feature expansion/engineering by taking all possible tuples (usually just second order but sometimes third) of features that are then hashed with collisions that explicitly create interactions that are often predictive whereas the individual features are not.

In most cases this technique combined with feature selection and elastic net regularization in LR acts very similar to a one hidden layer NN so it performs quite well in competitions.', 92, '2014-10-11 19:48:20.583', '47eb0a5f-5acd-49c2-ae76-d5df8845419a', 1255, 3247, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I need some help with a single layered perceptron with multiple classes.

What I need to do is classify a dataset with three different classes, by now I just learnt how to do it with two classes, so I have no really a good clue how to do it with three.

The dataset have three different classes: Iris-setosa, Iris-versicolor and Iris-versicolor.

The url with the dataset and the information is in : http://ftp.ics.uci.edu/pub/machine-learning-databases/iris/iris.data.

I really appreciate any help anyone can give to me.

Thanks a lot!', 4618, '2014-10-11 23:26:53.197', '4fbf9033-a776-436b-8047-b053ee8ec881', 2255, 4248, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Single Layer Perceptron with three classes', 4618, '2014-10-11 23:26:53.197', '4fbf9033-a776-436b-8047-b053ee8ec881', 2255, 4249, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset>', 4618, '2014-10-11 23:26:53.197', '4fbf9033-a776-436b-8047-b053ee8ec881', 2255, 4250, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I don''t think there is a way to build your graph from raw data without using at least basic programming skills. I''m not aware of a drag-and-drop interface for importing and displaying data. Graphs are just a bit too complex. Imagine trying to find the profit of selling a product if all you had was CSVs of receipts dropped into Excel. You''d need labels of the columns, some basic calculations, and so on before you had anything intelligible. Graphs are similar in this regard.

Thankfully, there are open source solutions, with some elbow grease and a few days of work, you can probably get a nice visualization.

Cypher queries are relatively simple to write. Using Neo4j and Cypher, you can create a basic visualization of your graph, which is displayed using D3.js

GraphAlchemist recently open-sourced their project Alchemy.js which specializes in graph visualization. https://github.com/GraphAlchemist/Alchemy
', 3466, '2014-10-12 02:53:51.650', '2348ef5b-1cfa-48ee-ac3f-a3e56515aa8b', 2256, 4251, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have some very complicated data about some movie sales online, first for each data entry, I have a key which is a combination of five keys, which are territory, day, etc, and then, for each key I have the sales for a period of time, and other information, like the movie''s box office and genre.

For each day, there is a delay for the data loading to the database, around ten hours, I try to fill the gap, do some data extrapolations.

For each movie we sell, there is some decay of selling since the new release of the movie, i.e. usually for each movie, it follows some sales decay pattern.

For a recent day, I pulled some data, and I found that some decay pattern:

![decay curve 1][1]


![decay curve 2][2]


![decay curve 3][3]


And for that day, the sales for each key can range from around $150000 to $0. The pic is as follow:

![one day sales curve][4]


  [1]: http://i.stack.imgur.com/H0lSJ.png
  [2]: http://i.stack.imgur.com/L5DAf.png
  [3]: http://i.stack.imgur.com/RJPjK.png
  [4]: http://i.stack.imgur.com/o7tsq.png

In the picture, the 15000 means there are around 15000 keys for each day.

found this article,

http://homepage.stat.uiowa.edu/~kcowles/s166_2009/Project_Lee&Pyo.pdf

And for each day, we what to come up a list of keys, and the extrapolated sales for each key.

I am not sure whether can be applied, and how to be applied here.

Thanks a lot in advance. ', 4619, '2014-10-12 05:27:17.687', 'a33c0ec5-352a-47af-a435-01000c0775e4', 2257, 4252, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to do this complicated data extrapolation, prediction modeling?', 4619, '2014-10-12 05:27:17.687', 'a33c0ec5-352a-47af-a435-01000c0775e4', 2257, 4253, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 4619, '2014-10-12 05:27:17.687', 'a33c0ec5-352a-47af-a435-01000c0775e4', 2257, 4254, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-mining><time-series>', 21, '2014-10-12 10:19:15.873', 'fad028df-1f43-4932-8976-0ee0dfa44228', 2257, 'edited tags', 4255, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First of all I know the question may be not suitable for the website but I''d really appreciate it if you just gave me some pointers.

I''m a 16 years old programmer, I''ve had experience with many different programming languages, a while ago I started a course at Coursera, titled introduction to machine learning and since that moment i got very motivated to learn about AI, I started reading about neural networks and I made a working perceptron using Java and it was really fun but when i started to do something a little more challenging (building a digit recognition software), I found out that I have to learn a lot of math, I love math but the schools here don''t teach us much, now I happen to know someone who is a math teacher do you think learning math (specifically calculus) is necessary for me to learn AI or should I wait until I learn those stuff at school?

Also what other things would be helpful in the path of me learning AI and machine learning? do other techniques (like SVM) also require strong math?

Sorry if my question is long, I''d really appreciate if you could share with me any experience you have had with learning AI.', 4620, '2014-10-12 11:23:26.493', 'dda09ea3-a6cc-4771-9eb3-50eca9bc34fe', 2258, 4256, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where to start on neural networks', 4620, '2014-10-12 11:23:26.493', 'dda09ea3-a6cc-4771-9eb3-50eca9bc34fe', 2258, 4257, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork><svm>', 4620, '2014-10-12 11:23:26.493', 'dda09ea3-a6cc-4771-9eb3-50eca9bc34fe', 2258, 4258, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Well , When it comes to AI I am an absolute beginner but here is my answer to your question based on my understandings :

a perceptron has only one activation function, therefore it can return only the values of true and false (in most cases true=0 and false=1), so because of that, I don''t think that you will be able to accomplish your goal using only one perceptron but you can absolutely do it using multiple perceptrons which essentially is a neural networks, of course training the network would be a lot harder than calculating the changes of weights as you do in perceptrons, You are gonna have to take advantage of a training algorithm such as backpropagation and a sigmoid activation function.
I hope my answer was helpful.', 4620, '2014-10-12 11:49:04.480', '010c9b25-a708-4099-a96a-b1580e66d53d', 2259, 4259, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The standard way to do this is called ''one versus all''... you train three perceptrons. First 1 target = is class a?, 2nd perceptron target = is class b? 3rd = is class c. You just train each perceptron separately, and then take max of the three perceptrons to decide class', 1256, '2014-10-12 16:35:21.260', '3eee226c-9423-41d3-8386-1607ca4e10fd', 2260, 4261, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('No, you should go ahead and learn the maths on your own. You will "only" need to learn calculus and linear algebra. The theory of neural networks is pretty primitive at this point -- it more of an art than a science -- so I think you can understand it if you try. Ipso facto, there are a lot of tricks that you need practical experience to learn.

Once you can understand the Coursera classes on ML and neural networks (Hinton''s), I suggest getting some practice. You might like [this](http://karpathy.github.io/neuralnets/) introduction.', 381, '2014-10-12 19:36:33.027', 'f292def4-b8b7-41b9-9bcb-04421efb21fc', 2261, 4262, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('you might want to look at http://deeplearning.net/software/jobman/intro.html

it was designed for deep learning (I guess), but it is application agnostic. It is effectively an API version of SeanEasters approach', 1256, '2014-10-12 19:52:55.430', '5a1bb98c-b366-4ee8-9949-45ef4d93ce7d', 2262, 4263, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have some very complicated data about some movie sales online, first for each data entry, I have a key which is a combination of five keys, which are territory, day, etc, and then, for each key I have the sales for a period of time, and other information, like the movie''s box office and genre.

For each day, there is a delay for the data loading to the database, around ten hours, I try to fill the gap, do some data extrapolations.

For each movie we sell, there is some decay of selling since the new release of the movie, i.e. usually for each movie, it follows some sales decay pattern.

For a recent day, I pulled some data, and I found that some decay pattern:

![decay curve 1][1]


![decay curve 2][2]


![decay curve 3][3]


And for that day, the sales for each key can range from around $150000 to $0. The pic is as follow:

![one day sales curve][4]


  [1]: http://i.stack.imgur.com/H0lSJ.png
  [2]: http://i.stack.imgur.com/L5DAf.png
  [3]: http://i.stack.imgur.com/RJPjK.png
  [4]: http://i.stack.imgur.com/o7tsq.png

In the picture, the 15000 means there are around 15000 keys for each day.

found this article,

http://homepage.stat.uiowa.edu/~kcowles/s166_2009/Project_Lee&Pyo.pdf

 I am trying to predict for each key, the sales amount, like for a movie, territory, day etc combination, the sales amount, how much dollars, means for that movie, that territory, that day, how much money we get from selling online. I tried ARIMA time series model, but there is some concerns for that model, seen from the pics, there is some seasonal thing, and decay thing for the movie, so the sales prediction can not be always flat, there may be a pump after a going down, it may happens on a weekend, since there is seasonal thing, and the decay trend, etc, how to capture these things. Thank you for your reply!

I am not sure whether can be applied, and how to be applied here.

Thanks a lot in advance. ', 4619, '2014-10-12 21:55:05.577', 'a06c6ed6-05cc-49d3-96aa-de5ac60e16a1', 2257, 'added 524 characters in body', 4264, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('No, you should go ahead and learn the maths on your own. You will "only" need to learn calculus, statistics, and linear algebra (like the rest of machine learning). The theory of neural networks is pretty primitive at this point -- it more of an art than a science -- so I think you can understand it if you try. Ipso facto, there are a lot of tricks that you need practical experience to learn. There are lot of complicated extensions, but you can worry about them once you get that far.

Once you can understand the Coursera classes on ML and neural networks (Hinton''s), I suggest getting some practice. You might like [this](http://karpathy.github.io/neuralnets/) introduction.', 381, '2014-10-13 05:27:54.170', 'fc3ac534-961b-4f1e-a57f-a8e129ad1745', 2261, 'added 141 characters in body', 4266, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was wondering if someone could point me to suitable database formats for building up a user database:

basically I am collecting logs of impressions data, and I want to compile a user database

which sites user visits, country/gender/..? and other categorisations with the aim of
a) doing searches: give me all users visiting games sites from france...
b) machine learning: eg clustering users by the sites they visit

so I am interested in storing info about millions of users

with indexes? on user, sites, geo-location

and the idea would be that this data would be continually updated ( eg nightly update to user database of new sites visited etc)

what are suitable database systems. Can someone suggest suitable reading material?
I was imagining HTable might be suitable...

 ', 1256, '2014-10-13 11:23:23.483', 'a6bbdd71-3c0e-4d46-9e60-68f113cf8d46', 2263, 4267, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('"Hadoop" formats for user database: online advertising', 1256, '2014-10-13 11:23:23.483', 'a6bbdd71-3c0e-4d46-9e60-68f113cf8d46', 2263, 4268, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<htable><bigtable>', 1256, '2014-10-13 11:23:23.483', 'a6bbdd71-3c0e-4d46-9e60-68f113cf8d46', 2263, 4269, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Perceptrons, strictly speaking, are binary classifiers.

To make a multi-class classifier, you should switch to a standard feed-forward neural net with a [softmax][1] output layer. Without any hidden layers this is equivalent to multinomial logistic regression.

You can also do the one-vs-all trick as @seanv507 suggests. This often works well in practice but there''s no strong basis for it in theory, and the true multi-class version is easier conceptually and practically in this case.


  [1]: http://en.wikipedia.org/wiki/Softmax_function', 1399, '2014-10-13 13:35:09.443', '4067341b-ed65-4569-a89d-8ede5b3229ec', 2264, 4270, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Check out the partial_fit method of [scikit''s SGD classifier][1]. You have control over what you call with it: you can do it "true" online learning by passing an instance at a time, or you can batch up instances into mini-batches if all your data are available in an array. If they are, you can slice the array to provide the minibatches.

  [1]: http://scikit-learn.org/0.15/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier', 1399, '2014-10-13 13:50:07.080', '7b8fbdd3-b3e6-4371-a063-6905410d9509', 2265, 4271, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It''s just logistic regression. Get a bunch of data about presentations of search results, along with whether an item was clicked on. An instance is a search result item, with possible features being rank, "quality" (not sure what you mean by this) etc. The what you''re asking about is a question of inference on the parameter related to rank.', 1399, '2014-10-13 13:53:36.027', '41b84cfb-5d74-4d5b-9759-1e2b31c250a1', 2266, 4272, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('you might want to look at this paper
[Predicting Clicks: Estimating the Click-Through Rate for New Ads][1]

> Whenever an ad is displayed on the search results page, it has
some chance of being viewed by the user. The farther down the
page an ad is displayed, the less likely it is to be viewed. As a
simplification, we consider the probability that an ad is clicked on
to be dependent on two factors: a) the probability that it is viewed,
and b) the probability that it is clicked on, given that it is viewed:





  [1]: http://research.microsoft.com/pubs/68148/predictingclicks.pdf', 1256, '2014-10-13 14:46:54.403', '22fce5a5-2c95-46df-b76d-da6dc102d038', 2267, 4273, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('

I am new to Natural Language Processing, I think NLP is a challenging field, the syntax and semantic ambiguities could cause a lot of problems. For example I think for these problems machine translation is a hart task.

Therefore probably many approaches and methods has been applied to the field, I would like to know what are the latest and most promising approaches and methods in the field of NLP?

Does these techniques are dependent highly to the target language?
', 3436, '2014-10-13 19:02:24.670', '68ec6ff8-c3ee-4e14-85e3-8a11ddc05dc9', 2268, 4274, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is state of the art in the field of NLP?', 3436, '2014-10-13 19:02:24.670', '68ec6ff8-c3ee-4e14-85e3-8a11ddc05dc9', 2268, 4275, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp>', 3436, '2014-10-13 19:02:24.670', '68ec6ff8-c3ee-4e14-85e3-8a11ddc05dc9', 2268, 4276, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am looking for an online console for the language R. Like I write the code and the server should execute and provide me with the output.

Similar to the website Datacamp.


', 4637, '2014-10-13 21:13:48.447', 'ac340938-3b86-4400-85b3-404a1f0f93b2', 2269, 4277, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Any Online R console?', 4637, '2014-10-13 21:13:48.447', 'ac340938-3b86-4400-85b3-404a1f0f93b2', 2269, 4278, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><statistics>', 4637, '2014-10-13 21:13:48.447', 'ac340938-3b86-4400-85b3-404a1f0f93b2', 2269, 4279, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes. I believe this is what you are looking for.

http://www.compileonline.com/execute_r_online.php', 3466, '2014-10-13 22:07:51.180', 'affae05d-983d-4a20-bd6b-675f0e16f535', 2270, 4280, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('
I am new to Natural Language Processing, I think NLP is a challenging field, the syntax and semantic ambiguities could cause a lot of problems. For example I think for these problems machine translation is a hard task.

Therefore there are probably many approaches and methods that have been applied to this field. But what are the latest and most promising approaches and methods in the field of NLP?

Are these techniques highly dependent on the target language?', 24, '2014-10-14 00:27:48.050', '5bb0a6ea-fe58-49a7-96bd-c34f8afccc80', 2268, 'spelling, grammar', 4281, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What is the state of the art in the field of NLP?', 24, '2014-10-14 00:27:48.050', '5bb0a6ea-fe58-49a7-96bd-c34f8afccc80', 2268, 'spelling, grammar', 4282, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-10-14 00:27:48.050', '5bb0a6ea-fe58-49a7-96bd-c34f8afccc80', 2268, 'Proposed by 24 approved by 84 edit id of 160', 4283, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You are probably aware that deep learning is all the rage these days, and it has touched NLP too. There is a tutorial on it from a recent conference: [Deep Learning for Natural Language Processing (without Magic)](http://nlp.stanford.edu/courses/NAACL2013/) by Richard Socher and Christopher Manning, who are from Stanford.', 381, '2014-10-14 01:19:37.067', 'eba59b2d-d710-4efb-835a-bf1431aed472', 2271, 4284, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('
[R-fiddle][1] provides an easy-to-use interactive R console that you can run from your browser.

![screenshot][2]


  [1]: http://www.r-fiddle.org
  [2]: http://i.stack.imgur.com/cZJu6.png', 2961, '2014-10-14 07:02:47.500', '72ec47cf-4a6c-49e7-8aa6-1a6b73310671', 2272, 4285, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Statement of problem: An ambulance is at the hospital dropping off a patient. The goal of the paramedic is to get released from the hospital as soon as possible. I am curious, what are the factors in how long an ambulance off loads a patient at the hospital? Can I predict how long an offload will take given certain variables. And how confident can I be in this model? The Dependent Variable is HospitalTime, it is a ratio type of data and is measured in seconds. The Independent Variables are:

 - Hospital, a nominal type of data recoded into integers, 1 would stand
   for Lee Memorial.
 - Ambulance, a nominal type of data recoded into integers, 9 would
   stand for ambulance #9
 - PatientPriority is an ordinal type of data recoded into integers. A 1
   is a high priority, 2 is a medium priority and 3 is low acuity.
 - MonthOfCall is an interval type of data recoded into integers. A 6
   would be June and 12 is December. A 12 (December) is not twice as
   much as a 6 (June) in this case.
 - HourOfCall is an interval type of data recoded into integers. Once
   again, an offload happening at 10:00 pm is not more than something
   happening at 10:00 am.
 - Officer1 and Officer2 are nominal data and are integers representing
   an EMT and a paramedic.

My question is this: Given this type of data and my goal to predict the off loading time at the hospital, what kind of regression model should I look into?

I have looked at my statistics books from university days and they are all using ratio data. My data is mixed with nominal, ordinal, interval and ratio.

I have as much data as you could ask for. I have at least 100,000 observations.

Can you please push me in the right direction? What kind of model should I use with this type of data?

Shown below are observations to give you a tiny peek at my data:

    IncidentID,HospitalTime,Hospital,Ambulance,PatientPriority,MonthOfCall,HourOfCall,Officer1,Officer2
    757620,1849,7,11,2,10,10,234,771,chr(10) 802611,2625,7,11,3,1,18,234,777,chr(10)
    765597,1149,7,12,3,11,2,234,777,chr(10) 770926,1785,7,12,3,11,15,234,777,chr(10)
    771689,3557,7,12,2,11,14,234,777,chr(10) 822758,1073,7,20,3,3,13,777,307,chr(10)
    767249,2570,7,22,2,11,11,560,778,chr(10) 767326,1998,7,22,1,11,18,560,777,chr(10)
    785903,1660,7,22,3,12,12,234,777,chr(10) 787644,2852,7,22,3,12,17,234,777,chr(10)
    760294,1327,7,23,2,10,14,498,735,chr(10) 994677,3653,7,32,2,2,15,181,159,chr(10)
    994677,3653,7,32,2,2,15,181,159,chr(10) 788471,2053,5,9,2,1,3,498,777,chr(10)
    788471,2053,5,9,2,1,3,498,777,chr(10) 759983,1342,5,11,2,10,8,474,777,chr(10)
    791243,1635,5,11,2,1,18,234,777,chr(10) 800796,1381,5,11,3,1,11,234,777,chr(10)

P.S. This question is cross-posted in Stack-Overflow under the same title and author.', 21, '2014-10-14 08:24:14.660', '6fd17b14-f571-499a-9122-1a71e0bf5a11', 1190, 'added 60 characters in body', 4286, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have one variable whose value I would like to predict, and I would like to use only one variable as predictor (for instance: predict traffic density based on weather). Initially I thought about using Self-Organizing Maps, which performs unsupervised clustering + regression, but since it has an important component of dimensionality reduction I see it as more appropriated for a large number of variables. Does it make sense to use it for a single variable as predictor? Maybe there are more adequate techniques for this *simple* case: I used "Data Mining" instead of "machine learning" in the title of my question, because I think maybe a linear regression could do the job...', 3159, '2014-10-14 08:50:53.907', '103b2122-b6f9-4a3b-bc93-afee351d9cc7', 2273, 4287, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the best Data Mining algorithm for prediction based on a single variable?', 3159, '2014-10-14 08:50:53.907', '103b2122-b6f9-4a3b-bc93-afee351d9cc7', 2273, 4288, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering><regression><correlation>', 3159, '2014-10-14 08:50:53.907', '103b2122-b6f9-4a3b-bc93-afee351d9cc7', 2273, 4289, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Common rule in machine learning is to **try simple things first**. For predicting continuous variables there''s nothing more basic than **simple linear regression**. "Simple" in the name means that there''s only one predictor variable used (+ intercept, of course):

    y = b0 + x*b1

where `b0` is an intercept and `b1` is a slope. For example, you may want to predict lemonade consumption in a park based on temperature:

    cons = b0 + temp * b1

Temperature is in well-defined **continuous** variable. But if we talk about something more abstract like "weather", then it''s harder to understand how we measure and encode it. It''s ok if we say that the weather takes values `{terrible, bad, normal, good, excellent}` and assign values numbers from -2 to +2 (implying that "excellent" weather is twice as good as "good"). But what if the weather is given by words `{shiny, rainy, cool, ...}`? We can''t give an order to these variables. We call such variables **categorical**. Since there''s no natural order between different categories, we can''t encode them as a single numerical variable (and linear regression expects numbers only), but we can use so-called **dummy encoding**:  instead of a single variable `weather` we use 3 variables - `[weather_shiny, weather_rainy, weather_cool]`, only one of which can take value 1, and others should take value 0. In fact, we will have to drop one variable because of [collinearity](http://en.wikipedia.org/wiki/Multicollinearity). So model for predicting traffic from weather may look like this:

    traffic = b0 + weather_shiny * b1 + weather_rainy * b2  # weather_cool dropped

where either `b1` or `b2` is 1, or both are 0.

Note that you can also encounter non-linear dependency between predictor and predicted variables (you can easily check it by plotting `(x,y)` pairs). Simplest way to deal with it without refusing linear model is to use polynomial features - simply add polynomials of your feature as new features. E.g. for temperature example (for dummy variables it doesn''t make sense, cause `1^n` and `0^n` are still 1 and 0 for any `n`):

    traffic = b0 + temp * b1 + temp^2 * b2 [+ temp^3 * b3 + ...]', 1279, '2014-10-14 13:10:39.773', 'dd5d84bf-9bcd-4cbc-8901-350437d187e8', 2274, 4290, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635

in the above paper, page 4, two models, non-adaptive regression model, adaptive regression model, the non-adaptive regression model''s parameter estimation method is "least squares", what is the parameter estimation for the adaptive regression model? is there any package in R to do parameter estimation for this kind of adaptive regression model? If I add more predictors in the adaptive regression model, can R still solve it? and how?

Thank you!', 4619, '2014-10-14 15:26:01.460', 'c5bf85c5-4c8e-4942-8932-4cffc0192e28', 2275, 4291, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('what are the parameter estimation methods for the two methods in this paper', 4619, '2014-10-14 15:26:01.460', 'c5bf85c5-4c8e-4942-8932-4cffc0192e28', 2275, 4292, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics>', 4619, '2014-10-14 15:26:01.460', 'c5bf85c5-4c8e-4942-8932-4cffc0192e28', 2275, 4293, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The kind of data you store and analyze is very much dependent upon the kind of data you can gather. So, without knowing what your ''impression data'' looks like, it is very hard to suggest how to normalize and store it.

Furthermore, the way you store data is also dependent upon how you wish to analyze it. For example, if you want to perform basic analytics like page view counts, how many pages a user visits per session, etc (SQL). . . data needs to be stored differently than if you want to build recommendations based on traffic patterns (graph database).

Please edit your question to include more detail. Apologies that I cannot simply leave a comment.', 3466, '2014-10-14 17:47:02.663', 'aaf41eee-15b8-4e95-a3b5-8e2c200af52c', 2276, 4294, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am more of an expert on data ETL and combining/aggregating than on the forumulas themselves. I work frequently with weather data. I like to give some suggestions on using weather data in analysis.

1. Two types of data are reported in US/Canada:<br/>
    A. Measurements<br/>
    B. Weather Type

As far as weather type (sunny, rainy, severe thunderstorm) they are either going to already be reflected in measurements (e.g., sunny, rainy) and are redundant or they are inclement weather conditions and are not necessarily reflected in the measurements.

For inclement weather types, I would have separate formulae.

For measurements, there are 7 standard daily measurements for Weather Station reporting in North America.

Temp Min/Max<br/>
Precipitation<br/>
Average Wind Speed<br/>
Average Cloudiness (percentage)<br/>
Total sunlight (minutes)<br/>
Snowfall<br/>
Snow Depth<br/>

Not all stations report all 7 daily measurements. Some report only Temp and Precipitation. So you may want to have one formula for Temp/Precipitation and an expanded formulae when all seven measurements are available.

The two links below are NOAA/NWS weather terms used in their datasets:

This document is the vocabulary for the annual summaries:

http://www1.ncdc.noaa.gov/pub/data/cdo/documentation/ANNUAL_documentation.pdf

This document is the vocabulary for the daily summaries

http://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf





    ', 3609, '2014-10-14 18:24:36.880', 'f5c4d120-0c73-4a4a-97db-916a5741fa33', 2277, 4295, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I was wondering if someone could point me to suitable database formats for building up a user database:

basically I am collecting logs of impressions data, and I want to compile a user database

which sites user visits, country/gender/..? and other categorisations with the aim of
a) doing searches: give me all users visiting games sites from france...
b) machine learning: eg clustering users by the sites they visit

so I am interested in storing info about millions of users

with indexes? on user, sites, geo-location

and the idea would be that this data would be continually updated ( eg nightly update to user database of new sites visited etc)

what are suitable database systems. Can someone suggest suitable reading material?
I was imagining Hbase might be suitable...

 ', 1256, '2014-10-14 23:31:23.803', 'd6f231ac-d42a-4cdb-8cfd-0f1d4e594ccb', 2263, 'correction', 4296, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigtable>', 1256, '2014-10-14 23:31:23.803', 'd6f231ac-d42a-4cdb-8cfd-0f1d4e594ccb', 2263, 'correction', 4297, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('is there anyone knowing the parameter estimation, prediction for the adaptive regression model very well?

http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635

I have a paper as in above link.

I am wondering how to use R to do the adaptive regression parameter estimation, prediction for the adaptive regression model on the 4th page in the paper from the above link.

Any one has any idea?

And if you know adaptive regression model very well, or can share some useful link or something, describe the model/parameter estimation/prediction very well, can also put here.

Thank you so much!', 4619, '2014-10-15 02:22:09.983', '4a8dd093-b834-4ba2-b349-57d09d52ca41', 2278, 4298, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('who knows the parameter estimation, prediction for the adaptive regression model very well?', 4619, '2014-10-15 02:22:09.983', '4a8dd093-b834-4ba2-b349-57d09d52ca41', 2278, 4299, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics>', 4619, '2014-10-15 02:22:09.983', '4a8dd093-b834-4ba2-b349-57d09d52ca41', 2278, 4300, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here is some notes for an R package ''earth'' for adaptive regression. This may be useful.

http://cran.r-project.org/web/packages/earth/vignettes/earth-notes.pdf', 4619, '2014-10-15 03:08:59.263', 'f36da531-2d89-4719-a903-bcdee5f6fe00', 2279, 4301, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In the case you mention, I recommend to keep the changes as a dictionary, for instance in a .csv file. Write a script that replaces the values in the original data based on the translation in your dictionary. That way, you separate the corrections from the script itself.', 906, '2014-10-15 11:37:47.090', '561bca25-458f-4675-8f40-b6eaa9977cdc', 2280, 4302, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('how to do the parameter estimation, prediction for the adaptive regression model?

http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635

I have a paper as in above link.

I am wondering how to use R to do the adaptive regression parameter estimation, prediction for the adaptive regression model on the 4th page in the paper from the above link.

Any one has any idea?

And if you know adaptive regression model very well, or can share some useful link or something, describe the model/parameter estimation/prediction very well, can also put here.

Thank you so much!', 4619, '2014-10-15 14:59:14.463', '389625c4-1bb7-45fb-8f20-20841f4b1770', 2278, 'deleted 24 characters in body; edited title', 4303, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('how to do the parameter estimation, prediction for the adaptive regression model?', 4619, '2014-10-15 14:59:14.463', '389625c4-1bb7-45fb-8f20-20841f4b1770', 2278, 'deleted 24 characters in body; edited title', 4304, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would say... it really depends. You may need to:

- *use* machine learning algorithms: this will be useful for specific applications you may have. In this situation what you need is some programming skills and the taste for testing (practicing will make you strong). Here maths are not so much required I would say.
- be able to *modify* existing algorithms. Your specific application may be reticent to regular algorithms, so you may need to adapt them to get maximum efficiency. Here maths come into play.
- understand the theory behind algorithms. Here maths are necessary, and will help you increase your knowledge of the field of machine learning, develop your own algorithms, speak the same langage as your peers... NN theory may be primitive as said by @Emre, but for instance this is not the case for SVM (the theory behind SVM requires e.g. to understand [reproducing kernel Hilbert spaces](http://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space)).

On the mid term for sure you will need strong maths. But you don''t need to wait for them to come to you, you can start right now with linear algebra, which is beautiful and useful for everything. And in case you encounter (possibly temporary) difficulties of any sort with maths, keep on practicing the way you already do (many people can talk about the perceptron but are not able to make a perceptron in Java), this is very valuable.
', 3317, '2014-10-15 20:37:40.633', 'f1892c8c-2791-4d49-a230-59a81efb83e2', 2281, 4305, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to identifies different queries in sentences.

Like - `Who is Bill Gates and where he was born?` or `Who is Bill Gates, where he was born?` contains two queries

1. Who is Bill Gates?
2. Where Bill Gates was born

I worked on Coreference resolution, so I can identify that `he` points to `Bill Gates` so resolved sentence is "Who is Bill Gates, where Bill Gates was born"

Like wise

    MGandhi is good guys, Where he was born?
    single query
    who is MGandhi and where was he born?
    2 queries
    who is MGandhi, where he was born and died?
    3 quries
    India won world cup against Australia, when?
    1 query (when India won WC against Auz)

I can perform Coreference resolution but not getting how can I distinguish queries in it.
How to do this?

I checked various sentence parser, but as this is pure nlp stuff, sentence parser does not identify it.

I tried to find "Sentence disambiguation" like "word sense disambiguation", but nothing exist like that.

Any help or suggestion would be much appreciable.



', 4662, '2014-10-16 05:44:40.183', 'e92a8149-da21-43b2-bf3e-b73eead0e174', 2284, 4310, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is it possible to identify different queries/questions in sentence?', 4662, '2014-10-16 05:44:40.183', 'e92a8149-da21-43b2-bf3e-b73eead0e174', 2284, 4311, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><nlp><social-network-analysis>', 4662, '2014-10-16 05:44:40.183', 'e92a8149-da21-43b2-bf3e-b73eead0e174', 2284, 4312, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Recently in a data analytic job interview for an e-commerce site, they asked me, do i have some knowledge of buyer classification problem. Unfortunately i heard this term for the first time.
After interview i tried to search a lot about it over google but didn''t find something meaningful. Please any one let me know if you have heard this term before and paste some links explaining this concept. Thanks   ', 713, '2014-10-16 06:37:09.887', '8a90a2a0-33b6-466a-b6fc-1c7a89143c63', 2285, 4313, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('what is buyer classification problem?', 713, '2014-10-16 06:37:09.887', '8a90a2a0-33b6-466a-b6fc-1c7a89143c63', 2285, 4314, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 713, '2014-10-16 06:37:09.887', '8a90a2a0-33b6-466a-b6fc-1c7a89143c63', 2285, 4315, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The basic thing, you can do in that situation, is to split your query into N simple sentences each of which should be processed in order to receive YES/NO answer considering if the sentence is a query. That way you will receive following results:

    Input: Gandhi is good guys, Where he was born?
    ->
    Gandhi is good guys - not query
    Where he was born?  - query
    ===
    1 query

    Input: who is MGandhi and where was he born?
    ->
    who is MGandhi     - query
    where was he born? - query
    ===
    2 queries

This approach will require anaphora resolution (in order to convert `he` into `Gandhi` in first example) and a parser to correctly divide complex sentence into simple ones.', 2573, '2014-10-16 08:03:15.593', '3a01a4d6-9f15-4e9f-a625-0a4ff5005565', 2286, 4317, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I want to identifies different queries in sentences.

Like - `Who is Bill Gates and where he was born?` or `Who is Bill Gates, where he was born?` contains two queries

1. Who is Bill Gates?
2. Where Bill Gates was born

I worked on Coreference resolution, so I can identify that `he` points to `Bill Gates` so resolved sentence is "Who is Bill Gates, where Bill Gates was born"

Like wise

    MGandhi is good guys, Where he was born?
    single query
    who is MGandhi and where was he born?
    2 queries
    who is MGandhi, where he was born and died?
    3 quries
    India won world cup against Australia, when?
    1 query (when India won WC against Auz)

I can perform Coreference resolution (Identifying and converting `he` to `Gandhi`) but not getting how can I distinguish queries in it.


How to do this?

I checked various sentence parser, but as this is pure nlp stuff, sentence parser does not identify it.

I tried to find "Sentence disambiguation" like "word sense disambiguation", but nothing exist like that.

Any help or suggestion would be much appreciable.



', 4662, '2014-10-16 09:26:18.960', 'c9374d37-f8ad-49d3-aad2-e12630db2f7e', 2284, 'added 50 characters in body', 4318, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am kind of a newbie on machine learning adn I would liek to ask some questions based on a problem I have .

Let''s say I have x y z as variable and I have values of these variables as time progresses like :

t0  = x0 y0 z0  <br>
t1  = x1 y1 z1  <br>
tn  = xn yn zn  <br>

Now I want a model that when it''s given 3 values of x , y , z I want a prediction of them like:

Input : x_test y_test z_test <b>
Output : x_prediction y_prediction z_prediction

These values are float numbers. What is the best model for this kind of problem?
Thanks in advance for all the answers.', 4668, '2014-10-16 12:15:32.017', '3e93b410-e85c-4a84-bdd1-6c0d7efa9c1a', 2287, 4319, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Regression Model for explain model(Details inside)', 4668, '2014-10-16 12:15:32.017', '3e93b410-e85c-4a84-bdd1-6c0d7efa9c1a', 2287, 4320, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><regression>', 4668, '2014-10-16 12:15:32.017', '3e93b410-e85c-4a84-bdd1-6c0d7efa9c1a', 2287, 4321, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am kind of a newbie on machine learning and I would like to ask some questions based on a problem I have .

Let''s say I have x y z as variable and I have values of these variables as time progresses like :

t0  = x0 y0 z0  <br>
t1  = x1 y1 z1  <br>
tn  = xn yn zn  <br>

Now I want a model that when it''s given 3 values of x , y , z I want a prediction of them like:

Input : x_test y_test z_test <b>
Output : x_prediction y_prediction z_prediction

These values are float numbers. What is the best model for this kind of problem?
Thanks in advance for all the answers.', 4668, '2014-10-16 12:32:12.077', 'dcfff686-97f7-49b5-ad69-b7b7e3076113', 2287, 'edited body; edited tags; edited title', 4322, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Regression Model for explained model(Details inside)', 4668, '2014-10-16 12:32:12.077', 'dcfff686-97f7-49b5-ad69-b7b7e3076113', 2287, 'edited body; edited tags; edited title', 4323, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><logistic-regression><predictive-modeling><regression>', 4668, '2014-10-16 12:32:12.077', 'dcfff686-97f7-49b5-ad69-b7b7e3076113', 2287, 'edited body; edited tags; edited title', 4324, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('AFAIK if you want to predict the value of one variable, you need to have one or more variables as predictors; i.e.: you assume the behaviour of one variable can be explained by the behaviour of other variables.
In your case you have three independent variables whose value you want to predict, and since you don''t mention any other variables, I assume that each variable depends on the others. In that case you could fit three models (for instance, regression models), each of which would predict the value of one variable, based on the others. As an example, to predict x:

    x_prediction=int+cy*y_test+cz*z_test

, where int is the intercept and cy, cz, the coefficients of the linear regression.
Likewise, in order to predict y and z:

    y_prediction=int+cx*x_test+cx*z_test
    z_prediction=int+cx*x_test+cy*y_test

', 3159, '2014-10-16 14:34:33.180', '639cc9a1-5448-4c87-a99b-ad4f733abc16', 2288, 4325, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Buyer classification is used to categorize users who purchase groups of items. Buyers are categorized in order to be targeted for advertising. When users buy similar items, they are more likely to buy similar items in the future. This is useful information when pricing items on a website.

A clear example, a website selling nutritional supplements may want to target different buyers by category. Men in their 20s are unlikely to purchase menopause supplements, and women in their 50s are also unlikely to buy creatine. So, by analyzing user purchase history and categorizing types of buyers, the site can send intelligent promotions - males in their 20s get ads for creatine while women in their 50s get ads for menopause supplements.

Also, if I want to run a sale to attract customers, I don''t want to offer lower prices on items which are often purchased together. I''d rathe price one low to attract the buyer, and then hope they buy the complimentary item at full price. Categorizing buyers also helps with this problem.

You may want to read up on shopping cart analysis, which is not a new problem. Department stores have been analyzing shopping carts and classifying buyers long before online shopping was popularized. That''s why you have to use membership cards to get the ''special'' prices.

Fine tuning these details can increase revenue substantially.', 3466, '2014-10-16 21:12:09.490', '20a66ff2-df00-4089-aeff-7c46bc745568', 2290, 4333, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to setup a Big Data Infra-Structure using Hadoop, Hive, Elastic Search (amongst others) and I would like to run some algorithms over these datasets. I would like the algorithms themselves, to be scalable, so that excludes using tools such as Weka, R, or even RHadoop.
The [Apache Mahout Library](https://mahout.apache.org) seems to be a good option, and it features [algorithms for regression and clustering tasks](https://mahout.apache.org/users/basics/algorithms.html). What I am struggling to find is a solution for anomaly or outlier detection.
Since Mahout features Hidden Markov Models and a variety of clustering techniques (including K-Means) I was wondering if it would be possible to build a model to detect outliers in time-series, using any of this. I would be grateful if somebody experienced on this could advice me a) if it is possible, and in case it is b) how-to do it, plus c) an estimation of the effort involved and d) accuracy/problems of this approach.', 3159, '2014-10-17 10:47:13.197', 'ee55e9c6-5584-4da0-8b2f-2825814af5e8', 2293, 4340, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Scalable Outlier/Anomaly Detection', 3159, '2014-10-17 10:47:13.197', 'ee55e9c6-5584-4da0-8b2f-2825814af5e8', 2293, 4341, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><data-mining><algorithms>', 3159, '2014-10-17 10:47:13.197', 'ee55e9c6-5584-4da0-8b2f-2825814af5e8', 2293, 4342, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It strongly depends on the environment/company you are working with. In my eyes there is a "big data" hype at the moment and a lot of companies try to enter the field with hadoop based solutions - what makes hadoop also a buzzword but its not always the best solution.

In my mind, a good Data Scientist should be able to ask the right questions and keep on asking again until its clear whats really needed. Than a good DataScientist - of course - needs to know how to address the problem (or at least know someone who can). Otherwise your stakeholder could be frustrated :-)

So, i would say its not absolutely necessary to learn Hadoop.', 4676, '2014-10-17 10:55:04.147', '82812bcd-4a7f-480d-869a-9692b50d2c49', 2294, 4343, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would take a look at [t-digest algorithm][1]. It''s [been merged into mahout][2] and also a part of some other libraries (github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/stream/quantile/TDigest.java) for big data streaming. You can get more about this algorithm particularly and big data anomaly detection in general in next resources:

 1. Practical machine learning anomaly detection book.(info.mapr.com/rs/mapr/images/Practical_Machine_Learning_Anomaly_Detection.pdf)
 2. Webinar: Anomaly Detection When You Don''t Know What You Need to Find (youtube.com/watch?v=i-mSV63Q9rA#t=757)
 3. Anomaly Detection in Elasticsearch (info.prelert.com/anomaly-detection-in-elasticsearch)
 4. Beating Billion Dollar Fraud Using Anomaly Detection: A Signal Processing Approach using Argyle Data on the Hortonworks Data Platform with Accumulo (oreilly.com/pub/e/3211)


  [1]: https://github.com/tdunning/t-digest
  [2]: https://issues.apache.org/jira/browse/MAHOUT-1361', 4679, '2014-10-17 12:50:56.243', '91382f0a-e897-499d-a775-c54a4c79171e', 2296, 4345, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m looking for a topic for my masters thesis. Machine learning is my primary domain and I want to work on probabilistic models and applied probability in Machine Learning. Please suggest some exciting new topics that would make for a good masters thesis subject.
Anything related to Markov chains Monte Carlo, Bayesian methods, Probabilistic graphical models, Markov models and so on in context of machine learning would be great!', 2475, '2014-10-17 13:34:48.540', '69671602-32f4-4c36-81c5-e40e56561b89', 2297, 4346, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Masters thesis topics in Applied probability and Probabilistic models in Machine Learning', 2475, '2014-10-17 13:34:48.540', '69671602-32f4-4c36-81c5-e40e56561b89', 2297, 4347, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 2475, '2014-10-17 13:34:48.540', '69671602-32f4-4c36-81c5-e40e56561b89', 2297, 4348, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve built a toy Random Forest model in `R` (using the `German Credit` dataset from the `caret` package), exported it in `PMML 4.0` and deployed onto Hadoop, using the `Cascading Pattern` library.

I''ve run into an issue where `Cascading Pattern` scores the same data differently (in a binary classification problem) than the same model in `R`. Out of 200 observations, 2 are scored differently.

Why is this? Could it be due to a difference in the implementation of Random Forests? ', 1127, '2014-10-17 13:58:39.353', 'dd10839d-9038-414d-b8b7-00fba217e4b2', 2298, 4349, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Differences in scoring from PMML model on different platforms', 1127, '2014-10-17 13:58:39.353', 'dd10839d-9038-414d-b8b7-00fba217e4b2', 2298, 4350, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><hadoop><random-forest><predictive-modeling>', 1127, '2014-10-17 13:58:39.353', 'dd10839d-9038-414d-b8b7-00fba217e4b2', 2298, 4351, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One of real challenges, I faced with R is different packages compatible with different versions.. quite a lot R packages are not available for latest version of R.. And R quite a few time gives error  due to library or package was written for older version..
', 4688, '2014-10-18 03:22:19.060', '784185d1-ab01-45b0-9143-1a34d8e8ee75', 2299, 4353, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to analyze the effectiveness and efficiency of kernel methods for which I would require 3 different data-set in 2 dimensional space for each of the following cases:

 1. BAD_kmeans: The data set for which the kmeans clustering algorithm
    will not perform well.
 2. BAD_pca: The data set for which the Principal Component Analysis
    (PCA) dimension reduction method upon projection of the original
    points into 1-dimensional space (i.e., the first eigenvector) will
    not perform well.
 3. BAD_svm: The data set for which the linear Support Vector Machine
    (SVM) supervised classification method using two classes of points
    (positive and negative)  will not perform well.


Which packages can I use in R to generate the random 2d data-set for each of the above cases ? A sample script in R would help in understanding', 3577, '2014-10-18 04:58:45.100', 'd325197d-87be-4878-9c01-fa6a0325cc21', 2302, 4358, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R Script to generate random dataset in 2d space', 3577, '2014-10-18 04:58:45.100', 'd325197d-87be-4878-9c01-fa6a0325cc21', 2302, 4359, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><r><clustering>', 3577, '2014-10-18 04:58:45.100', 'd325197d-87be-4878-9c01-fa6a0325cc21', 2302, 4360, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to learn both Python and R for usage in data science projects.

I am currently unemployed, fresh out of university, scouting around for jobs and thought it would be good if I get some Kaggle projects under my profile.

However, I have very little knowledge in either language. Have used Matlab and C/C++ in the past. But I haven''t produced production quality code or developed an application or software in either language. It has been dirty coding for academic usage all along.

I have used a little bit of Python, in a university project, but I dont know the fundamentals like what is a package , etc etc. ie havent read the intricacies of the language using a standard Python Textbook etc..

Have done some amount of coding in C/C++ way back (3-4 years back then switched over to Matlab/Octave).

I would like to get started in Python Numpy Scipy scikit-learn and pandas etc. but just reading up Wikipedia articles or Python textbooks is going to be infeasible for me.

And same goes with R, except that I have zero knowledge of R.

Does anyone have any suggestions?', 3223, '2014-10-18 08:08:25.513', '34b43597-5c81-48bc-95c7-a1dac766f687', 2303, 4361, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Python and R good tutorials?', 3223, '2014-10-18 08:08:25.513', '34b43597-5c81-48bc-95c7-a1dac766f687', 2303, 4362, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><python>', 3223, '2014-10-18 08:08:25.513', '34b43597-5c81-48bc-95c7-a1dac766f687', 2303, 4363, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is there a good java library for doing time series energy consumption forecasting based on weather data and other variables?

Thank you,
', 4692, '2014-10-18 08:19:00.607', '751081eb-fb1d-4355-aeb6-fc5fdc95b3a8', 2304, 4364, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Energy consumption time series forcasting', 4692, '2014-10-18 08:19:00.607', '751081eb-fb1d-4355-aeb6-fc5fdc95b3a8', 2304, 4365, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<time-series>', 4692, '2014-10-18 08:19:00.607', '751081eb-fb1d-4355-aeb6-fc5fdc95b3a8', 2304, 4366, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a variable whose value I would like to predict, and I would like to use only one variable as predictor. For instance, predict traffic density based on weather.

Initially, I thought about using [Self-Organizing Maps](http://en.wikipedia.org/wiki/Self-organizing_map) (SOM), which performs unsupervised clustering + regression. However, since it has an important component of dimensionality reduction, I see it as more appropriated for a large number of variables.

Does it make sense to use it for a single variable as predictor? Maybe there are more adequate techniques for this *simple* case: I used "Data Mining" instead of "machine learning" in the title of my question, because I think maybe a linear regression could do the job...', 84, '2014-10-18 13:25:13.107', '9b318e2d-642b-4950-b0a4-53f33ae45eae', 2273, 'Improving writing.', 4367, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I can only recommend [Advanced R](http://adv-r.had.co.nz/) by Hadley Wickham. I think it is at the same time incredibly rich in content and easy to read. You say you have zero knowledge in R, but I believe since you already have programming skills in other languages this book can complement very fruitfully any classical "R beginner manual" (for the latter see [here](http://cran.r-project.org/other-docs.html#english)). ', 3317, '2014-10-18 17:32:52.583', 'de7673d5-d3a3-4c1f-82bf-d74d35dc763f', 2305, 4368, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Most Treebank conversion which I found in the web are from constituency treebank to dependency treebank, I wonder why there is little jobs in the opposite direction?

', 3436, '2014-10-18 18:05:26.930', '75e3527b-0cde-447d-9a44-e708fd3d4a1c', 2306, 4369, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Treebank conversion from dependency to constituency?', 3436, '2014-10-18 18:05:26.930', '75e3527b-0cde-447d-9a44-e708fd3d4a1c', 2306, 4370, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp>', 3436, '2014-10-18 18:05:26.930', '75e3527b-0cde-447d-9a44-e708fd3d4a1c', 2306, 4371, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('    size <- 20             #length of random number vectors
    set.seed(1)
    x <- runif(size)          # generate samples from uniform distribution (0.0, 1.0)
    y <-runif(size)
    df <-data.frame(x,y)

    # other distributions: rpois, rmvnorm, rnbinom, rbinom, rbeta, rchisq, rexp, rgamma, rlogis, rstab, rt, rgeom, rhyper, rwilcox, rweibull.

See [this page][1] for tutorial on generating random samples from distributions


  [1]: http://statistics.ats.ucla.edu/stat/r/modules/prob_dist.htm', 609, '2014-10-19 03:24:22.877', '05d68875-8c30-4468-82ec-908294a3d5d3', 2307, 4372, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('None of the algorithms you mention are good with data that has uniform distribution.

    size <- 20             #length of random number vectors
    set.seed(1)
    x <- runif(size)          # generate samples from uniform distribution (0.0, 1.0)
    y <-runif(size)
    df <-data.frame(x,y)

    # other distributions: rpois, rmvnorm, rnbinom, rbinom, rbeta, rchisq, rexp, rgamma, rlogis, rstab, rt, rgeom, rhyper, rwilcox, rweibull.

See [this page][1] for tutorial on generating random samples from distributions.

---
For specific set of randomized data sets that are ''hard'' for these methods (e.r. linearly inseparable *n*-classes XOR patterns), see this blog post (incl. R code): http://tjo-en.hatenablog.com/entry/2014/01/06/234155.


  [1]: http://statistics.ats.ucla.edu/stat/r/modules/prob_dist.htm', 609, '2014-10-19 03:35:17.557', '15bda2f1-d53e-4da2-a0d2-1a46849fc89d', 2307, 'added reference to linearly inseparable n-classes XOR patterns, blog post', 4373, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am looking for a thesis to complete my master M2, I will work on a topic in the big data''s field (creation big data applications), using hadoop/mapReduce and Ecosystem ( visualisation, analysis ...), Please suggest some exciting topics that would make for a good masters thesis subject.<br>
Thanks ', 4705, '2014-10-19 11:02:44.397', '2c43084a-c3b8-4e99-b9ef-a6ef91a5ef57', 2308, 4374, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Masters thesis topics in big data', 4705, '2014-10-19 11:02:44.397', '2c43084a-c3b8-4e99-b9ef-a6ef91a5ef57', 2308, 4375, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop>', 4705, '2014-10-19 11:02:44.397', '2c43084a-c3b8-4e99-b9ef-a6ef91a5ef57', 2308, 4376, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First we need to understand why we need Deep learning. To build models ML need Test Data with Labels (supervised or unsupervised). In many domains as the data grows maintaining the data with labels is hard. Deep learning networks doesn''t need labeled data. The Deep learning algorithms can figure out the the labels. So this obviates the need for domain experts to come out with labels for the data which is very important in the areas of speech recognition, computer vision, and language understanding. Google Cat image recognition is very interesting experiment. Also it is interesting to know "Geoffrey hinton" the professor who was hired by Google.

http://www.wired.com/2014/01/geoffrey-hinton-deep-learning/

You may get the more insight as you explore in this framework.
', 4711, '2014-10-19 19:12:32.940', 'a1549f90-f720-457a-8021-463ef6583e86', 2311, 4379, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where is the difference between one-class, binary-class and multinominal-class classification?

If I would like to classify text in lets say four classes and also would like the system be able to tell me that none of these classes matches the unknown test-data.

Couldn''t I just use all the above mentioned methods to reach my goal? e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknown data-set ...

Just by saying, Training-Set for C1 contains class 1 (all good examples for C1) and class 0 (mix of all C2, C3 and C4 as bad examples for C1).

Is unlabeled-data C1 -> 1 or 0

Is unlabeled-data C2 -> 1 or 0 ...

and so on ...

For multinoninal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ...

But where is the difference between this two methods? (except of that I have to use different algorithms)

And how would I define a training-set for the above mentionend problem of categorizing data in those four classes using one class classfication (is that even possible)?

Excuse me if I''m completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =)
', 4717, '2014-10-20 06:38:16.490', '669a6452-4a6f-4b32-8180-c204cbf616a4', 2313, 4383, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine Learning - Where is the difference between one-class, binary-class and multinominal-class classification?', 4717, '2014-10-20 06:38:16.490', '669a6452-4a6f-4b32-8180-c204cbf616a4', 2313, 4384, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><classification><categorical-data>', 4717, '2014-10-20 06:38:16.490', '669a6452-4a6f-4b32-8180-c204cbf616a4', 2313, 4385, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Where is the difference between one-class, binary-class and multinominal-class classification?

If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data.

Couldn''t I just use all the methods that I mentioned above to reach my goal?
e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ...

Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1).

Is unlabeled-data C1 -> 1 or 0

Is unlabeled-data C2 -> 1 or 0
... and so on ...

For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ...

But where is the difference between this two methods? (except of that I have to use different algorithms)

And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)?

Excuse me if I''m completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =)', 4717, '2014-10-20 06:46:20.017', 'b8cfe849-d5be-4c03-9d20-96ee7d2a8480', 2313, 'deleted 5 characters in body', 4386, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A Random Forest (RF) is created by an ensemble of Decision Trees''s (DT). By using bagging, each DT is trained in a different data subset. Hence, **is there any way of implementing an on-line random forest by adding more decision tress on new data?**

For example, we have 10K samples and train 10 DT''s. Then we get 1K samples, and instead of training again the full RF, we add a new DT. The prediction is done now by the Bayesian average of 10+1 DT''s.

In addition, if we keep all the previous data, the new DT''s can be trained mainly in the new data, where the probability of picking a sample is weighted depending how many times have been already picked.', 4719, '2014-10-20 08:48:42.167', '0619bb83-bcee-4682-add1-3b698e945bc6', 2314, 4387, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('On-line random forests by adding more single Decisions Trees', 4719, '2014-10-20 08:48:42.167', '0619bb83-bcee-4682-add1-3b698e945bc6', 2314, 4388, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<random-forest><online-learning>', 4719, '2014-10-20 08:48:42.167', '0619bb83-bcee-4682-add1-3b698e945bc6', 2314, 4389, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Every ML algorithm with high complexity can overfit. However, the OP is asking wether an RF will not overfit when increasing the number of trees in the forest.

In general, ensemble methods reduces the prediction variance to almost nothing, improving the accuracy of the ensemble. If we define the variance of the expected generalization error of an individual randomized model as:

![enter image description here][1]

From [here][2], the variance of the expected generalization error of an ensemble corresponds to:

![enter image description here][3]

where `p(x)` is the Pearsons correlation coefficient between the predictions of two randomized models trained on the same data from two independent seeds. If we increase the number of DT''s in the RF, larger `M`, the variance of the ensemble decreases when `(x)<1`. Therefore, the variance of an ensemble is strictly smaller than the variance of an individual model.

In a nutshell, increasing the number of individual randomized models in an ensemble will never increase the generalization error.

  [1]: http://i.stack.imgur.com/ZUORL.gif
  [2]: http://arxiv.org/abs/1407.7502
  [3]: http://i.stack.imgur.com/5Zf9e.gif', 4719, '2014-10-20 09:31:18.650', '431d6f5b-3653-4ec7-aec0-1e7fd859d18c', 2315, 4390, '2');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-10-20 10:38:59.437', 'b1df1168-8904-4f9f-b764-ced60717bfc4', 2316, 'from http://stackoverflow.com/questions/26455402/machine-learning-where-is-the-difference-between-one-class-binary-class-and-m', 4391, '36');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where is the difference between one-class, binary-class and multinominal-class classification?

If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data.

Couldn''t I just use all the methods that I mentioned above to reach my goal?
e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ...

Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1).

Is unlabeled-data C1 -> 1 or 0

Is unlabeled-data C2 -> 1 or 0
... and so on ...

For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ...

But where is the difference between this two methods? (except of that I have to use different algorithms)

And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)?

Excuse me if I''m completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =)', 4717, '2014-10-19 21:03:35.390', 'e9afd7ef-9879-456e-88cc-61301ed288ff', 2316, 4392, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine Learning - Where is the difference between one-class, binary-class and multinominal-class classification?', 4717, '2014-10-19 21:03:35.390', 'e9afd7ef-9879-456e-88cc-61301ed288ff', 2316, 4393, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><data-mining>', 4717, '2014-10-19 21:03:35.390', 'e9afd7ef-9879-456e-88cc-61301ed288ff', 2316, 4394, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('*The Art of R Programming* by Normal Matloff is a great way to find your way towards being an R user.  I''ve recommended this book to several people navigating the tutorial / book universe and to my knowledge they''ve all stuck with it. ', 4724, '2014-10-20 15:32:58.097', 'a1edc438-27bd-4e27-876b-29a6e3bc1b39', 2317, 4395, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is an online data science "game" that takes you from learning how to use Python for loading a csv and using scikit to machine learning algorithms such as support vector machines. Here is a [blog post][1] with a demo video and the actual site is [Explore Data Science][2]. Personally, I think its genius.


  [1]: http://mentally-physically-spiritually-strong.com/2014/10/18/explore-data-science-game/
  [2]: https://exploredatascience.com', 3430, '2014-10-20 16:12:45.427', '7fd6f665-7339-4c84-b23e-2ec75db7648c', 2318, 4396, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To me Big Data is primarily about the tools (after all, that''s where it started); a "big" dataset is one that''s too big to be handled with conventional tools - in particular, big enough to demand storage and processing on a cluster rather than a single machine. This rules out a conventional RDBMS, and demands new techniques for processing; in particular, various Hadoop-like frameworks make it easy to distribute a computation over a cluster, at the cost of restricting the form of this computation. I''ll second the reference to http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html ; Big Data techniques are a last resort for datasets which are simply too big to handle any other way. I''d say any dataset for any purpose could qualify if it was big enough - though if the shape of the problem is such that existing "big data" tools aren''t appropriate, then it would probably be better to come up with a new name.

Of course there is some overlap; when I (briefly) worked at last.fm, we worked on the same 50TB dataset using Hadoop and also in an SQL database on a fairly ridiculous server (I remember it had 1TB RAM, and this is a few years ago). Which in a sense meant it both was and wasn''t big data, depending on which job you were working on. But I think that''s an accurate characterization; the people who worked on the Hadoop jobs found it useful to go to Big Data conferences and websites, while the people who worked on the SQL jobs didn''t.', 4730, '2014-10-20 19:40:34.193', 'fa228d3f-cc9f-4ad0-95bc-7061b69ae65c', 2320, 4402, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am drawing samples from two classes in the two-dimensional Cartesian space, each of which has the same covariance matrix (upper left to lower right: 2, 0, 0, 2).  One class has a mean of (1.5, 1) and the other has a mean of (1, 1.5).  If the priors are 4/7 for the former and 3/7 for the later, how would I derive the equation for the ideal decision boundary?

If it turns out that misclassifying the second class is twice as expensive as the first class, and the objective is to minimize the expected cost, what equation would I use for the best decision boundary?', 4736, '2014-10-20 22:13:32.420', '716531ff-ceef-4681-9bc2-d1114b9bd2be', 2321, 4403, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Bayes Optimal Decision Boundaries', 4736, '2014-10-20 22:13:32.420', '716531ff-ceef-4681-9bc2-d1114b9bd2be', 2321, 4404, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 4736, '2014-10-20 22:13:32.420', '716531ff-ceef-4681-9bc2-d1114b9bd2be', 2321, 4405, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are there any good sources that explain how decision trees can be implemented in a scalable way on a distributed computing system.  Where in a given source is this explained?', 4736, '2014-10-20 22:22:09.660', '2846433a-496e-4daf-a0a4-036ace504016', 2322, 4406, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Distributed Scalable Decision Trees', 4736, '2014-10-20 22:22:09.660', '2846433a-496e-4daf-a0a4-036ace504016', 2322, 4407, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><classification>', 4736, '2014-10-20 22:22:09.660', '2846433a-496e-4daf-a0a4-036ace504016', 2322, 4408, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can anyone explain how field aware factorization machines (FM) compare to standard FM?

Standard:
http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf

"Field Aware":
http://www.csie.ntu.edu.tw/~r01922136/kaggle-2014-criteo.pdf
', 1138, '2014-10-21 00:09:40.597', 'daf50710-e89f-46ec-bf09-4b2c01c8a22d', 2323, 4409, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Field Aware Factorization Machines', 1138, '2014-10-21 00:09:40.597', 'daf50710-e89f-46ec-bf09-4b2c01c8a22d', 2323, 4410, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><recommendation>', 1138, '2014-10-21 00:09:40.597', 'daf50710-e89f-46ec-bf09-4b2c01c8a22d', 2323, 4411, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Standard factorization machines have fields too. The "novelty" here seems to be the use of GBDT features and the application of the hashing tricks. Not to great effect, it seems: check out the minute range in performance on the last slide.', 381, '2014-10-21 01:34:13.407', '5b2321d7-e517-4121-9a41-678bc5d83baf', 2324, 4412, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''s [a recent paper on this subject](http://www.vision.cs.chubu.ac.jp/CV-R/pdf/AmirICCVW2009.pdf) (*On-line Random Forests*), coming from computer vision. [Here''s an implementation](http://www.ymer.org/amir/software/online-random-forests/), and a presentation: [Online random forests in 10 minutes](http://www.slideshare.net/CvilleDataScience/online-random-forest-in-10-minutes)', 381, '2014-10-21 02:43:26.790', '6e398d7d-a1da-4e15-bdd5-7e2378d81abe', 2325, 4413, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Apache Spark](http://spark.apache.org/) [can do it](http://spark-summit.org/2014/talk/scalable-distributed-decision-trees-in-spark-mllib), using the new [MLLib](http://spark.apache.org/docs/1.1.0/mllib-decision-tree.html) library. [Here''s a presentation](https://www.youtube.com/watch?v=3rRrcPXHu98), and [here are some benchmarks](http://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html). Bindings are available for python, scala, and java.', 381, '2014-10-21 06:12:51.887', '6c53b176-a17e-4d94-815e-fbf173f4f1cb', 2326, 4416, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have found the video tutorial/IPython notebook format really helped me get into the python ecosystem.

There were two tutorials at SciPy 2013 that cover sklearn ([part 1 of 1st tutorial][1], [github repo for notebooks][2]).

Similar tutorials, from PyCon2012 and PyData2012, are out there for pandas but I don''t have the rep to link searching for `pandas tutorial` on youtube should allow you to find them.


Since you mention Kaggle, I guess you will have seen their getting started with python tutorial for the titanic passenger dataset (I don''t have the rep here to provide a link but searching for `Getting Started with Python: Kaggle''s Titanic Competition` should get you there).


  [1]: https://www.youtube.com/watch?v=r4bRUvvlaBw
  [2]: https://github.com/jakevdp/sklearn_scipy2013
  [3]: https://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-python', 4739, '2014-10-21 07:06:21.580', 'da99b333-2597-4aae-9a58-0cf01269c2d4', 2327, 4417, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''re many data points, each of which is associated with two coordinates and a numeral value, or three coordinates. And I wish it is coloured.
I checked packages "scatterplot3d" and "plot3D" but I couldn''t find one like the example I give. It is like it has a fitting surface.

![enter image description here][1]


  [1]: http://i.stack.imgur.com/5whM7.png', 3443, '2014-10-21 09:42:51.610', 'ec57da2e-21e6-4e4e-a46c-334df42cfd69', 2328, 4418, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which packages or functions can I use in R to plot 3D data like this?', 3443, '2014-10-21 09:42:51.610', 'ec57da2e-21e6-4e4e-a46c-334df42cfd69', 2328, 4419, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><visualization>', 3443, '2014-10-21 09:42:51.610', 'ec57da2e-21e6-4e4e-a46c-334df42cfd69', 2328, 4420, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You could use the `wireframe` function from the `lattice` package:

    library("lattice")
    wireframe(volcano[1:30, 1:30], shade=TRUE, zlab="")

![wireframe plot example][1]


  [1]: http://i.stack.imgur.com/BQHGX.png', 2961, '2014-10-21 10:22:21.390', '4037f64d-1696-43e7-8f1e-494ad84e46a2', 2329, 4421, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There''re many data points, each of which is associated with two coordinates and a numeral value, or three coordinates. And I wish it is coloured.
I checked packages "scatterplot3d" and "plot3D" but I couldn''t find one like the example I give. It is like it has a fitting surface.

My data is basically like the following. In this way I think this kind of plot is gonna be perfectly suitble for this data:

        ki,kt,Top10AverageF1Score
        360,41,0.09371256716549396
        324,41,0.09539634212851525
        360,123,0.09473510831594467
        36,164,0.09773486852645874
        ...

But I also may have one more additional variable, which makes it like:

        NeighborhoodSize,ki,kt,Top10AverageF1Score
        10,360,41,0.09371256716549396
        15,324,41,0.09539634212851525
        15,360,123,0.09473510831594467
        20,36,164,0.09773486852645874
        ...

**Do you also have any good idea for visualizing the second case? What kind of plot and which packages and functions, etc.**
![enter image description here][1]


  [1]: http://i.stack.imgur.com/5whM7.png', 3443, '2014-10-21 12:50:19.847', 'c454f6e8-fcb9-4f26-949a-7082d539e0b8', 2328, 'added 746 characters in body', 4422, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How would I do parameter estimation and prediction for the adaptive regression model using R, as in the 4th page of the paper linked below?

http://papers.ssrn.com/sol3/papers.cfm?abstract_id=923635

Could anyone clarify this for me?

If you know adaptive regression models very well, share some useful link, or describe the model/parameter estimation/prediction, that would be very helpful.

Thank you so much!', 24, '2014-10-21 14:07:51.717', 'ec75b82c-7461-44ff-b871-a6fff59d6d48', 2278, 'Tightening up the wording to make the question clearer. Improved title and tags.', 4434, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Adaptive regression parameter estimation in R', 24, '2014-10-21 14:07:51.717', 'ec75b82c-7461-44ff-b871-a6fff59d6d48', 2278, 'Tightening up the wording to make the question clearer. Improved title and tags.', 4435, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<r><statistics><regression>', 24, '2014-10-21 14:07:51.717', 'ec75b82c-7461-44ff-b871-a6fff59d6d48', 2278, 'Tightening up the wording to make the question clearer. Improved title and tags.', 4436, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-10-21 14:07:51.717', 'ec75b82c-7461-44ff-b871-a6fff59d6d48', 2278, 'Proposed by 24 approved by 21 edit id of 162', 4437, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m just starting to develop a Machine Learning application for academic purposes. I''m currently using R and training myself in it. However, in a lot of places, I saw people using Python.

Could you tell me what people are using in academia and industry, and recommand one ?', 883, '2014-10-21 14:07:56.087', '3c9c87a0-819c-4f0f-8f6d-e83067185b19', 326, 'reformulation', 4438, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-10-21 14:07:56.087', '3c9c87a0-819c-4f0f-8f6d-e83067185b19', 326, 'Proposed by 883 approved by 21 edit id of 163', 4439, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I hope you can help me, as I have some questions on this topic. I''m new in the field of deep learning, and while I did some tutorials, I can''t relate or distinguish concepts from one another. I hope that some specialist of this community can answer a few questions and help me to understand whats deep learning really is. Neural networks? Special methods? Word vector representation? Information extraction? I need a clear picture of the field. Thanks a lot in advance, here are my questions:

a) Why is there a lot of interest in the NLP and ML communities for deep learning architectures? What are deep learning architectures? *I think Multi-Layer Neural (Tensor) Networks, so called deep networks, like Socher uses it?*

b) Why do the NLP and ML communities need approaches to learn complex non-linear relationships? *What''s a good example of complex nonlinear relationships? I think Single-Layer Neural networks can only distinguish linear input data according to their activation function, and because they have input data that can''t easily be separated with a straight line. Deep networks with ML are needed for processing complex data. Maybe they need them because they are using multi-dimensional word vector spaces?*

c) What are deep learning methods: for example back-propagation, or is it a neural tensor network, or is it the word vector representation? *I can''t distinguish what are deep learning architectures and methods? Or are they the same?*

e) And how can I relate deep learning to open information extraction? And what are good examples for the application of deep learning methods for open information extraction? *My approach is, that open information extraction is extraction of knowlegede on any domain with any relations from unstructured data (like webpages) with the target to aquiring knowledge. If you do this for a specific domain, you have to optimize your NLP tools manually to this domain and relations, but in other domain that doesn''t work well. Therefore "deep learning"(clearer: deep neural networks, recursive or tensor NN) architectures are interessting, because they use unsupervised learn to extract knowlegde and need no manual optimization relating to a topic.*
*A method are word vectors for representing words and not specific programmed model. The "deep learning" in the NLP field is, that the NN learn the before manual models automatically and mapping the words in a vector space. An exmaple is Socher''s Recursive Deep Models for Semantic Compositionality, where deep learning is used to extract the positive or negative statement of a sentence.*

*Is this approach true, or am I looking at this wrong?*

I would be grateful for any comments. Thanks in advance!
', 24, '2014-10-21 14:08:02.317', 'e90b512a-ead3-4347-88f2-b805c17e1b5f', 1253, 'spelling and grammar corrections', 4440, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Why the interest in deep learning within the NLP and Machine Learning communities?', 24, '2014-10-21 14:08:02.317', 'e90b512a-ead3-4347-88f2-b805c17e1b5f', 1253, 'spelling and grammar corrections', 4441, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-10-21 14:08:02.317', 'e90b512a-ead3-4347-88f2-b805c17e1b5f', 1253, 'Proposed by 24 approved by 21 edit id of 161', 4442, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"OriginalQuestionIds":[2313],"Voters":[{"Id":178,"DisplayName":"Christopher Louden"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-10-21 14:09:06.617', '1eaa2354-738a-4d09-9912-d6e69ad62385', 2316, '101', 4444, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am looking for a thesis to complete my master M2, I will work on a topic in the big data''s field (creation big data applications), using hadoop/mapReduce and Ecosystem ( visualisation, analysis ...), Please suggest some topics or project that would make for a good masters thesis subject.<br>
I add that I have bases in data warehouses, databases, data mining, good skills in programming, system administration and cryptgraphy ... <br>

Thanks ', 4705, '2014-10-21 20:36:21.553', '4cd0d19c-53b5-458d-9ef6-5455e69c8ea7', 2308, 'added 148 characters in body', 4446, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am looking for a thesis to complete my master M2, I will work on a topic in the big data''s field (creation big data applications), using hadoop/mapReduce and Ecosystem ( visualisation, analysis ...), Please suggest some topics or project that would make for a good masters thesis subject.<br>
I add that I have bases in data warehouses, databases, data mining, good skills in programming, system administration and cryptography ... <br>

Thanks ', 4705, '2014-10-21 21:17:25.727', '83b08d9d-8dc0-44f7-86e2-f97bc0cb797b', 2308, 'added 1 character in body', 4447, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Since it''s a master''s thesis, how about writing something regarding decision trees, and their "upgrades": boosting and Random Forests? And then integrate that with Map/Reduce, together with showing how to scale a Random Forest on Hadoop using M/R?', 1127, '2014-10-22 08:30:00.990', 'eae17e4b-d91d-4439-a0ad-b9b50469a3a9', 2332, 4449, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am trying to find a formula, method, or model to use to analyze the likelihood that a specific event influenced some longitudinal data. I am having difficultly figuring out what to search for on Google.

Here is an example scenario:

Image you own a business that has an average of 100 walk-in customers every day. One day, you decide you want to increase the number of walk-in customers arriving at your store each day, so you pull a crazy stunt outside your store to get attention. Over the next week, you see on average 125 customers a day.

Over the next few months, you again decide that you want to get some more business, and perhaps sustain it a bit longer, so you try some other random things to get more customers in your store. Unfortunately, you are not the best marketer, and some of your tactics have little or no effect, and others even have a negative impact.

What methodology could I use to determine the probability that any one individual event positively or negatively impacted the number of walk-in customers? I am fully aware that correlation does not necessarily equal causation, but what methods could I use to determine the likely increase or decrease in your business''s daily walk in client''s following a specific event?

I am not interested in analyzing whether or not there is a correlation between your attempts to increase the number of walk-in customers, but rather whether or not any one single event, independent of all others, was impactful.

I realize that this example is rather contrived and simplistic, so I will also give you a brief description of the actual data that I am using:

I am attempting to determine the impact that a particular marketing agency has on their client''s website when they publish new content, perform social media campaigns, etc. For any one specific agency, they may have anywhere from 1 to 500 clients. Each client has websites ranging in size from 5 pages to well over 1 million. Over the course of the past 5 year, each agency has annotated all of their work for each client, including the type of work that was done, the number of webpages on a website that were influenced, the number of hours spent, etc.

Using the above data, which I have assembled into a data warehouse (placed into a bunch of star/snowflake schemas), I need to determine how likely it was that any one piece of work (any one event in time) had an impact on the traffic hitting any/all pages influenced by a specific piece of work. I have created models for 40 different types of content that are found on a website that describes the typical traffic pattern a page with said content type might experience from launch date until present. Normalized relative to the appropriate model, I need to determine the highest and lowest number of increased or decreased visitors a specific page received as the result of a specific piece of work.

While I have experience with basic data analysis (linear and multiple regression, correlation, etc), I am at a loss for how to approach solving this problem. Whereas in the past I have typically analyzed data with multiple measurements for a given axis (for example temperature vs thirst vs animal and determined the impact on thirst that increased temperate has across animals), I feel that above, I am attempting to analyze the impact of a single event at some point in time for a non-linear, but predictable (or at least model-able), longitudinal dataset. I am stumped :(

Any help, tips, pointers, recommendations, or directions would be extremely helpful and I would be eternally grateful!
', 134, '2014-10-22 12:07:33.977', '2b41ca08-94a7-4087-b30d-78961bdf48a3', 497, 'You don''t need to put thanks, regards etc... ', 4455, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-10-22 12:07:33.977', '2b41ca08-94a7-4087-b30d-78961bdf48a3', 497, 'Proposed by 134 approved by 21 edit id of 164', 4456, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to analyze [MovieLens data set][1] and load on my machine the M1 file. I combine actually two data files (ratings.dat and movies.dat) and sort the table according `''userId''` column. The head of my DataFrame looks like here (all columns values are corresponding to the original data sets):

    In [36]: df.head(10)
    Out[36]:
            userId  movieId  Rating       Time                         movieName  \
    40034        1      150       5  978301777                  Apollo 13 (1995)
    77615        1     1028       5  978301777               Mary Poppins (1964)
    550485       1     2018       4  978301777                      Bambi (1942)
    400889       1     1962       4  978301753         Driving Miss Daisy (1989)
    787274       1     1035       5  978301753        Sound of Music, The (1965)
    128308       1      938       4  978301752                       Gigi (1958)
    497972       1     3105       5  978301713                 Awakenings (1990)
    28417        1     2028       5  978301619        Saving Private Ryan (1998)
    6551         1     1961       5  978301590                   Rain Man (1988)
    35492        1     2692       4  978301570  Run Lola Run (Lola rennt) (1998)

                                genre
    40034                       Drama
    77615   Children''s|Comedy|Musical
    550485       Animation|Children''s
    400889                      Drama
    787274                    Musical
    128308                    Musical
    497972                      Drama
    28417            Action|Drama|War
    6551                        Drama
    35492        Action|Crime|Romance

    [10 rows x 6 columns]

I can not understand that the same user with user Id 1 see the different movies (Apollo13 (Id:150), Mary Poppins (Id:1028) and Bambi (Id2018) exactly at the same time (till the milleseconds). If somebody works already with this data set, please, clear this situation.

  [1]: http://grouplens.org/datasets/movielens/', 3281, '2014-10-22 14:53:42.127', '17da3915-9436-40c2-984e-0b64cdb7364a', 2334, 4457, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('MovieLens data set', 3281, '2014-10-22 14:53:42.127', '17da3915-9436-40c2-984e-0b64cdb7364a', 2334, 4458, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><pandas>', 3281, '2014-10-22 14:53:42.127', '17da3915-9436-40c2-984e-0b64cdb7364a', 2334, 4459, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I want to analyze [MovieLens data set][1] and load on my machine the M1 file. I combine actually two data files (ratings.dat and movies.dat) and sort the table according `''userId''` and `''Time''` columns. The head of my DataFrame looks like here (all columns values are corresponding to the original data sets):

    In [36]: df.head(10)
    Out[36]:
            userId  movieId  Rating       Time                         movieName  \
    40034        1      150       5  978301777                  Apollo 13 (1995)
    77615        1     1028       5  978301777               Mary Poppins (1964)
    550485       1     2018       4  978301777                      Bambi (1942)
    400889       1     1962       4  978301753         Driving Miss Daisy (1989)
    787274       1     1035       5  978301753        Sound of Music, The (1965)
    128308       1      938       4  978301752                       Gigi (1958)
    497972       1     3105       5  978301713                 Awakenings (1990)
    28417        1     2028       5  978301619        Saving Private Ryan (1998)
    6551         1     1961       5  978301590                   Rain Man (1988)
    35492        1     2692       4  978301570  Run Lola Run (Lola rennt) (1998)

                                genre
    40034                       Drama
    77615   Children''s|Comedy|Musical
    550485       Animation|Children''s
    400889                      Drama
    787274                    Musical
    128308                    Musical
    497972                      Drama
    28417            Action|Drama|War
    6551                        Drama
    35492        Action|Crime|Romance

    [10 rows x 6 columns]

I can not understand that the same user with user Id 1 see or rated the different movies (Apollo13 (Id:150), Mary Poppins (Id:1028) and Bambi (Id:2018) exactly at the same time (till the milleseconds). If somebody works already with this data set, please, clear this situation.

  [1]: http://grouplens.org/datasets/movielens/', 3281, '2014-10-22 15:05:29.933', 'a57ed009-ea5a-430a-bfce-c68fe18de03b', 2334, 'added 24 characters in body', 4460, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I want to analyze [MovieLens data set][1] and load on my machine the M1 file. I combine actually two data files (ratings.dat and movies.dat) and sort the table according `''userId''` and `''Time''` columns. The head of my DataFrame looks like here (all columns values are corresponding to the original data sets):

    In [36]: df.head(10)
    Out[36]:
            userId  movieId  Rating       Time                         movieName  \
    40034        1      150       5  978301777                  Apollo 13 (1995)
    77615        1     1028       5  978301777               Mary Poppins (1964)
    550485       1     2018       4  978301777                      Bambi (1942)
    400889       1     1962       4  978301753         Driving Miss Daisy (1989)
    787274       1     1035       5  978301753        Sound of Music, The (1965)
    128308       1      938       4  978301752                       Gigi (1958)
    497972       1     3105       5  978301713                 Awakenings (1990)
    28417        1     2028       5  978301619        Saving Private Ryan (1998)
    6551         1     1961       5  978301590                   Rain Man (1988)
    35492        1     2692       4  978301570  Run Lola Run (Lola rennt) (1998)

                                genre
    40034                       Drama
    77615   Children''s|Comedy|Musical
    550485       Animation|Children''s
    400889                      Drama
    787274                    Musical
    128308                    Musical
    497972                      Drama
    28417            Action|Drama|War
    6551                        Drama
    35492        Action|Crime|Romance

    [10 rows x 6 columns]

I can not understand that the same user with user Id 1 see or rated the different movies (Apollo13 (Id:150), Mary Poppins (Id:1028) and Bambi (Id:2018) exactly at the same time (up to the milleseconds). If somebody works already with this data set, please, clear this situation.

  [1]: http://grouplens.org/datasets/movielens/', 3281, '2014-10-22 15:43:44.803', '9ea055cd-8a45-4630-8e06-dcce52844db1', 2334, 'added 1 character in body', 4461, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When you enter ratings on movie lens, you get pages with 10 movies or so. You set all the ratings, then submit by clicking "next page" or something.
So I guess all the ratings for the same page are received at the same time, when you submit the page.', 4760, '2014-10-22 15:50:43.230', '1d70aa1a-95d5-4d94-a65d-e46ab181c42d', 2335, 4462, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am not sure whether I formulated the question correctly. Basically, what I want to do is:

Let''s suppose I have a list of 1000 strings which look like this:

cvzxcvzx**string**cvzcxvz

otortorotr**string**grptprt

vmvmvmeop**string2**vmrprp

vccermpqp**string2**rowerm

proororor**string3**potrprt

mprto2435**string3**famerpaer

etc.

I''d like to extract these reoccuring strings that occur on the list. What solution should I use? Does anyone know about algorithm that could do this?', 4774, '2014-10-23 14:51:57.160', '58b3ad52-4ebc-4043-be49-141ebac040ae', 2337, 4466, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Clustering strings inside strings?', 4774, '2014-10-23 14:51:57.160', '58b3ad52-4ebc-4043-be49-141ebac040ae', 2337, 4467, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><text-mining><feature-extraction>', 4774, '2014-10-23 14:51:57.160', '58b3ad52-4ebc-4043-be49-141ebac040ae', 2337, 4468, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In my experience, the answer depends on the project at hand. For pure research, I prefer R for two reasons: 1) broad variety of libraries and 2) much of the data science literature includes R samples.

If the project requires an interactive interface to be used by laypersons, I''ve found R to be too constrained. Shiny is a great start, but it''s not flexible enough yet. In these cases, I''ll start to look at porting my R work over to Python or js.', 4776, '2014-10-23 15:50:09.820', '2613fb13-90b7-44b9-9c92-aab70d9049b5', 2338, 4472, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Interesting question! I have not encountered it before so here is a solution I just made up, inspired by the approach taken by the word2vec paper:

1. Define the pair-wise similarity based on the longest common substring (LCS), or the LCS normalized by the products of the string lengths. Cache this in a matrix for any pair of strings considered since it is expensive to calculate. Also consider approximations.

2. Find a Euclidean (hyperspherical, perhaps?) embedding that minimizes the error (Euclidean distance if using the ball, and the dot product is using the sphere). Assume random initialization, and using a gradient-based method by taking the Jacobian of the error.

3. Now you have a Hilbert space embedding, so cluster using your algorithm of choice!', 381, '2014-10-23 17:10:40.890', '53bbc1b4-3753-4dc7-87e8-9b8d3192d41a', 2339, 4473, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Interesting question! I have not encountered it before so here is a solution I just made up, inspired by the approach taken by the word2vec paper:

1. Define the pair-wise similarity based on the longest common substring (LCS), or the LCS normalized by the products of the string lengths. Cache this in a matrix for any pair of strings considered since it is expensive to calculate. Also consider approximations.

2. Find a Euclidean (hyperspherical, perhaps?) embedding that minimizes the error (Euclidean distance if using the ball, and the dot product is using the sphere). Assume random initialization, and using a gradient-based method by taking the Jacobian of the error.

3. Now you have a Hilbert space embedding, so cluster using your algorithm of choice!

*Response to first comment*: The bulk of the complexity lies in the first stage; the calculation of the LCS, so it depends on efficiently you do that. I''ve had luck with genetic algorithms. Anyway, what you''d do in this case is define a similarity _vector_ rather than a scalar, whose elements are the k-longest pair-wise LCS; see [this](http://cstheory.stackexchange.com/questions/8361/algorithm-find-the-first-k-longest-substrings-between-two-similar-strings) discussion for algorithms. Then I would define the error by the sum of the errors corresponding to each substring.

Something I did not address is how to choose the dimensionality of the embedding. The word2vec paper might provide some heuristics; see [this](https://groups.google.com/forum/#!topic/word2vec-toolkit/HRvNPIqe6mM) discussion. I recall they used pretty big spaces, on the order of a 1000 dimensions, but they were optimizing something more complicated, so I suggest you start at R^2 and work your way up. Of course, you will want to use a higher dimensionality for the multiple LCS case.', 381, '2014-10-23 18:46:30.427', '86f8e734-d06f-4da9-87b8-3e60a13e3f68', 2339, 'responded to comment', 4474, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you just want to get a forecast, use automatic forecasting software like Autobox, ForecastPro, SCA, etc.

- Comparison of different software: http://stats.stackexchange.com/questions/68253/expert-forecasting-software-evaluation
- Worked example: http://people.duke.edu/~rnau/autocomp.htm


If you are doing research on forecasting techniques, there are some Java packages, for example:

- JMotif http://code.google.com/p/jmotif/
- JTSA http://sourceforge.net/projects/jtsa/

However, Java is not very widely used for this; if you must use Java you will likely have a feeling of being out in the cold.   R and Python would have much more packages, examples, etc.  A tiny sample of what is available in R:

- http://www.amazon.com/Introductory-Time-Series-Paul-Cowpertwait/dp/0387886974/
- http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/
- http://www.r-bloggers.com/time-series-analysis-and-mining-with-r/
', 26, '2014-10-23 20:11:18.750', 'de45318e-868e-4027-8d28-297fcc0f8ea1', 2340, 4475, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would think about starting with a power analysis: i.e. how many data points do you need to measure the effect (or parameter) that you are interested to a specified level of confidence, *ceteris paribus*? Then, you estimate a cost. ', 4779, '2014-10-23 21:12:27.350', '6c6441c4-46f1-4f49-b7fc-166e7a452b88', 2341, 4476, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Is there a good java library for doing time series energy consumption forecasting based on weather data and other variables?', 21, '2014-10-23 21:56:54.693', '5369cb40-31a8-402b-b1a9-14433417f217', 2304, 'deleted 16 characters in body; edited tags', 4477, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<time-series><java><energy>', 21, '2014-10-23 21:56:54.693', '5369cb40-31a8-402b-b1a9-14433417f217', 2304, 'deleted 16 characters in body; edited tags', 4478, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You should read the paper from Google on PLANET, which was their distributed MapReduce-based implementation of random decision forests: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36296.pdf

You may or may not like the architecture but there are a number of interesting ideas about scaling up here.', 21, '2014-10-23 21:59:32.713', '6069fb75-97f7-4c76-8edb-44e855c64154', 2342, 4480, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<nlp><treebank>', 21, '2014-10-24 12:32:42.503', '587d57d0-36a2-430e-bbb4-c2eedec45cf9', 2306, 'edited tags', 4481, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Interesting question! I have not encountered it before so here is a solution I just made up, inspired by the approach taken by the word2vec paper:

1. Define the pair-wise similarity based on the longest common substring (LCS), or the LCS normalized by the products of the string lengths. Cache this in a matrix for any pair of strings considered since it is expensive to calculate. Also consider approximations.

2. Find a Euclidean (hyperspherical, perhaps?) embedding that minimizes the error (Euclidean distance if using the ball, and the dot product if using the sphere). Assume random initialization, and use a gradient-based optimization method by taking the Jacobian of the error.

3. Now you have a Hilbert space embedding, so cluster using your algorithm of choice!

*Response to deleted comment asking how to cluster multiple substrings*: The bulk of the complexity lies in the first stage; the calculation of the LCS, so it depends on efficiently you do that. I''ve had luck with genetic algorithms. Anyway, what you''d do in this case is define a similarity _vector_ rather than a scalar, whose elements are the k-longest pair-wise LCS; see [this](http://cstheory.stackexchange.com/questions/8361/algorithm-find-the-first-k-longest-substrings-between-two-similar-strings) discussion for algorithms. Then I would define the error by the sum of the errors corresponding to each substring.

Something I did not address is how to choose the dimensionality of the embedding. The word2vec paper might provide some heuristics; see [this](https://groups.google.com/forum/#!topic/word2vec-toolkit/HRvNPIqe6mM) discussion. I recall they used pretty big spaces, on the order of a 1000 dimensions, but they were optimizing something more complicated, so I suggest you start at R^2 and work your way up. Of course, you will want to use a higher dimensionality for the multiple LCS case.', 381, '2014-10-24 18:35:41.350', '04034d95-c014-48dc-b7f6-7228dd134b6f', 2339, 'added 55 characters in body', 4482, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I need some serious help. I am supposed to implement a project (Non-Existing as of now) for my Machine Learning course. I have no basics in AI or Data mining or Machine learning. I have been searching for a while and unable to find something that i can finish implementing in 3-4 weeks time. It carries a huge chunk of my final marks and no matter how much i try i am unable to understand how it works!
Can the machine learning masters please help me out with this. I need a project suggestion to start with. And i want to know how to proceed after gathering the data set. I am totally blank and running out of time for my graduation :(

Appreciate your suggestions! Thanks in advance.', 4794, '2014-10-24 23:13:07.803', 'e593903b-09e1-4391-8b9c-c51015dca768', 2343, 4483, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Small project ideas for Machine Learning', 4794, '2014-10-24 23:13:07.803', 'e593903b-09e1-4391-8b9c-c51015dca768', 2343, 4484, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><data-mining><clustering><algorithms>', 4794, '2014-10-24 23:13:07.803', 'e593903b-09e1-4391-8b9c-c51015dca768', 2343, 4485, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a question about memory usage.

I want to do 4 things:

    1) make a dataframe from one of several columns from a datasource, say a json string
    2) make the third column of the original dataset the index to the dataframe
    3) change the name of another column
    4) change the series i''ve created to a dataframe

My question is about memory efficiency. It seems that for step 1), I am first loading a whole dataframe, then run a concat command to concatenate the columns I want.

For step 2, I again need to resave the new dataframe as another object.

For step 3, it seems to stick so nothing there.

Please advise on a more efficient way to go about this, if that exists.

Command:

       df = pd.DataFrame(jsonobject)
       df = df.set_index("columnC")
       df.index.names= ["foo"]
       df1 = df["foo"].map(lambda x:x["id"])
       df2 = pd.DataFrame(df1)', 1223, '2014-10-25 01:36:27.483', '8d80a920-9ba1-4b38-bbbc-d987c85099c2', 2344, 4486, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('pandas dataframes memory', 1223, '2014-10-25 01:36:27.483', '8d80a920-9ba1-4b38-bbbc-d987c85099c2', 2344, 4487, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<pandas>', 1223, '2014-10-25 01:36:27.483', '8d80a920-9ba1-4b38-bbbc-d987c85099c2', 2344, 4488, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Gather million twitter user accounts. Then try to guess their gender based on their avatar, tweets and who they follow.', 4774, '2014-10-25 08:40:14.613', 'da170f6e-4ff5-4355-86b6-5a1c2bac4f85', 2345, 4489, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve looked at all the other sections on stackexchange and haven''t found a better fit for this question. Sorry if it''s misplaced. If it is, feel free to downvote, but could you please also drop a recommendation where the question would be better suited? Thanks!

I work in an analytical role at a a large financial services firm. We do a ton of daily reporting over metrics that rarely change in a meaningful way from day to day. From this daily reporting, our management is required to extract what was important yesterday and what important trends have developed / are developing over time.

I want to change this to a model of daily exception reporting and weekly trend reporting.

Features might include:

- User report consolidation (so there''s only one daily email)
- report ordering based upon level of variance from part performance (see the most important stuff first)
- html email support (with my audience, pretty counts)
- web interface to allow preference changes, including LDAP support (make administration easier)
- Unsubscribe feature at the report level

If tackled myself, I assume I''d be using a mix of Python, SQL, and powershell.
Is this being done in any open software? Is there anyplace that would be a good resource?

Thanks,
Brandon
', 4800, '2014-10-25 22:37:18.523', 'cd80e188-1a77-46f6-8632-81b47e212b60', 2346, 4490, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Business exception reporting', 4800, '2014-10-25 22:37:18.523', 'cd80e188-1a77-46f6-8632-81b47e212b60', 2346, 4491, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation>', 4800, '2014-10-25 22:37:18.523', 'cd80e188-1a77-46f6-8632-81b47e212b60', 2346, 4492, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here''s some practical advice from my own experiences-

1. Convince management the the change will be for the better.

2. Even if management agrees, they''ll still want to know why variances have occurred, and hence you''ll need to be able to supply more data.

3. Although exception reporting is best, management will want to see everything anyway, as doing so makes them feel as though they are doing something useful.

4. Don''t change everything at once- too big a change at once can cause resistance.

5. For how to best present the data, read Edward Tufte''s books, at the very least his first one.', 1241, '2014-10-25 23:07:15.843', '38bad640-7690-4e79-84bc-491c63187902', 2347, 4493, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have Train and Test data, how to calculate classification accuracy with evaluation top-label ?

    @attribute outlook {0, 1, 2}
    @attribute temperature {0, 1, 2}
    @attribute humidity {0, 1}
    @attribute windy {0, 1}
    @attribute play {0, 1}

Folds: 0

Train:

    1, 0, 0, 1, 0
    2, 1, 0, 1, 0
    2, 2, 1, 1, 0
    2, 2, 1, 0, 1
    1, 2, 1, 0, 0
    0, 1, 0, 1, 1
    0, 2, 1, 1, 0
    2, 1, 1, 1, 0
    0, 1, 1, 0, 0
    1, 1, 0, 0, 0
    1, 0, 1, 1, 0
    2, 1, 0, 0, 1

Test:

    0, 0, 0, 1, 1
    0, 0, 0, 0, 1


Rules found:

    (outlook,1) -> (play,0) [Support=0.33 , Confidence=1.00 , Correctly Classify= 2, 6, 11, 12]
    (humidity,1), (windy,1) -> (play,0) [Support=0.33 , Confidence=1.00 , Correctly Classify= 4, 8, 9]
    (outlook,2), (windy,1) -> (play,0) [Support=0.25 , Confidence=1.00 , Correctly Classify= 3]
    (outlook,0), (humidity,1) -> (play,0) [Support=0.17 , Confidence=1.00 , Correctly Classify= 10]
    (outlook,2), (windy,0) -> (play,1) [Support=0.17 , Confidence=1.00 , Correctly Classify= 5, 13]
    (outlook,0), (humidity,0) -> (play,1) [Support=0.08 , Confidence=1.00 , Correctly Classify= 7]

Top-label:

![enter image description here][1]

  [1]: http://i.stack.imgur.com/5kzZU.png', 3503, '2014-10-25 23:46:14.493', 'b0a7b671-80b6-4b34-9c94-8c381fa7c8fe', 2348, 4494, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to calculate classification accuracy?', 3503, '2014-10-25 23:46:14.493', 'b0a7b671-80b6-4b34-9c94-8c381fa7c8fe', 2348, 4495, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><accuracy>', 3503, '2014-10-25 23:46:14.493', 'b0a7b671-80b6-4b34-9c94-8c381fa7c8fe', 2348, 4496, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For my Computational Intelligence class I''m working on classifying short text. One of the papers that I''ve found makes a lot of use of granular computing, but I''m struggling to find a decent explanation of what exactly it is. From what I can gather from the paper it sounds to me like granular computing is very similar to fuzzy sets. So, what exactly is the difference. I''m asking about rough sets as well, because I''m curious about them and how they relate to fuzzy sets. If at all.

', 4804, '2014-10-26 13:12:23.597', 'ffd578ed-e274-436d-bb93-83178e7150df', 2349, 4497, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can someone explain rough vs fuzzy vs granular to me?', 4804, '2014-10-26 13:12:23.597', 'ffd578ed-e274-436d-bb93-83178e7150df', 2349, 4498, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><classification>', 4804, '2014-10-26 13:12:23.597', 'ffd578ed-e274-436d-bb93-83178e7150df', 2349, 4499, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For my Computational Intelligence class I''m working on classifying short text. One of the papers that I''ve found makes a lot of use of granular computing, but I''m struggling to find a decent explanation of what exactly it is. From what I can gather from the paper it sounds to me like granular computing is very similar to fuzzy sets. So, what exactly is the difference. I''m asking about rough sets as well, because I''m curious about them and how they relate to fuzzy sets. If at all.


Edit:
Here is the paper I''m referencing
http://ijcai.org/papers11/Papers/IJCAI11-298.pdf', 4804, '2014-10-26 13:18:38.950', '8bd00809-ddd6-4681-8807-9976bb7984dc', 2349, 'added 96 characters in body', 4500, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('"**Granularity**" refers to the ***resolution*** of the variables under analysis.  If you are analyzing *height* of people, you could use *course-grained variables* that have only a few possible values -- e.g. "above-average, average, below-average" -- or a *fine-grained variable*, with many or an infinite number of values -- e.g. integer values or real number values.

A measure is "**fuzzy**" if the distinction between alternative values is not crisp.  In the course-grained variable for *height*, a "crisp" measure would mean that any given individual could *only* be assigned one value -- e.g. a tall-ish person is either "above-average", or "average".  In contrast, a "fuzzy" measure allows for *degrees of membership* for each value, with "membership" taking values from 0 to 1.0.  Thus, a tall-ish person could be a value of "0.5 above-average", "0.5 average", "0.0 below-average".

Finally, a measure is "**rough**" when two values are given: upper and lower bounds as an estimate of the "crisp" measure.  In our example of a tall-ish person, the rough measure would be {UPPER = above-average, LOWER = average}.

Why use granular, fuzzy, or rough measures at all, you might ask?  Why not measure everything in nice, precise real numbers?  Because many real-world phenomena don''t have a good, reliable intrinsic measure and measurement procedure that results in a real number.  If you ask married couples to rate the quality of their marriage on a scale from 1 to 10, or 1.00 to 10.00, they might give you a number (or range of numbers), but how reliable are those reports? Using a course-grained measure (e.g. "happy", "neutral/mixed", "unhappy"), or fuzzy measure, or rough measure can be more reliable and more credible in your analysis.  Generally, it''s much better to use rough/crude measures well than to use precise/fine-grained measures poorly.', 609, '2014-10-26 19:36:07.113', 'fd999e7c-9f1a-45c6-b974-0d8c713d82b3', 2350, 4501, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think the most likely explanation is that the two libraries do not quite support TreeModel in PMML in the same way. Perhaps one only supports a subset of features, and ignores ones it does not understand. This could cause different scoring.

I''d also double check that upstream parsing code is the same in both cases. Maybe a missing value is treated differently upstream.', 21, '2014-10-26 20:10:06.923', '52c86be2-b15f-4f42-958e-fdac9e633fb1', 2351, 4502, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('

I have data collected from a computer simulation of football games which seem to have recurring patterns of the following form.

*if madrid plays arsernal and the match ends under 3 goal, then on their next match against each others, madrid will win. if madrid happens to loose and then plays against chelsea next, they will win 90% of the time.*

how do I find such inferences from simulation generated data like this. There are other forms of hidden patterns that I believe exists in the dataset.
', 4811, '2014-10-26 20:11:22.317', '1f5db5b9-358a-493e-a192-d97789ee5931', 2352, 4503, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('making logical inference from a simuation generated data', 4811, '2014-10-26 20:11:22.317', '1f5db5b9-358a-493e-a192-d97789ee5931', 2352, 4504, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining>', 4811, '2014-10-26 20:11:22.317', '1f5db5b9-358a-493e-a192-d97789ee5931', 2352, 4505, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I cant seem to figure out why I have a high percentage error. I''m trying to get a perceptron between X1 and X2 which are Gaussian distributed data sets with distinct means and identical covariances. My code:

    N=200;
    C= [2 1; 1 2]; %Covariance
    m1=[0 2];
    m2=[1.5 0];%mean
    X1 = mvnrnd(m1, C, N/2);
    X2 = mvnrnd(m2, C, N/2);

    X = [X1; X2];
    X = [X ones(N,1)]; %bias
    y = [-1*ones(N/2,1); ones(N/2,1)]; %classification

    %Split data into training and test
    ii = randperm(N);
    Xtr = X(ii(1:N/2),:);
    ytr = X(ii(1:N/2),:);
    Xts = X(ii(N/2+1:N),:);
    yts = y(ii(N/2+1:N),:);
    Nts = N/2;

    w = randn(3,1);
    eta = 0.001;
    %learn from training set
    for iter=1:500
    j = ceil(rand*N/2);
    if( ytr(j)*Xtr(j,:)*w < 0)
    w = w + eta*Xtr(j,:)'';
    end
    end

    %apply what you have learnt to test set
    yhts = Xts * w;
    disp([yts yhts])
    PercentageError = 100*sum(yts .*yhts < 0)/Nts;

Any help would be appreciated. Thank you
', 4813, '2014-10-26 20:35:39.767', '56b44798-0f08-4fc8-9a24-172b34640c3d', 2353, 4506, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('MATLAB Perceptron', 4813, '2014-10-26 20:35:39.767', '56b44798-0f08-4fc8-9a24-172b34640c3d', 2353, 4507, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 4813, '2014-10-26 20:35:39.767', '56b44798-0f08-4fc8-9a24-172b34640c3d', 2353, 4508, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think your best bet would be **[rosetta](http://pythonhosted.org/rosetta/)**. I''m finding it extremely useful and easy. Check its [pandas methods](http://pythonhosted.org/rosetta/#module-rosetta.parallel.pandas_easy).

You can get it by **pip**.', 4815, '2014-10-27 04:15:00.747', '3cd8f8af-2d9e-414f-a10d-60fda584a926', 2354, 4509, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Sorry, if this topic is not connect direct to the Data Science...
I try to understand now the [Graphlab tool][1] works. Firstly I want to execute the toy examples from Gallery site. When I try to execute the example code, everything is ok excepted one command: I can not see the graphlab plot after `show()`. The command `show()` return me some kind of object in IPython and nothing in IPython Notebook.
If the example code has the plot, which depends direct from matplotlib module, I can produce the real plots and save it on my machine. Consequently I suppose the main error is depended from the graphlab (or object from  its class).
If somebody already used this tool and got the plot, can he/she tell me, how I can execute the plots command.




  [1]: http://graphlab.com/learn/gallery/index.html', 3281, '2014-10-27 09:55:36.887', 'ff467c51-5a67-407d-80f8-34e2db4df350', 2355, 4510, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Graphlab, plots are invisible', 3281, '2014-10-27 09:55:36.887', 'ff467c51-5a67-407d-80f8-34e2db4df350', 2355, 4511, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<graphs>', 3281, '2014-10-27 09:55:36.887', 'ff467c51-5a67-407d-80f8-34e2db4df350', 2355, 4512, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Sorry, if this topic is not connect direct to the Data Science...
I try to understand now the [Graphlab tool][1] works. Firstly I want to execute the toy examples from Gallery site. When I try to execute the example code, everything is ok excepted one command: I can not see the graphlab plot after `show()`. The command `show()` return me some kind of object in IPython and nothing in IPython Notebook.
If the example code has the plot, which depends direct from matplotlib module, I can produce the real plots and save it on my machine. Consequently I suppose the main error is depended from the graphlab (or object from  its class).
If somebody already used this tool and got the plot, can he/she tell me, how I can execute the plots command.


    In [8]: import graphlab

    In [9]: from IPython.display import display

            from IPython.display import Image

            graphlab.canvas.set_target(''ipynb'')

    In [10]:import urllib

            url = ''https://s3.amazonaws.com/GraphLab-Datasets/americanMovies   /freebase_performances.csv''

            urllib.urlretrieve(url, filename=''freebase_performances.csv'')  # downloads an 8MB file to the working directory

    Out[10]: (''freebase_performances.csv'', <httplib.HTTPMessage instance at 0x7f44e153cf38>)

    In [11]: data = graphlab.SFrame.read_csv(''remote://freebase_performances.csv'', column_type_hints={''year'': int})

...
...
...

    In [15]:data.show()

No plot after this line
...
...

    In [19]:print data.show()

    <IPython.core.display.Javascript object at 0x7f44e14c0850>

The object of graphlab (?) after print command


  [1]: http://graphlab.com/learn/gallery/index.html', 3281, '2014-10-27 10:07:58.150', '1dc86af0-214f-4081-9687-b4a6c4f81f5d', 2355, 'added 891 characters in body', 4513, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Sorry, if this topic is not connect direct to the Data Science...
I try to understand how the [Graphlab tool][1] works. Firstly I want to execute the toy examples from Gallery site. When I try to execute the example code, everything is ok excepted one command: I can not see the graphlab plot after `show()`. The command `show()` return me some kind of object in IPython and nothing in IPython Notebook.
If the example code has the plot, which depends direct from matplotlib module, I can produce the real plots and save it on my machine. Consequently I suppose the main error is depended from the graphlab (or object from  its class).
If somebody already used this tool and got the plot, can he/she tell me, how I can execute the plots command.


    In [8]: import graphlab

    In [9]: from IPython.display import display

            from IPython.display import Image

            graphlab.canvas.set_target(''ipynb'')

    In [10]:import urllib

            url = ''https://s3.amazonaws.com/GraphLab-Datasets/americanMovies   /freebase_performances.csv''

            urllib.urlretrieve(url, filename=''freebase_performances.csv'')  # downloads an 8MB file to the working directory

    Out[10]: (''freebase_performances.csv'', <httplib.HTTPMessage instance at 0x7f44e153cf38>)

    In [11]: data = graphlab.SFrame.read_csv(''remote://freebase_performances.csv'', column_type_hints={''year'': int})

...
...
...

    In [15]:data.show()

No plot after this line
...
...

    In [19]:print data.show()

    <IPython.core.display.Javascript object at 0x7f44e14c0850>

The object of graphlab (?) after print command


  [1]: http://graphlab.com/learn/gallery/index.html', 3281, '2014-10-27 10:40:27.387', '3f7c85fd-0d52-49b7-8644-b843e1a10d50', 2355, 'edited body', 4514, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Sorry, if this topic is not connected direct to the Data Science...
I try to understand how the [Graphlab tool][1] works. Firstly I want to execute the toy examples from Gallery site. When I try to execute the example code, everything is ok excepted one command: I can not see the graphlab plot after `show()`. The command `show()` return me some kind of object in IPython and nothing in IPython Notebook.
If the example code has the plot, which depends direct from matplotlib module, I can produce the real plots and save it on my machine. Consequently I suppose the main error is depended from the graphlab (or object from  its class).
If somebody already used this tool and got the plot, can he/she tell me, how I can execute the plots command.


    In [8]: import graphlab

    In [9]: from IPython.display import display

            from IPython.display import Image

            graphlab.canvas.set_target(''ipynb'')

    In [10]:import urllib

            url = ''https://s3.amazonaws.com/GraphLab-Datasets/americanMovies   /freebase_performances.csv''

            urllib.urlretrieve(url, filename=''freebase_performances.csv'')  # downloads an 8MB file to the working directory

    Out[10]: (''freebase_performances.csv'', <httplib.HTTPMessage instance at 0x7f44e153cf38>)

    In [11]: data = graphlab.SFrame.read_csv(''remote://freebase_performances.csv'', column_type_hints={''year'': int})

...
...
...

    In [15]:data.show()

No plot after this line
...
...

    In [19]:print data.show()

    <IPython.core.display.Javascript object at 0x7f44e14c0850>

The object of graphlab (?) after print command


  [1]: http://graphlab.com/learn/gallery/index.html', 3281, '2014-10-27 10:46:37.337', 'e00787b1-5da9-458d-8153-c471e0f3cc4d', 2355, 'added 2 characters in body', 4515, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have Train and Test data, how to calculate classification accuracy with confusion matrix ? Thanks

    @attribute outlook {sunny, overcast, rainy}
    @attribute temperature {hot, mild, cool}
    @attribute humidity {high, normal}
    @attribute windy {TRUE, FALSE}
    @attribute play {yes, no}

Train:

    1 sunny     hot     high FALSE no
    2 sunny     hot     high TRUE no
    3 overcast hot     high FALSE yes
    4 rainy     mild high FALSE yes
    5 rainy     cool normal FALSE yes
    6 rainy     cool normal TRUE no
    7 sunny     cool normal FALSE yes
    8 rainy     mild normal FALSE yes
    9 sunny     mild normal TRUE yes
    10 overcast mild high TRUE yes
    11 overcast hot     normal FALSE yes
    12 rainy     mild high TRUE no


Test:

    overcast cool normal TRUE yes
    sunny     mild high FALSE no

Rules found:

    (humidity,normal), (windy,FALSE) -> (play,yes) [Support=0.33 , Confidence=1.00 , Correctly Classify= 4, 8, 9, 12]
    (outlook,overcast) -> (play,yes) [Support=0.25 , Confidence=1.00 , Correctly Classify= 2, 11]
    (outlook,rainy), (windy,FALSE) -> (play,yes) [Support=0.25 , Confidence=1.00 , Correctly Classify= 3]
    (outlook,sunny), (temperature,hot) -> (play,no) [Support=0.17 , Confidence=1.00 , Correctly Classify= 0, 1]
    (outlook,sunny), (humidity,normal) -> (play,yes) [Support=0.17 , Confidence=1.00 , Correctly Classify= 10]
    (outlook,rainy), (windy,TRUE) -> (play,no) [Support=0.17 , Confidence=1.00 , Correctly Classify= 5, 13]', 3503, '2014-10-27 11:13:51.847', 'c1446359-0f98-4d48-9506-ff518ad0cb97', 2348, 'added 254 characters in body', 4516, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to calculate classification accuracy with confusion matrix?', 3503, '2014-10-27 11:13:51.847', 'c1446359-0f98-4d48-9506-ff518ad0cb97', 2348, 'added 254 characters in body', 4517, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('OK, so values at time t-1 predict values at time t. That makes sense.

First you should decide whether you think these values are independent or not. Do the x predict the y or z at all? And, do you think just the previous 1 value is predictive, or the previous n?

Either way you could model this as a simple regression problem. What technique is best really depends on what you expect the relationship to be, and what these variables are; I am not sure that''s given here.

For example if they''re sensor values read fairly rapidly, and the sensor changes slowly, you''d expect some simple model like a moving average to do well. For other types of values this would not be predictive at all.

This looks like the Markov chain model, so you may look into that, but somehow I think it''s over-general for what I think the problem is.', 21, '2014-10-27 15:43:00.067', '5a28befa-344f-4b67-a84e-8e32b59b9ebb', 2356, 4518, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am kind of a newbie on machine learning and I would like to ask some questions based on a problem I have .

Let''s say I have x y z as variable and I have values of these variables as time progresses like :

t0  = x0 y0 z0  <br>
t1  = x1 y1 z1  <br>
tn  = xn yn zn  <br>

Now I want a model that when it''s given 3 values of x , y , z I want a prediction of them like:

Input : x_test y_test z_test <b>
Output : x_prediction y_prediction z_prediction


These values are float numbers. What is the best model for this kind of problem?
Thanks in advance for all the answers.

More details:
Ok so let me give some more details about the problems so as to be more specific.

I have run certain benchmarks and taken values of performance counters from the cores of a system per interval.

The performance counters are the x , y , z in the above example.They are dependent to each other.Simple example is x = IPC , y  =  Cache misses , z  = Energy at Core.

So I got this dataset of all these performance counters per interval .What I want to do is create a model that after learning from the training dataset , it will be given a certain state of the core ( the performance counters) and predict the performance counters that the core will have in the next interval.', 4668, '2014-10-27 16:04:23.527', '2d5e0380-f0a5-4c74-b463-1ea54eceff68', 2287, 'added 700 characters in body', 4519, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A confusion matrix is a cross tabulation of your predicted values against the true observed values, and (test) accuracy is emperical rate of correct predictions. So in this case you''ll need to

1. Predict the ''play'' attribute for your test set.  (Currently you don''t have a method to predict your second test case, so for the sake of argument let''s assume your model would predict yes for the sunny example.
2. The following method of keeping track of your predictions is reffered to as a confusion matrix,

     Predicted
|     | yes | no |
|-----|-----|----|
| yes | 1   | 1  |
| no  | 0   | 0  |

where the left hand axis is the true, observed values. (Here the first 1 is from your first test case and the second 1 is from the misclassified second test case.)

3. Calculate accuracy,

Accuracy = (# correct predictions)/(# total predictions) = 1 / 2 = .50.



', 4724, '2014-10-27 16:13:21.233', '33012e47-3c00-4668-aa6d-e9acf8ab5b9e', 2357, 4520, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A confusion matrix is a cross tabulation of your predicted values against the true observed values, and (test) accuracy is emperical rate of correct predictions. So in this case you''ll need to

1. Predict the ''play'' attribute for your test set.  (Currently you don''t have a method to predict your second test case, so for the sake of argument let''s assume your model would predict yes for the sunny example.
2. The following method of keeping track of your predictions is reffered to as a confusion matrix.  The top labels are prediced

<pre>
               Predicted
         +----------------+
         ¦     ¦ yes ¦ no ¦
Oserved  ¦ yes ¦ 1   ¦ 1  ¦
         ¦ no  ¦ 0   ¦ 0  ¦
         +----------------+
</pre>

Here the first 1 is from your first test case and the second 1 is from the misclassified second test case.

3. Calculate accuracy,

Accuracy = (# correct predictions)/(# total predictions) = 1 / 2 = .50.
', 4724, '2014-10-27 16:23:02.663', '817ce741-5d0f-47f3-909e-ef3548fbbb5b', 2357, 'added 56 characters in body', 4521, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It is classify test objects: "In classification, let R be the set of generated rules and T the training data. The basic idea of the proposed method is to choose a set of high confidence rules in R to cover T. In classifying a test object, the first rule in the set of rules that matches the test object condition classifies it. This process ensures that only the highest ranked rules classify test objects."

Suppose 1 test case is (overcast, cool, normal, TRUE). Look through the rules top to bottom and see if any of the conditions are matched. The first rule for example tests the outlook feature. The value doesn''t match, so the rule isn''t matched. Move on to the next rule. And so on. In this case, rule 2 matches the test case and the classification for the play variable is "yes". The second test case is misclassified.

Thanks

', 3503, '2014-10-27 17:39:18.997', 'cbb7d118-11a6-4b82-9009-90dc3092bf6f', 2358, 4522, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Our system allows an admin to manage a database of university courses. These courses have multiple fields, like the department, a title, and a description.

I am adding the ability to add learning objectives to a course. To simplify the problem, let''s say that learning objectives are just tags. Courses can have more than one learning objective associated with them. So a course like CHEM 101 might have "chemistry", "technology", "science", and several others.

Assuming I can reduce a course to a set of features, (using keywords/stemming/nlp, I suppose?), what kind of problem is this and what algorithm would you suggest? It seems very similar to a classification problem, but I want to provide a sorted list of suggestions with the most relevant at the top.', 4821, '2014-10-27 22:17:31.497', 'a5148132-465d-4dbb-8ac2-caa0a379bcbd', 2359, 4523, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which Machine Learning algorithm: Sorted list of tags given metadata?', 4821, '2014-10-27 22:17:31.497', 'a5148132-465d-4dbb-8ac2-caa0a379bcbd', 2359, 4524, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><algorithms>', 4821, '2014-10-27 22:17:31.497', 'a5148132-465d-4dbb-8ac2-caa0a379bcbd', 2359, 4525, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For item-ratings type of data with the restriction that an item''s rating should be between 1 and 10 after transformation, I would suggest using a simple rescaling such that the item''s transformed rating $x_t$ is given by:

$$x_t = 9*(\frac{x_i-x_{\mathrm{min}}}{x_{\mathrm{max}}-x_{\mathrm{min}}} + 1$$

where $x_{\mathrm{min}}$ and $x_{\mathrm{max}}$ are the minimum and maximum possible rating in the specific scale for the item and $x_i$ is the item rating.

In the case of the above scaling, the transformation applied is independent of the data where as the in the normalization, the transformation applied is dependent on the data (through mean and standard deviation) and might change as more data becomes available.

Section 4.3 on page 30 of [this][1] shows other ways of normalizing in which your restriction (transforming to the same absolute scale) might not be preserved.


  [1]: http://www.dai-labor.de/fileadmin/files/publications/DiplomaThesisStephanSpiegel.pdf', 847, '2014-10-27 23:24:46.673', '552cfabd-63b7-4ec2-8c0f-a3c5881cb4f4', 2360, 4526, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For item-ratings type of data with the restriction that an item''s rating should be between 1 and 10 after transformation, I would suggest using a simple rescaling such that the item''s transformed rating $x_t$ is given by:

$$x_t = 9\left(\frac{x_i-x_{\mathrm{min}}}{x_{\mathrm{max}}-x_{\mathrm{min}}}\right) + 1$$

where $x_{\mathrm{min}}$ and $x_{\mathrm{max}}$ are the minimum and maximum possible rating in the specific scale for the item and $x_i$ is the item rating.

In the case of the above scaling, the transformation applied is independent of the data where as the in the normalization, the transformation applied is dependent on the data (through mean and standard deviation) and might change as more data becomes available.

Section 4.3 on page 30 of [this][1] shows other ways of normalizing in which your restriction (transforming to the same absolute scale) might not be preserved.


  [1]: http://www.dai-labor.de/fileadmin/files/publications/DiplomaThesisStephanSpiegel.pdf', 847, '2014-10-27 23:34:12.113', '16e5b319-7dc5-4929-b253-3b7f4492e05f', 2360, 'added 11 characters in body', 4527, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This will most likely turn out to be multiple binary classification problems.

Instead of building just one classifier, you would have to build as many classifiers as there are tags. Each classifier''s task would be to predict whether a certain tag exists or not. A tag specific probability threshold can then be learnt by tuning over the precision-curve where course wise precision recall can be calculated.

The percentile of the predicted probability for each tag can be used to obtain a ordered (sorted) list of tag suggestions.

This way of converting a multi-class problem to multiple binary classification problems is known as binary relevance. While it is very efficient, it assumes that the labels themselves are independent. However, picking the probability threshold for each tag in order to optimize the course wise precision recall curve can lead to modeling dependence in the tags.

', 847, '2014-10-27 23:56:11.853', '1f32c202-98bf-4b96-b23c-d11de12cd8bc', 2361, 4528, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For item-ratings type of data with the restriction that an item''s rating should be between 1 and 10 after transformation, I would suggest using a simple rescaling such that the item''s transformed rating <img src="http://i.stack.imgur.com/nflvt.png" width="15">  is given by:

<img src="http://i.stack.imgur.com/8fhPw.png" width="250">

where <img src="http://i.stack.imgur.com/zXHST.png" width="30"> and <img src="http://i.stack.imgur.com/QDSDO.png" width="30"> are the minimum and maximum possible rating in the specific scale for the item and <img src="http://i.stack.imgur.com/kBi8d.png" width="19"> is the item rating.

In the case of the above scaling, the transformation applied is independent of the data where as the in the normalization, the transformation applied is dependent on the data (through mean and standard deviation) and might change as more data becomes available.

Section 4.3 on page 30 of [this][3] shows other ways of normalizing in which your restriction (transforming to the same absolute scale) might not be preserved.


  [1]: http://i.stack.imgur.com/zXHST.png
  [2]: http://i.stack.imgur.com/QDSDO.png
  [3]: http://www.dai-labor.de/fileadmin/files/publications/DiplomaThesisStephanSpiegel.pdf', 3430, '2014-10-28 02:23:27.850', '4c4c2f32-e4f7-4011-90cb-fe6cdfa091a4', 2360, 'replaced latex with images', 4529, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-10-28 02:23:27.850', '4c4c2f32-e4f7-4011-90cb-fe6cdfa091a4', 2360, 'Proposed by 3430 approved by 847 edit id of 166', 4530, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m training a NN with 8 features and 8000 training examples with a single output (0, 1) using the scipy.optimise CG algorithm and the results are somewhat inconsistent. The goal is to get the NN to be as ''precise'' as possible (recall doesn''t really matter too much) so I''ve set the threshold for y value quite high (0.75). Most of the time it gets a precision of around 80%, however sometimes it fails (using exactly the same parameters, lambda etc..) to generate any outputs which are above the 0.75 threshold, meaning the precision equals 0.

I''ve successfully trained NNs before without these unusual results (albeit the goal was a somewhat more conventional multi-class classifier with many more features).

I''m wondering if the training NNs with fewer features increases the chances of it getting stuck at a local optima; or getting stuck at local optima has a more significant impact on NNs with fewer features?

Any thoughts on what''s going on!?

 ', 4824, '2014-10-28 06:00:49.643', '45197268-eff9-4ca3-ae8b-07844174e3ab', 2362, 4531, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Neural Networks getting stuck at local optima', 4824, '2014-10-28 06:00:49.643', '45197268-eff9-4ca3-ae8b-07844174e3ab', 2362, 4532, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork>', 4824, '2014-10-28 06:00:49.643', '45197268-eff9-4ca3-ae8b-07844174e3ab', 2362, 4533, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The difference was, it appears, due to the different implementation of Random Forests in `R` and `Cascading Pattern` (as well as `openscoring` which I tried later) with respect to ties in the tree voting - i.e. when an even number of trees are built (say, 500) and exactly half classify an application as `Good`, and the other half as `Bad`, the handling of those situations differs. Solved it by growing and odd (501) number of trees.', 1127, '2014-10-28 06:27:40.090', '87b291f0-8894-4309-afda-17368220e6a0', 2363, 4535, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using unsupervised learning to reduce the dimensionality and then using supervised learning to obtain an accurate predictive model is commonly used. See for example [Bhat and Zaelit, 2012][1] where they first use PCA to reduce the dimension of a problem from 87 to 35. Then, they use L1 regression to obtain the best predictive model. This method beats non-linear tree based models built on the entire dataset and also its subset.

If your goal is to create more accurate classification of data into clusters, then a commonly used technique is to use supervised learning as a method to accurately pick the number of clusters [see Pan et al, 2013][2] for a recent example. The basic approach here is to choose the number of clusters such that a supervised multi-class method can learn these clusters and predict the clusters with the highest out of sample accuracy. This is one way to convince yourself that the clusters are both meaningful and predictable.

Another approach, if your goal is to classify documents as being from US/ USA/ Australia or for that matter discussing, soccer/ American football/ Australian football could be to solve three binary classification problems that independently predict if the document talks about soccer, American football or Australian football. Combining the results from these three classifiers (known as binary relevance), you could also have the ability of tagging a document as both soccer or American football or any combination of the above three tags.


  [1]: http://faculty.ucmerced.edu/hbhat/BhatZaelit2012.pdf
  [2]: http://jmlr.csail.mit.edu/papers/volume14/pan13a/pan13a.pdf', 847, '2014-10-28 07:00:01.593', '0b82fcd0-8908-4a5c-a891-5d00b3992b81', 2364, 4536, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Ok all sorted - Bit embarrassing but forgot to normalise the data!', 4824, '2014-10-28 07:14:06.873', 'd45db102-4638-4b9b-83ba-5fd136c8447a', 2365, 4537, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For item-ratings type of data with the restriction that an item''s rating should be between 1 and 10 after transformation, I would suggest using a simple rescaling such that the item''s transformed rating <img src="http://i.stack.imgur.com/nflvt.png" width="15">  is given by:

<img src="http://i.stack.imgur.com/8fhPw.png" width="250">

where <img src="http://i.stack.imgur.com/zXHST.png" width="30"> and <img src="http://i.stack.imgur.com/QDSDO.png" width="30"> are the minimum and maximum possible rating in the specific scale for the item and <img src="http://i.stack.imgur.com/kBi8d.png" width="19"> is the item rating.

In the case of the above scaling, the transformation applied is independent of the data where as in the normalization, the transformation applied is dependent on the data (through mean and standard deviation) and might change as more data becomes available.

Section 4.3 on page 30 of [this][3] shows other ways of normalizing in which your restriction (transforming to the same absolute scale) might not be preserved.


  [1]: http://i.stack.imgur.com/zXHST.png
  [2]: http://i.stack.imgur.com/QDSDO.png
  [3]: http://www.dai-labor.de/fileadmin/files/publications/DiplomaThesisStephanSpiegel.pdf', 847, '2014-10-28 09:46:07.187', 'cb8a2544-8ff9-44c0-9f28-ce0240ce4f8c', 2360, 'fixed grammar', 4538, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You are altering weights in the wrong direction for the negative cases.

The line

    w = w + eta*Xtr(j,:)'';

should be

    w = w + eta*Xtr(j,:)''*ytr(j);

With that change I got 12% error.', 836, '2014-10-28 17:07:32.723', '79ae3226-d843-43a6-9f89-8db80cda30be', 2366, 4539, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Instead of "recursive neural nets with back propagation" you might consider the approach used by Frantzi, et. al. at National Centre for Text Mining (NaCTeM) at University of Manchester for *Termine* (see: http://www.nactem.ac.uk/index.php and http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf) Instead of deep neural nets, they "combine linguistic and statistical information".

', 609, '2014-10-29 00:52:06.097', 'a2924e27-3ab7-4a01-a293-62175ed0f215', 2367, 4540, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the common/best practices to handle time data for machine learning application?

For example, if in data set there is a column with timestamp of event, such as "2014-05-05", how you can extract useful features from this column if any?

Thanks in advance!', 88, '2014-10-29 05:25:55.603', 'e34a65ab-1f24-4490-976c-42b1f1306ccf', 2368, 4541, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine learning - features engineering from date/time data', 88, '2014-10-29 05:25:55.603', 'e34a65ab-1f24-4490-976c-42b1f1306ccf', 2368, 4542, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><time-series><feature-selection>', 88, '2014-10-29 05:25:55.603', 'e34a65ab-1f24-4490-976c-42b1f1306ccf', 2368, 4543, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In several cases data and events inside a time series are seasonal. In such cases the month and the year of the event matters alot. Hence in such scenarios you can use binary variables to represent if the event is during a given month/year or not.

Hope this answers your question. If not kindly be a little more specific on what exactly are you trying to achieve  ', 4839, '2014-10-29 07:52:41.060', '2eea4c68-f31f-4be6-8e3e-4473df4fe3a6', 2369, 4544, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Ranked tag recommendation for university courses', 381, '2014-10-29 08:12:37.850', '73ac0752-cb4c-46e5-ab94-f5b5c9440f6f', 2359, 'copy-edited the title', 4545, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><classification><tagging><annotation>', 381, '2014-10-29 08:12:37.850', '73ac0752-cb4c-46e5-ab94-f5b5c9440f6f', 2359, 'copy-edited the title', 4546, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-10-29 08:12:37.850', '73ac0752-cb4c-46e5-ab94-f5b5c9440f6f', 2359, 'Proposed by 381 approved by 21 edit id of 167', 4547, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would start by graphing the time variable vs other variables and looking for trends.

## For example

![enter image description here][1]

In this case there is a periodic weekly trend and a long term upwards trend.  So you would want to encode two time variables:

- `day_of_week`
- `absolute_time`

## In general

There are several common time frames that trends occur over:

- `absolute_time`
- `day_of_year`
- `day_of_week`
- `month_of_year`
- `hour_of_day`
- `minute_of_hour`

Look for trends in all of these.

## Weird trends

Look for weird trends too.  For example you may see rare but persistent time based trends:

- `is_easter`
- `is_superbowl`
- `is_national_emergency`
- `etc.`

These often require that you cross reference your data against some external source that maps events to time.  While the general case can be automated pretty easily (just add them every time), these weird cases will often require a human eye and knowledge of the world to find.  This is the main reason that graphing is so important.

  [1]: http://i.stack.imgur.com/QGYUC.png', 4808, '2014-10-29 13:54:23.793', '3b1ac27b-198e-44eb-a87d-8b11014cebdf', 2370, 4548, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have found some solution and will post it here, because somebody, who works with graphlab, can have the same question.

We can look at the example here: [Six degrees of Kevin Bacon][1]

At te beginning of the program execution you need to run next command:

    graphlab.canvas.set_target(''ipynb'')

Exactly it is a key of the whole problem (at least by me:-)

At the beginning it is important to know, which parameter of `set_target()` command you want to execute. You can use two options for argument of this command:  `''ipynb''` (which is executed direct in iPython Notebook, like in example) or `''browser''` (which open the new window with the plots)
On my machine 64-bit, Ubuntu, I can not use the command `''ipynb''`. Only the `''browser''`-command get me the lot back. I don''t think, it is necessary to change hhtps to http, but you can do it anyway. We have also the other machine by us (32-bit, Ubuntu) and it execute the other command `''ipynb''`, but not `''browser''`


  [1]: http://graphlab.com/learn/gallery/notebooks/graph_analytics_movies.html', 3281, '2014-10-29 13:58:55.653', '33cff4ab-ea83-4ed9-8d0e-676057f77d84', 2371, 4549, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have found some solution and will post it here, because somebody, who works with graphlab, can have the same question.

We can look at the example here: [Six degrees of Kevin Bacon][1]

At te beginning of the program execution you need to run next command:

    graphlab.canvas.set_target(''ipynb'')

Exactly this is a key of the whole problem (at least by me:-)

At the beginning it is important to know, which parameter of `set_target()` command you want to execute. You can use two options for argument of this command:  `''ipynb''` (which is executed direct in iPython Notebook, like in example) or `''browser''` (which open the new window with the plots)
On my machine 64-bit, Ubuntu, I can not use the command `''ipynb''`. Only the `''browser''`-command get me the plot back. I don''t think, it is necessary to change https to http, but you can do it anyway. We have also the other machine by us (32-bit, Ubuntu) and it execute the other command `''ipynb''`, but not `''browser''` (without to change https to http)


  [1]: http://graphlab.com/learn/gallery/notebooks/graph_analytics_movies.html', 3281, '2014-10-29 14:08:05.387', '150205c3-cc81-4f2e-8406-eb4aafcc069b', 2371, 'added 2 characters in body', 4550, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Decision Trees are unstable learners and very sensitive to changes in the input parameters.
 ', 4847, '2014-10-29 14:37:00.910', '38b42218-a6f0-4014-b4f5-34fecf2658a7', 2372, 4551, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Here''s some practical advice from my own experiences-

1. Convince management the the change will be for the better.

2. Even if management agrees, they''ll still want to know why variances have occurred, and hence you''ll need to be able to supply more data.

3. Although exception reporting is best, management will want to see everything anyway, as doing so makes them feel as though they are doing something useful.

4. Don''t change everything at once- too big a change at once can cause resistance.

5. For how to best present the data, read Edward Tufte''s books, at the very least his first one, "The Visual Display of Quantitative Data".', 1241, '2014-10-29 16:05:25.840', '91cc8bd5-b046-42f7-92ee-284041561869', 2347, 'Revised final sentence.', 4552, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Here''s some practical advice from my own experiences-

1. Convince management the the change will be for the better.

2. Even if management agrees, they''ll still want to know why variances have occurred, and hence you''ll need to be able to supply more data.

3. Although exception reporting is best, management will want to see everything anyway, as doing so makes them feel as though they are doing something useful.

4. Don''t change everything at once- too big a change at once can cause resistance.

5. For how to best present the data, read Edward Tufte''s books, at the very least his first one, "The Visual Display of Quantitative Data".

6. Defining what''s an exception can be hard, because the recipients will each have their own  ideas. Using say, a 95% confidence interval is good, but it won''t be universally liked. Some people will consider any change above $X significant, and others will want to see everything that''s more than X% different from the prior period.', 1241, '2014-10-29 16:20:16.737', '1367e74c-6efc-4241-89b1-4dceda257495', 2347, 'Revised final sentence.', 4553, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-10-29 17:39:01.777', '30cf7ae7-d3f4-4b24-8e6c-d489c229db7a', 1204, '105', 4554, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The most online tutorials like to use a simple example to introduce to machine learning by classify unknown text in spam or not spam. They say that this is a binary-class problem. But why is this a binary-class problem? I think it is a one-class problem! I do only need positive samples of my inbox to learn what is not spam. If I do take a bunch of not spam textes as positiv samples and a bunch of spam-mails as negativ samples, then of course it''s possible to train a binary-classifier and make predictions from unlabeled data, but where is the difference to the onc-class-approach? There I would just define a training-set of all non spam examples and train some one-class classifier. What do you think?', 4717, '2014-10-29 21:57:15.603', '3847cfd8-8a7c-400b-97a4-a1299da32ee5', 2373, 4556, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Detecting Spam using Machine Learning', 4717, '2014-10-29 21:57:15.603', '3847cfd8-8a7c-400b-97a4-a1299da32ee5', 2373, 4557, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 4717, '2014-10-29 21:57:15.603', '3847cfd8-8a7c-400b-97a4-a1299da32ee5', 2373, 4558, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Your training data needs to be one set of data with samples of all the categories, because you are trying (I think) to create a model that will be fed such data.

Have you given any thoughts as to what model(s) you might be using? I''m asking because pure classification models will achieve a better fit if the amount of data in each class is pretty uniform in the training data. However, regression models need the data type proportions to match the expected input.', 1241, '2014-10-29 22:10:53.570', '3703cc31-d3c8-4cd8-8fb8-deb7eb77144b', 2374, 4559, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The problem arises if you want to classify a new example as either spam or not spam. A one-class method will only give you some score of how well a new instance fits to this class, but how do you turn this into a binary prediction without knowing how big the score would be for the other class?

If you look at the Naive Bayes classifier it essentially trains a "one-class" model for each class and arrives at a prediction by choosing the class with the highest score. But this requires you to have training examples for all classes.', 4852, '2014-10-30 07:31:03.903', 'b9c998a0-4eaf-4858-896a-2312595d5795', 2375, 4560, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have found some solution and will post it here, because somebody, who works with graphlab, can have the same question.

We can look at the example here: [Six degrees of Kevin Bacon][1]

At te beginning of the program execution you need to run next command:

    graphlab.canvas.set_target(''ipynb'')

Exactly this is a key of the whole problem (at least by me:-)

At the beginning it is important to know, which parameter of `set_target()` command you want to execute. You can use two options for argument of this command:  `''ipynb''` (which is executed direct in iPython Notebook, like in example) or `''browser''` (which open the new window with the plots)
On my machine 64-bit, Ubuntu, I can not use the command `''ipynb''`. Only the `''browser''`-command get me the plot back. I don''t think, it is necessary to change https to http, but you can do it anyway. We have also the other machine by us (32-bit, Ubuntu) and it executes the other command `''ipynb''`, but not `''browser''` (without to change https to http)


  [1]: http://graphlab.com/learn/gallery/notebooks/graph_analytics_movies.html', 3281, '2014-10-30 07:57:03.453', '34bd554e-9fe3-478a-aad1-3bb357303c25', 2371, 'added 1 character in body', 4561, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Here''s some practical advice from my own experiences-

1. The first thing to do is to convince management the the change will be for the better. A mock-up of a sample report can be very useful here.

2. Even if management agrees, they''ll still want to know why variances have occurred, and hence you''ll need to be able to supply more data.

3. Although exception reporting is best, management will want to see everything anyway, as doing so makes them feel as though they are doing something useful.

4. Don''t change everything at once- too big a change at once can cause resistance.

5. For how to best present the data, read Edward Tufte''s books, at the very least his first one, "The Visual Display of Quantitative Data".

6. Defining what''s an exception can be hard, because the recipients will each have their own  ideas. Using say, a 95% confidence interval is good, but it won''t be universally liked. Some people will consider any change above $X significant, and others will want to see everything that''s more than X% different from the prior period. Have fun with this part :(', 1241, '2014-10-30 13:40:45.283', 'c654f463-c14a-45e3-b1b7-9491afeefb28', 2347, 'expanded some paragraphs', 4562, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Strictly speaking, "one class classification" does not make sense as an idea. If there is only one possible state of a predicted value, then there is no prediction problem. The answer is always the single class.

Concretely, if you only have spam examples, you would always achieve 100% accuracy by classifying all email as spam. This is clearly wrong, and the only way to know how it is wrong is to know where the classification is wrong -- where emails are not in the spam class.

So-called [one-class classification][1] techniques are really anomaly detection approaches. They have an implicit assumption that things unlike the examples are not part of the single class, but, this is just an assumption about data being probably *not* within the class. There''s a binary classification problem lurking in there.

What is wrong with a binary classifier?

  [1]: http://en.wikipedia.org/wiki/One-class_classification', 21, '2014-10-30 14:48:34.907', 'b315bc15-fe7f-4e6a-ac6f-1d26567afd7d', 2376, 4563, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('
Basically your machine-learning problem is:
given [day-of-week, weather, departure-time, route], predict arrival time, then obtain travel time. Once your model is solid, you can then predict your travel time for each potential departure time and route, and choose the lowest.

You don''t really need to factor in the pit stops, if you think they''re a function of the rest, or a random variable altogether.

If you want a tool that can do the machine-learning aspect ''out of the box'' you should try Weka.
You''ll have to encode the data in the specific format it expects, but other than that you won''t have to do any coding (i.e. you won''t need to code any of the actual machine-learning algorithms).

I would discretize your departure time to make sure you have enough data.
Weka will let you try out the different algorithms, see which one is best.
Note that it''s a regression problem as opposed to a classification problem, so only a subset of the algorithms apply.
Once you''ve got the Weka part figured out, you can also call it programmatically, which will allow you to code the interface you want, and potentially include the automatic weather retrieval.
', 4760, '2014-10-30 14:59:13.243', 'e9720d89-6b5e-4616-9671-abc1cef6ae1c', 2377, 4564, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m doing some data analysis in a Statistical Pattern Recognition course using PRML. We analyzed a lot of matrix properties, like eigenvalues, column independence, positive semi-definite matrix, etc. When we are doing, for example, linear regression, we need to calculate some of those properties, and fit them into the equation.

So my question is, my question is about the intuition behind these matrix properties, and their implications in the ML/DM literature.

If anyone could answer, can you teach me what is the importance of eigenvalue, positive semi-definite matrix, and column independence for ML/DM. And possibly, other important matrix properties you think important in study the dataset, and why.

I''d be really appreciated if someone can answer this question.', 4859, '2014-10-30 18:22:18.907', 'e4959789-c4ec-40d2-b684-ac40449143f2', 2378, 4565, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Matrix properties and machine learning/data mining', 4859, '2014-10-30 18:22:18.907', 'e4959789-c4ec-40d2-b684-ac40449143f2', 2378, 4566, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><dataset>', 4859, '2014-10-30 18:22:18.907', 'e4959789-c4ec-40d2-b684-ac40449143f2', 2378, 4567, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A few things where the knowledge of Linear Algebra might be helpful in the context of Machine Learning:

 1. Dimensionality Reduction: There are lots of problems where PCA (a special case of an SVD) followed by a simple Machine Learning method applied on the reduced dataset produces much better results than a non parametric model on the full (non-reduced dataset). For an example see [Bhat and Zaelit, 2012][1] where PCA followed by linear regression performs better than more involved non-parametric models. It also suggests reasons why dimensionality reduction performs better in these cases.
 2. Visualizing data: Higher dimensional data is complicated to visualize and one often needs to be able to reduce the dimension of the dataset to view it. This comes very handy when one has to "view" the results of clustering on a higher dimensional dataset.
 3. Numerical Accuracy: Eigenvalues are generally handy in order to understand condition numbers of matrices and hence be able to figure out if results of Linear regression or other methods that require to solve Ax=b would be numerically accurate. Positive Definiteness of a matrix might also be able to guarantee bounds on numerical accuracies.
 4. Recommendations: Some methods like collaborative filtering use matrix factorization (SVD) tuned smartly to solve recommendation problems.
 5. Regularization: Regularization is commonly used to reduce over-fitting in machine learning problems. Most of these regularization techniques like Lasso, Tikhonov etc. have both optimization and Linear Algebra at their heart.

  [1]: http://faculty.ucmerced.edu/hbhat/BhatZaelit2012.pdf', 847, '2014-10-31 01:38:57.483', '622252db-7798-4965-888a-5e4adc3cff06', 2379, 4568, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The importance of a concept in mathematics depends on the circumstances of its application. Sometimes, its importance relies on the fact that it allows you to carry on with what you are doing.

For example, you usually need column independence (independent variables between predictors) because multiple regression will behave badly with highly correlated variables. Even worst, when some of your columns (or rows) are dependent, your matrix is not invertible. Why? Because matrix inversion A^-1 involves the determinant 1/|A|, which is 0 when columns or rows are linearly dependent.

Eigenvalues is a common occurrence in calculations related to maximization/minimization in machine learning. Let''s say you are interested in principal component analysis. A very important idea there is dimensional reduction (you have a dataset with many variables and want to reduce the number of variables without losing too much explanatory power.) One solution is to project your data onto a lower dimensional space (e.g. taking your data with 50 variables and reducing them to 5 variables.) Turns out a good projection to use is one that includes as much variation as possible and maximization of this variation results in the eigenvalue equation S u  =  u.

In other cases, you explicitly include the eigenvalue equation of some quantity of interest because in doing so, you''re changing the coordinate system in which you represent the variables. Take the case of a (multivariate) Gaussian distribution in which the argument of the exponent is given by  = (x-)^T  (x-). If you consider the eigenvalue equation of , the exponent can be written as  = y_1^2 / _1 + y_2^2 / _2 (in two dimensions) This is the equation of an ellipse only if _1 and _2 are positive. Therefore, you obtain the following graphical interpretation (Bishop, PRML, p.81):

![enter image description here][1]

Positive semi-definite matrices are used as a matter of convenience. They are well-behaved and well-understood. For instance, their eigenvalues are non-negative, and if you remember the previous paragraph, the argument  required positive eigenvalues. By now, you can see why some concepts are very popular: You need them for your calculations or they need each other.

I can recommend a couple of books:

1. [Linear Algebra: A Modern Introduction](http://www.amazon.com/Linear-Algebra-Modern-Introduction-CD-ROM/dp/0534998453/ref=ed_oe_h) by David Poole
2. [Understanding Complex Datasets: Data Mining with Matrix Decompositions ](http://www.amazon.com/Understanding-Complex-Datasets-Decompositions-Knowledge/dp/1584888326) by David Skillicorn.

The second recommendation is more specialized and requires a decent understanding of the basics, but it is of great help to understand matrix decompositions.

  [1]: http://i.stack.imgur.com/pNrLb.png', 4621, '2014-10-31 04:44:23.100', '4cecea77-5b31-48e1-9255-d3507c065e32', 2380, 4570, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve looked at all the other sections on stackexchange and haven''t found a better fit for this question. Sorry if it''s misplaced. If it is, feel free to downvote, but could you please also drop a recommendation where the question would be better suited? Thanks!

I work in an analytical role at a a large financial services firm. We do a ton of daily reporting over metrics that rarely change in a meaningful way from day to day. From this daily reporting, our management is required to extract what was important yesterday and what important trends have developed / are developing over time.

I want to change this to a model of daily exception reporting and weekly trend reporting.

Features might include:

- User report consolidation (so there''s only one daily email)
- report ordering based upon level of variance from part performance (see the most important stuff first)
- html email support (with my audience, pretty counts)
- web interface to allow preference changes, including LDAP support (make administration easier)
- Unsubscribe feature at the report level

Here''s what I''d like to know-
What are the practical problems I might run into?

What is the best way to display the new reports?

How should  I define an "exception"? how can I know if my definition is a good one?

I assume I''d be using a mix of Python, SQL, and powershell. Anything else I should consider, e.g. R?

What are some good resources?

Thanks,
Brandon
', 1241, '2014-10-31 08:30:41.357', '026bdb49-a126-4d7e-9cd2-e8f0ad1683c2', 2346, 'Rewrote last paragraph to have questions that are more specific and that make this question more suitable for this forum. added "visualization" tag.', 4571, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<recommendation><visualization>', 1241, '2014-10-31 08:30:41.357', '026bdb49-a126-4d7e-9cd2-e8f0ad1683c2', 2346, 'Rewrote last paragraph to have questions that are more specific and that make this question more suitable for this forum. added "visualization" tag.', 4572, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-10-31 08:30:41.357', '026bdb49-a126-4d7e-9cd2-e8f0ad1683c2', 2346, 'Proposed by 1241 approved by 21 edit id of 168', 4573, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m examining the activity of customers over the years which have about one event per year. This results in many short time-series for which I found the distributions (hit/miss over 4 years sorted by probability in the data):

    0000 : 0.31834
    0001 : 0.17582
    0010 : 0.13605
    0100 : 0.13554
    1000 : 0.12886
    0011 : 0.01717
    1100 : 0.01650
    0110 : 0.01578
    0101 : 0.01220
    1010 : 0.01117
    1001 : 0.00883
    0111 : 0.00571
    1110 : 0.00565
    1111 : 0.00496
    1101 : 0.00384
    1011 : 0.00351

Apparently a purely uncorrelated binomial model wouldn''t do, but one can observe that if both, the number of 1''s and 11''s coincide, then the probabilities are approximately equal (apart from a small recency effect of 0001).

Can you see a way to approach such data to deduce a probabilistic model? Basically where I have only a few probability parameters which roughly explain this distribution?', 723, '2014-10-31 08:50:23.160', 'b5aeaae1-2728-46b2-a57a-032219f6c039', 2381, 4574, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to find a model for a short discrete time-series?', 723, '2014-10-31 08:50:23.160', 'b5aeaae1-2728-46b2-a57a-032219f6c039', 2381, 4575, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<time-series>', 723, '2014-10-31 08:50:23.160', 'b5aeaae1-2728-46b2-a57a-032219f6c039', 2381, 4576, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m examining the activity of customers over the years which have about one event per year. This results is many short time-series for which I found the distributions (hit/miss over 4 years sorted by probability in the data):

    0000 : 0.31834
    0001 : 0.17582
    0010 : 0.13605
    0100 : 0.13554
    1000 : 0.12886
    0011 : 0.01717
    1100 : 0.01650
    0110 : 0.01578
    0101 : 0.01220
    1010 : 0.01117
    1001 : 0.00883
    0111 : 0.00571
    1110 : 0.00565
    1111 : 0.00496
    1101 : 0.00384
    1011 : 0.00351

Apparently a purely uncorrelated binomial model wouldn''t do, but one can observe that if both, the number of 1''s and 11''s coincide, then the probabilities are approximately equal (apart from a small recency effect of 0001).

Can you see a way to approach such data to deduce a probabilistic model? Basically where I have only a few probability parameters which roughly explain this distribution?', 723, '2014-10-31 10:02:13.603', '0038ff1e-60b5-4948-a63c-e0b9b6e6520d', 2381, 'edited body', 4577, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I need data set for energy forecasting for my project. Currently I am using east/west wind energy dataset from http://www.nrel.gov/. I need one more data set for solar energy. Please let me know where I can find one for free.

Thanks & Regards
Sachin Vittal', 4875, '2014-10-31 12:51:13.363', 'c671c49c-fced-4731-a851-5229d0307d5a', 2382, 4578, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data set: Time series forecasting', 4875, '2014-10-31 12:51:13.363', 'c671c49c-fced-4731-a851-5229d0307d5a', 2382, 4579, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset>', 4875, '2014-10-31 12:51:13.363', 'c671c49c-fced-4731-a851-5229d0307d5a', 2382, 4580, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('1. Gather app reviews for single app and try to find out the most important features that you can mine for the app developer

2. try predicting the next word in a sentence based on previous words.Eg. Swiftkey does this.

3. Get questions data from stack overflow and try to predict tags for every post on stack overflow.

', 3211, '2014-10-31 13:05:49.263', '8c1c5489-1fb9-4790-ba43-257f8e06ae4a', 2383, 4581, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would start by graphing the time variable vs other variables and looking for trends.

## For example

![enter image description here][1]

In this case there is a periodic weekly trend and a long term upwards trend.  So you would want to encode two time variables:

- `day_of_week`
- `absolute_time`

## In general

There are several common time frames that trends occur over:

- `absolute_time`
- `day_of_year`
- `day_of_week`
- `month_of_year`
- `hour_of_day`
- `minute_of_hour`

Look for trends in all of these.

## Weird trends

Look for weird trends too.  For example you may see rare but persistent time based trends:

- `is_easter`
- `is_superbowl`
- `is_national_emergency`
- `etc.`

These often require that you cross reference your data against some external source that maps events to time.

### Why graph?

There are two reasons that I think graphing is so important.

 - *Weird trends*
   While the general trends can be automated pretty easily (just add them
   every time), weird trends will often require a human eye and knowledge
   of the world to find.  This is one reason that graphing is so
   important.

 - *Data errors*
   All too often data has serious errors in it.  For example, you may find that the dates were encoded in two formats and only one of them has been correctly loaded into your program.  There are a myriad of such problems and they are surprisingly common.  This is the other reason I think graphing is important, not just for time series, but for any data.

  [1]: http://i.stack.imgur.com/QGYUC.png', 4808, '2014-10-31 13:08:33.763', '71a61691-1fc6-417e-acd4-9e004e715bcd', 2370, 'Elaborated on the reasons that graphing is useful.', 4582, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m planing to write a classification program that is able to classify unknown text in around 10 different categories and if none of them fits it would be nice to know that. It is also possible that more then one category is right.

My predefined categories are:

    c1 = "politics"
    c2 = "biology"
    c3 = "food"
    ...

I''m thinking about the right approach in how to represent my training-data or what kind of classification is the right one. The first challenge is about finding the right features. If I only have text (250 words each) what method would you recommend to find the right features? My first approach is to remove all stop-words and use the POS-Tagger ([Stanford NLP POS-Tagger][1]) to find nouns, adjective etc. I count them an use all frequently appeared words as features.

e.g. politics, I''ve around 2.000 text-entities. With the mentioned POS-Tagger I found:

    law:           841
    capitalism:    412
    president:     397
    democracy:     1007
    executive:     112
    ...

Would it be right to use only that as features? The trainings-set would then look like:

    Training set for politics:
    feature law         numeric
    feature capitalism  numeric
    feature president   numeric
    feature democracy   numeric
    feature executive   numeric
    class politics,all_others

    sample data:
    politics,5,7,1,9,3
    politics,14,4,6,7,9
    politics,9,9,9,4,2,1
    politics,5,8,0,7,6
    ...
    all_others,0,2,4,1,0
    all_others,0,0,1,1,1
    all_others,7,4,0,0,0
    ...

Would this be a right approach for binary-classification? Or how would I define my sets? Or is multi-class classification the right approach? Then it would look like:

    Training set for politics:
    feature law         numeric
    feature capitalism  numeric
    feature president   numeric
    feature democracy   numeric
    feature executive   numeric
    feature genetics    numeric
    feature muscle      numeric
    feature blood       numeric
    feature burger      numeric
    feature salad       numeric
    feature cooking     numeric
    class politics,biology,food

    sample data:
    politics,5,7,1,9,3,0,0,2,1,0,1
    politics,14,4,6,7,9,0,0,0,0,0,1
    politics,9,9,9,4,2,1,1,1,1,0,3
    politics,5,8,0,7,6,2,2,0,1,0,1
    ...
    biology,0,2,4,1,0,4,19,5,0,2,2
    biology,0,0,1,1,1,12,9,9,2,1,1
    biology,7,4,0,0,0,10,10,3,0,0,7
    ...

What would you say?


  [1]: http://nlp.stanford.edu/software/tagger.shtml', 4717, '2014-10-31 17:55:25.723', '8e9f2243-72f1-4a84-b712-e509ec7706dc', 2384, 4583, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Text-Classification-Problem, what is the right approach?', 4717, '2014-10-31 17:55:25.723', '8e9f2243-72f1-4a84-b712-e509ec7706dc', 2384, 4584, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 4717, '2014-10-31 17:55:25.723', '8e9f2243-72f1-4a84-b712-e509ec7706dc', 2384, 4585, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Kaggle][1] has a bunch of good practice datasets and basic tutorials.


  [1]: http://www.kaggle.com/competitions', 192, '2014-10-31 22:40:24.513', '02138924-c12b-4822-b411-af7038286408', 2385, 4586, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The following great article by Sebastian Raschka on *Bayesian approach to text classification* should be very helpful for your task. I also highly recommend his excellent **blog** on *data science* topics, as an additional general reference: http://sebastianraschka.com/articles.html.

You may also check this *educational report* on text classification: http://www.datasciencecentral.com/profiles/blogs/my-data-science-apprenticeship-project. It might provide you with some additional ideas.', 2452, '2014-11-01 10:07:09.843', '0413effc-a124-4fcd-b16a-a025ce0bf8f3', 2386, 4587, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Check **my answer** on the related question here: http://datascience.stackexchange.com/a/843/2452. However, given your situation, you might need to work on something simpler. For this, it might be a good idea to review **recommended literature** in the *open source data science master''s curriculum*: http://datasciencemasters.org. Many referenced there sources are free and easily available online. I''m pretty sure that many of these sources contain examples of simple projects that you could (re)implement, extend or otherwise use for your task.', 2452, '2014-11-01 10:16:55.557', 'ef3ee87d-29f9-4b17-89b5-c688e0c88ae6', 2387, 4588, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I need data set for energy forecasting for my project. Currently I am using east/west wind energy dataset from http://www.nrel.gov/. I need one more data set for solar energy. Please let me know where I can find one for free.', 21, '2014-11-01 16:08:55.033', 'aeb2405b-67b6-43eb-9615-d256b52aea06', 2382, 'deleted 36 characters in body; edited tags', 4589, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<dataset><energy>', 21, '2014-11-01 16:08:55.033', 'aeb2405b-67b6-43eb-9615-d256b52aea06', 2382, 'deleted 36 characters in body; edited tags', 4590, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am a newbie to data science with a typical problem. I have a data set with metric1, metric2 and metric3. All these metrics are interdependent on each other. I want to detect anomalies in metric3. Currently, I am using Nupic from numenta.org for my analysis and it doesn''t seem to be effective. Is there any ML library which can detect anomalies in multiple parameters please ?

', 4887, '2014-11-02 07:20:32.603', 'ede88e88-5b49-4f12-b569-3b0ea7dbef60', 2391, 4599, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Anomaly detection in multiple parametes', 4887, '2014-11-02 07:20:32.603', 'ede88e88-5b49-4f12-b569-3b0ea7dbef60', 2391, 4600, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><statistics>', 4887, '2014-11-02 07:20:32.603', 'ede88e88-5b49-4f12-b569-3b0ea7dbef60', 2391, 4601, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I tried to use OMP alogorithm avialable in scikit-learn. My net datasize which includes both target signal and dictionay ~ 1G. However when I ran the code, it exited with mem-error.
The machine has 16G RAM, so I dont think this should have happened. I tried with some logging where the error came and  found that the data got loaded completely into numpy arrays. And it was tha algorithm itself that caused the error. Can someone help me with this
or sugggest more memory efficient algorithm for feature selection, or is subsampling the
data my only option. Are there some deterministic good subsampling techniques
', 4889, '2014-11-02 11:36:28.143', 'b0cfc83d-d99e-4c18-8969-86099b1f2fe0', 2392, 4602, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('scikit-learn OMP mem error', 4889, '2014-11-02 11:36:28.143', 'b0cfc83d-d99e-4c18-8969-86099b1f2fe0', 2392, 4603, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><feature-selection><scalability><scikit>', 4889, '2014-11-02 11:36:28.143', 'b0cfc83d-d99e-4c18-8969-86099b1f2fe0', 2392, 4604, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think perhaps the first thing to decide that will help clarify some of your other questions is whether you want to perform binary classification or multi-class classification. If you''re interested in classifying each instance in your dataset into more than one class, then this brings up a set of new concerns regarding setting up your data set, the experiments you want to run, and how you plan to evaluate your classifier(s). My hunch is that you could formulate your task as a binary one where you train and test one classifier for each class you want to predict, and simply set up the data matrix so that there are two classes to predict - (1) the one you''re interested in classifying and (2) everything else.

In that case, instead of your training set looking like this (where each row is a document and columns 1-3 contain features for that document, and the class column is the class to be predicted):

    1           2           3           class
    feature1    feature2    feature3    politics
    feature1    feature2    feature3    law
    feature1    feature2    feature3    president
    feature1    feature2    feature3    politics

it would look like the following in the case where you''re interested in detecting the politics class against everything else:

    1           2           3           class
    feature1    feature2    feature3    politics
    feature1    feature2    feature3    non-politics
    feature1    feature2    feature3    non-politics
    feature1    feature2    feature3    politics

You would need to do this process for each class you''re interested in predicting, and then train and test one classifier per class and evaluate each classifier according to your chosen metrics (usually accuracy, precision, or recall or some variation thereof).

As far as choosing features, this requires quite a bit of thinking. Features can be highly dependent on the type of text you''re trying to classify, so be sure to explore your dataset and get a sense for how people are writing in each domain. Qualitative investigation isn''t enough to decide once and for all what are good features, but it is a good way to get ideas. Also, look into [TF-IDF][1] weighting of terms instead of just using their frequency within each instance of your dataset. This will help you pick up on (a) terms that are prevalent within a document (and possibly a target class) and (b) terms that distinguish a given document from other documents. I hope this helps a little.


  [1]: http://en.wikipedia.org/wiki/Tf%E2%80%93idf', 4897, '2014-11-03 03:22:30.393', '0ba039b7-5070-4298-b46f-18c9c45c5955', 2393, 4605, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am a newbie to data science with a typical problem. I have a data set with metric1, metric2 and metric3. All these metrics are interdependent on each other. I want to detect anomalies in metric3. Currently, I am using Nupic from numenta.org for my analysis and it doesn''t seem to be effective. Is there any ML library which can detect anomalies in multiple parameters?

', 21, '2014-11-03 08:27:42.083', 'b2f24ee2-f63a-4e71-8a36-f69ddd4dcf0a', 2391, 'deleted 8 characters in body; edited tags; edited title', 4607, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Anomaly detection in multiple parameters', 21, '2014-11-03 08:27:42.083', 'b2f24ee2-f63a-4e71-8a36-f69ddd4dcf0a', 2391, 'deleted 8 characters in body; edited tags; edited title', 4608, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><statistics><anomaly-detection>', 21, '2014-11-03 08:27:42.083', 'b2f24ee2-f63a-4e71-8a36-f69ddd4dcf0a', 2391, 'deleted 8 characters in body; edited tags; edited title', 4609, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In most data acquisition settings it is useful to tag your data with time and location. If I write the data to csv file, what are the best formats that I can use for this two variables? ', 3106, '2014-11-03 12:41:51.737', '2ae92d68-5119-4637-9911-c0a763a79a59', 2394, 4612, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Best format for recording time stamp and GPS', 3106, '2014-11-03 12:41:51.737', '2ae92d68-5119-4637-9911-c0a763a79a59', 2394, 4613, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-formats>', 3106, '2014-11-03 12:41:51.737', '2ae92d68-5119-4637-9911-c0a763a79a59', 2394, 4614, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a continuous variable, sampled over a period of a year at irregular intervals. Some days have more than one observation per hour, while other periods have nothing for days. This makes it particularly difficult to detect patterns in the time series, because some months (for instance October) are highly sampled, while others are not.

![enter image description here][1]

My question is what would be the best approach to model this time series?

 - I believe most time series analysis techniques (like ARMA) need a fixed frequency. I could aggregate the data, in order to have a constant sample or choose a sub-set of the data that is very detailed. With both options I would be missing some information from the original dataset, that could unveil distinct patterns.
 - Instead of decomposing the series in cycles, I could feed the model
   with the entire dataset and expect it to pick up the patterns. For
   instance, I transformed the hour, weekday and month in categorical
   variables and tried a multiple regression with good results (R2=0.71)

I have the idea that machine learning techniques such as ANN can also pick these patterns from uneven time series, but I was wondering if anybody has tried that, and could provide me some advice about the best way of representing time patterns in a Neural network.



  [1]: http://i.stack.imgur.com/7MEXt.png', 3159, '2014-11-03 16:51:47.467', '2a35dd63-5723-45fa-a5c0-17335f36f6de', 2395, 4615, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Modelling Unevenly Spaced Time Series', 3159, '2014-11-03 16:51:47.467', '2a35dd63-5723-45fa-a5c0-17335f36f6de', 2395, 4616, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork><time-series><regression>', 3159, '2014-11-03 16:51:47.467', '2a35dd63-5723-45fa-a5c0-17335f36f6de', 2395, 4617, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In most data acquisition settings it is useful to tag your data with time and location. If I write the data to csv file, what are the best formats that I can use for this two variables if I want to create a heatmap on Google Maps? ', 3106, '2014-11-03 16:55:47.653', '3b338571-b6c7-4556-8c8f-4b081692afdf', 2394, 'added 45 characters in body', 4618, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As Spacedman put it, "best" is pretty subjective. However, as we have found, a good format for time is [Unix time][1] (aka POSIX time, aka Epoch time). Most databases support it and it is still pretty human readable.

For location, we like [decimal degrees][2] as it is easy to read and stored and is compatible with Google Maps API. It''s also easy to convert to other formats if needed.


  [1]: http://en.wikipedia.org/wiki/Unix_time
  [2]: http://en.wikipedia.org/wiki/Decimal_degrees', 4907, '2014-11-03 22:10:13.263', '3a138d53-b8b3-4cc3-8f95-60d92db984ff', 2397, 4623, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''d like to apply some of the more complex supervised machine learning techniques in python - deep learning, generalized addative models, proper implementation of regularization, other cool stuff I dont even know about, etc.

Any recommendations how I could find expert ML folks that would like to collaborate on projects?
', 4910, '2014-11-04 00:58:00.137', 'c48d2314-f3e1-4d8d-b212-d391bb71123d', 2398, 4624, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Python Machine Learning Experts', 4910, '2014-11-04 00:58:00.137', 'c48d2314-f3e1-4d8d-b212-d391bb71123d', 2398, 4625, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><python>', 4910, '2014-11-04 00:58:00.137', 'c48d2314-f3e1-4d8d-b212-d391bb71123d', 2398, 4626, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Check out [LightSide][1] for a GUI introduction to text mining in general, and more specifically for generating features quickly. It was developed (and I believe is still being developed) by researchers at CMU and it''s how I got hooked on text mining. There is quite a bit of functionality right out of the box, and you can quickly import CSV data into the application, extract features, and start running experiments. It also makes use of suites of algorithms from several other prominent, well-regarded open source toolkits like Weka and LibLinear so you know you''re using something credible under the hood. That being said, both of these last mentioned toolkits are definitely worth checking out for added functionality, even if they have a bit of a steeper learning curve. Hope that helps.


  [1]: http://lightsidelabs.com/what/research/', 4897, '2014-11-04 04:23:37.107', 'b82b11ee-89a5-4763-b908-3a4cd2326799', 2399, 4627, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I tried to use OMP alogorithm avialable in scikit-learn. My net datasize which includes both target signal and dictionay ~ 1G. However when I ran the code, it exited with mem-error.
The machine has 16G RAM, so I dont think this should have happened. I tried with some logging where the error came and  found that the data got loaded completely into numpy arrays. And it was tha algorithm itself that caused the error. Can someone help me with this
or sugggest more memory efficient algorithm for feature selection, or is subsampling the
data my only option. Are there some deterministic good subsampling techniques

EDIT:
Relevant code piece
    `
    n=8;
    y=mydata[:,0];
    X=mydata[:,[1,2,3,4,5,6,7,8]];
    #print y;
    #print X;
    print "here";
    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=5,copy_X = False, normalize=True);
    omp.fit(X,y);
    coef = omp.coef_;
    print omp.coef_;
    idx_r, = coef.nonzero();
    for id in idx_r:
            print coef[id], vars[id],"\n";`

The error I get:

 ` File "/usr/local/lib/python2.7/dist-packages/sklearn/base.py", line 324, in score
    return r2_score(y, self.predict(X), sample_weight=sample_weight)
  File "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/metrics.py", line 2332, in r2_score
    numerator = (weight * (y_true - y_pred) ** 2).sum(dtype=np.float64)
MemoryError`
', 4889, '2014-11-04 09:47:09.123', '2e0fa9eb-9dbf-4b5a-bc85-ca01f3762737', 2392, 'added 756 characters in body', 4632, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You could try some competitions from [kaggle](http://kaggle.com).

Data Science courses from Coursera, edX, etc also provide forums for discussion.

Linkedin or freelance sites could be other possibilities.', 3051, '2014-11-04 13:41:52.240', 'f188a0ff-d2ec-4d9a-9dff-72b62df654cc', 2401, 4633, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I had a conversation with someone recently and mentioned my interest in data analysis and who I intended to learn the necessary skills and tools. They suggested to me that while it is great to learn the tools and build the skills there is little point in doing so unless i have specialized  knowledge in a specific field.

They basically summed it to that I''d just be like a builder with a pile of tools who could build a few wooden boxes and may be build better things (cabins, cupboards etc), but without knowledge in a specific field I''d never be a builder people would come to for a specific product.

Has anyone found this or have any input on what to make of this ? It would seem if it was true one would have to learn the data science aspects of things and then learn  a new field just to  become specialized.', 4836, '2014-11-05 09:24:58.003', '31506bc0-9781-4a74-9e47-9b0f0915a9f4', 2403, 4639, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data science without knowledge of a specific topic, is it worth pursuing as a career?', 4836, '2014-11-05 09:24:58.003', '31506bc0-9781-4a74-9e47-9b0f0915a9f4', 2403, 4640, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<education><beginner><career>', 4836, '2014-11-05 09:24:58.003', '31506bc0-9781-4a74-9e47-9b0f0915a9f4', 2403, 4641, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try out AirXcell : [AirXcell calculation software][1].
See documentation [Use AirXCell as an r Console][2]


  [1]: http://airxcell.com
  [2]: http://www.airxcell.com/doc/howTo/useConsole.html', 4929, '2014-11-05 15:16:36.867', '0f9ae0d8-9585-4f13-a6f7-2d8b5e56d026', 2405, 4643, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Drew Conway published the [Data Science Venn Diagram][1], with which I heartily agree:

![Data Science Venn Diagram][2]

On the one hand, you should really read his post. On the other hand, I can offer my own experience: my subject matter expertise (which I like better as a term than "Substantive Expertise", because you should really also have "Substantive Expertise" in math/stats and hacking) is in the retail business, my math/stats are forecasting and inferential statistics, and my hacking skills lie in R.

From this vantage point, I can talk to and understand retailers, and someone who doesn''t have at least a passing knowledge of this field will have to face a *steep* learning curve in a project with retailers. As a side gig, I do statistics in psychology, and it''s exactly the same there. And even with quite some knowledge of the hacking/math/stats part of the diagram, I would have a hard time getting up to speed in, say, credit scoring or some other new subject field.

Once you have a certain amount of math/stats and hacking skills, it is *much* better to acquire a grounding in one or more subjects than in adding *yet* another programming language to your hacking skills, or *yet* another machine learning algorithm to your math/stats portfolio. After all, once you have a solid math/stats/hacking grounding, you could if need be learn such new tools from the web or from textbooks in a relative short time period. But the subject matter expertise, on the other hand, you will likely not be able to learn from scratch if you start from zero. And clients will rather work with some data scientist A who understands their specific field than with another data scientist B who first needs to learn the basics - even if B is better in math/stats/hacking.

Of course, all this will also mean that you will never become an expert in *either* of the three fields. But that''s fine, because you are a data scientist, not a programmer or a statistician or a subject matter expert. There will always be people in the three separate circles who you can learn from. Which is part of what I like about data science.

  [1]: http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram
  [2]: http://i.stack.imgur.com/7W0VM.png', 2853, '2014-11-05 17:03:40.513', 'dcc7add5-d3d7-4c65-8302-d076c31c4282', 2406, 4644, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Another library that I use that has not been mentioned yet is gensim.  Its [Dictionary](http://radimrehurek.com/gensim/corpora/dictionary.html) module allows you to convert a list of words into a list of (id,count) pairs.  It also has an `allow_update` variable that updates the dictionary in the case that new words are encountered at runtime.

It also has built-in support for TF-IDF, LSI, and LDA, among other models.', 1097, '2014-11-05 17:07:02.563', 'fa8b071a-f373-4cbe-9403-525581478f52', 2407, 4645, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('ARIMA, Exponential Smoothing and others indeed require evenly spaced sample points. As you write, you could bucketize your data (say into days), but as you also write, you would lose information. In addition, you may end up with missing values, so you would need to impute, since ARIMA is not very good at handling missing values.

One alternative, as you again write, is to feed time dummies into a regression framework. I personally do not really like categorical dummies, because this implies a sharp cutoff between neighboring categories. This is usually not very natural. So I would rather look at periodic splines with different periodicities. This approach has the advantage of dealing with your uneven sampling and also with missing values.

Be *very* careful about interpreting $R^2$. In-sample fit is notoriously misleading as a measure of out-of-sample forecast accuracy ([see here][1]). I would argue that this disconnect between in-sample fit and out-of-sample forecast accuracy also means that there is no connection between in-sample fit and how well a model "understood" the data, even if your interest lies not in forecasting, but only in modeling per se. My philosophy is that if you can''t forecast a time series well, you haven''t understood it in any meaningful sense.

Finally, don''t overdo the modeling. Just from eyeballing your data, it is obvious that *something* happened in June, on one day in August and in September/October. I suggest you first find out what this *something* was and include this in your model, e.g., as explanatory variables (which you can include in ARIMAX if you want to). What happened there is obviously not seasonality.


  [1]: https://www.otexts.org/fpp/2/5', 2853, '2014-11-05 17:22:00.213', 'd8e09ffc-1a0d-424b-9ba5-ca45807eac84', 2410, 4648, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Sure, you can. Companies are clamoring for data scientists. Be careful though that they all interpret the term differently. Depending on the company you might find yourself asked to do anything from statistics to writing production code. Either one is a full-time job in itself and you have to be prepared for both, so asking for deep specialized knowledge on top of that''s not reasonable, in my opinion, and the companies I''ve talked to stressed the other two areas (esp. the programming). However, I found that it helps to be familiar with the types of problems that you might face. Depending on the sector, that could be anomaly detection, recommendation/personalization, prediction, record linkage, etc. These are things you can learn as examples at the same times as maths and programming.', 381, '2014-11-05 17:50:39.573', 'dd790a8d-7d0f-4ae3-8d7a-739e9a8386c7', 2411, 4649, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('# My Background

I am a graduate student in Civil Engineering. For the analyses of road traffic data (vehicle trajectories as time series) I work with big data sets mostly about a million data points or more.
I started using R language when MS Excel could not open the big data files. Using basic statistics knowledge and R code I developed few algorithms to identify certain patterns in the data which worked for many applications. But I still lack serious programming skills in R.
Now, I am familiar with basic inferential statistics and R packages (plyr, dplyr, ggplot2, etc). Recently I came to know that Machine Learning algorithms also help in defining patterns in the data through supervised/ unsupervised learning and their application might improve the accuracy of prediction of certain ''behaviors'' of drivers using the traffic data.

# Question

Having the basic knowledge of Statistics and R, I want to learn about the data science/ machine learning as a beginner. I know that some concepts in Stats. and ML overlap and that might bridge the gap in my learning of ML. Keeping my background in mind, what resources (books/ online courses) would you recommend me to start learning data science and apply it in my field?', 4933, '2014-11-05 18:57:55.037', '1c618438-2359-4723-b93c-d7ca54445a35', 2412, 4650, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Learning resources for Data Science for applications in road traffic data?', 4933, '2014-11-05 18:57:55.037', '1c618438-2359-4723-b93c-d7ca54445a35', 2412, 4651, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation>', 4933, '2014-11-05 18:57:55.037', '1c618438-2359-4723-b93c-d7ca54445a35', 2412, 4652, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One way to use both metric1 and metric2 in order to find anomalies in metric3 is to consider residual analysis.

In your case, this would require, creating a predictive model with metric1 and metric2 as the predictors and metric3 as the response variable.

Then, calculate the residuals for metric3 as its predicted value subtracted from its true value. Now, you can report the all members of the lowest decile [or any other percentile] as one kind of an anomaly and all the members of the highest decile [or any other percentile]   as another kind of an anomaly. ', 847, '2014-11-05 21:22:09.193', '6f8dc54e-99a6-4ad3-a98d-502cf61f79c2', 2413, 4653, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The best way to learn data science is through problem solving. I suggest you to head over to Kaggle and work through the for-knowledge problems.

To get a good start on Machine Learning problems, acclimate yourself with the tree package in R. This will help you understand how decision trees work, and building upon that, how random forests, gradient boosting machines and other sophisticated tree based algorithms work.

Then, there are SVMs and deep learning models.

To get an understanding of unsupervised learning problems, learn k-means and employ it for clustering.

Other general concepts/ ideas to understand are:

 1. cross-validation

 2. overfitting, regularization

 3. bias-variance trade off

 4. dimensionality reduction/ variable selection

 5. generalization error

 6. ensemble learning


For books, the most common recommendation to anyone who is familiar with statistics and wants to get into Machine Learning is ["The Elements of Statistical Learning"][1] by Hastie, Tibshirani, and Friedman.


  [1]: http://statweb.stanford.edu/~tibs/ElemStatLearn/', 847, '2014-11-05 21:55:43.587', '79e66a6f-f648-44c8-b2ff-a90676608022', 2414, 4654, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would start by graphing the time variable vs other variables and looking for trends.

## For example

![enter image description here][1]

In this case there is a periodic weekly trend and a long term upwards trend.  So you would want to encode two time variables:

- `day_of_week`
- `absolute_time`

## In general

There are several common time frames that trends occur over:

- `absolute_time`
- `day_of_year`
- `day_of_week`
- `month_of_year`
- `hour_of_day`
- `minute_of_hour`

Look for trends in all of these.

## Weird trends

Look for weird trends too.  For example you may see rare but persistent time based trends:

- `is_easter`
- `is_superbowl`
- `is_national_emergency`
- `etc.`

These often require that you cross reference your data against some external source that maps events to time.

## Why graph?

There are two reasons that I think graphing is so important.

 - *Weird trends*
   While the general trends can be automated pretty easily (just add them
   every time), weird trends will often require a human eye and knowledge
   of the world to find.  This is one reason that graphing is so
   important.

 - *Data errors*
   All too often data has serious errors in it.  For example, you may find that the dates were encoded in two formats and only one of them has been correctly loaded into your program.  There are a myriad of such problems and they are surprisingly common.  This is the other reason I think graphing is important, not just for time series, but for any data.

  [1]: http://i.stack.imgur.com/QGYUC.png', 4808, '2014-11-05 22:02:14.857', '170e8639-a9e6-44cc-9e2f-8564bd3cea3c', 2370, 'deleted 1 character in body', 4655, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to ask your opinion on how to choose a similarity measure. I have a set of vectors of length N, each element of which can contain either 0 or 1. The vectors are actually ordered sequences, so the position of each element is important. Suppose I have three vectors of length 10, x_1 x2, x3: x1 has three 1 at positions 6,7,8 (indexes start from 1. Both x2 and x3 have an additional 1, but x2 has it in position 9 while x3 has it in position 1. I am looking for a metric according to which x1 is more similar to x2 than to x3, in that the additional 1 is closer to the "bulk" of ones. I guess this is a relatively common problem, but I am confused on the best way to approach it.
Many thanks in advance!', 1004, '2014-11-06 07:38:33.490', '2dc38a1a-3321-4e6e-a187-5aee4a800bc7', 2416, 4657, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Similarity measure for ordered binary vectors', 1004, '2014-11-06 07:38:33.490', '2dc38a1a-3321-4e6e-a187-5aee4a800bc7', 2416, 4658, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<similarity>', 1004, '2014-11-06 07:38:33.490', '2dc38a1a-3321-4e6e-a187-5aee4a800bc7', 2416, 4659, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If speed isn''t a great concern you could use a [KDE](http://en.wikipedia.org/wiki/Kernel_density_estimation) with a high bandwidth to pick up the similarity between neighboring elements, then an appropriate metric like the [K-L divergence](http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence). Similarity and divergence are complementary, of course, so as a final step you would have to relate them; e.g., `sim(A, B) = exp[- KLD(A, B)]`', 381, '2014-11-06 08:52:22.617', '77ef4c20-c183-471d-8b66-2c12d84449f7', 2417, 4660, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The [R Programming Wikibook](http://en.wikibooks.org/wiki/R_Programming) is a nice collaborative handbook for R.', 187, '2014-11-06 13:56:02.497', '8648ab50-32e4-4009-85ef-f6fb82c8d072', 2418, 4661, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One thing you could do is fuzzify your vectors: replace each 1 by (for example) 0.4 in its position, 0.2 in the neighbouring positions, and 0.1 in the second position over. Then add up what''s in each position.
With these fuzzified vectors, you can apply a similarity metric either based on a distance or one like cosines similarity.
Your example would produce: (showing only first decimal)

0000011100 -> 0001378731

0000011110 -> 0001378873

1000011100 -> 4211378731


cos(x1, x2) = 0.9613,  cos(x1,x3) = 0.9469
', 4760, '2014-11-06 15:19:35.017', '470e45c1-0d05-486d-b3d7-2793e9438443', 2419, 4662, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Keep in mind that "big data" is an increasingly trendy thing for a company to say they''re involved in.  Higher ups might read an article about it in HBR, and say to themselves, "I''ve got to get me some of that" (not that they''re necessarily wrong).

What this means for you is that the advanced analytics isn''t as necessary for that company as just getting something up and running might be.

Luckily for you, most of the components said companies might need are free.  Moreover, I believe both Hortonworks and Cloudera have free "sandbox" virtual machines, which you can run on your PC, to play around with and get your bearings.

Advanced analytics on big data platforms are valuable, to be sure, but many companies need to learn to crawl before they can run. ', 4943, '2014-11-06 17:56:06.257', '0f8d8c57-312b-4361-be18-2df4e7be4905', 2420, 4663, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is a really strange question in my opinion. Why you''re going to move in a new direction if you are not sure that you love this new direction or at least find it very interesting? If you do love Big Data, why do you care about the PhD intelligent creatures that are already in the field? The same amount of PhD creatures are in every area of IT. Please have a quick read at this very nice article http://www.forbes.com/sites/louisefron/2013/09/13/why-you-cant-find-a-job-you-love/ and then ask yourself if you love Big Data enough and you are ready to add your grain of sand to the mountain of knowledge', 4944, '2014-11-06 21:36:51.897', '9d681814-6de4-4494-99f2-f2f402699b7f', 2421, 4664, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m just starting to work on a relatively large dataset after ML course in Coursera.
Trying to work on https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD.
Got an accuracy of 5.2 in training and test set with linear regression using gradient descent in octave.

I tried adding all possible quadratic features, but the code just won''t stop executing in my HP Pavilion g6 2320tx, with 4GB RAM in Ubuntu 14.04.

Is this beyond the data size capacity of Octave ?', 4947, '2014-11-07 04:13:44.583', '096af031-7f37-475b-b00f-6614ff7d45a0', 2422, 4665, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Clarification about Octave data size limit', 4947, '2014-11-07 04:13:44.583', '096af031-7f37-475b-b00f-6614ff7d45a0', 2422, 4666, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<octave>', 4947, '2014-11-07 04:13:44.583', '096af031-7f37-475b-b00f-6614ff7d45a0', 2422, 4667, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You have about 4GB of RAM on your machine and Octave is an in memory application.

If you want to work with 515345 instances and 4275 features, assuming that you are using double precision (i.e. 8 bytes), you would need a memory of 515345*4275*8/1000000/1024 bytes ~ 17.6 GB. Even if you were using 4 bytes for each data point, you would require at least 9 GB for the computation to go through. ', 847, '2014-11-07 05:53:25.600', 'bb955abe-6104-4d16-86d0-ad93b5e635ff', 2423, 4668, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You have about 4GB of RAM on your machine and Octave is an in memory application.

If you want to work with 515345 instances and 4275 features, assuming that you are using double precision (i.e. 8 bytes), you would need a memory of 515345*4275*8/1000000/1024 bytes ~ 17.6 GB. Even if you were using 4 bytes for each data point, you would require at least 9 GB for the computation to go through.

This issue might not be the Octave memory restriction in this case. See [here][1] for further details on Octave''s memory usage.


  [1]: http://stackoverflow.com/questions/565806/is-anyone-using-64-bit-build-of-octave', 847, '2014-11-07 06:04:59.140', '9ae0c595-c087-4f3a-9506-115fd57e7d82', 2423, 'added 225 characters in body', 4669, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would recommend those materials:

 * Python
   * [Python for Data Analysis](http://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1449319793) - book which nicely covers Pandas workflow with IPython.
 * R
   * [Coursera Data Science Specialisation](https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage) - free nine one month time consuming courses which introduces R step by step from beginning to machine learning.
   * [Data Camp courses](https://www.datacamp.com/courses) - at least 4 free courses covering topics from data analysis.', 82, '2014-11-07 08:09:40.260', '58acec3f-c28c-43dd-bef0-9cf8a3ef7c33', 2424, 4670, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":471,"DisplayName":"Spacedman"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-11-07 08:24:49.463', '539f0b7d-be6c-4d56-af6d-6cc8c3d76a85', 2398, '102', 4671, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m just starting to work on a relatively large dataset after ML course in Coursera.
Trying to work on https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD.
Got an accuracy of 5.2 in training and test set with linear regression using gradient descent in octave.

I tried adding all possible quadratic features (515345 instances and 4275 features), but the code just won''t stop executing in my HP Pavilion g6 2320tx, with 4GB RAM in Ubuntu 14.04.

Is this beyond the data size capacity of Octave ?', 847, '2014-11-07 08:28:07.227', 'e9d63ae2-0d05-49aa-ac87-4ca8871c246c', 2422, 'data size made clear', 4673, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-07 08:28:07.227', 'e9d63ae2-0d05-49aa-ac87-4ca8871c246c', 2422, 'Proposed by 847 approved by 21 edit id of 170', 4674, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<r><neuralnetwork><time-series><forecast>', 97, '2014-11-07 08:28:10.637', '2cf993fe-f532-466b-82a2-63ffb331a4fc', 1059, 'Add more relevant tags', 4675, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-07 08:28:10.637', '2cf993fe-f532-466b-82a2-63ffb331a4fc', 1059, 'Proposed by 97 approved by 21 edit id of 169', 4676, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Another possibility is the [Earth Mover''s Distance][1]. I applied it to Computer Vision, but I think that it may be adapted to your specific problem.


  [1]: http://en.wikipedia.org/wiki/Earth_mover''s_distance', 2576, '2014-11-07 09:04:17.603', 'f4c355cc-4a15-4df2-afa7-681e798102c6', 2425, 4678, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a data set that is pivoted in to the following format:

[key] [id] [0] [1] [5] [10] [15] [60] [120] [180],.. [365]

So key could be

[Products] [1000] [15,000] [4000]... etc

Where products is the category of item being reviewed and key is the identifier for the product; the only fields (0, 1,... 180,.. [365]) are individual daily samples identify how many of "x" product were logged as either sold, in-stock etc.

What I need to do is perform some kind of analysis on an entire slew of products and their inventory levels. i.e. each import of data I need to make sure the incoming data is accurate or predictably accurate and that some human did not typo a stock level. The problem is, using a simple average or rolling average can introduce significant variance and smoothing out the average renders my analysis less reliable. Ideally this analysis would trigger an alarm that someone would have to investigate.

Is there a better and more accurate way of performing this analysis?

Thanks!', 4956, '2014-11-07 21:36:00.080', '94648e7d-a5b6-4f25-8789-be9f90786a1b', 2426, 4679, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Validity of data', 4956, '2014-11-07 21:36:00.080', '94648e7d-a5b6-4f25-8789-be9f90786a1b', 2426, 4680, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><statistics>', 4956, '2014-11-07 21:36:00.080', '94648e7d-a5b6-4f25-8789-be9f90786a1b', 2426, 4681, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The meaning of multi-class classification rules

    Example: I have two classification rules:
    (Refund, No)  (Cheat, No) Support = 0.4, Confidence = 0.57
    (Refund, No)  (Cheat, Yes) Support = 0.3, Confidence = 0.43

=> multi-class classification rules:

    (Refund, No)  (Cheat, No) v (Cheat, Yes)

When predicted classification for test data, (Cheat, No) will be selected priority so why we need to have (Cheat, Yes) in multi-class classification rules here?

Please help me solve this problem, I need it very urgently.

Thanks, Dung.', 3503, '2014-11-08 12:22:17.920', '69999049-2831-43e6-b4c6-97e57b0e59fd', 2427, 4684, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The meaning of multi-class classification rules', 3503, '2014-11-08 12:22:17.920', '69999049-2831-43e6-b4c6-97e57b0e59fd', 2427, 4685, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 3503, '2014-11-08 12:22:17.920', '69999049-2831-43e6-b4c6-97e57b0e59fd', 2427, 4686, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The meaning of multi-class classification rules

    Example: I have two classification rules:
    (Refund, No)  (Cheat, No) Support = 0.4, Confidence = 0.57
    (Refund, No)  (Cheat, Yes) Support = 0.3, Confidence = 0.43

=> multi-class classification rules:

    (Refund, No)  (Cheat, No) v (Cheat, Yes)

When predicted classification for test data, (Cheat, No) will be selected priority so why we need to have (Cheat, Yes) in multi-class classification rules here?
', 836, '2014-11-08 15:16:49.740', 'f90305b7-4cdf-4375-87f4-c2b5c9fa42b4', 2427, 'Remove personal messages of urgency and thanks (the first may attract unwanted remarks, and never results in better or faster answers anyway)', 4687, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-08 15:16:49.740', 'f90305b7-4cdf-4375-87f4-c2b5c9fa42b4', 2427, 'Proposed by 836 approved by 3503 edit id of 171', 4688, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('
    (Cheat, No) will be selected (cheat will be classified as No) for the rule (Refund, No).

In a binary response variable like cheat, all the information can be inferred from just the first rule:

    (Refund, No)  (Cheat, No) Support = 0.4, Confidence = 0.57

The other rule is redundant.

However, in the case of a multi-class response variable, we would like to have all the rules written out so we exactly know the likelihood of the rule implying each of the different classes in the response variable. To keep things consistent, this is also done for the case when the response variable is binary. ', 847, '2014-11-08 18:33:37.063', 'cec10a7f-9d03-4f8c-b8e0-4aaf2152c891', 2428, 4689, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The first thing you should do is to identify how large an error your analysis can handle. That will make your job much easier because you won''t have to find everything.

A standard way of identifying "suspicious" data is is Benford''s Law, which predicts the distribution of the first digit of each number. It can also be generalized for for other digits. http://en.wikipedia.org/wiki/Benford''s_law

As for finding outliers, I''d probably use boxplots, particularly because you can achieve high data density with them, reducing the time to manually skim them.

One thing that might be useful is to compare the ratio of one variable to another- in my company we use this method all the time.', 1241, '2014-11-08 20:29:37.377', 'ca00c066-38f7-4d16-a21e-d0ea34226f04', 2429, 4690, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The meaning of multi-class classification rules

    Example: I have two classification rules (Refund is a predictor and Cheat is a binary response):
    (Refund, No)  (Cheat, No) Support = 0.4, Confidence = 0.57
    (Refund, No)  (Cheat, Yes) Support = 0.3, Confidence = 0.43

=> multi-class classification rules:

    (Refund, No)  (Cheat, No) v (Cheat, Yes)

When predicted classification for test data, (Cheat, No) will be selected priority so why we need to have (Cheat, Yes) in multi-class classification rules here?
', 847, '2014-11-08 22:52:49.717', '323b2b91-47d9-406f-9be2-4dcd60436660', 2427, 'explained the predictor and response variables for the example provided in the question ', 4691, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-08 22:52:49.717', '323b2b91-47d9-406f-9be2-4dcd60436660', 2427, 'Proposed by 847 approved by 3503 edit id of 172', 4692, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-11-08 23:12:05.950', '69c87747-263d-4bca-9b26-38aae19534f2', 2403, '102', 4695, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In my experience to have a PhD doesn''t mean necessarily be good in the enviroment of data science company, I work as data scientist and I''m just an engineer but I''ve known some universitary teachers who works in collaboration with my company and sometimes I''ve said them that Their point of view was not right because despite of their ideas and reasonings were right they are not applicables to the company activities, so we had to modify some data models to make them usefull for the company and the results lost their value so we had to seek new models. What I mean is that Data Science is a multidisciplinar area so many different people working together is needed so I think that your skills could be very useful in a data scientist team, you only have to find where you fit ;)', 4966, '2014-11-09 10:36:51.290', '6b096f0f-4436-4799-8fa7-11e073550080', 2431, 4700, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('By "large", I mean in the range of 100m to 10b rows.

I''m currently using both Hadoop MapReduce and Amazon RedShift. MapReduce has been a little disappointing here. Redshift works very well if the data is distributed well for the given query.

Are there other technologies that I should be looking at here? If so, what are the trade offs?', 4967, '2014-11-09 14:11:18.350', 'b5a47f6f-b587-44a6-961c-9141b61579f9', 2432, 4701, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What technologies are fastest at performing joins on large datasets?', 4967, '2014-11-09 14:11:18.350', 'b5a47f6f-b587-44a6-961c-9141b61579f9', 2432, 4702, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><performance><map-reduce><aws>', 4967, '2014-11-09 14:11:18.350', 'b5a47f6f-b587-44a6-961c-9141b61579f9', 2432, 4703, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was going through an IEEE Research paper which has used Fuzzy ARTMAP for predicting the price of electricity given some highly correlated data.

As per my basic understanding about Fuzzy ARTMAP it is a classification algorithm, so how will it be able to predict continuous data?

The text from research paper is:

> In the architecture of the FA network, the preprocessing stages take
> the input vector and contribute to produce complement coding, which
> avoids category proliferation, i.e., the creation of a relatively
> large number of categories to represent the training data. A sequence
> of input vectors (price and demand) and their respective target
> vectors are introduced to the FA network in order to classify the
> input pattern correctly. The classied input patterns are then grouped
> into labels using membership functions.

I was using MATLAB to implement the same, so is there a library in MATLAB to approach towards the solution.
', 4973, '2014-11-10 00:10:26.947', '243cb3d2-884e-40db-8c55-95d9ef5ff727', 2433, 4705, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('FUZZY ARTMAP for continuous data', 4973, '2014-11-10 00:10:26.947', '243cb3d2-884e-40db-8c55-95d9ef5ff727', 2433, 4706, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><regression>', 4973, '2014-11-10 00:10:26.947', '243cb3d2-884e-40db-8c55-95d9ef5ff727', 2433, 4707, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('More importantly than the technology is the type of join you are using. For instance if the join keys are sorted, you can use sort merge joins and use join orders to get a better performance.

That being said, you can use in memory solutions for fastest joins if the size of your intermediate results will not blow up your cluster memory. Look at Spark SQL or Mem-SQL for instance.', 4974, '2014-11-10 02:15:27.147', '1613e0a3-f8d4-4e41-be91-a6b3bc4c5374', 2434, 4708, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you are working on collaborative filtering you should pose the problem as a low-rank matrix approximation, wherein both the users are items are co-embedded into the same low-dimensionality space. Similarity search will be much simpler then. I recommend using LSH, as you suggested. Another fruitful avenue for dimensionality reduction not yet mentioned is [random projection](http://en.wikipedia.org/wiki/Random_projection).', 381, '2014-11-10 09:40:26.753', '0f792aee-233f-40fe-994d-aed4550f1f84', 2435, 4710, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I need to collect several large datasets (thousands of samples, dozens of features) for regression with only categorical inputs. I already look for such datasets in the UCI repository, but I did not find any suitable one.

Does anybody know of any such dataset, or of any additional dataset repository on the Internet?


', 2576, '2014-11-10 09:45:13.063', '53012a1d-c1d2-4a93-b6ef-e6dfc7274cf5', 2436, 4711, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Large categorical dataset for regression', 2576, '2014-11-10 09:45:13.063', '53012a1d-c1d2-4a93-b6ef-e6dfc7274cf5', 2436, 4712, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><categorical-data>', 2576, '2014-11-10 09:45:13.063', '53012a1d-c1d2-4a93-b6ef-e6dfc7274cf5', 2436, 4713, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('n this paper ([here][1])  they suppose a "unified architecture for NLP" with deep neural networks with multitask learning

My problem is to understand the layer architecture in figure 1, see below:
![unified deep learning architecture for NLP][2]

1. Is someone able to give me a concrete, reproducible example how this architecture processing 3 sentences through their layers?
2. What are the outputs after each layer?
3. Why they choose which layer?

Thans in advance!


  [1]: http://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf
  [2]: http://i.stack.imgur.com/RoU0P.png', 3615, '2014-11-10 11:13:07.987', '6f6d8c91-9cd1-48e1-bcc9-d20681032011', 2437, 4714, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How does the supposed "Unified Architecture for NLP" from Collobert and Weston 2008 really works?', 3615, '2014-11-10 11:13:07.987', '6f6d8c91-9cd1-48e1-bcc9-d20681032011', 2437, 4715, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><neuralnetwork>', 3615, '2014-11-10 11:13:07.987', '6f6d8c91-9cd1-48e1-bcc9-d20681032011', 2437, 4716, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can easily have an RStudio server installed in Digital Ocean using this package:
[https://github.com/sckott/analogsea][1]


  [1]: https://github.com/sckott/analogsea', 4978, '2014-11-10 11:16:07.537', 'fe3acee7-8f38-46d4-b36b-96648ae0efc0', 2438, 4717, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a [database][1] of 3190 instances of DNA consisting of 60 sequential DNA nucleotide positions classified according to 3 types: EI, IE, Other.

I want to formulate a supervised classifier.

My present approach is to formulate a 2nd order Markov Transition Matrix for each instance and apply the resulting data to a Neural Network.

How best to approach this classification problem, given that the Sequence of the data should be relevant? Is there a better approach than the one I came up with?


  [1]: https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Splice-junction+Gene+Sequences%29', 136, '2014-11-10 16:20:22.647', 'fcb78011-0100-4cd9-996d-f417447ebdb6', 2439, 4718, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Classification of DNA Sequences', 136, '2014-11-10 16:20:22.647', 'fcb78011-0100-4cd9-996d-f417447ebdb6', 2439, 4719, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<sequence><sequential><dna>', 136, '2014-11-10 16:20:22.647', 'fcb78011-0100-4cd9-996d-f417447ebdb6', 2439, 4720, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You should probably start with a very basic approach: bag-of-words representation (vector as long as your vocabulary, 1 if the word is found in the text, 0 if it''s not), and a simple classifier like naive bayes. This works surprisingly well to find topics (a little less for sentiment classification).
For preprocessing you would probably want to do stop-word removal and stemming (in order to reduce the vocabulary) rather than POS tagging.

The problem with the basic approach is that you would have a n-class classifier, and no "this fits multiple categories" or "this fits 0 categories" answers. If you want to include that aspect, then the best is to design n 2-class classifiers, one for each of your classes, where each classifier decides whether the text fits the class or not.

But I would try out-of-the box naive bayes first, just to see how it works. You can use Weka, it''s free, open-source, and can be integrated with java. You can also do the preprocessing (stemming) with the Python NLTK.

', 4760, '2014-11-10 18:46:42.993', 'd845c4b7-e668-46e3-86b6-9fa27b45c92f', 2441, 4724, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('All you need are data sets with enough records and enough features for your purposes. You can simply convert any continuous variables into categorical ones by grouping.', 1241, '2014-11-10 19:27:03.397', '5e0426cc-00ae-4187-bf49-22f475a37bac', 2442, 4725, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('May be it will be a little offtopic, but I''d like to highly recommend you to go through this MOOC https://www.coursera.org/course/statistics. This is a very good and clear introduction to statistics. It give you a base principles about core field in data science. I hope it will be a good start point for beginning friendship between you and statistics.', 4842, '2014-11-11 08:34:22.210', '383461e9-41d6-4487-b181-2daf28fe0389', 2443, 4726, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a [database][1] of 3190 instances of DNA consisting of 60 sequential DNA nucleotide positions classified according to 3 types: EI, IE, Other. So its a multi-class classification problem with 3190 instances and 60 predictors.

My present approach to solve this classification problem is to formulate a 2nd order Markov Transition Matrix for each instance and apply the resulting data to a Neural Network.

How best to approach this classification problem, given that the Sequence of the data should be relevant? Is there a better approach than the one I came up with?


  [1]: https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Splice-junction+Gene+Sequences%29', 847, '2014-11-11 11:28:08.323', 'db313a5c-c089-467c-a188-a59b5a447e17', 2439, 'edited the structure of the question to make the classification aspect of the problem clearer', 4730, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-11 11:28:08.323', 'db313a5c-c089-467c-a188-a59b5a447e17', 2439, 'Proposed by 847 approved by 21 edit id of 173', 4731, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I need to analyse a dataset about mobile phone usage (#calls, #sms, #internetConnections) per each cell and hour in the different days.
My purpouse is detecting similarities in this usage (Monday-Tuesday is similar... or Monday night is different...). After this, I''d like to find the reason they are similar/dissimilar.

What can I apply?


PS: I really hope this is the right place to ask this question', 989, '2014-11-11 15:03:51.680', 'd488c084-ab93-45ea-b2d1-eb511b0ad696', 2445, 4735, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Analyzing mobile usage', 989, '2014-11-11 15:03:51.680', 'd488c084-ab93-45ea-b2d1-eb511b0ad696', 2445, 4736, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><clustering>', 989, '2014-11-11 15:03:51.680', 'd488c084-ab93-45ea-b2d1-eb511b0ad696', 2445, 4737, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I need to analyse a dataset about mobile phone usage (#calls, #sms, #internetConnections) per each cell and hour in the different days.
My purpouse is detecting similarities in the data (Monday-Tuesday is similar... or Monday night is different...). After this, I''d like to find the reason they are similar/dissimilar.

What can I apply?


PS: I really hope this is the right place to ask this question', 989, '2014-11-11 15:17:16.140', '72979787-5c31-4e3a-838c-3a21e2f0d017', 2445, 'deleted 2 characters in body', 4738, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a biosemi bdf of EEG data which contains 32 channel.
I''ve opened it using biosig, everything works great, a first list is channel and inside each list there are eeg data.
But if I open it using MNE it the first list is eeg data, and the second list (inside the list of eeg data) are two list of eeg data.

this is how I open the data using MNE

    raw_file=read_raw_edf("E:\eeg DATA\\256\s02_reduced.bdf",preload=True,verbose=True)

am I missing something here?', 273, '2014-11-11 18:34:40.213', 'cfd5ab2d-a10f-4328-82ab-4b0ee12c463f', 2446, 4739, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Opening biosemi bdf data using MNE and biosig using python', 273, '2014-11-11 18:34:40.213', 'cfd5ab2d-a10f-4328-82ab-4b0ee12c463f', 2446, 4740, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python>', 273, '2014-11-11 18:34:40.213', 'cfd5ab2d-a10f-4328-82ab-4b0ee12c463f', 2446, 4741, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to create a logistic regression model in jpmml, then write the PMML to a file. The problem I''m having, is that I can''t find any way to create a custom tag, such as "shortForm" and "longForm" in the following example:

    <MapValues outputColumn="longForm">
      <FieldColumnPair field="gender" column="shortForm"/>
      <InlineTable>
        <row><shortForm>m</shortForm><longForm>male</longForm>
        </row>
        <row><shortForm>f</shortForm><longForm>female</longForm>
        </row>
      </InlineTable>
    </MapValues>

Here''s what I have so far:

    MapValues mv = new MapValues("output")
      .withFieldColumnPairs(
   new FieldColumnPair( new FieldName("gender"), "shortForm" )
   ).withInlineTable(
   new InlineTable().withRows(
     new Row().with???( new ??? )
 )))

In short, I am asking for an API call I can use to instantiate the "shortForm" element in the example, and attach it to the "row" object. I''ve been all through the API, examples, and Google/SO, and can''t find a thing.

Thanks for your help!', 4999, '2014-11-11 19:22:00.073', '57069f5c-eb9a-4f6b-b146-af0621b7220a', 2447, 4742, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How can I create a custom tag in jpmml?', 4999, '2014-11-11 19:22:00.073', '57069f5c-eb9a-4f6b-b146-af0621b7220a', 2447, 4743, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<logistic-regression><java>', 4999, '2014-11-11 19:22:00.073', '57069f5c-eb9a-4f6b-b146-af0621b7220a', 2447, 4744, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I need to analyse a dataset about mobile phone usage (#calls, #sms, #internetConnections) per each cell and hour in the different days.

    [date] [CGI/Position] [#calls] [#sms] [#internetConnections]

My purpouse is detecting similarities in the data (Monday-Tuesday is similar... or Monday night is different...). After this, I''d like to find the reason they are similar/dissimilar.

What can I apply?


PS: I really hope this is the right place to ask this question', 989, '2014-11-11 21:36:08.950', '5a748d98-61cf-4770-9026-feba3bfa074c', 2445, 'added 70 characters in body', 4745, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('We have a ruby-on-rails platform (w/ postgreSQL db) for people to upload various products to trade. Of course, many of these products listed are the same, while they are described differently by the consumer (either through spelling, case etc.) "lots of duplicates"

For the purposes of analytics and a better UX, we''re aiming to create an evolving "master product list", or "whitelist", if you will, that will have users select from an existing list of products they are uploading, OR request to add a new one. We also plan to enrich each product entry with additional information from the web, that would be tied to the "master product".

Here are some methods we''re proposing to solve this problem:

A) Take all the "items" listed in the website (~90,000), de-dupe as much as possible by running select "distinct" queries (while maintaining a key-map back to original data by generating an array of item keys from each distinct listing in a group-by.)

THEN

A1) Running this data through mechanical turk, and asking each turk user to list data in a uniform format.

OR

A2) Running each product entry through the Amazon products API and asking the user to identify a match.

or

A3) A better method?
', 5004, '2014-11-12 01:31:01.377', 'e3f718ba-f286-4d5e-8240-642d870d50bf', 2448, 4746, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Method to create master product database to validate entries, and enrich data set', 5004, '2014-11-12 01:31:01.377', 'e3f718ba-f286-4d5e-8240-642d870d50bf', 2448, 4747, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-cleaning><sql>', 5004, '2014-11-12 01:31:01.377', 'e3f718ba-f286-4d5e-8240-642d870d50bf', 2448, 4748, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Clustering Approach**:

Run a clustering algorithm. Something like k-means should work well with this kind of a dataset. While doing this, I would not feed the day_of_week information into the clustering algorithm.

I would suggest running k-means (after normalizing each of the columns). Choose a small number of clusters that is easy to investigate (or you could use the number of clusters that maximizes the BIC).

Investigate the clusters to understand membership by day_of_week in each of these clusters.


**Multi-class Classification Approach**:

Treat the day_of_week as the response that you would like to predict. Build a decision tree of a fixed depth to predict the day_of_week given the columns. By examining this tree, you can easily tell, which decisions led to a set of leaves being labeled Sunday vs the set of decisions that led to a set of leaves being labeled Monday. These decisions will also help you understand the similarities between different days.

  ', 847, '2014-11-12 03:12:11.550', '18447e98-848c-41f8-b65e-b4f339bfcd26', 2449, 4749, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try the [1998 KDD Cup dataset][1]. Its a regression problem with categorical and integer predictors. For your task, you could either treat integer predictors as categorical or ignore them completely.


  [1]: https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data', 847, '2014-11-12 06:14:15.090', '0b286cd8-9657-43d8-ba53-7569e30eed4e', 2450, 4750, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There are two straight forward (vanilla) ways without going for any fancy featurization:

**Clustering**:

Run a clustering algorithm. Something like k-means should work well with this kind of a dataset. While doing this, I would not feed the day_of_week information into the clustering algorithm.

I would suggest running k-means (after normalizing each of the columns). Choose a small number of clusters that is easy to investigate (or you could use the number of clusters that maximizes the BIC).

Investigate the clusters to understand membership by day_of_week in each of these clusters.


**Multi-class Classification**:

Treat the day_of_week as the response that you would like to predict. Build a decision tree of a fixed depth to predict the day_of_week given the columns. By examining this tree, you can easily tell, which decisions led to a set of leaves being labeled Sunday vs the set of decisions that led to a set of leaves being labeled Monday. These decisions will also help you understand the similarities between different days.

  ', 847, '2014-11-12 06:23:54.947', '45bd4c53-5219-4c89-843c-1ba565858cd4', 2449, 'added 75 characters in body', 4751, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For problems where the data represents online fraud or insurance (where each row represents a transaction), it is typical for the response variable to denote the value of fraud committed in dollars. Such a response value might have less than 5% non-zero values denoting fraudulent transactions.

I have two questions regarding such a dataset:

 1. What algorithms can we use to ensure that the model not only predicts the fraudulent transactions accurately, but also predicts the value of fraud associated with these.
 2. Assuming that we can quantify the cost involved in each false positive (tagging a non-fraudulent transaction as fraudulent) and cost incurred due to a false negative (tagging a fraudulent transaction as non-fraudulent), how can we optimize the model to maximize savings?

', 847, '2014-11-12 09:21:19.490', '1b4b0968-7103-434c-91a6-d308d98e1b97', 2451, 4752, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Modeling when the response variable has too many 0''s and few continuous values?', 847, '2014-11-12 09:21:19.490', '1b4b0968-7103-434c-91a6-d308d98e1b97', 2451, 4753, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><regression><predictive-modeling><unbalanced-classes>', 847, '2014-11-12 09:21:19.490', '1b4b0968-7103-434c-91a6-d308d98e1b97', 2451, 4754, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I need to analyse a dataset about mobile phone usage (#calls, #sms, #internetConnections) per each cell and hour in the different days.

    [date] [CGI/Position] [#calls] [#sms] [#internetConnections]

My purpose is detecting similarities in the data (Monday-Tuesday is similar... or Monday night is different...). After this, I''d like to find the reason they are similar/dissimilar.

What can I apply?', 847, '2014-11-12 09:41:43.677', '898523ae-cb87-46d4-b731-76beb1d10c5d', 2445, 'fixed spelling error', 4755, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-12 09:41:43.677', '898523ae-cb87-46d4-b731-76beb1d10c5d', 2445, 'Proposed by 847 approved by 989 edit id of 174', 4756, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I need to analyse a dataset about mobile phone usage (#calls, #sms, #internetConnections) per each cell and hour in the different days.

    [date] [CDR/Position] [#calls] [#sms] [#internetConnections]

My purpose is detecting similarities in the data (Monday-Tuesday is similar... or Monday night is different...). After this, I''d like to find the reason they are similar/dissimilar.

What can I apply?', 989, '2014-11-12 09:42:00.080', '339fd8f5-6ec9-4a52-b629-6e951a0a20ff', 2445, 'edited body', 4757, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Analyzing mobile usage. What kind of approach should I apply?', 989, '2014-11-12 09:42:00.080', '339fd8f5-6ec9-4a52-b629-6e951a0a20ff', 2445, 'edited body', 4758, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can/should use a generic Java Architecture for XML Binding (JAXB) approach.

Simply put, call `Row#withContent(Object...)` with instances of `org.w3c.dom.Element` that represent the desired XML content.

For example:

    Document document = documentBuilder.newDocument();
    Element shortForm = document.createElement("shortForm");
    shortForm.setTextContent("m");
    Element longForm = document.createElement("longForm");
    longForm.setTextContent("male");
    row = row.withContent(shortForm, longForm);', 5012, '2014-11-12 11:08:44.973', 'ea18dda0-8f97-48fc-821a-91711c0c6ff8', 2452, 4759, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('All you need are data sets with enough records and enough features for your purposes. You can simply convert any continuous variables into categorical ones by grouping. Some sources for large sets can be found by a search for "large free data sets".', 1241, '2014-11-12 11:25:19.497', 'ce32e6d6-07fe-4844-9663-ea610677c0e1', 2442, 'Added possible sources', 4760, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('All you need are data sets with enough records and enough features for your purposes. You can simply convert any continuous variables into categorical ones by grouping. Some sources for large sets can be found by a search for "large free data sets". If you are dead set on lots of categorical data, try insurance data (given that I''m an actuary, I should have thought of that earlier). Those tend to be laden with categorical variables, as I well know from first person experience.', 1241, '2014-11-12 11:45:02.060', '2a731554-8d11-42c4-a95d-d92f8fbb6343', 2442, 'Added suggestion about insurance databases', 4761, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you prefer quick hands-on/interactive tutorials, below are my suggestions -

Python - [codeacademy][1], Google Python Class

R - CodeSchool''s [''Try R''][2] and DataCamp (suggested above)


  [1]: http://www.codecademy.com/
  [2]: http://tryr.codeschool.com/', 782, '2014-11-12 14:51:42.213', 'f51a69c4-c261-4fac-87f2-e338755ce277', 2453, 4762, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('let''s assume that I want to train a stochastic gradient descent regression algorithm using a dataset that has N samples. Since the size of the dataset is fixed, I will reuse the data T times. At each iteration or "epoch", I use each training sample exactly once after randomly reordering the whole training set.

My implementation is based on Python and Numpy. Therefore, using vector operations can remarkably decrease computation time. Coming up with a vectorized implementation of batch gradient descent is quite straightforward. However, in the case of stochastic gradient descent I can not figure out how to avoid the outer loop that iterates through all the samples at each epoch.

Does anybody know any vectorized implementation of stochastic gradient descent?

**EDIT**: I''ve been asked why would I like to use online gradient descent if the size of my dataset is fixed.

From [1], one can see that online gradient descent converges slower than batch gradient descent to the minimum of the empirical cost. However, it converges faster to the minimum of the expected cost, which measures generalization performance. I''d like to test the impact of these theoretical results in my particular problem, by means of cross validation. Without a vectorized implementation, my online gradient descent code is much slower than the batch gradient descent one. That remarkably increases the time it takes to the cross validation process to be completed.

[1] "Large Scale Online Learning", L. Bottou, Y. Le Cunn, NIPS 2003.


', 2576, '2014-11-12 15:30:27.423', 'ed6b50fc-f8b4-4f88-a47c-79e5a43bda21', 1246, 'added 761 characters in body', 4763, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For problems where the data represents online fraud or insurance (where each row represents a transaction), it is typical for the response variable to denote the value of fraud committed in dollars. Such a response value might have less than 5% non-zero values denoting fraudulent transactions.

I have two questions regarding such a dataset:

 1. What algorithms can we use to ensure that the model not only predicts the fraudulent transactions accurately, but also predicts the value of fraud associated with these.
 2. Assuming that we can quantify the cost involved in each false positive (tagging a non-fraudulent transaction as fraudulent) and cost incurred due to a false negative (tagging a fraudulent transaction as non-fraudulent), how can we optimize the model to maximize savings (or minimize losses)?

', 847, '2014-11-12 16:39:52.363', '28ab8c39-23b9-4c88-915f-950d934a635f', 2451, 'added 21 characters in body', 4764, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('We''re currently using Redshift as our data warehouse, which we''re very happy with. However, we now have a requirement to do machine learning against the data in our warehouse. Given the volume of data involved, ideally I''d want to run the computation in the same location as the data rather than shifting the data around, but this doesn''t seem possible with Redshift. I''ve looked at MADlib, but this is not an option as Redshift does not support UDFs (which MADlib requires). I''m currently looking at shifting the data over to EMR and processing it with the Apache Spark machine learning library (or maybe H20, or Mahout, or whatever). So my questions are:

 1. is there a better way?
 2. if not, how should I make the data accessible to Spark? The options I''ve identified so far include: use Sqoop to load it into HDFS, use DBInputFormat, do a Redshift export to S3 and have Spark grab it from there. What are the pros/cons for these different approaches (and any others) when using Spark?

Note that this is off-line batch learning, but we''d like to be able to do this as quickly as possible so that we can iterate experiments quickly.

', 5019, '2014-11-12 17:27:57.850', '5778607c-43ba-4a70-9fe8-6daecc03929d', 2454, 4765, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Processing data stored in Redshift', 5019, '2014-11-12 17:27:57.850', '5778607c-43ba-4a70-9fe8-6daecc03929d', 2454, 4766, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop><aws>', 5019, '2014-11-12 17:27:57.850', '5778607c-43ba-4a70-9fe8-6daecc03929d', 2454, 4767, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('NLP is very vast and varied. Here are a few basic tools in NLP:

 1. Sentence splitting: Identifying sentence boundaries in text
 2. Tokenization: Splitting a sentence into individual words
 3. Lemmatization: Converting a word to its root form. E.g. says, said,
    saying will all map to root form - say
 4. Stemmer: It is similar to a
    lemmatizer, but it stems a word rather than get to the root form.
    e.g. laughed, laughing will stem to laugh. However, said, saying
    will map to sa - which is not particularly enlightening in terms of
    what "sa" means
 5. POS tagger: Tags a word with the Part of Speech - what is a noun, verb, preposition etc.
 6. Parser: Links words with POS tags to other words with POS tags. E.g. John ate an apple. Here John and apple are nouns linked by the verb - eat. John is the subject of the verb, and apple is the object of the verb.

If you are looking for the state of the art for these tools, check out [StanfordCoreNLP][1], which has most of these tools and a trained model to identify the above from a document. There is also an online demo to check out stanfordCoreNLP before downloading and using it with your application.

NLP has several subfields. Here are a few of them:

 1. Machine Translation: Automatic Translation from one language to another
 2. Information Retrieval: Something like a search engine, that retrieves relevant information from a large set of documents based on a search query
 3. Information Extraction: Extract concepts and keywords - such as names of people, locations, times, synonyms etc.
 4. Deep Learning has lately become a new field of NLP, where a system tries to understand a document like a human understands it.






  [1]: http://nlp.stanford.edu/software/corenlp.shtml', 5021, '2014-11-12 18:22:07.253', '339ee852-83ea-457c-b92a-e44136ca6073', 2455, 4768, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You have a great idea going, and it might work for your specific project. However there are a few considerations you should take into account:

 1. In your first sentence, Obama in incorrectly classified as an organization, instead of a person. This is because the training model used my NLTK probably does not have enough data to recognize Obama as a PERSON. So, one way would be to update this model by training a new model with a lot of labeled training data. Generating labeled training data is one of the most expensive tasks in NLP - because of all the man hours it takes to tag sentences with the correct Part of Speech as well as semantic role.

 2. In sentence 2, there are 2 concepts - "Former Vice President", and "Dick Cheney". You can use co-reference to identify the relation between the 2 NNPs. Both the NNP are refering to the same entity, and the same entity could be referenced as - "former vice president" as well as "Dick Cheney". Co-reference is often used to identify the Named entity that pronouns refer to. e.g. "Dick Cheney is the former vice president of USA. He is a Republican". Here the pronoun "he" refers to "Dick Cheney", and it should be identified by a co-reference identification tool. ', 5021, '2014-11-12 18:41:33.053', '36480e53-ea92-4d76-bf5f-e44d4d9e8986', 2456, 4769, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('TF IDF will give you the degree of measure of how relevant a document is to your query. owever, to evaluate your IR system you need to use metrics such as - Precision, Recall and F score.

Precision: Out of all the documents that your system retrieves, which ones are relevant? This measures how much noise there is in the output of your IR system.

Recall: Out of all the documents that are relevant, which ones did your system retrieve? This measures how much coverage does your IR system have?

It is possible to get 100% recall all the time by basically retrieving ALL documents from a collection for any query. However, the precision in this case will be very low.

It is possible to get a very high precision by hand modeling an IR system ti produce very accurate results. However, it would produce a very bad recall as there will not be coverage over all the documents.

So we need to measure F score- which is the harmonic mean between Precision and Recall
Check out [Chapter 8 of the Stanford IR book][1].

If you are looking for datasets only here are a few that are relevant:

 - http://zitnik.si/mediawiki/index.php?title=Datasets
 - http://www.daviddlewis.com/resources/testcollections/
 - http://boston.lti.cs.cmu.edu/callan/Data/

  [1]: http://nlp.stanford.edu/IR-book/pdf/08eval.pdf', 5021, '2014-11-12 19:00:05.013', '0d9290a4-058f-4223-b1b0-a055590cf5a8', 2457, 4770, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are really so many good resources now.  If you want to stay away from textbooks, both O''Reilly Media and Packt Publishing offer much lighter but effective reading on a lot of great topics.  These books are much more applied in practice.

As far as learning the languages go, Coursera, Udacity, Code Acadmey, and Code School have great tutorials.  I would recommend taking a look at the following:

**Coursera AI and Stats Courses**
https://www.coursera.org/courses?orderby=upcoming&lngs=en&cats=stats,cs-ai

**Udacity Data Science courses**
https://www.udacity.com/courses#!/data-science

', 5022, '2014-11-12 19:31:27.170', '7127b3cc-558e-4f08-911b-b378323f68d2', 2458, 4771, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Also, **Green Tea Press** offers free books on related topics such as an intro to Python and using python with Probability and Stats.  http://www.greenteapress.com/index.html', 5022, '2014-11-12 19:35:01.550', '0347ab98-ae1c-48b6-b50e-c6f5c26a75bb', 2459, 4772, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-11-13 04:38:15.907', 'cfc26aa9-e01d-4c14-b2a6-8173e1eca2f9', 2268, '104', 4776, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-11-13 04:38:24.880', 'b063f480-0cf1-4f3d-9843-93e681a56c94', 2303, '105', 4777, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How can I create a custom tag in JPMML?', 21, '2014-11-13 04:44:06.957', '603aa4e4-9b65-4225-99aa-1a67ec156ba0', 2447, 'edited tags; edited title', 4779, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<logistic-regression><java><pmml><jpmml>', 21, '2014-11-13 04:44:06.957', '603aa4e4-9b65-4225-99aa-1a67ec156ba0', 2447, 'edited tags; edited title', 4780, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One way would be to create 20 features (each feature representing a codon). In this way, you would have a dataset with 3190 instances and 20 categorical features. There is no need to treat the sequence as a Markov chain.

Once the dataset has been featurized as suggested above, any supervised classifier can work well. I would suggest using a gradient boosting machine as it might be better suited to handle categorical features. ', 847, '2014-11-13 07:17:18.147', '8e60dfef-9a71-431a-a60a-49505644d960', 2462, 4781, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a data set that keep tracks of who referred someone to a program and the geo coords of both parties, what will be the best way to visualize this kind of data set. This visualization should also be able to use the geo coordinates to place this entities in the map to form clusters or on a real map.', 5027, '2014-11-13 08:58:25.847', 'c4afb2f9-b584-4623-9221-2b4a25d66d8e', 2463, 4782, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Geospatial Social Network Analysis Visualization', 5027, '2014-11-13 08:58:25.847', 'c4afb2f9-b584-4623-9221-2b4a25d66d8e', 2463, 4783, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<social-network-analysis>', 5027, '2014-11-13 08:58:25.847', 'c4afb2f9-b584-4623-9221-2b4a25d66d8e', 2463, 4784, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to use Latent Dirichlet Allocation for a project and I am using Python with the gensim library. After finding the topics I would like to cluster the documents using an algorithm such as k-means(Ideally I would like to use a good one for overlapping clusters so any recommendation is welcomed). I managed to get the topics but they are in the form of:

0.041*Minister + 0.041*Key + 0.041*moments + 0.041*controversial + 0.041*Prime

In order to apply a clustering algorithm, and correct me if I''m wrong, I believe I should find a way to represent each word as a number using either tfidf or word2vec.

Do you have any ideas of how I could "strip" the textual information from e.g. a list, in order to do so and then place them back in order to make the appropriate multiplication?

For instance the way I see it if the word Minister has a tfidf weight of 0.042 and so on for any other word within the same topic I should be to compute something like:

0.041*0.42 + ... + 0.041*tfidf(Prime) and get a result that will be later on used in order to cluster the results.

Thank you for your time.', 5029, '2014-11-13 09:19:22.797', 'd217ea21-981f-4842-95d6-01deb559b1c2', 2464, 4785, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Clustering of documents using the topics derived from Latent Dirichlet Allocation', 5029, '2014-11-13 09:19:22.797', 'd217ea21-981f-4842-95d6-01deb559b1c2', 2464, 4786, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><clustering><lda>', 5029, '2014-11-13 09:19:22.797', 'd217ea21-981f-4842-95d6-01deb559b1c2', 2464, 4787, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m looking for a supervised learning algorithm that can take 2d data for input and output. As an example of something similar to my data, consider a black image with some sparse white dots. Blur that image using a full range of grayscale. Then create a machine that can take the blurred image as input and produce the original sharp image as output.

I could make some sample 1D data by taking a region/radius around the original sharp point, but I don''t know the exact radius. It would be significant data duplication and a lot of guessing.

Any good algorithm suggestions for this problem? Thanks for your time.', 847, '2014-11-14 08:36:25.320', '09a51354-fc9e-45d0-b86b-5b922413349d', 1213, 'changed image to consider; image was probably meant to be imagine', 4790, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-14 08:36:25.320', '09a51354-fc9e-45d0-b86b-5b922413349d', 1213, 'Proposed by 847 approved by 21 edit id of 176', 4791, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have measurements of 4 devices at two different points of time. A measurement basically consists of an array of ones and zeros corresponding to a bit value at the corresponding location:

    whos measurement1_dev1_time1

    Name                         Size               Bytes  Class      Attributes

    measurement1_dev1_time1      4096x8             32768  logical


I assume that for a specific device the changes between time 1 and 2 of the measurements are unique. However, since I am dealing with 32768 bits at different locations, it is quite hard to visualize if there is some kind of dependency.

As every bit at location ``x``can be regarded as one dimension of an observation I thought to use PCA to reduce the number of dimensions.

Thus, for every of the 5 devices:

 1. I randomly sample ``n`` measurements at point ``t1``and ``t2`` seperatly
 2. I prepare an array as input for ``pca()`` with ``m``*n columns (``m``< 32768; its a subset of all the observed bits, as the original data might be too big for pca) and 4 rows (one row for each device).
 3. On this array ``A`` I calculate the pca: ``[coeff score latent] = pca(zscore(A))```
 4. Then I try to visualize it using ``biplot``: ``biplot(coeff(:,1:2), ''score'', score(:,1:2))``

However, this gives me really strange results. Maybe PCA is not the right approach for this problem? I also modified the input data to do the PCA not on the logical bit array itself. Instead, I created a vector, which holds the indices where there is a ''1'' in the original measurement array. Also this produces strange results.

As I am completely new to PCA I want to ask you if you either see a flaw in the process or if PCA is just not the right approach for my goal and I better look for other dimension reduction approaches or clustering algorithms.', 5038, '2014-11-14 08:53:21.510', '50b57c65-7cfa-4403-bfb8-0bce04db7ff7', 2466, 4792, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Dimension reduction for logical arrays', 5038, '2014-11-14 08:53:21.510', '50b57c65-7cfa-4403-bfb8-0bce04db7ff7', 2466, 4793, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dimensionality-reduction>', 5038, '2014-11-14 08:53:21.510', '50b57c65-7cfa-4403-bfb8-0bce04db7ff7', 2466, 4794, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a [database][1] of 3190 instances of DNA consisting of 60 sequential DNA nucleotide positions classified according to 3 types: EI, IE, Other.

I want to formulate a supervised classifier.

My present approach is to formulate a 2nd order Markov Transition Matrix for each instance and apply the resulting data to a Neural Network.

How best to approach this classification problem, given that the Sequence of the data should be relevant? Is there a better approach than the one I came up with?


  [1]: https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Splice-junction+Gene+Sequences%29', 136, '2014-11-14 11:36:06.290', 'ac38268d-f05c-470d-9ee5-b72730670e8f', 2439, 'Rollback to [fcb78011-0100-4cd9-996d-f417447ebdb6]', 4798, '8');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('### Storing user profiles

If you just want to store all user profiles... just save them into normal **RDBMS**. Assuming one user profiles takes 10Kb of storage, you need only ~9.5Gb for every million of users, which is pretty little and gives you all advantages of mature relational databases.

It makes sense to use **HBase** only when you have really many users (say, > 1B) or when data is very sparse (most columns are empty). But don''t expect it to be as convenient as good old SQL databases.

In advertising, and especially in [real-time bidding][1], very fast retrieval of user profiles is needed. [**Aerospike**][2] becomes more and more popular for this task.

### Analysing data slices

Common use of business logs is to analyse specific slices of data, e.g. number of users from France that visited sites from "game" category on November 1-14, 2014. Standard way to manage such data efficiently is to organize them into [**data cubes**][3]. You won''t get individual records (e.g. users), but you''ll get aggregated statistics really fast.

Such cubes may have many different dimensions, but in 99% of cases they have **date field** that they are **partitioned** by. It makes great sense, because almost every query includes time period to get data from.

As for software, **Vertica** is great for such aggregations. Cheaper* solution from Hadoop world is **Impala**, which is also great.

(* - if you count only license price)

### Machine learning

It really depends on concrete tasks and ML toolkit in use. For real-time bidding you would want blazing fast access to user profile vectors and would probably prefer Aerospike. For online learning **Spark Streaming** may be used as a data source, and no storage used at all. For offline machine learning there''s excellent [**MLlib**][4] from the same Spark project, which works with a variety of sources.




[1]: http://en.wikipedia.org/wiki/Real-time_bidding
[2]: http://www.aerospike.com/
[3]: http://en.wikipedia.org/wiki/Data_cube
[4]: https://spark.apache.org/docs/latest/mllib-guide.html', 1279, '2014-11-14 15:15:20.617', '65e101b8-9ff3-4f17-9f52-b4fc22a7dbdd', 2468, 4799, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I did small survey and get such data:

| ------------| Yes | No | Dont_Know |
|-------------|     |    |           |
| Employee    | 60  | 5  | 5         |
| Workers     | 17  | 0  | 1         |
| Businessmen | 71  | 5  | 10        |
| Jobless     | 4   | 30 | 0         |

(Sorry, I don''t know how to show tables here. Markdown is not working, HTML also.)

R code

    dt <- data.frame(workers = c("Employee",
                             "Workers",
                             "Businessmen",
                             "Jobless"),
                 yes = c(60,17,71,4),
                 no = c(5,0,5,30),
                 dont_know = c(5,1,10,0)
                 )

1. What kind of test I must do, if I want to show, that the Jobless people are often choosing **No** answer?
2. Is the difference between Jobless and Businessmen answers significant?
3. And what is about other groups?
4. What another information I can get from such data or what kind questions I can ask from such data?
  ', 3377, '2014-11-14 15:20:44.463', '5f6bd154-aeff-4e86-b72e-271be4b976c6', 2469, 4800, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('survey data analysis (descreate data)', 3377, '2014-11-14 15:20:44.463', '5f6bd154-aeff-4e86-b72e-271be4b976c6', 2469, 4801, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r>', 3377, '2014-11-14 15:20:44.463', '5f6bd154-aeff-4e86-b72e-271be4b976c6', 2469, 4802, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I did small survey and get such data:

    |-------------| Yes | No | Dont_Know |
    |-------------|     |    |           |
    | Employee    | 60  | 5  | 5         |
    | Workers     | 17  | 0  | 1         |
    | Businessmen | 71  | 5  | 10        |
    | Jobless     | 4   | 30 | 0         |

R code

    dt <- data.frame(workers = c("Employee",
                                 "Workers",
                                 "Businessmen",
                                 "Jobless"),
                     yes = c(60,17,71,4),
                     no = c(5,0,5,30),
                     dont_know = c(5,1,10,0)
                    )

1. What kind of test I must do, if I want to show, that the Jobless people are often choosing **No** answer?
2. Is the difference between Jobless and Businessmen answers significant?
3. And what is about other groups?
4. What another information I can get from such data or what kind questions I can ask from such data?
  ', 2961, '2014-11-14 15:48:17.037', '66dfc8f3-0982-4571-9214-38fb67626402', 2469, 'improved formatting; edited title', 4803, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('survey data analysis (discrete data)', 2961, '2014-11-14 15:48:17.037', '66dfc8f3-0982-4571-9214-38fb67626402', 2469, 'improved formatting; edited title', 4804, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-14 15:48:17.037', '66dfc8f3-0982-4571-9214-38fb67626402', 2469, 'Proposed by 2961 approved by 3377 edit id of 177', 4805, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I was wondering if someone could point me to suitable database formats for building up a user database:

basically I am collecting logs of impressions data, and I want to compile a user database

which sites user visits, country/gender/..? and other categorisations with the aim of
a) doing searches: give me all users visiting games sites from france...
b) machine learning: eg clustering users by the sites they visit

so I am interested in storing info about 100''s of millions of users

with indexes? on user, sites, geo-location

and the idea would be that this data would be continually updated ( eg nightly update to user database of new sites visited etc)

what are suitable database systems. Can someone suggest suitable reading material?
I was imagining Hbase might be suitable...

 ', 1256, '2014-11-14 16:00:29.030', '420bc3be-4ae3-46ad-8731-d06327c69cc6', 2263, 'added 9 characters in body; edited tags', 4806, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigtable><hbase>', 1256, '2014-11-14 16:00:29.030', '420bc3be-4ae3-46ad-8731-d06327c69cc6', 2263, 'added 9 characters in body; edited tags', 4807, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have two classes (A,B) that I want to classify using a SVM. Say that I have a class C and a function f. Can I do this:

```
A'' =  f(A,C) = |A-C|
B'' =  f(B,C) = |B-C|
```

and then perform the classification on A'' and B'' instead? In the context of my problem A and B are classes where elements are vectors. The f function measures the Mahalanobis distance of each vector with respect to the distribution imposed by C.', 5042, '2014-11-14 16:39:29.663', '5b917317-eb68-45be-a599-85e01c4fe8df', 2470, 4808, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Non-linear transformations input dataset for support vector machines', 5042, '2014-11-14 16:39:29.663', '5b917317-eb68-45be-a599-85e01c4fe8df', 2470, 4809, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<svm>', 5042, '2014-11-14 16:39:29.663', '5b917317-eb68-45be-a599-85e01c4fe8df', 2470, 4810, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would definitely use a graph (Though, this clearly depends on the final application, maybe you could add more information)

For the nodes, you should consider as nodes not only tanks but also points were pipelines change name or bifurcate. For instance, following your example:

        e1     e2        e3          e4        e5          e7
        +---+----------.---------+----------.----------+-----------
        |          |         |                     |          |
    [R tank 1]  [S tank 1] [S tank 2]              |e6     [S tank 3]
                                                   |
                                               [S tank 4]

Now adding the nodes:

        e1 n2  e2     n4   e3   n6   e4    n8  e5     n9   e7
    n1 +---+----------.---------+----------.----------+----------+
       |          |         |                     |              |
      n3         n5        n7                     |e6           n11
                                                  |
                                                 n10
Lastly, you need some kind of mapping.
Some of the nodes will map to tanks:

    [R tank 1]     n3
    [S tank 1]     n5
    [S tank 2]     n7
    [S tank 4]     n10
    [S tank 3]     n11

And the pipelines will be represented by paths in the graph

    Pipeline 1     e1 e2
    Pipeline 2     e3 e4
    Pipeline 3     e5 e7
    Pipeline 4     e6
', 5041, '2014-11-14 17:11:13.060', '74de5718-055c-499b-8e61-62abd2ac974f', 2471, 4811, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You are just predicting if Play = Yes or Play = No.

The confusion matrix would look like this:


                 Predicted
              +------+------+
              |  Yes |  No  |
        +-------------------+
    A   |     |      |      |
    c   | Yes |  TP  |  FP  |
    t   |     |      |      |
    u   +-------------------+
    a   |     |      |      |
    l   | No  |  FN  |  TN  |
        |     |      |      |
        +-----+------+------+

    TP: True positives
    FP: False positives
    FN: False negatives
    TN: True negatives', 5041, '2014-11-14 17:32:26.050', '7a81686f-968c-48aa-8f5e-48e9771b14b2', 2472, 4812, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am dealing with a lot of categorical data right now and I would like to use an appropriate data mining method in any tool [preferably R] to find the effect of each parameter [categorical parameters] over my target variable. To give a brief notion about the data that am dealing with, my target variable denotes the product type [say, disposables and non-disposables] and I have parameters like root cause,symptom,customer name, product name etc. As my target can be considered as a binary value, I tried to find the combination of values leading to the desired categories using Apriori but, I have more than 2 categories in that attribute and I want to use all of them and find the effect of the mentioned parameters over each category. I really wanted to try SVM and use hyperplanes to separate the content and get n-dimensional view. But, I do not have enough knowledge to validate the technique, functions am using to do the analysis. Currently I have like 9000 records and each of them represents a complaint from the user. There are lot of columns available in the dataset which is what I am trying to use to determine the target variable [ myForumla <- Target~. ] I tried with just 4 categorical columns too. Not getting a proper result.

Can just the categorical variables be used to develop a SVM model and get visualization with n hyper planes? Is there any appropriate data mining technique available for dealing with just the categorical data?', 5043, '2014-11-14 19:03:35.603', 'b60dfbac-6b39-4696-8e4d-87fe9f658bb5', 2473, 4814, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Choosing the right data mining method to find the effect of each parameter over the target', 5043, '2014-11-14 19:03:35.603', 'b60dfbac-6b39-4696-8e4d-87fe9f658bb5', 2473, 4815, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><classification><r><svm><categorical-data>', 5043, '2014-11-14 19:03:35.603', 'b60dfbac-6b39-4696-8e4d-87fe9f658bb5', 2473, 4816, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think this is something that experienced programmers do all the time. But, given my limited programming experience, please bear with me.


I have an excel file which has particular cell entries that read

*[[{"from": "4", "response": true, "value": 20}, {"from": "8", "response": true, "value": 20}, {"from": "9", "response": true, "value": 20}, {"from": "3", "response": true, "value": 20}], [{"from": "14", "response": false, "value": 20}, {"from": "15", "response": true, "value": 20}, {"from": "17", "response": false, "value": 20}, {"from": "13", "response": true, "value": 20}]]*


Now, for each such entry I want to take the information in each of the curly brackets and make a row of data out of it. Each such row would have 3 columns. For example, the row formed from the first entry within curly brackets should have the entries "4" "true" and "20" respectively. The part I posted should give me 6 such rows, and for n such repetitions I should end up with a matrix of 6n rows, and 4 columns ( an identifier, plus the 3 columns mentioned).

What would be most efficient way to do this? By "doing this" I mean learning the trick, and then implementing it. I have access to quite a few software packages(Excel, Stata, Matlab, R) in my laboratory, so that should not be an issue.
', 5044, '2014-11-14 22:48:10.173', '7e2e01c3-676a-41fc-bf4f-6fee076a1848', 2474, 4817, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Parsing data from a string', 5044, '2014-11-14 22:48:10.173', '7e2e01c3-676a-41fc-bf4f-6fee076a1848', 2474, 4818, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<parsing>', 5044, '2014-11-14 22:48:10.173', '7e2e01c3-676a-41fc-bf4f-6fee076a1848', 2474, 4819, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This lies in the area of `[Online Algorithms][1]`. Online algorithms are specially suited for very large scale tasks where it is impractical to run an algorithm over all the data at the same time. So we run the algorithm piecemeal, and observe the changing trends in teh data.

Other examples it can be used for:

1) topic detection - observing the topics change with time
2) Clustering - observing clusters change with time



  [1]: http://en.wikipedia.org/wiki/Online_algorithm', 5021, '2014-11-14 23:55:51.953', '0357ee76-e48c-4998-a09a-c20511b0cf5d', 2475, 4820, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here are some things to try.

 1. Plot a bar graph. The bar graph will clearly show jobless people are often choosing NO. Try an 1-way ANOVA test. If the p < delta (i.e. delta=0.05), try a post-hoc test (i.e. Tukey''s HSD) to do a pairwise comparison.
 3. Like I said earlier, try a multiple comparison test (1-way ANOVA) first, if there is a statistically significant difference, you can try a pairwise comparison test (post-hoc test).
 3. Maybe try a clustering algorithm? Be careful, because the marginal sums (by rows or columns) are not equal. Maybe create a similarity matrix by profession? To me, it seems that Employees and Businessmen are in one group (very similar), while Workers and Jobless are each in their own group. If you turn those frequencies into proportions, then you might just have 2 groups; one for employees + workers + businessmen, and one for jobless.
 4. Use contingency table analysis to see if the responses (yes/no/don''t know) are associated with profession.

', 3083, '2014-11-15 01:26:16.583', 'f6eab279-c513-489b-b45d-41c87a40735d', 2476, 4822, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How about

 1. Ordinary Least Square (OLS) regression? Since you have a class imbalance, you might want to combine that with boosting algorithms.
 2. If you have a function to quantify the cost involved with FP''s and FN''s, use any optimization technique you can find. My favorite is genetic algorithms. You may also try linear programming. ', 3083, '2014-11-15 01:30:00.870', 'd79d5813-42c2-4944-a72f-991987e29c86', 2477, 4823, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data volume is not the only criterion for using Hadoop. Big Data is often characterized by the 3 V''s:

 - volume,
 - velocity, and
 - variety.

More V''s than these 3 have been invented since. I suppose the V''s were a catchy way to characterize what is Big Data.

But as hinted, computational intensity is a perfect reason for using Hadoop (if your algorithm is computationally expensive). And, as hinted, the problem you describe is perfect for Hadoop, especially since it is embarrassingly parallel in nature.

Is Hadoop a good choice for you? I would argue, yes. Why? Because

 1. Hadoop is open source (compared with proprietary systems which may be expensive and black boxes),
 2. your problem lends itself well to the MapReduce paradigm (embarassingly parallel, shared-nothing),
 3. Hadoop is easily scalable with commodity hardware (as opposed to specialized hardware, and you should get linear speed-up in your performance with just throwing hardware at the problem, and you can just spin a cluster as needed on cloud service providers),
 4. Hadoop allows multiple client languages (Java is only one of many supported languages),
 5. there might be a library available already to do your cross-product operation, and
 6. you''re shipping compute code, not data, around the network (which you should benefit from, and as opposed to other distributed platforms where you are shipping data to compute nodes which is the bottleneck).

Please note, Hadoop is not a distributed file system (as mentioned, and corrected already). Hadoop is distributed storage and processing platform. The distributed storage component of Hadoop is called the Hadoop Distributed File System (HDFS), and the distributed processing component is called MapReduce.

Hadoop has now evolved slightly. They keep the HDFS part for distributed storage. But they have a new component called YARN (Yet Another Resource Negotiator), which serves to appropriate resources (CPU, RAM) for any compute task (including MapReduce).

On the "overhead" part, there is noticeable overhead with starting/stopping a Java Virtual Machine (JVM) per tasks (map tasks, reduce tasks). You can specify for your MapReduce Jobs to reuse JVMs to mitigate this issue.

If "overhead" is really an issue, look into Apache Spark, which is part of the Hadoop ecosystem, and they are orders of magnitude faster than MapReduce, especially for iterative algorithms.

I have used Hadoop to compute pairwise comparisons (e.g. correlation matrix, similarity matrix) that are O(N^2) (n choose 2) in worst case running time complexity. Imagine computing the correlations between 16,000 variables (16,000 choose 2); Hadoop can easily process and store the results if you have the commodity resources to support the cluster. I did this using the preeminent cloud service provider (I won''t name it, but you can surely guess who it is), and it cost me < $100 and under 18 hours.  ', 3083, '2014-11-15 03:18:13.223', '46e08d7b-1512-41f7-b3d2-8136d4a69f18', 2478, 4824, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can try Bayesian belief networks (BBNs). BBNs can easily handle categorical variables and give you the picture of the multivariable interactions. Furthermore, you may use sensitivity analysis to observe how each variable influences your class variable. ', 3083, '2014-11-15 09:34:47.653', '1839d1d3-a595-44b5-8915-123c627dd254', 2479, 4825, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You can try Bayesian belief networks (BBNs). BBNs can easily handle categorical variables and give you the picture of the multivariable interactions. Furthermore, you may use sensitivity analysis to observe how each variable influences your class variable.

Once you learn the structure of the BBN, you can identify the Markov blanket of the class variable. The variables in the Markov blanket of the class variable is a subset of all the variables, and you may use optimization techniques to see which combination of values in this Markov blanket maximizes your class prediction.', 3083, '2014-11-15 09:40:33.473', 'd9804a31-891f-4564-b5f0-313e98722757', 2479, 'added 324 characters in body', 4826, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You are just predicting if Play = Yes or Play = No.

The confusion matrix would look like this:


                 Predicted
              +------+------+
              |  Yes |  No  |
        +-------------------+
    A   |     |      |      |
    c   | Yes |  TP  |  FP  |
    t   |     |      |      |
    u   +-------------------+
    a   |     |      |      |
    l   | No  |  FN  |  TN  |
        |     |      |      |
        +-----+------+------+

    TP: True positives
    FP: False positives
    FN: False negatives
    TN: True negatives

The accuracy can then be calculated as (TP + TN)/(TP + FP + TN + FN). ', 847, '2014-11-15 14:27:22.397', 'b97e8b7f-5af4-40e5-afbc-28c56d963a86', 2472, 'added accuracy description into the confusion matrix.', 4828, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-15 14:27:22.397', 'b97e8b7f-5af4-40e5-afbc-28c56d963a86', 2472, 'Proposed by 847 approved by 2452, 21 edit id of 178', 4829, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-11-15 14:28:46.633', 'fd4f9ce8-acb8-4fad-b5c4-fc0c06a70c09', 1079, '102', 4830, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am new to D3 programming (any programming, for that matter). I have protein-protein interaction data in JSON format and csv format. I would like to use that data for network visualization.

Data attributes: Protein Name, Protein Group, Protein type, Protein Source Node, Protein Target Node

Can anyone suggest good network visualizations for such data. How does it work with hive plots?', 2647, '2014-11-15 15:49:07.213', 'ae9460eb-b9da-4c99-9c81-5148e6788c05', 2480, 4831, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Visualization using D3', 2647, '2014-11-15 15:49:07.213', 'ae9460eb-b9da-4c99-9c81-5148e6788c05', 2480, 4832, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization><javascript>', 2647, '2014-11-15 15:49:07.213', 'ae9460eb-b9da-4c99-9c81-5148e6788c05', 2480, 4833, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why dont you have a look at the following example?
http://bl.ocks.org/mbostock/2066421

You can also find a fiddle here: http://jsfiddle.net/boatrokr/rk2s5/

Pay attention to the part where the links are defined.', 5041, '2014-11-16 01:05:02.603', 'e1f3ccc2-a2d4-41e1-b827-978f891fcc35', 2482, 4840, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you have R is quite simple

1. Copy the lines into a file, let''s say: "mydata.json"

2. Be sure you have installed the rjson package

         install.packages("rjson")

3. Import your data

         library("rjson")
         json_data <- fromJSON(file="mydata.json")', 5041, '2014-11-16 01:13:21.363', '8a7fb6d9-a8a9-400e-880c-d72b3af1db9e', 2483, 4841, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was wondering if anyone had anyone was aware of any methods for visualizing a SVM model such as that there are more than three continuous explanatory variables. In my particular situation, my response variable is binomial, with 6 continuous explanatory variables, one categorical explanatory variable. I have already reduced the number of explanatory variables and I am primarily using R for my analysis.

(I am unaware if such a task is possible/ worth pursuing.)

Thanks for your time.', 5023, '2014-11-16 21:03:09.037', 'bd97e5c8-dc29-4f09-8335-db33e1b18ff3', 2486, 4848, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Visualizing Support Vector Machines (SVM) with Multiple Explanatory Variables', 5023, '2014-11-16 21:03:09.037', 'bd97e5c8-dc29-4f09-8335-db33e1b18ff3', 2486, 4849, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><r><visualization><svm>', 5023, '2014-11-16 21:03:09.037', 'bd97e5c8-dc29-4f09-8335-db33e1b18ff3', 2486, 4850, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a data set that keep tracks of who referred someone to a program and the geo coords of both parties, what will be the best way to visualize this kind of data set. This visualization should also be able to use the geo coordinates to place this entities in the map to form clusters or on a real map.

Am interested in Algorithm and/or Library to do this preferable Java, Python, Scala or NodeJS lib.  The record can be a big as thousand or hundreds of thousands.

Thanks.', 5027, '2014-11-17 09:59:11.510', '76fc3d22-b8c2-48d5-86ac-4c276ec5ad2f', 2463, 'added 177 characters in body; edited tags', 4852, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<algorithms><scalability><social-network-analysis><library>', 5027, '2014-11-17 09:59:11.510', '76fc3d22-b8c2-48d5-86ac-4c276ec5ad2f', 2463, 'added 177 characters in body; edited tags', 4853, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('this is my first ever stack exchange question.

I''m trying to build a tool right now and one of the features of the tool is the ability to break down a product or service into it''s associated attributes/properties/classes/keywords/entities. (Choose which word best suits, as I have no idea).

For example if we had a Camera as the product. I would like to be able to generate a breakdown of everything that is associated to a camera. Such as;

Digital, Film, Optical, LCD, Glass, CCD, CMOS, RGB, Lens, Shutter, Negative, Polaroid, Darkroom, Flash, Resolution, Stabilisation, Batteries, Zoom, Angle, Telephoto, Macro, Filters, Memory, CF, SD

The list could go on for quite some time, those were jsut a few off the top of my head.

How on earth could I go about retrieving such attributes automatically? Is there a database out there that has such info? Are there any special tricks anyone has up their sleeve to be able to accumulate datasets such as the example above?

Very interested in your answers.

Thanks :)', 5066, '2014-11-17 11:54:39.933', '50df34c4-a258-4de8-b70f-3c476cc1ee67', 2487, 4854, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Ontology database', 5066, '2014-11-17 11:54:39.933', '50df34c4-a258-4de8-b70f-3c476cc1ee67', 2487, 4855, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 5066, '2014-11-17 11:54:39.933', '50df34c4-a258-4de8-b70f-3c476cc1ee67', 2487, 4856, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It seems to me as if a good starting point would be to read up on the semantic web, perhaps starting with [DBpedia][1] and maybe LinkedData. You could go from there and build up your own database.

Example of a [SPARQL query][2] starting with the DBpedia page of ''Camera'':

    select ?label where {
      ?prod dbpedia-owl:product :Camera .
      ?prod dcterms:subject ?categories .
      ?entity dcterms:subject ?categories.
      ?entity rdf:type yago:PhysicalEntity100001930 .
      ?entity rdfs:label ?label .
      filter langMatches( lang(?label), ''en'').
    }

Generating a lot of words somehow related to ''Camera''.

    ...
    "Shutter button"@en
    "Rangefinder camera"@en
    "Still camera"@en
    "Lomo LC-A"@en
    "Flexaret"@en
    "Land Camera"@en
    "Robot (camera)"@en
    "Speed Graphic"@en
    "Ansco Panda"@en
    "Image trigger"@en
    "Still video camera"@en
    "Hidden camera"@en
    "Mainichi Shimbun"@en
    "hiradai Station"@en
    "Depth-of-field adapter"@en
    "Banquet camera"@en
    "Digital versus film photography"@en
    "Fernseh"@en
    "Remote camera"@en
    "Professional video camera"@en
    ....

*The above result is just an excerpt.*

  [1]: http://dbpedia.org/About
  [2]: http://dbpedia.org/snorql/?query=%0D%0Aselect%20%3Flabel%20where%20%7B%0D%0A%20%20%23%3ACamera%20dbpedia-owl%3Aabstract%20%3Fabstract.%0D%0A%20%20%23FILTER%20langMatches%28%20lang%28%3Fabstract%29%2C%20%27en%27%29.%0D%0A%20%20%23%3Fprod%20dbpedia2%3Aproducts%20%3Frelated%20.%0D%0A%20%20%3Fprod%20dbpedia-owl%3Aproduct%20%3ACamera%20.%0D%0A%20%20%3Fprod%20dcterms%3Asubject%20%3Fcategories%20.%0D%0A%20%20%3Fentity%20dcterms%3Asubject%20%3Fcategories.%0D%0A%20%20%3Fentity%20rdf%3Atype%20yago%3APhysicalEntity100001930%20.%0D%0A%20%20%3Fentity%20rdfs%3Alabel%20%3Flabel%20.%0D%0A%20%20filter%20langMatches%28%20lang%28%3Flabel%29%2C%20%27en%27%29.%0D%0A%7D', 5073, '2014-11-18 01:26:51.900', '63bf8031-a170-40ef-8723-50aab9de3557', 2488, 4857, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The questionnaire for the data is [here][1]

The first question takes multiple entry for the same question, I want to reduce this to a single variable. How do I do it?

The clean data is available [here][2].
NB: The Column CompuPlat has missing values.


  [1]: http://www.cc.gatech.edu/gvu/user_surveys/survey-1997-10/questions/general.html
  [2]: http://wikisend.com/download/586046/DataRaw.arff', 5075, '2014-11-18 09:07:00.867', 'e88a19ec-543f-4075-8f8c-66971a4d0e9a', 2489, 4858, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Reduction of multiple answers to single variable', 5075, '2014-11-18 09:07:00.867', 'e88a19ec-543f-4075-8f8c-66971a4d0e9a', 2489, 4859, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset>', 5075, '2014-11-18 09:07:00.867', 'e88a19ec-543f-4075-8f8c-66971a4d0e9a', 2489, 4860, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Your dataset can be viewed as a directed graph. The party''s location (latitude and longitude) can be denoted as a node and the directed edge can denote who referred whom. Once the dataset can be viewed as this, the problem boils down to joining co-ordinates with lines. ', 847, '2014-11-18 09:48:19.333', 'a5889547-2282-4796-bbc6-b380fef52c06', 2490, 4861, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data sample contains a single feature: random integer number from 1 to 4.

Is it possble to change `1,2,3,4` representation on the filter card to some custom names, say: `Type1,Type2,Type3,Type4`

![enter image description here][1]


  [1]: http://i.stack.imgur.com/4ZPYM.png', 97, '2014-11-18 09:56:06.157', '7e6cb5e7-8ac2-444f-9c77-90c4905d472f', 2491, 4862, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Change aliases of filter items in Tableau', 97, '2014-11-18 09:56:06.157', '7e6cb5e7-8ac2-444f-9c77-90c4905d472f', 2491, 4863, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization><tableau>', 97, '2014-11-18 09:56:06.157', '7e6cb5e7-8ac2-444f-9c77-90c4905d472f', 2491, 4864, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Data sample contains a single feature: random integer number from 1 to 4.

Is it possble to change `1,2,3,4` representation on the filter card to some custom names, say: `Type1,Type2,Type3,Type4`? (not changing data set)

![enter image description here][1]


  [1]: http://i.stack.imgur.com/4ZPYM.png', 97, '2014-11-18 10:02:47.110', 'dc4a64d3-2342-4be3-8390-82c3c48a71a7', 2491, 'some details ', 4865, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The questionnaire for the data is [here][1]

The first question takes multiple entry for the same question, I want to reduce this to a single variable. How do I do it?

The clean data is available [here][2].
NB: The Column CompuPlat has missing values.

part of dataset
<table>
<tr></tr>
<tr></tr>
<tr></tr>
<tr></tr>
<tr></tr>
<tr></tr>
</table>

  [1]: http://www.cc.gatech.edu/gvu/user_surveys/survey-1997-10/questions/general.html
  [2]: http://wikisend.com/download/586046/DataRaw.arff', 5075, '2014-11-18 10:11:23.793', '458cdb4c-bba6-473e-9fcd-7a226419d95d', 2489, 'added 102 characters in body', 4866, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The variable represents the answer to the first question.

One straightforward way is to allow for all possible categories in this variable. For example, if there are 5 options in this answer, you will have to treat it as a categorical variable with 2^5 = 32 categories. ', 847, '2014-11-18 10:19:59.510', '0a48a0c0-ebdc-41c4-8948-84a41619ceaa', 2492, 4867, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The questionnaire for the data is [here][1]

The first question takes multiple entry for the same question, I want to reduce this to a single variable. How do I do it?

The clean data is available [here][2].
NB: The Column CompuPlat has missing values.

part of dataset

<code>CMFam CMHobb CMNone CMOther CMPol CMProf CMRel
0 0 1 0 0 0 0
0 0 0 0 0 0 0
1 1 0 0 0 1 0
0 0 0 1 0 0 0
0 0 0 0 1 1 0
1 0 0 0 0 1 1
Community Membership_Family
Community Membership_Hobbies
Community Membership_None
Community Membership_Other
Community Membership_Political
Community Membership_Professional
Community Membership_Religious
Community Membership_Support
</code>
I want to club all of them in a variable CM

  [1]: http://www.cc.gatech.edu/gvu/user_surveys/survey-1997-10/questions/general.html
  [2]: http://wikisend.com/download/586046/DataRaw.arff', 5075, '2014-11-18 10:20:14.107', '8e5b018f-fb1e-43d7-a613-0dbd40d69d03', 2489, 'added 102 characters in body', 4868, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The variable represents the answer to the first question.

One straightforward way is to allow for all possible categories in this variable. For example, if there are 5 options in this answer, you will have to treat it as a categorical variable with 2^5 = 32 categories.

However, the number of categories increase exponentially with the number of options (check boxes) provided for the answer. In that case, it might be better to restrict the number of categories to, for example, 5. This can be done by leaving the top 4 choices/ options (by count) as they are and treating every other choice as "other".', 847, '2014-11-18 10:32:32.317', 'fad3529d-e997-4142-a61c-0d36a367d82d', 2492, 'edited the answer after discussions (through comments) with the questioner ', 4869, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Currently we are regularly analyzing sets of paragraphs every month. I would like to automate this and split each paragraphs into chunks of data.

To do this I would like to employ a neural network. However, I am not really very familiar with creating neural networks.

Any ideas or starting point on how to do this using Neuroph?', 5077, '2014-11-18 11:04:28.837', 'c48f8191-6981-4cc9-82de-9695915454e2', 2493, 4870, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Analyze paragraphs using Neuroph', 5077, '2014-11-18 11:04:28.837', 'c48f8191-6981-4cc9-82de-9695915454e2', 2493, 4871, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><text-mining><neuralnetwork><java>', 5077, '2014-11-18 11:04:28.837', 'c48f8191-6981-4cc9-82de-9695915454e2', 2493, 4872, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Currently we are regularly analyzing sets of paragraphs every month. I would like to automate this and split each paragraphs into chunks of data.

To do this I would like to employ a neural network. However, I am not really very familiar with creating neural networks.

Any ideas or starting point on how to do this using Neuroph or maybe in other framework/approaches?', 5077, '2014-11-18 11:14:51.377', 'afce340a-04bf-465c-901b-38693f6200f7', 2493, 'Changed tags', 4873, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<text-mining><clustering><neuralnetwork><java>', 5077, '2014-11-18 11:14:51.377', 'afce340a-04bf-465c-901b-38693f6200f7', 2493, 'Changed tags', 4874, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have 1-4 gram text data from wikipedia for 14 categories, which I am using for NE classification.
I feed named entity from sentence to lucene indexer which searches named entity from these 14 categories.
Issue I am facing is, for single entity I get multiple classes as a result with same score.
like while search `titanic`, indexer gives this result

Score    - 11.23
Title    - titanic
Category - Book

Score    - 11.23
Title    - titanic
Category - Movie

Score    - 11.23
Title    - titanic
Category - Product

now problem is which class to be considered?

I already tried with classifiers (NB,ME in nltk,scikit learn), but as it consider each entity from dataset as feature, it works as indexer only.

Why lucene?

![enter image description here][1]


  [1]: http://i.stack.imgur.com/Tz8Uy.jpg', 5079, '2014-11-18 12:41:54.973', '4fb1b005-9a76-4aad-a099-14e3c2e31387', 2494, 4875, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the best practice to classify category of named entity in sentence', 5079, '2014-11-18 12:41:54.973', '4fb1b005-9a76-4aad-a099-14e3c2e31387', 2494, 4876, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><classification><nlp>', 5079, '2014-11-18 12:41:54.973', '4fb1b005-9a76-4aad-a099-14e3c2e31387', 2494, 4877, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am running SVM algorithm in R.It is taking long time to run the algorithm.I have system with 32GB RAM.How can I use that whole RAM memory to speed my process.', 3551, '2014-11-18 14:03:26.760', '58e8372e-a10b-4f7b-9e98-ddadc8427a3a', 2495, 4878, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to run R programs on multicore using doParallel package?', 3551, '2014-11-18 14:03:26.760', '58e8372e-a10b-4f7b-9e98-ddadc8427a3a', 2495, 4879, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r>', 3551, '2014-11-18 14:03:26.760', '58e8372e-a10b-4f7b-9e98-ddadc8427a3a', 2495, 4880, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Below is the dataset where the response variable is play with two labels (yes, and no),

    No. outlook temperature humidity    windy   play
    1   sunny       hot     high        FALSE   no
    2   sunny       hot     high        TRUE    no
    3   overcast    hot     high        FALSE   yes
    4   rainy       mild    high        FALSE   yes
    5   rainy       cool    normal      FALSE   yes
    6   rainy       cool    normal      TRUE    no
    7   overcast    cool    normal      TRUE    yes
    8   sunny       mild    high        FALSE   no
    9   sunny       cool    normal      FALSE   yes
    10  rainy       mild    normal      FALSE   yes
    11  sunny       mild    normal      TRUE    yes
    12  overcast    mild    high        TRUE    yes
    13  overcast    hot     normal      FALSE   yes
    14  rainy       mild    high        TRUE    no


Here are the decisions with their respective classifications:

    1: (outlook,overcast) -> (play,yes)
    [Support=0.29 , Confidence=1.00 , Correctly Classify= 3, 7, 12, 13]

    2: (humidity,normal), (windy,FALSE) -> (play,yes)
    [Support=0.29 , Confidence=1.00 , Correctly Classify= 5, 9, 10]

    3: (outlook,sunny), (humidity,high) -> (play,no)
    [Support=0.21 , Confidence=1.00 , Correctly Classify= 1, 2, 8]

    4: (outlook,rainy), (windy,FALSE) -> (play,yes)
    [Support=0.21 , Confidence=1.00 , Correctly Classify= 4]

    5: (outlook,sunny), (humidity,normal) -> (play,yes)
    [Support=0.14 , Confidence=1.00 , Correctly Classify= 11]

    6: (outlook,rainy), (windy,TRUE) -> (play,no)
    [Support=0.14 , Confidence=1.00 , Correctly Classify= 6, 14]


Thanks. ', 847, '2014-11-18 16:48:58.960', 'acc8a7bd-b18a-4abc-82e3-df63b76cb6a3', 1189, 'make thing clearer ', 4882, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to define confusion matrix for classification?', 847, '2014-11-18 16:48:58.960', 'acc8a7bd-b18a-4abc-82e3-df63b76cb6a3', 1189, 'make thing clearer ', 4883, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-18 16:48:58.960', 'acc8a7bd-b18a-4abc-82e3-df63b76cb6a3', 1189, 'Proposed by 847 approved by 21 edit id of 179', 4884, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Does it matter that the model is created in the form of SVM?

If no, I have seen a clever 6-D visualization. Its varieties are becoming popular in medical presentations.
3 dimensions are shown as usual, in orthographic projection.
Dimension 4 is color (0..255)
Dimension 5 is thickness of the symbol
Dimension 6 requires animation. It is a frequency of vibration of a dot on the screen.
In static, printed versions, one can replace frequency of vibration by blur around the point, for a comparable visual perception.

If yes, and you specifically need to draw separating hyperplanes, and make them look like lines\planes, the previous trick will not produce good results. Multiple 3-D images are better.', 5083, '2014-11-18 19:52:54.050', 'b115a16a-9869-499a-8c4b-567b1333085a', 2496, 4885, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m looking for an (ideally free) API that would have time series avg/median housing prices by zip code or city/state.

Quandl almost fits the bill, but it returns inconsistent results across different zip codes and the data is not as up to date as I''d like (it''s mid November, and the last month is August).

I also looked at Zillow, but storing their data is against TOS, and at 1,000 calls daily--it would take forever to pull in the necessary data.

Any suggestions (even if they aren''t free) would be much appreciated!', 5086, '2014-11-19 01:07:10.283', '3f4380c3-6f99-499a-90d2-766fce88327a', 2499, 4892, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('API for historical housing prices', 5086, '2014-11-19 01:07:10.283', '3f4380c3-6f99-499a-90d2-766fce88327a', 2499, 4893, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset>', 5086, '2014-11-19 01:07:10.283', '3f4380c3-6f99-499a-90d2-766fce88327a', 2499, 4894, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a list user data : user name, age, sex , address, location etc and

a set of product data : Product name, Cost , description etc

Now i would like to build a recommendation engine that will be able to :

1 Figure out similar products

2 Recommend relevant products fomr the product list to user

How can I implement this using mahout ?', 5091, '2014-11-19 09:26:58.700', '06170af0-857c-4664-b889-b564744a07a1', 2500, 4896, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Recommendation engine with mahout', 5091, '2014-11-19 09:26:58.700', '06170af0-857c-4664-b889-b564744a07a1', 2500, 4897, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><recommendation>', 5091, '2014-11-19 09:26:58.700', '06170af0-857c-4664-b889-b564744a07a1', 2500, 4898, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a list user data: user name, age, sex, address, location etc., and

a set of product data: Product name, Cost, description etc.

Now I would like to build a recommendation engine that will be able to:

1 Figure out similar products

2 Recommend relevant products from the product list to a user

How can I implement this using mahout?', 847, '2014-11-19 10:28:16.017', '26862355-e746-4dba-a3e6-047749b4c07a', 2500, 'corrected spelling, fixed grammar and presentation', 4899, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-19 10:28:16.017', '26862355-e746-4dba-a3e6-047749b4c07a', 2500, 'Proposed by 847 approved by 5091 edit id of 181', 4900, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Currently we are regularly analyzing sets of paragraphs every month. I would like to automate this and split each paragraphs into chunks of data.

To do this I would like to employ a neural network. However, I am not really very familiar with creating neural networks.

Any ideas or starting point on how to do this using Neuroph or maybe in other framework/approaches?

Edit for more info as suggested.

I have very little experience on neural networks though I have some introduction with it in college. However I am very much familar with java

The data is around 3 megabytes only and consists of rules and relationships for a single domain. This means that the data is complex but relatively limited though still free-form English language.', 5077, '2014-11-19 11:34:03.507', '4ece1dd2-cac5-4ae7-a206-6cebe3aa3fb2', 2493, 'added 381 characters in body', 4901, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I do at the moment some data experiments with the [Graphlab toolkit][1]. I have at the first next SFrame, with the three columns:

    Users Items Rating

The pair in the same row of every `Users` and `Items` values build the unique key and the `Rating` is the corresponded float value. These values are not normalised. First of all, I do someself next normalization:

1. Division every rating value of specific user by the rating maximum from this user (scale between 0 and 1)
2. Take the logarithm by every rating value

Afterward I create a recommender model and evaluate the basic metrics for it.

In this topic I invite everybody to discuss another interesting normalisation methods. If anybody could tell some good method for data preparation, it would be great. The results could be evaluated because of the metrics and I can publish it here.



  [1]: http://graphlab.com/products/create/docs/', 3281, '2014-11-19 12:38:05.597', '8f91c7a8-6074-4d19-8114-e4394dcd3e6c', 2501, 4902, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data scheduling for recommender', 3281, '2014-11-19 12:38:05.597', '8f91c7a8-6074-4d19-8114-e4394dcd3e6c', 2501, 4903, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation><data-cleaning>', 3281, '2014-11-19 12:38:05.597', '8f91c7a8-6074-4d19-8114-e4394dcd3e6c', 2501, 4904, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I do at the moment some data experiments with the [Graphlab toolkit][1]. I have at the first next SFrame, with the three columns:

    Users Items Rating

The pair in the same row of every `Users` and `Items` values build the unique key and the `Rating` is the corresponded float value. These values are not normalised. First of all, I do someself next normalisation:

1. Division of every rating value of specific user by the rating maximum from this user (scale between 0 and 1)
2. Take the logarithm by every rating value

Afterward I create a recommender model and evaluate the basic metrics for it.

In this topic I invite everybody to discuss another interesting normalisation methods. If anybody could tell some good method for data preparation, it would be great. The results could be evaluated because of the metrics and I can publish it here.



  [1]: http://graphlab.com/products/create/docs/', 3281, '2014-11-19 12:44:53.533', '7e22041c-8254-4949-8116-66db88d20ede', 2501, 'added 3 characters in body', 4905, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a list user data: user name, age, sex, address, location etc., and

a set of product data: Product name, Cost, description etc.

Now I would like to build a recommendation engine that will be able to:

1 Figure out similar products

eg :

name  :   category   :  cost    :   ingredients

x     :     x1   :        15  :       xx1, xx2, xx3

y     :    y1   :        14   :     yy1, yy2, yy3

z     :    x1  :          12   :     xx1, xy1


here x and z are similar.


2 Recommend relevant products from the product list to a user

How can I implement this using mahout?', 5091, '2014-11-19 12:46:27.353', '0d790941-95a9-4232-96f4-a6dffc085c4a', 2500, 'added 224 characters in body', 4906, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I do at the moment some data experiments with the [Graphlab toolkit][1]. I have at the first next SFrame, with the three columns:

    Users Items Rating

The pair in the same row from every `Users` and `Items` values build the unique key and the `Rating` is the corresponded float value. These values are not normalised. First of all, I do someself next normalisation:

1. Division of every rating value of specific user by the rating maximum from this user (scale between 0 and 1)
2. Take the logarithm by every rating value

Afterward I create a recommender model and evaluate the basic metrics for it.

In this topic I invite everybody to discuss another interesting normalisation methods. If anybody could tell some good method for data preparation, it would be great. The results could be evaluated because of the metrics and I can publish it here.



  [1]: http://graphlab.com/products/create/docs/', 3281, '2014-11-19 13:08:42.127', '40171530-9c54-4e1d-9cb8-df04368669b7', 2501, 'added 2 characters in body', 4907, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m working through the [Coursera NLP course by Jurafsky & Manning][1], and the [lecture on Good-Turing smoothing][2] struck me odd.

The example given was:

> You are fishing (a scenario from Josh Goodman), and caught:
> 10 carp, 3 perch, 2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish
> ...
> How likely is it that the next species is new (i.e. catfish or bass)
> Let''s use our estimate of things-we-saw-once to estimate the new things.
> 3/18 (because N_1=3)

I get the intuition of using the count of uniquely seen items to estimate the number of unseen item types (N = 3), but the next steps seem counterintuitive.

Why is the denominator left unchanged instead of incremented by the estimate of unseen item types? I.e., I would expect the probabilities to become:

> Carp : 10 / 21
> Perch : 3 / 21
> Whitefish : 2 / 21
> Trout : 1 / 21
> Salmon : 1 / 21
> Eel : 1 / 21
> Something new : 3 / 21

It seems like the Good-Turing count penalizes seen items too much (trout, salmon, & eel are each taken down to 1/27); coupled with the need to adjust the formula for gaps in the counts (e.g., Perch & Carp would be zeroed out otherwise), it just feels like a bad hack.

  [1]: https://www.coursera.org/course/nlp
  [2]: https://class.coursera.org/nlp/lecture/32', 5095, '2014-11-19 19:25:02.770', 'fc11b16c-9b5f-4179-bd4b-8316eeac39cb', 2502, 4908, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Good-Turing Smoothing Intuition', 5095, '2014-11-19 19:25:02.770', 'fc11b16c-9b5f-4179-bd4b-8316eeac39cb', 2502, 4909, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp>', 5095, '2014-11-19 19:25:02.770', 'fc11b16c-9b5f-4179-bd4b-8316eeac39cb', 2502, 4910, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('This lies in the area of `[Online Algorithms][1]`. Online algorithms are specially suited for very large scale tasks where it is impractical to run an algorithm over all the data at the same time. So we run the algorithm piecemeal, and observe the changing trends in teh data.

Other examples it can be used for:

1) topic detection - observing the topics change with time
2) Clustering - observing clusters change with time

  [1]: http://en.wikipedia.org/wiki/Online_algorithm
http://en.wikipedia.org/wiki/Online_algorithm', 5021, '2014-11-19 23:18:40.920', '6e332b89-01be-42bf-99b0-c1e4a43cb234', 2475, 'It seems like there is a bug where stackExchange  is not showing the reference link.', 4911, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to build an item-item similarity matching recommendation engine with mahout. The data set is as in the following format ( attributes are in text not in numerals format )

    name : category : cost : ingredients

    x : xx1 : 15 : xxx1, xxx2, xxx3

    y : yy1 : 14 : yyy1, yyy2, yyy3

    z : xx1 : 12 : xxx1, xxy1

So in-order to use this data set for mahout to train, what is the right way to convert this in to numeric (as CSV Boolean data set) format accepted by mahout.

', 5091, '2014-11-20 05:38:27.303', 'b05d2c39-394d-44e3-90ce-c13724a71e44', 2503, 4912, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Creating Data model for mahout recommendation engine', 5091, '2014-11-20 05:38:27.303', 'b05d2c39-394d-44e3-90ce-c13724a71e44', 2503, 4913, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><dataset><recommendation>', 5091, '2014-11-20 05:38:27.303', 'b05d2c39-394d-44e3-90ce-c13724a71e44', 2503, 4914, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a big data problem with a large dataset (take for example 50 million rows and 200 columns). The dataset consists of about 100 numerical columns and 100 categorical columns and a response column that represents a binary class problem. The cardinality of each of the categorical columns is less than 50.

I want to know a priori whether I should go for deep learning methods or ensemble tree based methods (for example gradient boosting, adaboost, or random forests). Are there some exploratory data analysis or some other techniques that can help me decide for one method over the other? ', 847, '2014-11-20 06:49:00.357', 'c885b06f-f1d4-4651-bd12-ead868e5f6aa', 2504, 4915, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Deep Learning vs gradient boosting: When to use what?', 847, '2014-11-20 06:49:00.357', 'c885b06f-f1d4-4651-bd12-ead868e5f6aa', 2504, 4916, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><deep-learning>', 847, '2014-11-20 06:49:00.357', 'c885b06f-f1d4-4651-bd12-ead868e5f6aa', 2504, 4917, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I do at the moment some data experiments with the [Graphlab toolkit][1]. I have at the first next SFrame, with the three columns:

    Users Items Rating

The pair in the same row from every `Users` and `Items` values build the unique key and the `Rating` is the corresponded float value. These values are not normalised. First of all, I do someself next normalisation:

1. Division of every rating value of specific user by the rating maximum from this user (scale between 0 and 1)
2. Take the logarithm by every rating value

Afterward I create a recommender model and evaluate the basic metrics for it.

In this topic I invite everybody to discuss another interesting normalisation methods. If anybody could tell some good method for data preparation, it would be great. The results could be evaluated because of the metrics and I can publish it here.


**PS**

My dataset is comming from some music site, the users rated some tracks. I have approximately 300 000 users and 200 000 tracks. Total number of ratings is over 3 millions (actually the matrix is sparse). This is the most simple data set, which I analyze now. In the future I can (and will) use some additional information about the users and tracks (f.e. duration, year, genre, band etc). At the moment I just interest to collect some methods for rating normalisation without to use additional information (users & items features). My problem is, the data set doesn''t have any `Rating` at the first. I create someself the column `Rating`, based on the number of events for unique `User-Item` pair (I have this information). You can of course understand that some users can hear some tracks many times, and another users only one time. Consequently the dispersion is very high and I want to reduce it (normalise the ratings value).


  [1]: http://graphlab.com/products/create/docs/', 3281, '2014-11-20 07:43:12.023', '33eec79c-8ead-4833-9fb3-270322b1ffa0', 2501, 'added 867 characters in body', 4918, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Have you looked into tourr package in R. This package does hyperplane reduction. In addition it has an optimizer that tries to find the best reduction.

There is a very nice video in <https://www.youtube.com/watch?v=iSXNfZESR5I> That shows what  R is capable even beyound tourr package.

Also I refer you to <http://stackoverflow.com/questions/8017427/plotting-data-from-an-svm-fit-hyperplane>', 5100, '2014-11-20 09:33:20.320', '9ac2825a-e3b0-46b5-8382-688c4aa369df', 2505, 4919, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Just go through Neo4j (graph data base and will be useful for social network analysis) also.. may be helpful ', 5091, '2014-11-20 13:07:45.903', '01dce8b1-45ef-440c-864b-abb53a066983', 2506, 4920, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to build a data set on several log files of one of our products.

The different log files have their own layout and own content; I successfully grouped them together, only one step remaining...

Indeed, the log "messages" are the best information. I don''t have the comprehensive list of all those messages, and it''s a bad idea to hard code based on those because that list can change every day.

What I would like to do is to separate the indentification text from the value text (for example: "Loaded file XXX" becomes (identification: "Loaded file", value: "XXX")). Unfortunately, this example is simple, and in real world there are different layouts and sometimes multiple values.

I was thinking about using string kernels, but it is intended for clustering ... and cluseting is not applicable here (I don''t know the number of different types of messages and eventhough, it would be too much).

Do you have any idea?

Thanks for your help.

P.S: For those who programs, this can be easier to understand. Let''s say that the code contains as logs printf("blabla %s", "xxx") -> I would like to have "blabla" and "xxx" seperatated', 3024, '2014-11-20 14:26:10.463', '448499af-b364-4c5e-9e5f-1db0adedc8e5', 2507, 4921, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Log file analysis: extracting information part from value part', 3024, '2014-11-20 14:26:10.463', '448499af-b364-4c5e-9e5f-1db0adedc8e5', 2507, 4922, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining><clustering>', 3024, '2014-11-20 14:26:10.463', '448499af-b364-4c5e-9e5f-1db0adedc8e5', 2507, 4923, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I do at the moment some data experiments with the [Graphlab toolkit][1]. I have at the first next SFrame, with the three columns:

    Users Items Rating

The pair in the same row from every `Users` and `Items` values build the unique key and the `Rating` is the corresponded float value. These values are not normalised. First of all, I do someself next normalisation:

1. Division of every rating value of specific user by the rating maximum from this user (scale between 0 and 1)
2. Take the logarithm by every rating value

Afterward I create a recommender model and evaluate the basic metrics for it.

In this topic I invite everybody to discuss another interesting normalisation methods. If anybody could tell some good method for data preparation, it would be great. The results could be evaluated because of the metrics and I can publish it here.


**PS**

My dataset is comming from some music site, the users rated some tracks. I have approximately 100 000 users and 300 000 tracks. Total number of ratings is over 3 millions (actually the matrix is sparse). This is the most simple data set, which I analyze now. In the future I can (and will) use some additional information about the users and tracks (f.e. duration, year, genre, band etc). At the moment I just interest to collect some methods for rating normalisation without to use additional information (users & items features). My problem is, the data set doesn''t have any `Rating` at the first. I create someself the column `Rating`, based on the number of events for unique `User-Item` pair (I have this information). You can of course understand that some users can hear some tracks many times, and another users only one time. Consequently the dispersion is very high and I want to reduce it (normalise the ratings value).


  [1]: http://graphlab.com/products/create/docs/', 3281, '2014-11-20 14:56:14.813', '70a4a511-dbda-4667-9f3b-dca907e9cdd8', 2501, 'edited body', 4924, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to run an R script using a single command (e.g. bat file or shortcut).

This R script asks the user to choose a file and then plots information about that file. All is done via dialog boxes.

I don''t want the user to go inside R - because they don''t know it at all.

So, I was using r cmd and other similar stuffs, but as soon as the plots are displayed, R exits and closes the plots.

What can I do?

Thanks for your help.', 3024, '2014-11-20 15:02:17.937', '5750d980-50d3-4001-8f07-baa8d936ecaa', 2508, 4925, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to run R scripts without closing X11', 3024, '2014-11-20 15:02:17.937', '5750d980-50d3-4001-8f07-baa8d936ecaa', 2508, 4926, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r>', 3024, '2014-11-20 15:02:17.937', '5750d980-50d3-4001-8f07-baa8d936ecaa', 2508, 4927, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How about considering each string as a process trace and applying alpha-algorithm?
That would give you a graph and nodes with a big number out-edges will most likely point to values.

You can mark these nodes and for every new string parse/traverse the graph until you reach those areas.', 5041, '2014-11-20 16:28:48.343', '86c6d7a5-c4ef-4fc3-bf41-7234c5c4bf57', 2509, 4928, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('This lies in the area of . [Online algorithms][1] are specially suited for very large scale tasks where it is impractical to run an algorithm over all the data at the same time. So we run the algorithm piecemeal, and observe the changing trends in teh data.

Other examples it can be used for:

1) topic detection - observing the topics change with time
2) Clustering - observing clusters change with time

  [1]: http://en.wikipedia.org/wiki/Online_algorithm', 847, '2014-11-20 17:41:46.610', '41ac5ad4-44f9-45cd-94ae-835d5655d81c', 2475, 'added link as the questioner intended', 4929, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-20 17:41:46.610', '41ac5ad4-44f9-45cd-94ae-835d5655d81c', 2475, 'Proposed by 847 approved by 5021 edit id of 182', 4930, '24');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('user5007', 'I''m currently finishing up a B.S. in mathematics and would like to attend graduate school (a master''s degree for starters, with the possibility of a subsequent Ph.D.) with an eye toward entering the field of data science. I''m also particularly interested in machine learning.

What are the graduate degree choices that would get me to where I want to go?

Is there a consensus as to whether a graduate degree in applied mathematics, statistics, or computer science would put me in a better position to enter the field of data science?

Thank you all for the help, this is a big choice for me and any input is very much appreciated. Typically I ask my questions on Mathematics Stack Exchange, but I thought asking here would give me a broader and better rounded perspective.', '2014-11-20 17:54:11.990', '0f9d7e4c-f4d9-463c-93f1-e9d5cf915461', 2510, 4931, '2');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('user5007', 'Graduate Degree Choices for Data Science', '2014-11-20 17:54:11.990', '0f9d7e4c-f4d9-463c-93f1-e9d5cf915461', 2510, 4932, '1');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('user5007', '<career>', '2014-11-20 17:54:11.990', '0f9d7e4c-f4d9-463c-93f1-e9d5cf915461', 2510, 4933, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would add a comment but I do not have enough reputation points. I might suggest Using "R revolution open". It is a Build of R that includes a lot of native support for multi-core processing. I have not used it much as my computer is very old, but it is defiantly worth looking at. Plus it is free. ', 5023, '2014-11-20 21:11:12.187', '6cd3a4e2-79b7-4b8e-b00c-77b99b9c705d', 2511, 4937, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('let''s assume that I want to train a stochastic gradient descent regression algorithm using a dataset that has N samples. Since the size of the dataset is fixed, I will reuse the data T times. At each iteration or "epoch", I use each training sample exactly once after randomly reordering the whole training set.

My implementation is based on Python and Numpy. Therefore, using vector operations can remarkably decrease computation time. Coming up with a vectorized implementation of batch gradient descent is quite straightforward. However, in the case of stochastic gradient descent I can not figure out how to avoid the outer loop that iterates through all the samples at each epoch.

Does anybody know any vectorized implementation of stochastic gradient descent?

**EDIT**: I''ve been asked why would I like to use online gradient descent if the size of my dataset is fixed.

From [1], one can see that online gradient descent converges slower than batch gradient descent to the minimum of the empirical cost. However, it converges faster to the minimum of the expected cost, which measures generalization performance. I''d like to test the impact of these theoretical results in my particular problem, by means of cross validation. Without a vectorized implementation, my online gradient descent code is much slower than the batch gradient descent one. That remarkably increases the time it takes to the cross validation process to be completed.

**EDIT**: I include here the pseudocode of my on-line gradient descent implementation, as requested by ffriend. I am solving a regression problem.

    Method: on-line gradient descent (regression)
    Input: X (nxp matrix; each line contains a training sample, represented as a length-p vector), Y (length-n vector; output of the training samples)
    Output: A (length-p+1 vector of coefficients)

    Initialize coefficients (assign value 0 to all coefficients)
    Calculate outputs F
    prev_error = inf
    error = sum((F-Y)^2)/n
    it = 0
    while abs(error - prev_error)>ERROR_THRESHOLD and it<=MAX_ITERATIONS:
        Randomly shuffle training samples
        for each training sample i:
            Compute error for training sample i
            Update coefficients based on the error above
        prev_error = error
        Calculate outputs F
        error = sum((F-Y)^2)/n
        it = it + 1

[1] "Large Scale Online Learning", L. Bottou, Y. Le Cunn, NIPS 2003.

', 2576, '2014-11-21 10:02:39.520', 'f9d6250f-c471-485d-afa1-3467a3fbb206', 1246, 'added 935 characters in body', 4938, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('UCL - CSML. It covers computer science, machine learning and statistics.', 5110, '2014-11-21 10:24:18.623', '06179b92-d49f-48a1-8bc4-9d51c93895ba', 2512, 4939, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From [A_Roadmap_to_SVM_SMO.pdf](http://nshorter.com/ResearchPapers/MachineLearning/A_Roadmap_to_SVM_SMO.pdf), pg 12.


![a busy cat](http://s13.postimg.org/9dx9t4w47/whatwhat.png)

Assume I am using linear kernel, how will I be able to get both the first and second inner product?

My guess, inner product of all datapoints with datapoints labelled class A for the first inner product of the equation and inner product of all datapoints with datapoints labelled class B for second inner product?', 5110, '2014-11-21 10:34:31.673', '9dc640ae-fd1c-424d-afd0-4f169656fc68', 2513, 4940, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Please enlighten me with Platt''s SMO algorithm (for SVM)', 5110, '2014-11-21 10:34:31.673', '9dc640ae-fd1c-424d-afd0-4f169656fc68', 2513, 4941, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<svm>', 5110, '2014-11-21 10:34:31.673', '9dc640ae-fd1c-424d-afd0-4f169656fc68', 2513, 4942, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I was wondering if anyone was aware of any methods for visualizing an SVM model where there are more than three continuous explanatory variables. In my particular situation, my response variable is binomial, with 6 continuous explanatory variables (predictors), one categorical explanatory variable (predictor). I have already reduced the number of predictors and I am primarily using R for my analysis.

(I am unaware if such a task is possible/ worth pursuing.)

Thanks for your time.', 847, '2014-11-21 10:39:38.337', '5aa8262d-ceff-4433-8913-9cc89e024194', 2486, 'variables must be referred to as predictors for the larger community; fixed some grammar', 4943, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-21 10:39:38.337', '5aa8262d-ceff-4433-8913-9cc89e024194', 2486, 'Proposed by 847 approved by 21 edit id of 183', 4944, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('From [A_Roadmap_to_SVM_SMO.pdf](http://nshorter.com/ResearchPapers/MachineLearning/A_Roadmap_to_SVM_SMO.pdf), pg 12.


![a busy cat](http://s13.postimg.org/9dx9t4w47/whatwhat.png)

Assume I am using linear kernel, how will I be able to get both the first and second inner product?

My guess, inner product of datapoint with datapoint j labelled class A for the first inner product of the equation and inner product of datapoint j with datapoints labelled class B for second inner product?', 5110, '2014-11-21 10:41:27.877', 'a28f29b4-a4ff-4a43-aa40-64308b5a61b5', 2513, 'deleted 7 characters in body', 4945, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are no _unseen_ item types in the given data, by definition. 3 is the count of items seen _once_, and they are already included in the denominator 18. If the next item were _previously_ unseen, it would become seen _once_ when it appears. Since 3-of-18 examples were seen-once items, this is an estimate of the probability that the next item will be seen-once too on its first appearance.

It is certainly a heuristic. There is no way to know whether there are 0 or 1000 other types out there.', 21, '2014-11-21 10:44:06.783', '04c0a961-0d29-49c4-b6ed-e0f386c28916', 2514, 4946, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('UCL - CSML. It covers computer science, machine learning and statistics.

Firstly, reputation of the university.
Secondly, you are from Mathematics background, hence I assume you don''t have sufficient programming knowledge.
Thirdly, Statistics and Machine Learning dominates this field. Employers would prefer these 2 before Mathematics.

In short, this course provides everything that you are lacking. HOWEVER, they don''t teach programming languages like Java, C++,... but Matlab, R, and Mathematica. Hence, it would be essential if you pick up the former from somewhere.', 5110, '2014-11-21 10:47:20.207', '95807ba2-cdf6-4d94-abce-00b25e619538', 2512, 'added 506 characters in body', 4947, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First of all, word "sample" is normally used to describe [_subset_ of population](http://en.wikipedia.org/wiki/Sample_%28statistics%29), so I will refer to the same thing as "example".

Your SGD implementation is slow because of this line:

    for each training example i:

Here you explicitly use exactly one example for each update of model parameters. By definition, vectorization is a technique for converting operations on one element into operations on a vector of such elements. Thus, no, you cannot process examples one by one and still use vectorization.

You can, however, approximate true SGD by using **mini-batches**. Mini-batch is a small subset of original dataset (say, 100 examples). You calculate error and parameter updates based on mini-batches, but you still iterate over many of them without global optimization, making the process stochastic. So, to make your implementation much faster it''s enough to change previous line to:

    batches = split dataset into mini-batches
    for batch in batches:

and calculate error from batch, not from a single example.

Though pretty obvious, I should also mention vectorization on per-example level. That is, instead of something like this:

    theta = np.array([...])  # parameter vector
    x = np.array([...])      # example
    y = 0                    # predicted response
    for i in range(len(example)):
        y += x[i] * theta[i]
    error = (true_y - y) ** 2  # true_y - true value of response

you should definitely do something like this:

    error = (true_y - sum(np.dot(x, theta))) ** 2

which, again, easy to generalize for mini-batches:

    true_y = np.array([...])     # vector of response values
    X = np.array([[...], [...]]) # mini-batch
    errors = true_y - sum(np.dot(X, theta), 1)
    error = sum(e ** 2 for e in errors)', 1279, '2014-11-21 11:50:47.717', '68ff45ea-7ade-4596-8595-4faed2628777', 2515, 4948, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To all:

I have been wracking my brain at this for a while and thought maybe someone here would know of a package or algorithm to handle the following:

I have nominal multivariant timeseries that look like the following:

              Time Var1 Var2 Var3 Var4 Var5 ... VarN
                 0     A     A   B    C    A   ... H
                 1     A     A   B    D    D   ... H
                 2     B     A   C    D    D   ... H
                 ..

And so on from times 0 to 1,000,000. What I would like to do is search the time series for rules of the type:

Given Var3 is in state B in the previous step and Var5 is in state D in the previous step, than Var1 will be in state B. What I want to do is have the rules that include the time interval explicitly. A simpler case of interest would simply be to reduce the time series to

                   Time    Var1 Var2 Var3 Var4 Var5 ... VarN
                    0        0    0    0     0   0   ... 0
                    1        0    0    0     1   1   ... 0
                    2        1    0    1     0   0   ... 0

Where the the variable is 1 if its state is different from the previous step and zero otherwise. Then I just want to have rules that say something like:

If Var4 and Var5 changed in the previous step than Var1 will change in the current step. Which would be easy for a lag of one, as I could just make the data into something like:

       Var1 Var2 Var3 Var4 Var5 ... VarN Var1_t-1 Var2_t-1 Var3_t-1 ...
and then do sequence mining, but if I want to have rules that aren''t just a single lag but could be lags from 1 to 500 than my data set begins to be a little difficult to work with.

Any help would be greatly appreciated. ', 5113, '2014-11-21 15:23:37.360', 'bb4a05be-4f3f-4d71-aa03-dbcb3fba0ae6', 2516, 4949, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Relation mining of multivariant categorical timeseries without excluding the temporal nature', 5113, '2014-11-21 15:23:37.360', 'bb4a05be-4f3f-4d71-aa03-dbcb3fba0ae6', 2516, 4950, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><statistics><text-mining><time-series><categorical-data>', 5113, '2014-11-21 15:23:37.360', 'bb4a05be-4f3f-4d71-aa03-dbcb3fba0ae6', 2516, 4951, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Dimension reduction (like PCA) is an excellent way to visualize the results of classification on a high-dimensional feature space.

The simplest approach is to project the features to some low-d (usually 2-d) space and plot them. Then either project the decision boundary onto the space and plot it as well, or simply color/label the points according to their predicted class. You can even use, say, shape to represent ground-truth class, and color to represent predicted class.

This is true for any categorical classifier, but here''s an SVM-specific example: http://www.ece.umn.edu/users/cherkass/predictive_learning/Resources/Visualization%20and%20Interpretation%20of%20SVM%20Classifiers.pdf

In particular, see figures 1a and 2a.', 1156, '2014-11-21 18:55:07.860', 'd4bde491-5550-435d-9c0d-2cb0d733a15c', 2517, 4952, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try using the **item-based similarity** algorithm available under *Apache Mahout*. It is easy to implement and you will have a good sense how the recommendation system for your data set will work. You could provide ingredients and category as the major inputs to get the similar products.As a neophyte to this field, I would say that this approach is an easy way for all the neophytes to get a good heads up of what kind of a result one can expect from building a recommendation system of their own.   ', 5043, '2014-11-21 21:33:06.777', 'bdb7d798-5d93-422e-8387-21d8fc2123e6', 2518, 4953, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[This][1] looks like a similar kind of a problem.

Solutions: (taken from above source)

1. Just sleep via  Sys.sleep(10) which would wait ten seconds.

2. Wait for user input via readLines(stdin()) or something like that [untested]

3. Use the tcltk package which comes with R and is available on all platforms to pop up a window the user has to click to make the click disappear. That solution has been posted a few times over the years on r-help.

2nd option is better to use for the user.

P.S. Since I did not come up with the answer myself, I tried to put it in comment but my reputation is too low for that.

  [1]: http://stackoverflow.com/questions/24220676/r-script-using-x11-window-only-opens-for-a-second', 4622, '2014-11-21 21:39:29.617', 'c65ea08e-2276-401d-beec-a88d68b3ae31', 2519, 4954, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Every field has their own variation of "data science," so I would suggest choosing a subject that interest you and going from there.

I can''t offer what the go to subject is for your particular interest.  A graduate degree that would "get you where you want to go" is quite a personal understanding, so I can'' answer that.  But what I will say is, from my own personal experience, when I graduated with my undergrad degree in economics, I was really interested in data science, and economics allowed me to use data science in a field I''m really interested in.  So I applied to Ph.D programs to further my knowledge and am using data science extensively in many different forms of analysis.

My suggestion is to apply to graduate degrees that have interesting subject matter to you and will allow you to use data science as understanding.  You would fit well in an economics degree because of your background :)

', 4697, '2014-11-22 08:10:40.480', '7375abd1-5962-45f2-9b34-cc57c5baaf90', 2520, 4963, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I browsed a sample for available data at http://dbpedia.org/page/Sachin_Tendulkar. I wanted these properties as columns, so I downloaded the CSV files from http://wiki.dbpedia.org/DBpediaAsTables.

Now, when I browse the data for the same entity "Sachin_Tendulkar", I find that many of the properties are not available. e.g. the property "dbpprop:bestBowling" is not present.

How can I get all the properties that I can browse through the direct resource page.', 5126, '2014-11-22 11:54:54.780', 'e3945b89-0fea-4ece-9939-6b37f41af752', 2521, 4964, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('DBPedia as Table not having all the properties', 5126, '2014-11-22 11:54:54.780', 'e3945b89-0fea-4ece-9939-6b37f41af752', 2521, 4965, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset>', 5126, '2014-11-22 11:54:54.780', 'e3945b89-0fea-4ece-9939-6b37f41af752', 2521, 4966, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why not do an MSc in ooh... **Data Science**?

I wrote a quick review of [UK Data Science Masters][1]'' offerings recently. That should help you get an idea what is offered. Mostly they are mashups of stats and computing, but there are specialisms (health, finance for example) that might interest you.


  [1]: http://barry.rowlingson.com/blog/uk-data-science-masters.html', 471, '2014-11-22 16:08:49.147', '335219d9-b642-4f67-8f7a-91ed44d85715', 2522, 4967, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Why not do an MSc in ooh... **Data Science**?

I wrote a quick review of [UK Data Science Masters][1]'' offerings recently. That should help you get an idea what is offered. Mostly they are mashups of stats and computing, but there are specialisms (health, finance for example) that might interest you.

Note that list was compiled for courses that have already started, so some of those courses might not be available for starting next October, or have different syllabus contents.

  [1]: http://barry.rowlingson.com/blog/uk-data-science-masters.html', 471, '2014-11-22 18:16:40.737', '88b1899f-1090-47be-8e13-f2b72539011e', 2522, 'added 180 characters in body', 4968, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Common model validation statistics like the KS, AUROC, and Gini are all functionally related. However, my question has to do with proving how these are all related. I am curious if anyone can help me prove these relationships. I haven''t been able to find anything online, but I am just genuinely interested how the proofs work. For example, I know Gini=2AUROC-1, but my best proof involves pointing at a graph. I am interested in formal proofs. Any help would be greatly appreciated!

', 5132, '2014-11-23 01:05:06.473', '437cdb6e-4146-4567-949f-8462897c53dc', 2523, 4970, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Relationship between KS, AUROC, and Gini', 5132, '2014-11-23 01:05:06.473', '437cdb6e-4146-4567-949f-8462897c53dc', 2523, 4971, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><statistics><predictive-modeling><accuracy>', 5132, '2014-11-23 01:05:06.473', '437cdb6e-4146-4567-949f-8462897c53dc', 2523, 4972, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The official answer:

    Date: Sun, 23 Nov 2014 03:08:19 +0100
    From: Petar Ristoski <petar.ristoski@informatik.uni-mannheim.de>
    To: ''Barry Carter'' <carter.barry@gmail.com>
    Subject: RE: CSV tables don''t have all properties?

    Hi Carter,

    The question was already answered on the DBpedia mailing list, but I will
    try to clarify it again. On the DBpedia as Tables web page says that "For
    each class in the DBpedia ontology (such as Person, Radio Station, Ice
    Hockey Player, or Band) we provide a single CSV/JSON file which contains all
    instances of this class. Each instance is described by its URI, an English
    label and a short abstract, the MAPPING-BASED INFOBOX data describing the
    instance (extracted from the English edition of Wikipedia), geo-coordinates,
    and external links." As you can see we only provide the mapping-based
    infobox properties (dbpedia-owl namespace), while the properties from the
    dbpprop (raw infobox properties) namespace are completely ignored.
    Therefore, dbpprop:bestBowling is missing from the file.

    Also, there is a section "Generating your own Custom Tables" [1], where we
    explain how to generate your own tables that will contain the properties you
    need.

    Regards,

    Petar

    [1] http://wiki.dbpedia.org/DBpediaAsTables#h347-4

', 4710, '2014-11-23 02:26:42.470', '65af7689-0a86-4aca-badb-feef9212bafd', 2524, 4973, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am having some difficulty in seeing connection between PCA on second order moment matrix in estimating parameters of Gaussian Mixture Models. Can anyone connect the above??', 4686, '2014-11-23 02:27:10.670', '94676049-0804-4652-880f-2989ea2f3a55', 2525, 4974, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can some one explain how PCA is relevant in extracting parameters of Gaussian Mixture Models', 4686, '2014-11-23 02:27:10.670', '94676049-0804-4652-880f-2989ea2f3a55', 2525, 4975, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering>', 4686, '2014-11-23 02:27:10.670', '94676049-0804-4652-880f-2989ea2f3a55', 2525, 4976, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The question was already answered on the DBpedia-discussion mailing list, by Daniel:

> Hi Abhay,
>
> the DBpediaAsTables dataset only contains the properties in the
> dbpedia-owl namespace (mapping-based infobox data) and not those from
> the dbpprop (raw infobox properties) namespace (regarding the
> differences see \[1]).
>
> However, as you are only interested in the data about specific
> entities, take a look at the CSV link at the bottom of the entity''s
> description page, e.g., for your example this link is \[2].
>
> Cheers,
> Daniel
>
> \[1] wiki.dbpedia.org/Datasets#h434-10

> \[2] dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE+%3Chttp://dbpedia.org/resource/Sachin_Tendulkar%3E&format=text%2Fcsv

On the [DBpediaAsTables web page][1], you can find out which datasets were used to generate the tables: instance_types_en, labels, short_abstracts_en, mappingbased_properties_en, geo_coordinates_en.
Also, I want to clarify that DBpediaAsTables contains all instances from DBpedia 2014, and with "we provide some of the core DBpedia data" we want to say that not all datasets are included in the tables (but only the 5 I stated before)

If you want to generate your own tables that will contain custom properties, please refer to the section [Generate your own Custom Tables][2].

Cheers,

Petar

  [1]: http://wiki.dbpedia.org/DBpediaAsTables
  [2]: http://wiki.dbpedia.org/DBpediaAsTables#h347-4', 5133, '2014-11-23 02:27:31.947', 'f28f999f-e0a2-4e4f-9325-10acfcd9e82a', 2526, 4977, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hi this is my first question in the Data Science stack. I want to create an algorithm for text classification. Suppose i have a large set of text and articles. Lets say around 5000 plain texts. I first use a simple function to determine the frequency of all the four and above character words. I then use this as the feature of each training sample. Now i want my algorithm to be able to cluster the training sets to according to their features, which here is the frequency of each word in the article. (Note that in this example, each article would have its own unique feature since each article has a different feature, for example an article has 10 "water and 23 "pure" and another has 8 "politics" and 14 "leverage"). Can you suggest the best possible clustering algorithm for this example?', 5138, '2014-11-23 14:58:34.127', '223ef50f-d16c-42e8-8e1f-5127f6a02d92', 2527, 4978, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using Clustering in text processing', 5138, '2014-11-23 14:58:34.127', '223ef50f-d16c-42e8-8e1f-5127f6a02d92', 2527, 4979, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining><clustering>', 5138, '2014-11-23 14:58:34.127', '223ef50f-d16c-42e8-8e1f-5127f6a02d92', 2527, 4980, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you want to proceed on your existing path I suggest normalizing each term''s frequency by its popularity in the entire corpus, so rare and hence predictive words are promoted. Then use random projections to reduce the dimensionality of these very long vectors down to size so your clustering algorithm will work better (you don''t want to cluster in high dimensional spaces).

But there are other ways of topic modeling. Read [this](http://www.cs.princeton.edu/~blei/papers/Blei2012.pdf) tutorial to learn more.', 381, '2014-11-23 17:49:04.487', 'd0a481f4-1949-4025-9878-58147481bedf', 2528, 4981, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Store the edges (relations) in your server:

    (TeamID, playerID)

When you want to find common elements just filter all edges where:

    TeamID="TeamA" or TeamID="TeamB"

(You could use indexes to speed is up, etc)

Then group by playerID and check how many items are in each group. The groups with two items belong to both teams and are shared.
    ', 5041, '2014-11-23 23:00:30.007', 'a50bfbf4-a587-439a-b19b-3d8322026272', 2529, 4982, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I recommend you to take a look to Oryx (https://github.com/cloudera/oryx). Oryx is based on Apache Mahout (actually one of the creators of Mahout Sean Owen built it) and provides recommendation using collaborative filtering. Oryx is a very practical tool for implementing recommendation. I have used it in several projects: recommending products in retail stores (small businesses), building an e-commerce recommender and user similarity from mobile app interaction.

You just have to represent data in the form:
UserId ItemId Value

Where value is a measure (subjective) of the *importance* or *influence* of the interaction between that user and the item. User and item can be anything actually, and the same procedure can be used for tagging. For example, for recommending songs, finding similar songs and bands, and finding similar users according to their music tastes you can represent data as

UserId SongId NumberOfPlays

Where NumberOfPlays is the amount of times a song has been played by user (in an online music service for example). This exampl was given in Myrrix the predecessor of Oryx. They also show how to recommend tags to questions in StackOverflow.

The github site is not that well documented but it will be enough to get it running (and working :))



', 5143, '2014-11-24 09:20:47.543', '1c4911f0-e713-4869-aa78-4883ce04550c', 2530, 4983, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('To all:

I have been wracking my brain at this for a while and thought maybe someone here would know of a package or algorithm to handle the following:

I have nominal multivariant timeseries that look like the following:

              Time Var1 Var2 Var3 Var4 Var5 ... VarN
                 0     A     A   B    C    A   ... H
                 1     A     A   B    D    D   ... H
                 2     B     A   C    D    D   ... H
                 ..

And so on from times 0 to 1,000,000. What I would like to do is search the time series for rules of the type:

Given Var3 is in state B in the previous step and Var5 is in state D in the previous step, than Var1 will be in state B. What I want to do is have the rules that include the time interval explicitly. A simpler case of interest would simply be to reduce the time series to

                   Time    Var1 Var2 Var3 Var4 Var5 ... VarN
                    0        0    0    0     0   0   ... 0
                    1        0    0    0     1   1   ... 0
                    2        1    0    1     0   0   ... 0

Where the the variable is 1 if its state is different from the previous step and zero otherwise. Then I just want to have rules that say something like:

If Var4 and Var5 changed in the previous step than Var1 will change in the current step. Which would be easy for a lag of one, as I could just make the data into something like:

       Var1 Var2 Var3 Var4 Var5 ... VarN Var1_t-1 Var2_t-1 Var3_t-1 ...
and then do sequence mining, but if I want to have rules that aren''t just a single lag but could be lags from 1 to 500 than my data set begins to be a little difficult to work with.

Any help would be greatly appreciated.


Edit to respond to comment:
Each column could be in one of 7 different states. As far as a target, it is non-specific, any rules between the columns would be of interest. However, predicting columns 30-40 and 62-75 would be particularly interesting.', 5134, '2014-11-24 13:47:38.860', '70667472-afd4-4e76-8a71-cc620be0f341', 2516, 'Response to comments', 4984, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-24 13:47:38.860', '70667472-afd4-4e76-8a71-cc620be0f341', 2516, 'Proposed by 5134 approved by 5113 edit id of 184', 4985, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD

According to the description given in the above link,
the Attribute information specifies "average and covariance over all ''segments'', each segment being described by a 12-dimensional timbre vector". So the covariance matrix should have 12*12 = 144 elements. But why is the number of timbre covariance features only 78 ?', 4947, '2014-11-25 03:08:34.843', '2fdfe07a-4081-4e5d-bd0d-6e58d80fac88', 2534, 4995, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Confused about description of YearPrediction Dataset', 4947, '2014-11-25 03:08:34.843', '2fdfe07a-4081-4e5d-bd0d-6e58d80fac88', 2534, 4996, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset>', 4947, '2014-11-25 03:08:34.843', '2fdfe07a-4081-4e5d-bd0d-6e58d80fac88', 2534, 4997, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You are right, the covariance matrix should have n^2 elements. However, since cov_{i,j} = cov_{j,i}, there is no need to have a repeated feature cov_{j,i} if cov_{i,j} is already accounted for. Hence there will be only n*(n+1)/2 = 12*13/2 = 78 unique covariances and thus only 78 unique covariance based features (n of those will be variances). ', 847, '2014-11-25 03:38:33.600', 'e54928fb-6ba1-4879-a7cd-1d59ba03dc50', 2535, 4998, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This problem is one of estimating the lag. Once that is estimated, you could create additional features representing the lagged values and move forward with "sequence mining" as you have already suggested in the question itself.

For each variable, Var_i, you will have to estimate its lag l_i. This lag can be calculated by estimating the order of a Markov chain with seven symbols (you could use either [BIC][1] or [AIC][2] to estimate this order; both would require calculating likelihood of candidate orders and pick the order that maximizes either of these criteria). Once you are done calculating the order of the Markov chain for each of the variables, then you could represent your dataset such that each row will have the current value of Var_i and its preceding values, all the way back to its estimated lag l_i. While this methodology is laborious, it pays rich dividends as its automated and parsimonious way of representing the necessary information.


  [1]: http://en.wikipedia.org/wiki/Bayesian_information_criterion
  [2]: http://en.wikipedia.org/wiki/Akaike_information_criterion', 847, '2014-11-25 09:17:45.020', '7d0b76c8-6c7c-403b-b764-ac428ec14844', 2536, 4999, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('GBMs, like random forests, build each tree on a different sample of the dataset and hence, going by the spirit of ensemble models, produce higher accuracies. However, I have not seen GBM being used with dimension sampling at every split of the tree like is common practice with random forests.

Are there some tests that show that dimensional sampling with GBM would decrease its accuracy because of which this is avoided, either in literature form or in practical experience? ', 847, '2014-11-25 09:40:20.040', '7373d3d1-5615-4478-af6c-7fcd8f9ba0dd', 2537, 5000, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why isn''t dimension sampling used with gradient boosting machines (GBM)?', 847, '2014-11-25 09:40:20.040', '7373d3d1-5615-4478-af6c-7fcd8f9ba0dd', 2537, 5001, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<random-forest><accuracy><gbm><ensemble-modeling>', 847, '2014-11-25 09:40:20.040', '7373d3d1-5615-4478-af6c-7fcd8f9ba0dd', 2537, 5002, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have historic error of time series.  I want to analyze error series to improve forecast series. Are there any methods to do this?', 5099, '2014-11-25 10:30:17.567', '1fe85880-4984-4e7c-81cf-e5b7cc62d90f', 2538, 5003, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Error analysis for better accuracy', 5099, '2014-11-25 10:30:17.567', '1fe85880-4984-4e7c-81cf-e5b7cc62d90f', 2538, 5004, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<time-series><forecast>', 5099, '2014-11-25 10:30:17.567', '1fe85880-4984-4e7c-81cf-e5b7cc62d90f', 2538, 5005, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I couldn''t quite think of how best to title this, so recommendations are welcome. Same goes for the tags (I don''t have the reputation to use the tags that I thought were appropriate). The question is this:

"Suppose you have N pairs of observations, (x,y), and you have a model with some unknown parameters, B, that estimates the relationship between x and y, F(x,B) -> y. Now suppose you determine B using the method of least-squares (and, implicitly, that all the assumptions of least-squares are satisfied). The parameters, B, are themselves random variables, each with its own variance. Is there any way to estimate the reduction (or increase) in the variance of B that would result from applying the same method of least-squares to N+1 pairs of observations?"

The question is asked in the context of experimentation. If each data point costs $X, an affirmative answer to the question would go a long way in determining whether or not to continue testing.

Many thanks.', 847, '2014-11-25 18:27:15.063', '765d26d6-c0f3-413f-877b-dc510f1d76d9', 1155, 'added tag, made it look professional', 5010, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<statistics><algorithms><experiments>', 847, '2014-11-25 18:27:15.063', '765d26d6-c0f3-413f-877b-dc510f1d76d9', 1155, 'added tag, made it look professional', 5011, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-25 18:27:15.063', '765d26d6-c0f3-413f-877b-dc510f1d76d9', 1155, 'Proposed by 847 approved by -1 edit id of 185', 5012, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I couldn''t quite think of how best to title this, so recommendations are welcome. Same goes for the tags (I don''t have the reputation to use the tags that I thought were appropriate). The question is this:

"Suppose you have N pairs of observations, (x,y), and you have a model with some unknown parameters, B, that estimates the relationship between x and y, F(x,B) -> y. Now suppose you determine B using the method of least-squares (and, implicitly, that all the assumptions of least-squares are satisfied). The parameters, B, are themselves random variables, each with its own variance. Is there any way to estimate the reduction (or increase) in the variance of B that would result from applying the same method of least-squares to N+1 pairs of observations?"

The question is asked in the context of experimentation. If each data point costs $X, an affirmative answer to the question would go a long way in determining whether or not to continue testing.', 84, '2014-11-25 18:27:15.063', '606d6520-c4e9-4f29-a398-3b3ce61a626c', 1155, 'added tag, made it look professional', 5013, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have just finished my Ph.D. and have used some NLP in it. My university didn''t offer any NLP courses. So I ended up teaching myself NLP. I used the following book:

http://www.nltk.org/book_1ed/

Which serves as a great introduction to NLP using NLTK (Natural Language Tool Kit). It gives a good introduction into programming with Python. So handy if you''ve never programmed in Python before too.

I would highly suggest using nltk from nltk.org (sorry can''t post more than two links)

The book I used is now out of date as NLTK is now on version 3.0, the book mentioned previously is for NLTK 2.x. But you''re in luck. The Authors are working on a new version of the book for NLTK 3.X You can view the unfinished book over at

http://www.nltk.org/book/


I would highly suggest using NLTK and if you''re new to natural language processing. I would highly suggest you try and get yourself a copy of the following book:

Foundations of Statistical Natural Language Processing by Manning and Shutze

Even though it doesn''t contain any code, it servers a great introduction to natural language processing.', 5170, '2014-11-26 02:07:06.460', '23eea3b0-8742-42b0-8a4c-f3631eee437f', 2540, 5019, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('
What is the standard way for evaluating and comparing different algorithms while developing recommendation system? Whether we need to have a predetermined annotated ranked dataset and then compare with precision/recall/F measure of  different algorithms ? Is this the best way for evaluation ? Or is there any other way to compare results of various recommendation algorithms ?', 5091, '2014-11-26 04:40:17.840', 'c3011fd2-2566-42f5-bad2-8685fa8aa6ef', 2541, 5020, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Evaluating Recommendation engines', 5091, '2014-11-26 04:40:17.840', 'c3011fd2-2566-42f5-bad2-8685fa8aa6ef', 2541, 5021, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><dataset><statistics><recommendation>', 5091, '2014-11-26 04:40:17.840', 'c3011fd2-2566-42f5-bad2-8685fa8aa6ef', 2541, 5022, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m from programming background. I''m now learning Analytics. I''m learning concepts from basic statistics to model building like linear regression, logistic regression, time-series analysis, etc.,

As my previous experience is completely on programming, I would like to do some analysis on the data which programmer has.

Say, Lets have the details below(I''m using SVN repository)

personname, code check-in date, file checked-in, number of times checkedin, branch, check-in date and time, build version, Number of defects, defect date, file that has defect, build version, defect fix date, defect fix hours, (please feel free to add/remove how many ever variables needed)

I Just need a trigger/ starting point on what can be done with these data. can I bring any insights with this data.

or can you provide any links that has information about similar type of work done.
', 5172, '2014-11-26 08:47:49.650', '3cb86147-913e-4755-9427-daa597a342cf', 2543, 5024, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can Analytics be applied in software developement', 5172, '2014-11-26 08:47:49.650', '3cb86147-913e-4755-9427-daa597a342cf', 2543, 5025, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><statistics><recommendation><predictive-modeling>', 5172, '2014-11-26 08:47:49.650', '3cb86147-913e-4755-9427-daa597a342cf', 2543, 5026, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Can Machine Learning be applied in software developement', 97, '2014-11-26 09:35:10.677', '348510c4-9a12-4eac-95f0-fb6d9ef53af2', 2543, 'Change to more relevant title', 5027, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-26 09:35:10.677', '348510c4-9a12-4eac-95f0-fb6d9ef53af2', 2543, 'Proposed by 97 approved by 5172 edit id of 186', 5028, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Definately - Yes.
Good question.
I was thinking about it myself.

**(1) Collect the data**.
The first problem you have: gather enough data.
All the attributes you mentioned (date, name, check-in title/comment, N of deffects etc) are potentially useful - gather as much as possible.
As soon as you have a big project, a number of developers, many branches, frequent commits and you have started collecting all the data, you a ready to go further.

**(2) Ask good questions**.
The next question you should ask yourself: what effect are you going to measure, estimate and maybe predict.
Frequency of possible bugs? Tracking inaccurate "committers"? Risky branches?

**(3) Select the model**.
As soon as you get the questions formulated, you should follow the general approach at data science - extract needed features in your data, select appropriate model, train you model and apply it.', 97, '2014-11-26 09:37:43.970', 'a0a205cb-eaf3-420e-8e2f-4aab559c1bec', 2544, 5029, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><recommendation><predictive-modeling><software-develpment>', 97, '2014-11-26 09:43:21.757', 'c82923ef-a50c-4c01-939e-644e9ed8a150', 2543, 'Retagging to relevant tags', 5030, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-11-26 09:43:21.757', 'c82923ef-a50c-4c01-939e-644e9ed8a150', 2543, 'Proposed by 97 approved by 5172 edit id of 187', 5031, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Definately - Yes.
Good question.
Was thinking about it myself.

**(1) Collect the data**.
The first problem you have: gather enough data.
All the attributes you mentioned (date, name, check-in title/comment, N of deffects etc) are potentially useful - gather as much as possible.
As soon as you have a big project, a number of developers, many branches, frequent commits and you have started collecting all the data, you a ready to go further.

**(2) Ask good questions**.
The next question you should ask yourself: what effect are you going to measure, estimate and maybe predict.
Frequency of possible bugs? Tracking inaccurate "committers"? Risky branches? Want to see some groups of users/bugs/commits according to some metrics?

**(3) Select the model**.
As soon as you get the questions formulated, you should follow the general approach at data science - extract needed features in your data, select appropriate model, train you model and test it, apply it.', 97, '2014-11-26 09:52:45.610', 'b8fc1855-7215-4d27-b9d6-1163c6b11357', 2544, 'More detatils.', 5032, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As you are also looking for examples, then github is a good place to check out.

I took a random repository and went to "Graphs" on the right hand side, which opens up [contribution frequency graph][1]. There''s several tabs next to it that display other aspects of a repository and commit history graphically - commits, code frequency, punch card, etc.


  [1]: https://github.com/mbostock/d3/graphs/contributors', 587, '2014-11-26 09:57:00.047', '44b83cb7-abdf-4050-8b92-bd38ae0cc0f2', 2545, 5033, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Definately - Yes.
Good question.
Was thinking about it myself.

**(1) Collect the data**.
The first problem you have: gather enough data.
All the attributes you mentioned (date, name, check-in title/comment, N of deffects etc) are potentially useful - gather as much as possible.
As soon as you have a big project, a number of developers, many branches, frequent commits and you have started collecting all the data, you a ready to go further.

**(2) Ask good questions**.
The next question you should ask yourself: what effect are you going to measure, estimate and maybe predict.
Frequency of possible bugs? Tracking inaccurate "committers"? Risky branches? Want to see some groups of users/bugs/commits according to some metrics?

**(3) Select the model**.
As soon as you have the questions formulated, you should follow the general approach in data science - extract needed features in your data, select appropriate model, train you model and test it, apply it. This is too broad process to discuss it this thread, so please use this site to get right answers.', 97, '2014-11-26 09:58:47.527', 'f7b87384-9be8-4f33-9c03-03c41318fb36', 2544, 'More detatils.', 5034, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you''re simply trying to separate textual and numeric information then there is a solution based on regular expressions or even just string splitting.

You could even do something like finding the first numeric character and split the text in half right before that.

With regular expressions you can match all numeric characters that follow eachother. The pattern would be `([0-9]+)` with a global flag. It would match all the groups of numbers and you can do whatever you with with them afterwards.

[Regex Tester][1] is good for playing around with that stuff.


  [1]: http://www.regextester.com/index.html', 587, '2014-11-26 11:18:13.227', '8c534e69-645d-4fab-bf50-fddcf8ad916b', 2546, 5035, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to determine whether or not we are 90% confident that the mean of a *proposed* *population* is at least 2 times that of the mean of the *incumbant* *population* based on samples from each population which is all the data I have right now. Here are the data.

incumbantvalues = (7.3, 8.4, 8.4, 8.5, 8.7, 9.1, 9.8, 11.0, 11.1, 11.9)

proposedvalues =  (17.3, 17.9, 19.2, 20.3, 20.5, 20.6, 21.1, 21.2, 21.3, 21.7)

I have no idea if either population is or will be normal.

The ratio of the *sample* means does exceed 2.0 but how does that translate to confidence that the proposed population mean will be at least twice that of the mean of the incumbant population with 90% confidence ?


Can re-sampling (bootstrapping with replacement) help answer this question ?
', 5180, '2014-11-26 20:01:23.187', '14266eb1-4650-4f96-879c-cf32488c02da', 2547, 5037, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Statistical comparison of 2 small data sets for 2X increase in the population mean', 5180, '2014-11-26 20:01:23.187', '14266eb1-4650-4f96-879c-cf32488c02da', 2547, 5038, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics><sampling>', 5180, '2014-11-26 20:01:23.187', '14266eb1-4650-4f96-879c-cf32488c02da', 2547, 5039, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Without a doubt you can. The key is to have a set of hypotheses (i.e. assumptions \ scenarios that you want to evaluate) and wrangle the data together to prove \ disprove what you thought is true.

Here are a few things to watch-out for:

 - **Be ready for Disappointments:** Often times, once you have invested time and energy in building these models, analysts tend to get biased towards publishing results (publication bias). Treat this as an exploration that with a lot of dead-ends and the goal should be to find the ones that are not.

 - **Know your Data:** You cannot will your data into doing things magically without truly understanding it. Ensure that you know the different attributes (predictors and dependents) very well. Knowing your data well will allow you to cleanse it and think about appropriate models. All models don''t work equally well on all data - data that has a lot of categorical variables might require creative solutions like Dimension reduction before it can be modeled.

 - **Know the "Operational" Processes:** Knowing how things operate within your firm will help you refine the set of hypothesis that you want to test. For e.g. in your scenario above, knowing how developers work with your change management software and what types of administrative setups have been done will help you figure out why the data is coming in the way it is. Some developers might only be focused on certain modules that are more mature than others, might work only on certain shifts and that might limit how many lines of code are checked in, how many bugs are found etc.

Having said that here are some scenarios you might want to test:

 - Developer Effectiveness :
How different developers working on same modules overtime has resulted into increase or decrease in bugs.
Does more line of code results in more bugs? Maybe this might be an indicator that the programs need to be split further into smaller components
Folks might be more productive during certain times of day than others - does time of day affect bug introductions?

 - Module Maturity:
Which Modules have the most number of issues?
Are they worked upon by more developers or less?
Do defects keep aging for a long time before they are fixed?

Of course, these questions will change depending on what you are working on.

Hope this helps.', 5182, '2014-11-26 21:20:52.973', '4657ba7f-8bfd-4a80-9056-c3a851624065', 2549, 5041, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am doing a text classification task(5000 essays evenly distributed by 10 labels). I explored `LinearSVC` and got an accuracy of 80%. Now I guess whether accuracy could be raised by using `ensemble` classifier with `SVM` as base estimator?

However, I do not know how to employ an `ensemble` classifier incorporating all the features? Please note that I do not want to combine the different features directly in a single vector.

Therefore, **My first question:** in order to improve the current accuracy, is it possible to use `ensemble` classifier with `svm` as base estimator?
**My second question** How to employ an `ensemble` classifier incorporating all features?', 4950, '2014-11-27 03:21:11.110', '4e3518ad-1854-4958-b33c-f8f6531858c2', 2550, 5042, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to ensemble classifier incorporating all features in python?', 4950, '2014-11-27 03:21:11.110', '4e3518ad-1854-4958-b33c-f8f6531858c2', 2550, 5043, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><python><nlp><sklearn><ensemble-modeling>', 4950, '2014-11-27 03:21:11.110', '4e3518ad-1854-4958-b33c-f8f6531858c2', 2550, 5044, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If these data are available in the actual excel spreadsheet cells (ie, before you export them to the JSON format provided in your question), you can use the following to get them into R:

 1. highlight the region of interest within excel
 2. copy it to the clipboard (eg. Ctrl-C)
 3. At an R prompt type:

        d <- read.delim(''clipboard'')

The data will now be available as a data.frame in R.

    d
      from response value
    1    4     TRUE    20
    2    8     TRUE    20
    3    9     TRUE    20
    4    3     TRUE    20
    5   14    FALSE    20
    6   15     TRUE    20
    7   17    FALSE    20
    8   13     TRUE    20  ', 5153, '2014-11-27 03:27:46.193', 'b0e65e84-7924-494c-a5d5-eac0f9b1e185', 2551, 5045, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The standard way to evaluate a recommendation engine is by using the **[RMSE (root mean square error)][1]** of the predicted values and the ground truth.

It is almost a SOP that, after finishing developing a recommendation engine, we will evaluate this engine by comparing its RMSE with other famous, common recommendation algorithms like **[SVD][2]**, **[tranditional CF][3]**, even [RBM][4], etc.

Some terms mentioned above do not seem to be related with recommendation, but you can easily find on the internet how these techniques can be used in this topic.

  [1]: http://en.wikipedia.org/wiki/Root-mean-square_deviation
  [2]: http://en.wikipedia.org/wiki/Singular_value_decomposition
  [3]: http://en.wikipedia.org/wiki/Collaborative_filtering
  [4]: http://en.wikipedia.org/wiki/Restricted_Boltzmann_machine', 5184, '2014-11-27 03:37:08.010', '90f330a7-7b6d-48fe-bf4e-800cda7e16e8', 2552, 5046, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes, in principle, resampling can help answer this question.

    incumbant <- c(7.3, 8.4, 8.4, 8.5, 8.7, 9.1, 9.8, 11.0, 11.1, 11.9)
    proposed  <- c(17.3, 17.9, 19.2, 20.3, 20.5, 20.6, 21.1, 21.2, 21.3, 21.7)

    set.seed(42)

    M  <- 2000
    rs <- double(M)

    for (i in 1:M) {
        rs[i] <- mean(sample(proposed, replace=T)) - 2 * mean(sample(incumbant, replace=T))
    }

Using Hall''s method, the 90% confidence interval for the difference in the (weighted) means

    ci.hall <- 2 * (mean(proposed)-2*mean(incumbant)) - rev(quantile(rs,prob=c(0.05, 0.95)))
    names(ci.hall) <- rev(names(ci.hall))
    ci.hall

       5%   95%
    -0.29  2.95

You would expect that lower cutoff > 0 for your hypothesis to be true.

The proportion of resample means >= 0 gives the estimate that mean(proposed) is at least twice mean(incumbant):

    sum(rs>=0)/M

    [1] 0.8915


The problem is that the samples are really rather small and resampling estimates
can be unstable for small n. The same issue applies if you want to assess normality and go with parametric comparisons. If you can get to, say, n >= 30, the approach described here should be fine.

', 5153, '2014-11-27 04:23:31.500', 'cc6a9225-9520-426b-8c82-617112d0f773', 2553, 5047, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Yes, in principle, resampling can help answer this question.

    incumbant <- c(7.3, 8.4, 8.4, 8.5, 8.7, 9.1, 9.8, 11.0, 11.1, 11.9)
    proposed  <- c(17.3, 17.9, 19.2, 20.3, 20.5, 20.6, 21.1, 21.2, 21.3, 21.7)

    set.seed(42)

    M  <- 2000
    rs <- double(M)

    for (i in 1:M) {
        rs[i] <- mean(sample(proposed, replace=T)) - 2 * mean(sample(incumbant, replace=T))
    }

Using Hall''s method, the 90% confidence interval for the difference in the (weighted) means

    ci.hall <- 2 * (mean(proposed)-2*mean(incumbant)) - rev(quantile(rs,prob=c(0.05, 0.95)))
    names(ci.hall) <- rev(names(ci.hall))
    ci.hall

       5%   95%
    -0.29  2.95


The proportion of resample means >= 0 gives the estimate that mean(proposed) is at least twice mean(incumbant):

    sum(rs>=0)/M

    [1] 0.8915


The problem is that the samples are really rather small and resampling estimates
can be unstable for small n. The same issue applies if you want to assess normality and go with parametric comparisons.

If you can get to, say, n >= 30, the approach described here should be fine.

', 5153, '2014-11-27 04:35:05.763', '73745581-62cc-4ae9-a6b3-da025705af10', 2553, 'formatting', 5048, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Yes, in principle, resampling can help answer this question.

    incumbent <- c(7.3, 8.4, 8.4, 8.5, 8.7, 9.1, 9.8, 11.0, 11.1, 11.9)
    proposed  <- c(17.3, 17.9, 19.2, 20.3, 20.5, 20.6, 21.1, 21.2, 21.3, 21.7)

    set.seed(42)

    M  <- 2000
    rs <- double(M)

    for (i in 1:M) {
        rs[i] <- mean(sample(proposed, replace=T)) - 2 * mean(sample(incumbent, replace=T))
    }

To make the assessment, you should choose *one* (not both) of the following:

A. The (two-tailed) 90% confidence interval for the difference in the (weighted) means using Hall''s method is:

    ci.hall <- 2 * (mean(proposed)-2*mean(incumbent)) - rev(quantile(rs,prob=c(0.05, 0.95)))
    names(ci.hall) <- rev(names(ci.hall))
    ci.hall

       5%   95%
    -0.29  2.95

This is appropriate if you have any concern about missing the possibility that mean(proposed) might actually be less than 2 * mean(incumbent).

B. The proportion of resample means >= 0 provides the (one-tailed) estimate that mean(proposed) is at least twice mean(incumbent):

    sum(rs>=0)/M

    [1] 0.8915

The problem is that the samples are really rather small and resampling estimates
can be unstable for small n. The same issue applies if you want to assess normality and go with parametric comparisons.

If you can get to, say, n >= 30, the approach described here should be fine.


', 5153, '2014-11-27 05:03:58.203', 'c949afdd-ea35-4e04-9f37-ed5b09c0add8', 2553, 'added 268 characters in body', 5052, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Check these :

Repository of Test Domains for Information Extraction : http://www.isi.edu/info-agents/RISE/repository.html

DBpedia : http://wiki.dbpedia.org/Downloads32


', 5091, '2014-11-27 07:21:56.277', '0dc5ddba-1034-439c-a46e-51806319035e', 2555, 5053, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here is what I programmed within a loop.

1. randomly take 10 values (with replacement) from the incumbant sample, determine its mean
2. randomly take 10 values (with replacement) from the proposed sample, determine its mean
3. form the ratio of the above two means and append it to a master list
4. repeat steps 1 thru 3 many times (I chose 1 million)
5. % Confidence=(number of ratios that equal or exceed 2.0/1000000)*100

Results:
Exactly 897450 ratios were found to be greater than or equal to 2.0, producing a confidence of 89.745%.

Conclusion:
We are less than 90% confident that the proposed population will have a mean at least twice that of the incumbant population.
', 5180, '2014-11-27 21:58:15.113', 'e1595dc7-855c-4cf0-bc30-33b353f12363', 2557, 5061, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are some possible techniques for smoothing proportions across very large categories, in order to take into account the sample size? The application of interest here is to use the proportions as input into a predictive model, but I am wary of using the raw proportions in cases where there is little evidence and I don''t want to overfit.

Here is an example, where the ID denotes a customer and impressions and clicks are the number of ads shown and clicks the customer has made, respectively.



![enter image description here][1]


  [1]: http://i.stack.imgur.com/3oHzQ.jpg', 1138, '2014-11-28 03:02:44.323', '13654a78-82b0-4585-96fd-0b6860332b91', 2558, 5062, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Smoothing Proportions :: Massive User Database', 1138, '2014-11-28 03:02:44.323', '13654a78-82b0-4585-96fd-0b6860332b91', 2558, 5063, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><predictive-modeling><feature-extraction>', 1138, '2014-11-28 03:02:44.323', '13654a78-82b0-4585-96fd-0b6860332b91', 2558, 5064, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data analysis is always driven by the request. It could be: "I want to find out this, so I need to collect those data first. Then I would use this model to analyze". If you just want to practice, by reviewing your data set, there is one:

**Task**: Which issue affects the "number of check in " most?

**Data set**: what you have

**Model**: Correlation (e.g. Spearman, which is nonparametric measure of statistical dependence between two variables)', 5198, '2014-11-28 06:02:59.210', 'd3242220-63ac-4450-8113-6f005facb26b', 2559, 5065, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Cannot say it is the best one, but Latent Semantic Analysis could be one option. Basically it is based on co-occurrence, you need to weight it first.

http://en.wikipedia.org/wiki/Latent_semantic_analysis

http://lsa.colorado.edu/papers/dp1.LSAintro.pdf

The problem is that LSA does not have firm statistic support.

Have fun', 5198, '2014-11-28 06:17:34.480', '3dbdf9c3-7b2b-4dd7-826e-c0c7ddbae3c0', 2561, 5067, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I don''t know if you ever read SenseCluster by Ted Pedersen : http://senseclusters.sourceforge.net/. Very good paper for sense clustering.

Also, when you analyze words, think that "computer", "computers", "computering", ... represent one concept, so only one feature. Very important for a correct analysis.

To speak about the clustering algorithm, you could use a [hierarchical clustering][1]. At each step of the algo, you merge the 2 most similar texts according to their features (using a measure of dissimilarity, euclidean distance for example). With that measure of dissimilarity, you are able to find the best number of clusters and so, the best clustering for your texts and articles.

Good luck :)


  [1]: http://en.wikipedia.org/wiki/Hierarchical_clustering', 5165, '2014-11-28 08:55:25.813', '91b2d6e2-65f3-4acc-815d-8591244469b4', 2562, 5068, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a visualization problem.

Creating a comparison report of PR event efficiency. Say, show or exhibition.

There are two dimensions of comparison:

- compare vs the same event performance in the past years
- compare vs another type of analogical/competitive events

There is also a number of comparison aspects:

- Audience
- Media Coverage
- Social Buzz
- ROI
- .... etc

Each aspect is a set of some final KPI-s (just numbers, which can be compared vs another "dimensions"), plus maybe some descriptive text and pictures (which couldn''t be a metric but should be attached to the report).

So finaly it looks like a three-dimensional coube:

 1. Years
 2. Another Events
 3. Aspects

If I put it in plain Word or PPT it will look like a document with dozen of slides/papers and linear structure.

Any ideas how to compile an elegant user-friendly report?', 97, '2014-11-28 09:19:32.867', 'ce7d9cf0-327d-4fd0-a8a9-84634880889d', 2563, 5069, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Visualization of three-dimensional report', 97, '2014-11-28 09:19:32.867', 'ce7d9cf0-327d-4fd0-a8a9-84634880889d', 2563, 5070, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<marketing><infographics><data-visualization><reporting>', 97, '2014-11-28 09:19:32.867', 'ce7d9cf0-327d-4fd0-a8a9-84634880889d', 2563, 5071, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A short while ago, I came across this ML framework that has implemented several different algorithms ready for use. The site also provides a handy API that you can access with an API key.

I have need of the framework to solve a website classification problem where I basically need to categorize several thousand websites based on their HTML content. As I don''t want to be bound to their existing API, I wanted to use the framework to implement my own.

However, besides some introductory-level data mining courses and associated reading, I know very little as to what exactly I would need to use. Specifically, I''m at a loss as to what exactly I need to do to train the classifier and then model the data.

The framework already includes some classification algorithms like NaiveBayes, which I know is well suited to the task of text classification, but I''m not exactly sure how to apply it to the problem.

Can anyone give me a rough guidelines as to what exactly I would need to do to accomplish this task?', 5199, '2014-11-28 11:39:09.537', '5e5050bd-b3b9-4443-b187-fc5a3ccc5175', 2564, 5074, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using the Datumbox Machine Learning Framework for website classification - guidelines?', 5199, '2014-11-28 11:39:09.537', '5e5050bd-b3b9-4443-b187-fc5a3ccc5175', 2564, 5075, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><java>', 5199, '2014-11-28 11:39:09.537', '5e5050bd-b3b9-4443-b187-fc5a3ccc5175', 2564, 5076, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('AUC and accuracy are fairly different things. AUC applies to binary classifiers that have some notion of a decision threshold internally. For example logistic regression returns positive/negative depending on whether the logistic function is greater/smaller than a threshold, usually 0.5 by default. When you choose your threshold, you have a classifier. You have to choose one.

For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set.

AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else. More importantly, AUC is not a function of threshold. It is an evaluation of the classifier as threshold varies over all possible values. It is in a sense a broader metric, testing the quality of the internal value that the classifier generates and then compares to a threshold. It is not testing the quality of a particular choice of threshold.

AUC has a different interpretation, and that is that it''s also the probability that a randomly chosen positive example is ranked above a randomly chosen negative example, according to the classifier''s internal value for the examples.

AUC is computable even if you have an algorithm that only produces a ranking on examples. AUC is not computable if you truly only have a black-box classifier, and not one with an internal threshold. These would usually dictate which of the two is even available to a problem at hand.

AUC is, I think, a more comprehensive measure, although applicable in fewer situations. It''s not strictly better than accuracy; it''s different. It depends in part on whether you care more about true positives, false negatives, etc.

_F-measure is more like accuracy in the sense that it''s a function of a classifier and its threshold setting. But it measures precision vs recall (true positive rate), which is not the same as either above._', 21, '2014-11-28 12:48:14.443', 'eb904ff9-b6de-4b20-96b2-6e3027a196dd', 2565, 5079, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('NARMAX Methodology and Residual analysis both address this issue. Search for the following articles:(Error = Residual = Noise)

1. Chaotic Time Series Prediction with residual Analysis Method Using Hybrid ElmanNARX Neural Networks, Muhammad Ardalani-Farsa (2010)

2. Orthogonal Least Squares Methods and their Application to Non-Linear System Identification, S. Chen, S. A. Billings, W. Luo (1989)

3. Any article working on NARMAX, NARMA and Residual Analysis. Remember in NARX and NAR there is no error estimation and analysis.

Notice in general you can follow this steps:

1. Estimate a time series and calculate Error or Residuals using any .

2. Consider errors or residuals as a new time series. Try to estimate Error-Time-Series. Now you can add this estimations to your initial model.

3. You can do this residual analysis as many times as you need. In practice 2 or 3 times suffices. Remember in practice, residual time series are noisy and SNR in this time series is so small. So you should use some Noise-Robust methods for residual analysis.', 5200, '2014-11-28 13:16:02.520', '57db05bf-da62-478b-bddb-e1ecc0188f29', 2566, 5080, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to make a prediction for the result of the parliamentary elections. My output will be the % each party receives. There is more than 2 parties so logistic regression is not a viable option. I could make a separate regression for each party but in that case the results would be in some manner independent from each other. It would not ensure that the sum of the results would be 100%.

What regression (or other method) should I use? Is it possible to use this method in R or Python via a specific library?', 5211, '2014-11-29 16:05:08.810', '541fe66b-8efc-48b8-b3e9-daf6ab31448e', 2567, 5082, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What regression to use to calculate the result of election in a multiparty system?', 5211, '2014-11-29 16:05:08.810', '541fe66b-8efc-48b8-b3e9-daf6ab31448e', 2567, 5083, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><python><regression><predictive-modeling>', 5211, '2014-11-29 16:05:08.810', '541fe66b-8efc-48b8-b3e9-daf6ab31448e', 2567, 5084, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have some text files containing moview reviews I need to find out whether the review is good or bad. I tried the following code but its not working:

    import nltk
    with open("c:/users/user/desktop/datascience/moviesr/movies-1-32.txt", ''r'') as m11:
        mov_rev = m11.read()
    mov_review1=nltk.word_tokenize(mov_rev)
    bon="crap aweful horrible terrible bad bland trite sucks unpleasant boring dull moronic dreadful disgusting distasteful flawed ordinary slow senseless unoriginal weak wacky uninteresting unpretentious "
    bag_of_negative_words=nltk.word_tokenize(bon)
    bop="Absorbing Big-Budget Brilliant Brutal Charismatic Charming Clever Comical Dazzling Dramatic Enjoyable Entertaining Excellent Exciting  Expensive Fascinating Fast-Moving First-Rate Funny Highly-Charged Hilarious Imaginative Insightful Inspirational Intriguing Juvenile Lasting Legendary Pleasant Powerful Ripping Riveting Romantic Sad  Satirical Sensitive  Sentimental Surprising Suspenseful Tender Thought Provoking Tragic Uplifting Uproarious"
    bop.lower()
    bag_of_positive_words=nltk.word_tokenize(bop)
    vec=[]
    for i in bag_of_negative_words:
        if i in mov_review1:
            vec.append(1)
        else:
            for w in bag_of_positive_words:
                if w in moview_review1:
                    vec.append(5)

so i am trying to check whether the review contains a positive word or a negative word. If it contains negative word then a value 1 will be assigned to the vector vec else a value of 5 will be assigned.
but the output i am getting is an empty vector.

please help..
Also please suggest others way of solving this problem.
thanks in advance', 5214, '2014-11-30 00:42:28.237', '76d9d859-eb0f-4c62-a7d9-ef2a39df4eb1', 2568, 5085, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Sentiment analysis using python', 5214, '2014-11-30 00:42:28.237', '76d9d859-eb0f-4c62-a7d9-ef2a39df4eb1', 2568, 5086, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><nlp>', 5214, '2014-11-30 00:42:28.237', '76d9d859-eb0f-4c62-a7d9-ef2a39df4eb1', 2568, 5087, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<visualization><marketing><infographics><data-visualization><reporting>', 97, '2014-11-30 08:44:27.780', '4d17ec07-4c52-449a-9306-ed130098b3de', 2563, 'retagging//', 5088, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Robert is right, multinomial logistic regression is the best tool to use. Although you would need to have a integer value representing the party as the dependent variable, for example:

1= Conservative majority, 2= Labour majority, 3= Liberal majority....(and so on)

You can perform this in R using the nnet package. Here is a good place to quickly run through how to use it: http://www.ats.ucla.edu/stat/r/dae/mlogit.htm

Hope this helps.', 5219, '2014-11-30 20:01:23.930', 'c4d3a3f0-ce0f-4356-b3a4-eff2db4a7c17', 2569, 5089, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('On what do you want to base your prediction? I''ve tried to predict multiparty election results for my thesis based on previous years and then using results for some polling stations from this year predict the results in all other polling stations. For this the linear model with which I compared estimated the number of votes each party would obtain by regressing over the votes from previous years. If you have the estimated number of votes for all parties you can calculate the percentage from that. See http://amstat.tandfonline.com.proxy.ubn.ru.nl/doi/abs/10.1198/016214504000001835#.VHt5tDGG-OM for the relevant paper, which extends the linear model.', 5220, '2014-11-30 20:14:15.560', '00aa0837-15e9-463d-9240-72693c5bbdcb', 2570, 5090, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The United States Census Bureau has many free housing datasets (some of which are updated more than once every 10 years). There is an [API for American Community Survey 1 Year Data](http://www.census.gov/data/developers/data-sets/acs-survey-1-year-data.html) that includes housing data. There are raw data sets at [American Fact Finder](http://factfinder2.census.gov/faces/nav/jsf/pages/searchresults.xhtml?refresh=t).', 1330, '2014-12-01 00:23:30.580', '576ec400-c3a6-4365-b920-323f3bacf47e', 2571, 5091, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is real estate data for sale at [DataQuick](http://www.dataquick.com/) or [Real Quest](http://www.realquest.com/).', 1330, '2014-12-01 00:26:39.927', '3ddbbb96-7aa9-49bc-ac59-647ae0a58064', 2572, 5092, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('try

    vec =[]

    for word in bag_of_negative_words:
        if word in mov_review1:
            vec.append(1)

    for word in bag_of_positive_words:
        if word in moview_review1:
             vec.append(5)', 5091, '2014-12-01 09:58:00.280', 'd02b56f1-c84e-47c3-9e07-a41159d8ee21', 2573, 5093, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I suggest using machine learning libraries with already functional linear regression
[Spark MLlib](https://spark.apache.org/docs/1.1.0/mllib-guide.html) or [hivemall](https://github.com/myui/hivemall).', 5224, '2014-12-01 12:23:03.083', '4deaaa66-3a91-409c-9b52-62669f7e4acc', 2574, 5094, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In this [wiki page](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF) there is a function `corr()` that calculates the Pearson coefficient of correlation, but my question is that: is there any function in Hive that enables to calculate the Kendall coefficient of correlation of a pair of a numeric columns in the group?', 5224, '2014-12-01 14:52:31.827', '706fdf0d-4c3a-4e6c-bf77-be00ee94a0ec', 2575, 5095, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to calculate the Kendall coefficient of correlation of a pair of a numeric columns in the group?', 5224, '2014-12-01 14:52:31.827', '706fdf0d-4c3a-4e6c-bf77-be00ee94a0ec', 2575, 5096, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop><correlation>', 5224, '2014-12-01 14:52:31.827', '706fdf0d-4c3a-4e6c-bf77-be00ee94a0ec', 2575, 5097, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Hive: How to calculate the Kendall coefficient of correlation of a pair of a numeric columns in the group?', 5224, '2014-12-01 15:33:21.720', 'c525425c-998a-4e7b-b2be-045a5b5f385a', 2575, 'edited title', 5098, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m looking for API suggestions for enriching data on companies. Currently I use the Crunchbase API to look up a company''s name or domain and I am trying to gather the domain/name (if I don''t already have both), contact email (this one is a long shot), and the location of their headquarters.

This works incredibly well if Crunchbase has the company in their API, but I''d say this only happens about 25% of the time.

I''d love to get some suggestions on some free APIs that I could use along with Crunchbase. I''d also love to see if anyone has had positive or negative experiences with paid APIs! ', 5227, '2014-12-01 20:10:29.967', 'e2b760c2-8a73-4889-8d73-53937b80a570', 2576, 5099, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('API for Company Data Enrichment Suggestions', 5227, '2014-12-01 20:10:29.967', 'e2b760c2-8a73-4889-8d73-53937b80a570', 2576, 5100, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset>', 5227, '2014-12-01 20:10:29.967', 'e2b760c2-8a73-4889-8d73-53937b80a570', 2576, 5101, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is not a regression but a multi-class classification problem. The output is typically the probabilities of all classes for any given test instance (test row). So in your case, the output for any given row will be of the form:

    prob_1, prob_2, prob_3,..., prob_k

where prob_i denotes the probability of the i-th class (in your case i-th party) and there are k classes in the response variable. The class prediction in this case is going to be the class that has the maximum probability.

There are many classifiers in R that do multi-class classification. You could use logistic regression with multi-class support through the [nnet][1] package in R and invoking the `multinom` command.

As an alternative, you could also use the [gbm][2] package in R and invoke the `gbm` command. To create a multi-class classifier, just use `distribution="multinomial" while using the `gbm` function.


  [1]: http://cran.r-project.org/web/packages/nnet/nnet.pdf
  [2]: http://cran.r-project.org/web/packages/gbm/gbm.pdf', 847, '2014-12-01 21:59:47.540', '1437bd13-5c84-4b6a-90aa-b360c666d5b0', 2577, 5102, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('This is not a regression but a multi-class classification problem. The output is typically the probabilities of all classes for any given test instance (test row). So in your case, the output for any given test row from the trained model will be of the form:

    prob_1, prob_2, prob_3,..., prob_k

where prob_i denotes the probability of the i-th class (in your case i-th party), assuming  there are k classes in the response variable. Note that the sum of these k probabilities is going to be 1. The class prediction in this case is going to be the class that has the maximum probability.

There are many classifiers in R that do multi-class classification. You could use logistic regression with multi-class support through the [nnet][1] package in R and invoking the `multinom` command.

As an alternative, you could also use the [gbm][2] package in R and invoke the `gbm` command. To create a multi-class classifier, just use `distribution="multinomial" while using the `gbm` function.


  [1]: http://cran.r-project.org/web/packages/nnet/nnet.pdf
  [2]: http://cran.r-project.org/web/packages/gbm/gbm.pdf', 847, '2014-12-01 22:18:54.130', 'b8d8f1d2-7f91-49ca-a950-0b071b0ee353', 2577, 'added that sum of predicted probabilities for multi-class problem is 1 ', 5103, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have some question regarding to the choice of the better implementation. I would know the differences and advantages of [Mahout Apache][1] (Java implementation) versus [Graphlab][2] (Python implementation) in the area of the data sciences. Specially in the area of recommenders and classifiers. Can anybody here get some (qualified) feedback about both possibilities?


  [1]: https://mahout.apache.org/
  [2]: http://graphlab.com/index.html', 3281, '2014-12-02 10:58:30.950', '7d231b6b-4636-4b09-b723-6190c7201810', 2578, 5104, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Graphlab vs Mahout', 3281, '2014-12-02 10:58:30.950', '7d231b6b-4636-4b09-b723-6190c7201810', 2578, 5105, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><bigdata><python><java>', 3281, '2014-12-02 10:58:30.950', '7d231b6b-4636-4b09-b723-6190c7201810', 2578, 5106, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which of the following is best (or widely used) for calculating item-item similarity measure in mahout and why ?

    Pearson Correlation
    Spearman Correlation
    Euclidean Distance
    Tanimoto Coefficient
    LogLikelihood Similarity

Is there any thumb-rule to chose from these set of algorithm also how to differentiate each of them ?', 5091, '2014-12-02 11:12:06.103', '687417ee-ae6c-4041-b242-87923ac53d1b', 2579, 5107, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Mahout Similarity algorithm comparison', 5091, '2014-12-02 11:12:06.103', '687417ee-ae6c-4041-b242-87923ac53d1b', 2579, 5108, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><statistics><algorithms><recommendation>', 5091, '2014-12-02 11:12:06.103', '687417ee-ae6c-4041-b242-87923ac53d1b', 2579, 5109, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The advantage of mahout is that it is scalable, apache license , good community and documentation support.', 5091, '2014-12-02 14:35:33.447', '980c8960-6d68-4e47-a9d7-f7d4bb7057e8', 2580, 5110, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The advantage of mahout is that it is scalable, apache license , good community and documentation support. Also Fast-prototyping and evaluation
to evaluate a different configuration of the same algorithm we just need to update a parameter and run again.
', 5091, '2014-12-02 16:00:07.753', 'f93ca5ca-484a-4f7e-b563-c8b287cdb259', 2580, 'added 150 characters in body', 5111, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to match new product description with the existing ones. Product description looks like this: Panasonic DMC-FX07EB digital camera silver. These are steps to be performed:

1. Tokenize description and recognize attributes: Panasonic => Brand, DMC-FX07EB => Model, etc.<br/>
2. Get few candidates with similar features<br/>
3. Get the best candidate.<br/>

I am having problem with the first step (1). In order to get ''Panasonic => Brand'', DMC-FX07EB => Model, silver => color, I need to have index where each token of the product description correspond to certain attribute name (Brand, model, color, etc.) in the existing database. The problem is that in my database product descriptions are presented as one atomic attribute e.g. ''description'' (no separated product attributes).

Basically I don''t have training data, so I am trying to build index of all product attributes so I can build training data. So far I have attributes from bestbuy.com and semantics3.com APIs, but both sources lack most of attributes or contain irrelevant ones. Any suggestions for better APIs to get product attributes? Better approach to do this?

P.S. For every product there is a matched product description in the Database, which is as well in a form of one atomic attribute. I have checked this [question on SO][1], it helped me and it seems we have same approach but I am still trying to get training data.


  [1]: http://stackoverflow.com/questions/18496925/how-to-parse-product-titles-unstructured-into-structured-data', 5241, '2014-12-02 16:09:35.333', '98422f9d-9b76-45cd-90e1-f44b18208d88', 2581, 5112, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Attributes extraction from unstructured product descriptions', 5241, '2014-12-02 16:09:35.333', '98422f9d-9b76-45cd-90e1-f44b18208d88', 2581, 5113, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><nlp><feature-extraction>', 5241, '2014-12-02 16:09:35.333', '98422f9d-9b76-45cd-90e1-f44b18208d88', 2581, 5114, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am currently using SVM and scaling my training features to the range of [0,1].
I first fit/transform my training set and then apply the __same__ transformation to my testing set. For example:

        ### Configure transformation and apply to training set
        min_max_scaler = MinMaxScaler(feature_range=(0, 1))
        X_train = min_max_scaler.fit_transform(X_train)

        ### Perform transformation on testing set
        X_test = min_max_scaler.transform(X_test)

Let assume that a given feature in the training set has a range of [0,100], and that same feature in the testing set has a range of [-10,120]. In the training set that feature will be scaled appropriately to [0,1], while in the testing set that feature will be scaled to a range outside of that first specified, something like [-0.1,1.2].

I was wondering what the consequences of the testing set features being out of range of those being used to train the model? Is this a problem?', 802, '2014-12-02 16:19:19.043', 'f1d11122-f5c4-4603-9fc8-fb07e87aa2cd', 2582, 5115, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Consequence of Feature Scaling', 802, '2014-12-02 16:19:19.043', 'f1d11122-f5c4-4603-9fc8-fb07e87aa2cd', 2582, 5116, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><svm><feature-scaling>', 802, '2014-12-02 16:19:19.043', 'f1d11122-f5c4-4603-9fc8-fb07e87aa2cd', 2582, 5117, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try to search from the database''s of officials "bad words" that google publish in this link [Goolgles official list of bad words...][1]
Also here is the link for the good words [Not official list of good words...][2]

For the code i would do it like this...

    textArray = file(''dir_to_your_text'',''r'').read().split()

    #Bad words should be listed like this for the split function to work
    # "*** ****** **** ****" the stars are for the cenzuration :P
    badArray = file(''dir_to_your_bad_word_file).read().split()
    goodArray = file(''dir_to_your_good_word_file).read().split()

    #Than you use maching algoritem from difflib on good and bad word for ewery word in array of words
    import difflib

    goodMachingCouter = 0;
    badMacihngCouter = 0;


    for iGood in range(0, len(goodArray)):
        for iWord in range(0, len(textArray)):
            goodMachingCounter += difflib.SequenceMatcher(None, goodArray[iGood], textArray[iWord]).ratio()

    for iBad in range(0, len(badArray)):
        for iWord in range(0, len(textArray)):
            badMachingCounter += difflib.SequenceMatcher(None, badArray[ibad], textArray[iWgoodord]).ratio()

    goodMachingCouter *= 100/(len(goodArray)*len(textArray))
    badMacihngCouter *= 100/(len(badArray)*len(textArray))

    print(''Show the good measurment of the text in %: ''+goodMachingCouter)
    print(''Show the bad measurment of the text in %: ''+badMacihngCouter)
    print(''Show the hootnes of the text: '' + len(textArray)*goodMachingCounter)

The code will be slow but accurate :) I didn''t run and test it pls do it for me and post the correct code :) because i wana test it too :)

  [1]: http://fffff.at/googles-official-list-of-bad-words/
  [2]: http://www.enchantedlearning.com/wordlist/positivewords.shtml', 5244, '2014-12-02 20:07:14.887', 'e1a4f9ce-5685-4afb-9118-78dacbcd1c86', 2583, 5118, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Left you a quick response on SO. The gist is that you can collect a lot of information from electronics shops and manufacturers'' web sites, and lots you can annotate manually. If your goal is to only get training data, that''s all you need.', 5249, '2014-12-02 22:23:33.700', '541cb249-98f5-4e00-896c-7cfee2d59917', 2584, 5119, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This was meant as a comment but it is too long.

The fact that your test set has a different range **might** be a sign that the training set is not a good representation of the test set. However, if the difference is really small as in your example, it is likely that it won''t affect your predictions. Unfortunately, I don''t think I have a good reason to think it won''t affect a SVM in any circumstance.

Notice that the rationale for using MinMaxScalar is (according to the documentation):

> The motivation to use this scaling include robustness to very small
> standard deviations of features and preserving zero entries in sparse
> data.

Therefore, it is important for you to make sure that your data fits that case.

If you are really concerned about having a difference range, you should use a regular standardization (such as `preprocessing.scale`) instead.', 4621, '2014-12-03 05:00:30.500', '3550e60d-9597-4573-95d7-4074c00e2376', 2585, 5120, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><classification><python><recommendation><java>', 3466, '2014-12-03 07:43:18.727', '5ec52ea2-b7cb-44d5-8dfc-fc82b0f3bc83', 2578, 'Added tags.', 5121, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-12-03 07:43:18.727', '5ec52ea2-b7cb-44d5-8dfc-fc82b0f3bc83', 2578, 'Proposed by 3466 approved by 3281 edit id of 189', 5122, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am very passionate about how computers can be made able to think intelligently and independently (in our favour of course!). I am currently studying Bachelors science of information technology at UTS (University of Technology:Sydney). I have two months before I start my second year and have not yet been able to decide on which major should I select that can lead myself towards dedicated study of Artificial Intelligence (which I love with my life).

I have the following majors available:

Internetworking and Applications
Data Analytics
(there are other two as well, but business oriented).

Here(uts.edu.au) is the link to my subjects.

I believe that being able to play with data is a sign of intelligence (I may be wrong too!).

Will one of these subjects form me a good foundation for my further study in AI? or should I jump into Engineering? or Pure Science?', 5185, '2014-12-03 07:46:30.967', 'c76447d5-6c8b-4fa5-9999-67a15111f22f', 2586, 5123, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('can data analytics be a basis for artificial intelligence?', 5185, '2014-12-03 07:46:30.967', 'c76447d5-6c8b-4fa5-9999-67a15111f22f', 2586, 5124, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 5185, '2014-12-03 07:46:30.967', 'c76447d5-6c8b-4fa5-9999-67a15111f22f', 2586, 5125, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is there a method/class available in mahout to perform n-fold cross validation?
If yes how it can be done ?', 5091, '2014-12-03 11:05:59.490', '5b669044-7c95-4dc6-aaf5-5e5533d6b29d', 2587, 5126, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('N - fold cross validation in mahout', 5091, '2014-12-03 11:05:59.490', '5b669044-7c95-4dc6-aaf5-5e5533d6b29d', 2587, 5127, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><recommendation><java>', 5091, '2014-12-03 11:05:59.490', '5b669044-7c95-4dc6-aaf5-5e5533d6b29d', 2587, 5128, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The difference between these methods is the assumptions they make about the task.  [Multi-class classification](http://en.wikipedia.org/wiki/Multiclass_classification) assumes that each document has exactly one label.  So a document can either be about sports or weather, not both.
[Multi-label classification](http://en.wikipedia.org/wiki/Multi-label_classification) allows a document to have any combination of labels, including none.  So a document can be about only sports, only weather, sports AND weather, or neither.

You could train a multi-label classifier with data where each document has exactly one label, but there is no guarantee that the predictions made at test time will have only one label.  Also you are forcing the classifier to do more work (and potentially make more errors) by considering more possible labelings than it needs to.
Therefore, if the multi-class assumption makes sense for your problem, you are better off with a multi-class classifier.

The method that you describe for training individual binary classifiers corresponds to multi-label classification.  The binary classifiers that you use could each be trained from one-class data or two-class data.  However, this is only one of the many ways to do multi-label classification (see the wikipedia page above for more).

Unfortunately, the problem that you describe does not cleanly fit into either multi-class or multi-label classification, since you want each document to have _at most_ one label.
', 5263, '2014-12-03 13:21:34.153', 'eab0524a-b8e2-4244-8c5c-ff9961b56e3c', 2588, 5129, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<classification><r><python><regression><predictive-modeling>', 847, '2014-12-03 13:50:12.587', '97277930-ad96-4c44-ba0d-b3b44ed4d7cb', 2567, 'added a tag', 5130, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-12-03 13:50:12.587', '97277930-ad96-4c44-ba0d-b3b44ed4d7cb', 2567, 'Proposed by 847 approved by 21 edit id of 188', 5131, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Is there a method/class available in Apache Mahout to perform n-fold cross validation?
If yes how it can be done?', 21, '2014-12-03 13:50:48.697', '0c2df64a-a585-407f-85fc-4399927b1d30', 2587, 'added 6 characters in body; edited tags', 5132, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><java><apache-mahout>', 21, '2014-12-03 13:50:48.697', '0c2df64a-a585-407f-85fc-4399927b1d30', 2587, 'added 6 characters in body; edited tags', 5133, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I believe the claim that you are referring to is that the _maximum-likelihood estimate_ of the component means in a GMM must lie in the span of the eigenvectors of the second moment matrix.  This follows from two steps:

1. Each component mean in the maximum-likelihood estimate is a linear combination of the data points.  (You can show this by setting the gradient of the log-likelihood function to zero.)
2. Any linear combination of the data points must lie in the span of the eigenvectors of the second moment matrix.  (You can show this by first showing that any individual data point must lie in the span, and therefore any linear combination must also be in the span.)', 5263, '2014-12-03 13:55:16.150', '1383bc8b-afa9-421d-81e1-22cae78acd46', 2589, 5134, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to pose a question about how to treat additional holders in the propensity-to-buy models of banking products.

Up to now I was only taking into considerations the clients as first holders.
For example, if a client 1 appears as the first holder of a saving account A with a balance at the end of the month of 100 and as an additional holder of a saving account B with a balance at the end of the month of 50, the saving balance at the end of the month for the client is considered to be just 100.
Moreover, if a client only appears as an additional holder (and he/she is not a first account holder of ANY product), he/she is dismissed by the model.

However I have been told to include additional holders in the models (additional holders have the same rights of the first holders).

One possibility is to recalculate all the variables summing up the position as first and additional holder (in the previous example, the balance at the end of the month of client 1 would be 150). Together with this, I would create some variable that represents the maximum degree of intervention of the client in the account (ex. ''first holder'', ''second holder'').

Another possibility would be to double all the variables, considering the client as first and additional holder (in the example, we would create two variables:  the balance at the end of the month as FH =100, :  the balance at the end of the month as AH =50).

Did any of you encounter a similar problem?It would be very helpful to understand how you solved it.


Thanks
', 5265, '2014-12-03 14:18:34.003', 'fb2d9edb-4ffc-45c6-9636-deae9a7baf71', 2590, 5135, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Cross-sell models and additional holders', 5265, '2014-12-03 14:18:34.003', 'fb2d9edb-4ffc-45c6-9636-deae9a7baf71', 2590, 5136, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><dataset><predictive-modeling><logistic-regression><data-cleaning>', 5265, '2014-12-03 14:18:34.003', 'fb2d9edb-4ffc-45c6-9636-deae9a7baf71', 2590, 5137, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to create a model to predict the propensity to buy a certain product. As my proportion of 1''s is very low, I decided to apply oversampling (to get a 10% of 1''s and a 90% of 0''s). Now, I want to discretize some of the variables. To do so I run a tree for each variable against the target. My question is...shall I define the prior probabilities when I do this (run the trees), or it doesn''t matter and I can use the over-sampled dataset just like that?

Thanks.
', 5265, '2014-12-03 14:23:38.830', '3ad20dce-3389-4e87-9012-c36de1fcbc84', 2591, 5138, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Decision trees, categorizacion and oversampling', 5265, '2014-12-03 14:23:38.830', '3ad20dce-3389-4e87-9012-c36de1fcbc84', 2591, 5139, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><predictive-modeling><sampling>', 5265, '2014-12-03 14:23:38.830', '3ad20dce-3389-4e87-9012-c36de1fcbc84', 2591, 5140, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Left you a quick response on SO. The gist is that you can collect a lot of information from electronics shops and manufacturers'' web sites, and lots you can annotate manually. If your goal is to only get training data, that''s all you need:

My answer form the cross-post:
"Having developed a commercial analyzer of this kind, I can tell you that there is no easy solution for this problem. But there are multiple shortcuts, especially if your domain is limited to cameras/electronics.

Firstly, you should look at more sites. Many have product brand annotated in the page (proper html annotations, bold font, all caps in the beginning of the name). Some sites have entire pages with brand selectors for search purposes. This way you can create a pretty good starter dictionary of brand names. Same with product line names and even with models. Alphanumeric models can be extracted in bulk by regular expressions and filtered pretty quickly.

There are plenty of other tricks, but I''ll try to be brief. Just a piece of advice here: there is always a trade-off between manual work and algorithms. Always keep in mind that both approaches can be mixed and both have return-on-invested-time curves, which people tend to forget. If your goal is not to create an automatic algorithm to extract product brands and models, this problem should have limited time budget in your plan. You can realistically create a dictionary of 1000 brands in a day, and for decent performance on known data source of electronic goods (we are not talking Amazon here or are we?) a dictionary of 4000 brands may be all you need for your work. So do the math before you invest weeks into the latest neural network named entity recognizer."', 5249, '2014-12-03 14:35:06.510', '35c470c8-7285-4eea-a4bb-72105ad2ad0b', 2584, 'added 1479 characters in body', 5143, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to write a data-mining service in [Google Go](http://golang.org) which collects data through scraping and APIs.

However as Go lacks good ML support I would like to do the ML stuff in Python.

Having a web background I would connect both services with something like RPC but as I believe that this is a common problem in data science I think that there is some better solution.

For example most (web) protocols lack at:

- buffering between processes
- clustering over multiple instances

**So what (type of libraries) do data scientists use to connect different languages/processes?**

Bodo

', 5266, '2014-12-03 15:56:50.687', 'ab74748d-c8ed-4b46-a88e-f4fc999148d6', 2593, 5144, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to connect data-mining with machine learner process', 5266, '2014-12-03 15:56:50.687', 'ab74748d-c8ed-4b46-a88e-f4fc999148d6', 2593, 5145, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining>', 5266, '2014-12-03 15:56:50.687', 'ab74748d-c8ed-4b46-a88e-f4fc999148d6', 2593, 5146, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For those not familiar, item-item recommenders calculate similarities between items, as opposed to user-user (or user-based) recommenders, which calculate similarities between users. Although some algorithms can be used for both, this question is in regard to item-item algorithms (thanks for being specific in your question).


Accuracy or effectiveness of recommenders is evaluated based on comparing recommendations to a previously collected data set (training set). For example, I have shopping cart data from the last six months; I''ll use the first 5 months as training data, then run my various algorithms, and compare the quality against what really happened during the 6th month.

The reason Mahout ships with so many algorithms is because different algorithms are more or less effective in each data set you may work with. So, ideally, you do some testing as I described with many algorithms and compare the accuracy, then choose the winner.

Interestingly, you can also take other factors into account, such as the need to minimize the data set (for performance reasons), and run your tests only with a certain portion of the training data available. In such a case, one algorithm may work better with the smaller data set, but another may work with the complete set. Then, you get to weigh performance VS accuracy VS challenge of implementation (such as deploying on a Hadoop cluster).

Therefore, different algorithms are suited for different project. However, there are some general rules:

1. All algorithms always do better with unreduced data sets (more data is better).
2. More complex algorithms aren''t necessarily better.

I suggest starting with a simple algorithm and ensuring you have high quality data. If you have additional time, you can implement more complex algorithms and create a comparison which is unique to your data set.

Most of my info comes from [This study][1]. You''ll find lots of detail about implementation there.


  [1]: http://ai.arizona.edu/intranet/papers/comparative.ieeeis.pdf', 3466, '2014-12-03 18:22:29.333', '0527152f-9cbb-414b-b465-2a7ea769d227', 2594, 5147, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''d argue that you are not applying the same tranformation but instead calculate a new one.

IMHO applying the same calculation means:

1. During training, estimate min and max or better offset and scaling factor from the training data.
2. For prediction (and testing), apply the offset and scaling factor obtained during training.

---

Here''s a situation that will make very clear that re-calculating offset and scaling from the prediction data is wrong:
Consider submitting only one single case to the prediction routine. In that case, talking of the range of the data is not even meaningful. Yet the prediction should obviously work for a single case.

Here''s another thought that points into the same direction:
The scaling will be the more unstable the fewer cases enter the calculation. No surprise so far. But re-calculating the scaling offset and factor for the prediction data means that the stability of the predictions would depend on the size of the prediction data set. And that should ring all alarm bells.

---

Note that the same thoughts apply to e.g. doing PCA for feature generation. Also there, you need to apply the rotation as estimated from the trainig data.', 5268, '2014-12-03 18:36:00.880', '3fd804a6-ce19-4e56-bec7-53d7229932b3', 2595, 5148, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":5268,"DisplayName":"cbeleites"}]}', 5268, '2014-12-03 18:37:21.060', 'b645bc83-f3c6-46f9-b2fd-e583bfa415a5', 2595, 'via Vote', 5149, '12');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Within each class, you''ll have distributions of values for the features. That in itself is not a reason for concern.

From a slightly theoretical point of view, you can ask yourself why you should scale your features and why you should scale them in exactly the chosen way.
One reason may be that your particular training algorithm is known to converge faster (better) with values around 0 - 1 than with features which cover other orders of magnitude. In that case, you''re probably fine. My guess is that your SVM is fine: you want to avoid too large numbers because of the inner product, but a max of 1.2 vs. a max of 1.0 won''t make much of a difference.
(OTOH, if you e.g. knew your algorithm to not accept negative values you''d obviously be in trouble. )

The practical question is whether your model performs well for cases that are slightly out of the range covered by training. This I believe can best and possibly only be answered by testing with such cases / inspecting test results for performance drop for cases outside the training domain. It is a valid concern and looking into this would be part of the validation of your model.

Observing differences of the size you describe is IMHO a reason to have a pretty close look at model stability. ', 5268, '2014-12-03 18:57:22.773', '0155a55e-cc67-40ca-9264-7814a892d712', 2595, 'added 62 characters in body', 5150, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":5268,"DisplayName":"cbeleites"}]}', 5268, '2014-12-03 18:57:28.143', '1310faae-ea74-4d87-a9cb-52850dffa171', 2595, 5151, '13');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve been toying with this idea for a while. I think there is probably some method in the text mining literature, but I haven''t come across anything just right...

What is/are some methods for tackling a problem where the number of variables it its self a variable.  This is not a missing data problem, but one where the nature of the problem fundamentally changes. Consider the following example:

Suppose I want to predict who will win a race, a simple multinomial classification problem. I have lots of past data on races, plenty to train on.  Lets further suppose I have observed each contestant run multiple races. The problem however is that the number or racers is variable. Sometimes there are only 2 racers, sometimes there are as many as 100 racers.

One solution might be to train a separate model for each number or racers, resulting in 99 models in this case, using any method I choose.  E.g. I could have 100 random forests.

Another solution might be to include an additional variable called ''number_of_contestants'' and have input field for 100 racers and simply leave them blank when no racer is present.  Intuitively, it seems that this method would have difficulties predicting the outcome of a 100 contestant race if the number of racers follows a Poisson distribution (which I didn''t originally specify in the problem, but I am saying it here).

Thoughts?', 5247, '2014-12-03 21:47:34.907', 'f3b3b01e-4c14-40d2-be83-f56ce6755b68', 2596, 5152, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Method for solving problem with variable number of predictors', 5247, '2014-12-03 21:47:34.907', 'f3b3b01e-4c14-40d2-be83-f56ce6755b68', 2596, 5153, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><classification><statistics><nlp>', 5247, '2014-12-03 21:47:34.907', 'f3b3b01e-4c14-40d2-be83-f56ce6755b68', 2596, 5154, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I don''t see the problem. All you need is a learner to map a bit string as long as the _total number of contestants_, representing the subset who are taking part, to another bit string (with only one bit set) representing the winner, or a ranked list, if you want them all (assuming you have the whole list in your training data). In the latter case you would pose it as a learning-to-rank problem.', 381, '2014-12-04 03:42:48.357', '130a80e9-e981-4466-9070-4ee07e288374', 2597, 5155, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to know how exactly mahout user based and item based recommendation differ from each other.

It defines that

[User-based][1]: Recommend items by finding similar users. This is often harder to scale because of the dynamic nature of users.

[Item-based][2]: Calculate similarity between items and make recommendations. Items usually don''t change much, so this often can be computed off line.

But though there are two kind of recommendation available, what I understand is that both these will take some data model ( say 1,2 or 1,2,.5 as item1,item2,value or user1,user2,value where value is not mandatory) and will perform all calculation as the similarity measure and recommender build-in function we chose and we can run both user/item based recommendation on the same data ( is this a correct assumption ?? ).

So I would like to know how exactly and in which all aspects these two type of algorithm differ.

  [1]: https://mahout.apache.org/users/recommender/userbased-5-minutes.html
  [2]: https://mahout.apache.org/users/recommender/intro-itembased-hadoop.html', 5091, '2014-12-04 05:18:03.720', '8218692c-eb81-4655-919c-ee4a43b1ff5d', 2598, 5156, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Item based and user based recommendation differnce in Mahout', 5091, '2014-12-04 05:18:03.720', '8218692c-eb81-4655-919c-ee4a43b1ff5d', 2598, 5157, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><algorithms><recommendation>', 5091, '2014-12-04 05:18:03.720', '8218692c-eb81-4655-919c-ee4a43b1ff5d', 2598, 5158, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You are correct that both models work on the same data without any problem. Both items operate on a matrix of user-item ratings.

In the user-based approach the algorithm produces a rating for an item `i` by a user `u` by combining the ratings of other users `u''` that are similar to `u`. Similar here means that the two users have a high Pearson correlation or cosine similarity.

In the item-based approach we produce a rating for `i` by `u` by looking at the set of items `i''` that are similar to `i` (in the same sense as above) that `u` has rated and then combines the ratings by `u` of `i''` into a predicted rating  by `u` for `i`.

The item-based approach was invented at Amazon (http://dl.acm.org/citation.cfm?id=642471) to address their scale challenges with user-based filtering. The number of things they sell is much less than the number of users so the item-item similarities can be computed offline and accessed when needed.

', 2724, '2014-12-04 06:08:25.000', 'ab42f7bf-9bf6-4251-ad44-efd2f1e6cf0f', 2599, 5159, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You are correct that both models work on the same data without any problem. Both items operate on a matrix of user-item ratings.

In the user-based approach the algorithm produces a rating for an item `i` by a user `u` by combining the ratings of other users `u''` that are similar to `u`. Similar here means that the two users have a high Pearson correlation or cosine similarity.

In the item-based approach we produce a rating for `i` by `u` by looking at the set of items `i''` that are similar to `i` (in the same sense as above) that `u` has rated and then combines the ratings by `u` of `i''` into a predicted rating  by `u` for `i`.

The item-based approach was invented at Amazon (http://dl.acm.org/citation.cfm?id=642471) to address their scale challenges with user-based filtering. The number of things they sell is much less and much less dynamic than the number of users so the item-item similarities can be computed offline and accessed when needed.

', 2724, '2014-12-04 06:14:47.783', '27c80f35-d896-4cb5-ad92-abcdead2b468', 2599, 'added 22 characters in body', 5160, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am interested in graph problems like 2-color, max-clique, stable sets, etc but the documentation for scipy.optimize.anneal seems to be for ordinary functions. How would one apply this library towards graph formulations?', 5273, '2014-12-04 07:47:32.740', '6f87129d-ad1f-4490-a11d-b768c9a43fd7', 2600, 5161, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How does one feed graph optimization problems into Python''s anneal function in SciPy?', 5273, '2014-12-04 07:47:32.740', '6f87129d-ad1f-4490-a11d-b768c9a43fd7', 2600, 5162, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><graphs><optimization>', 5273, '2014-12-04 07:47:32.740', '6f87129d-ad1f-4490-a11d-b768c9a43fd7', 2600, 5163, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I came across the sann package in R for simulated annealing. The "sann" function uses parameters fn and gr as respectively the function to optimize and to select new points. For something like the max-clique or max-stable set problems, fn would be a summing function, but it''s less clear how one would formulate gr to fix these graph computations. In these cases, how would gr "select"?', 5273, '2014-12-04 08:54:32.863', 'ddb6d496-15ab-4b98-b50b-8d19e40b2b8d', 2601, 5164, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to use "sann" package in R to solve graph problems?', 5273, '2014-12-04 08:54:32.863', 'ddb6d496-15ab-4b98-b50b-8d19e40b2b8d', 2601, 5165, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><graphs>', 5273, '2014-12-04 08:54:32.863', 'ddb6d496-15ab-4b98-b50b-8d19e40b2b8d', 2601, 5166, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You are correct that both models work on the same data without any problem. Both items operate on a matrix of user-item ratings.

In the user-based approach the algorithm produces a rating for an item `i` by a user `u` by combining the ratings of other users `u''` that are similar to `u`. Similar here means that the two users have a high Pearson correlation or cosine similarity or something of the like.

In the item-based approach we produce a rating for `i` by `u` by looking at the set of items `i''` that are similar to `i` (in the same sense as above) that `u` has rated and then combines the ratings by `u` of `i''` into a predicted rating  by `u` for `i`.

The item-based approach was invented at Amazon (http://dl.acm.org/citation.cfm?id=642471) to address their scale challenges with user-based filtering. The number of things they sell is much less and much less dynamic than the number of users so the item-item similarities can be computed offline and accessed when needed.

', 2724, '2014-12-04 12:42:25.033', '8a391c3c-5db3-45c8-95b1-a26365afc33f', 2599, 'added 25 characters in body', 5167, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You are correct that both models work on the same data without any problem. Both items operate on a matrix of user-item ratings.

In the user-based approach the algorithm produces a rating for an item `i` by a user `u` by combining the ratings of other users `u''` that are similar to `u`. Similar here means that the two user''s ratings have a high Pearson correlation or cosine similarity or something similar.

In the item-based approach we produce a rating for `i` by `u` by looking at the set of items `i''` that are similar to `i` (in the same sense as above except now we''d be looking at the ratings that items have received from users) that `u` has rated and then combines the ratings by `u` of `i''` into a predicted rating  by `u` for `i`.

The item-based approach was invented at Amazon (http://dl.acm.org/citation.cfm?id=642471) to address their scale challenges with user-based filtering. The number of things they sell is much less and much less dynamic than the number of users so the item-item similarities can be computed offline and accessed when needed.

', 2724, '2014-12-04 13:02:30.360', 'e687cfb5-3d2a-4789-9029-e5b2ef2b748f', 2599, 'added 83 characters in body', 5168, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Item based recommenders.pdf][1]


  [1]: http://ir.cs.georgetown.edu/cs422/files/DM-ItemRecommenders.pdf

Check out the link given above. It has a good explanation for item and user based recommendation system. Hope this helps you.                                    ', 5043, '2014-12-04 15:12:57.327', '3c1d8d69-17e9-4b56-9075-5cb62a2a52af', 2602, 5169, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Custom Google Search**

You can use the Custom Google Search for datasets:

[Google Custom Search: Datasets](https://www.google.com/cse/publicurl?cx=002720237717066476899:v2wv26idk7m)

It includes 230 sources and meta-sources of datasets, including all mentioned in this question. Please, feel free to exclude .gov and any other websites from results by adding " -.gov" or " -site.com" to the search line. Other Google Search Operators work.

Don''t hesitate to contact me if you have ideas what websites to add.


**IOGDS**

The following service categorizes more than 1,000,000 public datasets:

[IOGDS: International Open Government Dataset Search](http://logd.tw.rpi.edu/node/9903)', 5279, '2014-12-04 18:36:57.947', '722134a9-5d37-46ed-aa0d-70d81f5ea579', 2604, 5171, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**Item Based Algorithm**

    for every item i that u has no preference for yet

      for every item j that u has a preference for

        compute a similarity s between i and j

        add u''s preference for j, weighted by s, to a running average

     return the top items, ranked by weighted average

**User Based Algorithm**

    for every item i that u has no preference for yet

     for every other user v that has a preference for i

       compute a similarity s between u and v

       add v''s preference for i, weighted by s, to a running average

     return the top items, ranked by weighted average

Item vs User based:

1) Recommenders scale with the number of items or users they must deal with, so there are scenarios in which each type can perform better than the other

2) Similarity estimates between items are more likely to converge over time than similarities between users

3) We can compute and cache similarities that converge, which can give item based recommenders a performance advantage

4) Item based recommenders begin with a list of a user''s preferred items and therefore do not need a nearest item neighborhood as user based recommenders do', 5043, '2014-12-04 19:08:33.323', '4fb6ede4-8b33-4cd8-858d-c9a112223194', 2602, 'added 961 characters in body', 5172, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":381,"DisplayName":"Emre"}]}', 381, '2014-12-04 19:44:42.160', 'e419d954-101d-4698-b180-483c60ce0272', 2597, 'via Vote', 5173, '12');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":381,"DisplayName":"Emre"}]}', 381, '2014-12-04 19:44:45.173', 'ad9f25be-8d76-41dd-a1e8-3c29f52556b1', 2597, 5174, '13');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I don''t see the problem. All you need is a learner to map a bit string as long as the _total number of contestants_, representing the subset who are taking part, to another bit string (with only one bit set) representing the winner, or a ranked list, if you want them all (assuming you have the whole list in your training data). In the latter case you would have a learning-to-rank problem.

If the contestant landscape can change it would help to find a vector space embedding for them so you can use the previous embeddings as an initial guess and rank anyone, even hypothetical, given their vector representation. As the number of users increases the embedding should stabilize and retraining should become less costly. The question is how to find the embedding, of course. If you have a lot of training data, you could probably find a randomized one along with the ranking function. If you don''t, you would have to generate the embedding by some algorithm and estimate only the ranking function. I have not faced your problem before so I can''t direct you to a particular paper, but the recent NLP literature should give you some inspiration. I still think it is feasible.', 381, '2014-12-04 19:53:31.667', 'e125b2b9-462d-403b-ae61-0c2c2ee0b53d', 2597, 'added 781 characters in body', 5175, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I don''t see the problem. All you need is a learner to map a bit string as long as the _total number of contestants_, representing the subset who are taking part, to another bit string (with only one bit set) representing the winner, or a ranked list, if you want them all (assuming you have the whole list in your training data). In the latter case you would have a learning-to-rank problem.

If the contestant landscape can change it would help to find a vector space embedding for them so you can use the previous embeddings as an initial guess and rank anyone, even hypothetical, given their vector representation. As the number of users increases the embedding should stabilize and retraining should become less costly. The question is how to find the embedding, of course. If you have a lot of training data, you could probably find a randomized one along with the ranking function. If you don''t, you would have to generate the embedding by some algorithm and estimate only the ranking function. I have not faced your problem before so I can''t direct you to a particular paper, but the recent NLP literature should give you some inspiration, e.g. [this](http://jmlr.org/papers/volume13/shalit12a/shalit12a.pdf). I still think it is feasible.', 381, '2014-12-04 20:11:17.383', '1e03d493-0b6c-47ca-8d37-4e30c75b871e', 2597, 'added 70 characters in body', 5176, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes, see the following links:

http://www.quora.com/In-what-way-can-I-use-the-CrossFoldLearner-class-in-Mahout-v0-8-0-9-to-perform-K-fold-cross-validation

https://mahout.apache.org/users/classification/logistic-regression.html


', 5280, '2014-12-04 20:25:54.433', 'a27f3653-2261-4d71-9562-bc2d3006a394', 2605, 5177, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Classes related to Artificial Intelligence are typically taught in Computer Science departments. Looking at the [IT Project Subjects offered by your university](https://my.feit.uts.edu.au/pages/course/undergraduate/it_project_subjects), I suspect Data Analytics would indeed be more relevant to AI than Internetworking and Applications.

Looking at the courses offered by your department, the following likely involve aspects of AI:

 - Image Processing and Pattern Recognition
 - Intelligent Agents
 - Building Intelligent Agents

For self-directed study in AI, I recommend starting with Russell & Norvig''s essential textbook [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/).

As to what it will take to create a human-like strong AI, I recommend this collection of essays: [The Philosophy of Artificial Intelligence](http://www.amazon.com/Philosophy-Artificial-Intelligence-Oxford-Readings/dp/0198248547)... even though the material is getting a bit out-of-date by now.

Good luck!', 819, '2014-12-04 22:13:30.747', 'bf2a94a1-f5b0-451d-8c94-1e23eb963021', 2606, 5178, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Would it be possible to use Approximate Bayesian computation (ABC)? If you assume a distribution for the number of competitors (e.g. Poisson), select a subset of competitors each iteration and simulate your data using multinomial distributions with probabilities based on competitors'' features, after discarding parameters that don''t match your training data, you should be able to obtain parameters for each competitor (that is, posterior distributions) and generate more races.

This might not work if the number of competitors is so important that it affects the coefficients of features for each competitor.', 4621, '2014-12-04 23:26:15.803', '08a9f460-b257-4024-8742-7b8ced0d3929', 2607, 5179, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My data looks like this:

![enter image description here][1]


  [1]: http://i.stack.imgur.com/9LgwU.png

Why is this error showing up?', 2647, '2014-12-05 00:26:12.983', 'ede2b22c-5be9-408a-b4f5-16efdf747112', 2608, 5180, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Simple Excel Question: VLookup Error', 2647, '2014-12-05 00:26:12.983', 'ede2b22c-5be9-408a-b4f5-16efdf747112', 2608, 5181, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-cleaning>', 2647, '2014-12-05 00:26:12.983', 'ede2b22c-5be9-408a-b4f5-16efdf747112', 2608, 5182, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can anyone suggest any good books to learn hadoop and map reduce basics?

Also something for Spark, and Spark Streaming?

Thanks', 5283, '2014-12-05 05:50:29.903', '2bb13f70-9718-43a3-8017-d01e6404f7b3', 2609, 5183, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Good books for Hadoop, Spark, and Spark Streaming', 5283, '2014-12-05 05:50:29.903', '2bb13f70-9718-43a3-8017-d01e6404f7b3', 2609, 5184, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop>', 5283, '2014-12-05 05:50:29.903', '2bb13f70-9718-43a3-8017-d01e6404f7b3', 2609, 5185, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('for hadoop try [hadoop in action][1]  or [Hadoop: The Definitive Guide][2]


  [1]: http://www.amazon.in/Hadoop-Action-Chuck-Lam/dp/8177228137?tag=googinhydr18418-21
  [2]: http://www.amazon.in/Hadoop-The-Definitive-Guide-White/dp/9350237563?tag=googinhydr18418-21', 5091, '2014-12-05 11:05:15.257', '228a8856-9c16-4a60-b9ff-8432f983ed8e', 2610, 5186, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here''s a good [open source book for spark][1]


  [1]: http://databricks.gitbooks.io/databricks-spark-reference-applications/', 4679, '2014-12-05 11:22:01.423', 'df74a415-45af-4603-81a3-2835183450ae', 2611, 5187, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Although I have seen a few good questions asked about data anonymization, I was wondering if there were answers to this more specific variant.

I am seeking a tool (or to design one) that will anonymize human names from a specific country: particularly first names in unstructured text. Many of the tools that I have seen have considered the wider dimensions of data anonymization; with an equal focus on dates of birth, addresses, etc.

An imperative aspect is that it needs to have near absolute recall. The major pitfalls, as far as I can see, are diminutive variants ("Tommy" instead of "Thomas", "Ben" instead of "Benjamin", etc.) and typos. These two factors prevent a simple regex based on a database of names (based on censuses, etc.)', 5290, '2014-12-05 14:03:58.777', '6e6a3a67-9395-4e3f-841d-d225c02149da', 2612, 5188, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Name Anonymization Software', 5290, '2014-12-05 14:03:58.777', '6e6a3a67-9395-4e3f-841d-d225c02149da', 2612, 5189, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining><anonymization>', 5290, '2014-12-05 14:03:58.777', '6e6a3a67-9395-4e3f-841d-d225c02149da', 2612, 5190, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Ruby together with [Nokogiri](http://www.nokogiri.org/) allows to access HTML and XML documents via XPath and CSS selectors. Here is a [tutorial](http://thaiwood.io/screen-scraping-with-a-saw-a-nokogiri-tutorial-with-examples/).', 223, '2014-12-05 15:28:39.640', 'f8081415-cd1a-4d7b-8532-ca1f81742c9d', 2613, 5191, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I don''t think you really need some special software, but rather to employee existing tools, such as encryption algorithms.

Why not just encrypt the names with any key-based algorithm and store the key securely?

If you didn''t need to be able to recover the names, but just to identify variation to the level of differences in diminutives, then you could simply use hashing rather than encryption.

I''m not sure what environment you want to carry this out it, but any language such as `R` or SQL/NoSQL database could easily carry this out programmatically.', 2723, '2014-12-05 15:29:04.743', 'df537026-5255-426e-ae43-dc8856a985fd', 2614, 5192, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is a common result of imprecise matching, such as with whitespace problems.

For example, if you imagine that the grey shading used in codeblocks on StackExchange represents the whitespaces in your Excel cells, then
  `AK`
is technically the same as

    AK

You can trim and/or use wildcard chars to allow matching to skip spaces. For example:

`=VLOOKUP(CONCATENATE("*",TRIM(A2),"*"), $E$2:$F$5, 2,FALSE)`

', 2723, '2014-12-05 15:31:06.383', '220cd320-fec7-40cd-832a-e59b0bd6958c', 2615, 5193, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''s such an overwhelming amount of literature that with programming, databases, and Big Data I like to stick to the O''reilly series as my go-to source. O''reilly books are extremely popular in the industry and I''ve been very satisfied.

A current version of

 1. [Hadoop: The Definitive Guide][1],
 2. [MapReduce Design Patterns][2], and
 3. [Learning Spark][3]

might suit your needs by providing high quality, immediately useful information and avoiding information overload -- all are published by O''reilly.

Spark Streaming is covered in Chapter 13 of "Learning Spark".


  [1]: http://shop.oreilly.com/product/0636920021773.do
  [2]: http://shop.oreilly.com/product/0636920025122.do
  [3]: http://shop.oreilly.com/product/0636920028512.do', 2723, '2014-12-05 15:38:09.867', '878f2b94-0117-486b-9648-0593c4ffe4de', 2616, 5194, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In terms of open source NLG components, I''m most familiar with Mumble and FUF/SURGE. They''ve got both similarities and differences, so it''s hard to say which is better...

[Mumble](https://code.google.com/p/sparser/source/browse/#svn%2Ftrunk%2FMumble):

 - written in Lisp
 - [EPL license](http://www.eclipse.org/legal/epl-v10.html)
 - based on tree-adjoining grammar
 - focuses on linguistic message planning

[FUF/SURGE](http://www.cs.bgu.ac.il/surge/index.html):

 - written in Lisp
 - GPL license
 - based on functional unification grammar
 - focuses on syntactic realization

Since it sounds like you''re interested in abstractive summarization (which is much harder than traditional extractive summarization), I''d recommend the following academic papers:

 - [Text Generation for Abstractive Summarization](http://www.nist.gov/tac/publications/2010/participant.papers/Rali.proceedings.pdf)
 - [Framework for Abstractive Summarization using Text-to-Text Generation](http://www.aclweb.org/anthology/W11-1608)
 - [Towards a Framework for Abstractive Summarization of Multimodal Documents](http://dl.acm.org/citation.cfm?id=2000990) -- full disclosure: I''m the author of this one

Also, consider checking out this textbook to get started: [Building Natural Language Generation Systems](http://www.amazon.com/Building-Natural-Language-Generation-Processing/dp/0521620368)', 819, '2014-12-05 15:43:40.720', '0411e64d-7061-49ef-8f25-bb3fdd74a72d', 2617, 5195, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Few things in life give me pleasure like scraping structured and unstructured data from the Internet and making use of it in my models.

For instance, the Data Science Toolkit (or `RDSTK` for R programmers) allows me to pull lots of good location-based data using IP''s or addresses and the `tm.webmining.plugin` for R''s `tm` package makes scraping financial and news data straightfoward. When going beyond such (semi-) structured data I tend to use `XPath`.

However, I''m constantly getting throttled by limits on the number of queries you''re allowed to make. I think Google limits me to about 50,000 requests per 24 hours, which is a problem for Big Data.

From a *technical* perspective getting around these limits is easy -- just switch IP addresses and purge other identifiers from your environment. However, this presents both ethical and financial concerns (I think?).

Is there a solution that I''m overlooking?

', 2723, '2014-12-05 15:51:54.690', '558aa1aa-546d-4896-beb2-7a1331aee88a', 2618, 5198, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Ethically and Cost-effectively Scaling Data Scrapes', 2723, '2014-12-05 15:51:54.690', '558aa1aa-546d-4896-beb2-7a1331aee88a', 2618, 5199, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining>', 2723, '2014-12-05 15:51:54.690', '558aa1aa-546d-4896-beb2-7a1331aee88a', 2618, 5200, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m not sure I fully understand your question, but it seems to me that you''re trying to determine the category of the string/entity "titanic" out of context. Your data tells you that "titanic" could be a book, a movie, or a product, and you want to figure out which one is correct -- is that what you''re trying to do?

If so, the problem is that you''ve dropped the context in which the string/entity "titanic" appears in your original text. For example...

 - In the sentence "I couldn''t stop reading Titanic," the word "titanic" refers to a book.
 - In the sentence "Titanic was one of the highest-grossing films of all time," the word "titanic" refers to a movie.
 - In the sentence "The Titanic was the world''s largest ocean liner," the word "titanic" refers to a product.

Without that context, there''s no way to know which is the correct category. I''d suggest looking into how named entity recognition tools like [Stanford NER](http://nlp.stanford.edu/software/CRF-NER.shtml) work -- that will help you better understand how to do something like this. You''ll see that the input to an NER tool generally needs to be a sentence, in order to take advantage of the context to properly categorize the extracted entities.', 819, '2014-12-05 15:57:07.737', 'f203d7d4-0ddf-4641-a53b-e226027fedf2', 2619, 5201, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From an older version of the [OpenNLP README](http://opennlp.sourceforge.net/README.html):

> **Training the Tools**
>
> There are training tools for all components expect the coref component. Please consult the help message of the tool and the javadoc to figure out how to train the tools.
>
> The tutorials in our wiki might also be helpful.
>
> The following modules currently support training via the WordFreak opennlp.plugin v1.4 (http://wordfreak.sourceforge.net/plugins.html).
>
> * coreference: org.annotation.opennlp.OpenNlpCoreferenceAnnotator (use opennlp 1.4.3 for training, models are compatible)
>
> **Note:** In order to train a model you need all the training data. There is not currently a mechanism to update the models distributed with the project with additional data.

As you can see, OpenNLP does not provide training tools for the coreference component. However, it seems at one point it was possible to train new models for OpenNLP''s coref component using the third-party WordFreak plugin... however, it hasn''t been updated in over a decade, so your mileage may vary.', 819, '2014-12-05 16:14:01.097', 'f050638f-11fc-4a07-a567-7326bd2b894a', 2620, 5202, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First, some clarification on terminology.

A *package* in R is a collection of R functions, data, and compiled code in a well-defined format.

SANN (`sann`) is not a package. Depending on which package you''re using, `sann` is either a function or, more often, a *method* used within an optimization function.

Packages containing `sann` include `optim`, `trustOptim`, `consPlan`, and `constrOptim`.

In the package `optim`, the `sann` method is implemented as:

    > func <- function(x){
    +  out <- (x[1]-2)^2 + (x[2]-1)^2
    +  return <- out
    +  }>
    > optim(par=c(0,0), fn=func, gr = NULL,
    +       method = c("SANN"),
    +       lower = -Inf, upper = Inf,
    +       control = list(), hessian = T)

As you said, for the "SANN" (`sann`) method `gr` is used to generate a new candidate point. If it is NULL a default Gaussian Markov kernel is used.

Now in your use case -- the case of a graph -- what you probably want to do is to [use `par` and `value` to pass values to `fn` and `gr`][1]. This is a nice feature of this implementation of SANN in `optim` which is covered a little more than half way  through this [documentation page][2].

`par` is the best set of parameters found and `value` is the value of `fn` corresponding to `par`.

  [1]: http://www.inside-r.org/r-doc/graphics/par
  [2]: http://www.inside-r.org/r-doc/stats/optim', 2723, '2014-12-05 16:35:42.910', '24ac5086-efc8-4f08-9f10-b2ab5a92f257', 2621, 5203, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The `Data Science Toolkit` is [a powerful library][1] (or collection of libraries, technically) which are available in a number of languages. For instance, I use the implementation called `RDSTK` in R.

In the case of your preferred language, Google Go, there''s [a list][2] of web-related libraries here which looks very useful.


  [1]: http://www.datasciencetoolkit.org/
  [2]: https://code.google.com/p/go-wiki/wiki/Projects#Web_Libraries', 2723, '2014-12-05 16:37:54.327', 'e07e22df-2072-40c2-83ea-250396edcb47', 2622, 5204, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The `mnlogit` package in R allows for the fast estimation of multinomial logit models.

The specification of forumlas is a bit different from most other regression models/packages in R, however.

Using the `Fish` dataset as a reproducible example,

    > require(mnlogit)
    Loading required package: mnlogit

    Package: mnlogit
    Version: 1.1.1
    Multinomial Logit Choice Models.
    Scientific Computing Group, Sentrana Inc, 2013.

    > data(Fish, package =''mnlogit'')
    > head(Fish)
               mode   income     alt   price  catch chid
    1.beach   FALSE 7083.332   beach 157.930 0.0678    1
    1.boat    FALSE 7083.332    boat 157.930 0.2601    1
    1.charter  TRUE 7083.332 charter 182.930 0.5391    1
    1.pier    FALSE 7083.332    pier 157.930 0.0503    1
    2.beach   FALSE 1250.000   beach  15.114 0.1049    2
    2.boat    FALSE 1250.000    boat  10.534 0.1574    2

I''m trying to understand the difference between the model specification of

    fm <- formula(mode ~ 0 + price | income | catch)

and

    fm <- formula(mode ~ 0 + price | income + catch)

while the documentation covers the detail of such changes  in the general coeffcient area of the forumla (i.e. where `price` is), I don''t see an explanation of how operators like `+` affect the alternative-specific area of the formula/code, relative to `|`.', 2723, '2014-12-05 16:54:13.967', '22a8e713-8b5e-4ce9-8683-4b67847e1d3d', 2623, 5205, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to Interpret Multinomial Specification in R''s `mnlogit` package', 2723, '2014-12-05 16:54:13.967', '22a8e713-8b5e-4ce9-8683-4b67847e1d3d', 2623, 5206, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><logistic-regression><regression>', 2723, '2014-12-05 16:54:13.967', '22a8e713-8b5e-4ce9-8683-4b67847e1d3d', 2623, 5207, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A bit of shameless self promotion here but I wrote a python package specifically for handling this problem.

https://github.com/rawkintrevo/angemilner

https://pypi.python.org/pypi/angemilner/0.2.0

It requires a mongodb daemon and basically you make a page for each one of your keys. So you have 4 email addresses each with a separate key assigned. When you load the key in you specify the maximum calls per day and minimum time between uses.

Load keys:

    l.new_api_key("your_assigned_key1", ''noaa'', 1000, .2)
    l.new_api_key("your_assigned_key2", ''noaa'', 1000, .2)

Then when you run your scraper for instance the NOAA api:

    url= ''http://www.ncdc.noaa.gov/cdo-web/api/v2/stations''
    payload= { ''limit'': 1000,
    ''datasetid'':  ''GHCND'',
   ''startdate'': ''1999-01-01'' }

    r = requests.get(url, params=payload, headers= {''token'': ''your_assigned_key''})

becomes:

    url= ''http://www.ncdc.noaa.gov/cdo-web/api/v2/stations''
    payload= { ''limit'': 1000,
       ''datasetid'':  ''GHCND'',
       ''startdate'': ''1999-01-01'' }

    r = requests.get(url, params=payload, headers= {''token'': l.check_out_api_key(''noaa'')[''key'']})

so if you have 5 keys, `l.check_out_api_key` returns the key that has the least uses and waits until enough time has elapsed for it to be used again.

Finally to see how often your keys have been used / remaining useage available:

    pprint(l.summary())


I didn''t write this for R because most scraping is done in python (most of MY scraping).  It could be easily ported.

Thats how you can *technically* get around rate limiting.  *Ethically* ...', 5247, '2014-12-05 17:13:14.073', '18800fb7-7c87-442d-877e-9da54673cab3', 2624, 5208, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For many APIs (most I''ve seen) ratelimiting is a function of your API Key or OAuth credentials. (Google, Twitter, NOAA, Yahoo, Facebook, etc.) The good news is you won''t need to spoof your IP, you just need to swap out credentials as they hit there rate limit.

A bit of shameless self promotion here but I wrote a python package specifically for handling this problem.

https://github.com/rawkintrevo/angemilner

https://pypi.python.org/pypi/angemilner/0.2.0

It requires a mongodb daemon and basically you make a page for each one of your keys. So you have 4 email addresses each with a separate key assigned. When you load the key in you specify the maximum calls per day and minimum time between uses.

Load keys:

    l.new_api_key("your_assigned_key1", ''noaa'', 1000, .2)
    l.new_api_key("your_assigned_key2", ''noaa'', 1000, .2)

Then when you run your scraper for instance the NOAA api:

    url= ''http://www.ncdc.noaa.gov/cdo-web/api/v2/stations''
    payload= { ''limit'': 1000,
    ''datasetid'':  ''GHCND'',
   ''startdate'': ''1999-01-01'' }

    r = requests.get(url, params=payload, headers= {''token'': ''your_assigned_key''})

becomes:

    url= ''http://www.ncdc.noaa.gov/cdo-web/api/v2/stations''
    payload= { ''limit'': 1000,
       ''datasetid'':  ''GHCND'',
       ''startdate'': ''1999-01-01'' }

    r = requests.get(url, params=payload, headers= {''token'': l.check_out_api_key(''noaa'')[''key'']})

so if you have 5 keys, `l.check_out_api_key` returns the key that has the least uses and waits until enough time has elapsed for it to be used again.

Finally to see how often your keys have been used / remaining useage available:

    pprint(l.summary())


I didn''t write this for R because most scraping is done in python (most of MY scraping).  It could be easily ported.

Thats how you can *technically* get around rate limiting.  *Ethically* ...', 5247, '2014-12-05 17:21:55.873', 'f9944b61-004b-4748-ba7a-8a41e7aa5213', 2624, 'added 265 characters in body', 5209, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For many APIs (most I''ve seen) ratelimiting is a function of your API Key or OAuth credentials. (Google, Twitter, NOAA, Yahoo, Facebook, etc.) The good news is you won''t need to spoof your IP, you just need to swap out credentials as they hit there rate limit.

A bit of shameless self promotion here but I wrote a python package specifically for handling this problem.

https://github.com/rawkintrevo/angemilner

https://pypi.python.org/pypi/angemilner/0.2.0

It requires a mongodb daemon and basically you make a page for each one of your keys. So you have 4 email addresses each with a separate key assigned. When you load the key in you specify the maximum calls per day and minimum time between uses.

Load keys:

    from angemilner import APIKeyLibrarian
    l= APIKeyLibrarian()
    l.new_api_key("your_assigned_key1", ''noaa'', 1000, .2)
    l.new_api_key("your_assigned_key2", ''noaa'', 1000, .2)

Then when you run your scraper for instance the NOAA api:

    url= ''http://www.ncdc.noaa.gov/cdo-web/api/v2/stations''
    payload= { ''limit'': 1000,
    ''datasetid'':  ''GHCND'',
   ''startdate'': ''1999-01-01'' }

    r = requests.get(url, params=payload, headers= {''token'': ''your_assigned_key''})

becomes:

    url= ''http://www.ncdc.noaa.gov/cdo-web/api/v2/stations''
    payload= { ''limit'': 1000,
       ''datasetid'':  ''GHCND'',
       ''startdate'': ''1999-01-01'' }

    r = requests.get(url, params=payload, headers= {''token'': l.check_out_api_key(''noaa'')[''key'']})

so if you have 5 keys, `l.check_out_api_key` returns the key that has the least uses and waits until enough time has elapsed for it to be used again.

Finally to see how often your keys have been used / remaining useage available:

    pprint(l.summary())


I didn''t write this for R because most scraping is done in python (most of MY scraping).  It could be easily ported.

Thats how you can *technically* get around rate limiting.  *Ethically* ...
', 5247, '2014-12-05 17:52:33.167', '3a65337f-521f-4076-862e-f490f733e6c9', 2624, 'updated code a bit for clarity', 5210, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If your only motivation for using Google Go is webscraping, and you want to do you ML in python, I would recommend the following stack:

[Python requests for scraping data][1]

[MongoDB for caching data][2]

[pymongo for interfacing python and mongodb][3]

[scikit-learn for doing your machine learning][4]

This all happens in python and you can extend it multiple processors with
[multiprocessing][5] or to multiple nodes with django


  [1]: http://docs.python-requests.org/en/latest/
  [2]: http://www.mongodb.org/
  [3]: http://api.mongodb.org/python/current/
  [4]: http://scikit-learn.org/stable/
  [5]: https://docs.python.org/2/library/multiprocessing.html', 5247, '2014-12-05 18:09:27.670', '17f29b2e-a41d-471e-844a-354474effd46', 2625, 5211, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('If your only motivation for using Google Go is webscraping, and you want to do you ML in python, I would recommend the following stack:

[Python requests for scraping data][1]

[MongoDB for caching data][2] (MongoDB''s page oriented format makes it a natural home for storing JSON objects commonly returned by APIs)

[pymongo for interfacing python and mongodb][3]

[scikit-learn for doing your machine learning][4]

This all happens in python and you can extend it multiple processors with
[multiprocessing][5] or to multiple nodes with django


  [1]: http://docs.python-requests.org/en/latest/
  [2]: http://www.mongodb.org/
  [3]: http://api.mongodb.org/python/current/
  [4]: http://scikit-learn.org/stable/
  [5]: https://docs.python.org/2/library/multiprocessing.html', 5247, '2014-12-05 18:43:57.927', '36c39dad-537c-49a2-b5f3-866d400e00ac', 2625, 'added bit about why I recommend MongoDB ', 5212, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For many APIs (most I''ve seen) ratelimiting is a function of your API Key or OAuth credentials. (Google, Twitter, NOAA, Yahoo, Facebook, etc.) The good news is you won''t need to spoof your IP, you just need to swap out credentials as they hit there rate limit.

A bit of shameless self promotion here but I wrote a python package specifically for handling this problem.

https://github.com/rawkintrevo/angemilner

https://pypi.python.org/pypi/angemilner/0.2.0

It requires a mongodb daemon and basically you make a page for each one of your keys. So you have 4 email addresses each with a separate key assigned. When you load the key in you specify the maximum calls per day and minimum time between uses.

Load keys:

    from angemilner import APIKeyLibrarian
    l= APIKeyLibrarian()
    l.new_api_key("your_assigned_key1", ''noaa'', 1000, .2)
    l.new_api_key("your_assigned_key2", ''noaa'', 1000, .2)

Then when you run your scraper for instance the NOAA api:

    url= ''http://www.ncdc.noaa.gov/cdo-web/api/v2/stations''
    payload= { ''limit'': 1000,
    ''datasetid'':  ''GHCND'',
   ''startdate'': ''1999-01-01'' }

    r = requests.get(url, params=payload, headers= {''token'': ''your_assigned_key''})

becomes:

    url= ''http://www.ncdc.noaa.gov/cdo-web/api/v2/stations''
    payload= { ''limit'': 1000,
       ''datasetid'':  ''GHCND'',
       ''startdate'': ''1999-01-01'' }

    r = requests.get(url, params=payload, headers= {''token'': l.check_out_api_key(''noaa'')[''key'']})

so if you have 5 keys, `l.check_out_api_key` returns the key that has the least uses and waits until enough time has elapsed for it to be used again.

Finally to see how often your keys have been used / remaining useage available:

    pprint(l.summary())


I didn''t write this for R because most scraping is done in python (most of MY scraping).  It could be easily ported.

Thats how you can *technically* get around rate limiting.  *Ethically* ...

**UPDATE** The example uses Google Places API [here][1]


  [1]: https://github.com/rawkintrevo/angemilner/blob/master/example2.py', 5247, '2014-12-05 18:53:24.863', '34e44f17-e5b2-4c54-87d3-a6803ec7a8ee', 2624, 'added 135 characters in body', 5213, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you are willing to pay for a vendor solution, [Teradata](http://www.teradata.com/) is designed to solve the problem of large scale joins with low latency.', 1330, '2014-12-05 20:13:17.793', '0c4830fb-6faf-4d7a-96f6-121c2e4d519f', 2626, 5214, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I came across a package in R which has a function called `sann` for simulated annealing.

`sann` uses parameters `fn` and `gr` to optimize and to select new points, respectively.

For something like the max-clique or max-stable set problems, `fn` would be a summing function, but it''s less clear how one would formulate `gr` to fix these graph computations. In these cases, how would `gr` "select"?', 2723, '2014-12-05 23:04:05.373', '362bcde1-2baa-4863-bb8e-599834293dc4', 2601, 'Realized he could''ve been using several different packages -- optimx, consPlan, optim, etc -- and updated the question as such', 5215, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to use "sann" function in R to solve graph problems?', 2723, '2014-12-05 23:04:05.373', '362bcde1-2baa-4863-bb8e-599834293dc4', 2601, 'Realized he could''ve been using several different packages -- optimx, consPlan, optim, etc -- and updated the question as such', 5216, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-12-05 23:04:05.373', '362bcde1-2baa-4863-bb8e-599834293dc4', 2601, 'Proposed by 2723 approved by 21 edit id of 190', 5217, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to create 3D bars on this map. Can anyone please advise if this is possible, and how?

http://leafletjs.com/examples/choropleth.html

My data: UFO sightings in the USA (location wise).
Count of these sightings per location will be the height of the 3D bar.
Base map is a choropleth with US population density values.

I don''t mind integrating Javascript or d3.js into the code to create the 3D bars.', 2647, '2014-12-06 00:41:24.933', '36f2b5c4-0fdb-48ab-b2be-a19a44acf05f', 2627, 5219, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('3D map using leaflet', 2647, '2014-12-06 00:41:24.933', '36f2b5c4-0fdb-48ab-b2be-a19a44acf05f', 2627, 5220, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<javascript><data-visualization>', 2647, '2014-12-06 00:41:24.933', '36f2b5c4-0fdb-48ab-b2be-a19a44acf05f', 2627, 5221, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using SAS Studio (online, student version)...
Need to do a "nested likelihood ratio test" for a logistic regression.
Entirety of instructions are: "Perform a nested likelihood ratio test comparing your full model (all predictors included)to a reduced model of interest."
The two models I have are:

    Proc Logistic Data=Project_C;
    Model Dem (event=''1'') = VEP TIF Income NonCit Unemployed Swing;
    Run;
and

    Proc Logistic Data=Project_C;
    Model Dem (Event=''1'') = VEP TIF Income / clodds=Wald clparm=Wald expb rsquare;
    Run;

I honestly have no idea where to even start.
Any suggestions would be appreciated.
Thanks!', 5298, '2014-12-06 01:10:30.817', '13cd4c4a-2158-407c-8c95-4ce5e05d7798', 2628, 5222, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('SAS Nested Likelihood Ratio Test for a Logistic Model', 5298, '2014-12-06 01:10:30.817', '13cd4c4a-2158-407c-8c95-4ce5e05d7798', 2628, 5223, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<logistic-regression>', 5298, '2014-12-06 01:10:30.817', '13cd4c4a-2158-407c-8c95-4ce5e05d7798', 2628, 5224, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In recent years, the term "data" seems to have become a term widely used without specific definition. Everyone seems to use the phrase. Even people as technology-impaired as my grandparents use the term and seem to understand words like "data breach." But I don''t understand what makes "data science" a new discipline. Data has been the foundation of science for centuries. Without data, there would be no Mendel, no Schrödinger, etc. You can''t have science without interpreting  and analyzing data.

But clearly it means something. Everyone is talking about it.  So what exactly do people mean by data when they use terms like "big data" and why has this become a discipline in itself? Also, if it is an emerging discipline, where can I find more serious/in-depth information so I can better educate myself?

Thanks!', 5301, '2014-12-06 06:53:14.617', 'b26588ce-eb62-43ba-a6a0-6b4c09c6fe51', 2629, 5225, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is "data science"?', 5301, '2014-12-06 06:53:14.617', 'b26588ce-eb62-43ba-a6a0-6b4c09c6fe51', 2629, 5226, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><definitions>', 5301, '2014-12-06 06:53:14.617', 'b26588ce-eb62-43ba-a6a0-6b4c09c6fe51', 2629, 5227, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I get asked this question all the time, so earlier this year I wrote an article ([**What is Data Science?**](http://oss4ds.com/what-is-data-science.html)) based on a presentation I''ve given a few times. Here''s the gist...

First, a few definitions of data science offered by others:

[**Josh Wills** from **Cloudera** says](https://twitter.com/josh_wills/status/198093512149958656) a data scientist is someone "who is better at statistics than any software engineer and better at software engineering than any statistician."

[A frequently-heard **joke**](https://twitter.com/nivertech/status/180109930139893761) is that a "Data Scientist" is a Data Analyst who lives in California.

[According to **Big Data Borat**](https://twitter.com/BigDataBorat/status/372350993255518208), Data Science is statistics on a Mac.

In [**Drew Conway''s** famous Data Science Venn Diagram](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram), it''s the intersection of Hacking Skills, Math & Statistics Knowledge, and Substantive Expertise.

Here''s another good definition I found on the [**ITProPortal** blog](http://www.itproportal.com/2014/02/11/how-to-pick-a-data-scientist-the-right-way/):
> "A data scientist is someone who understands the domains of programming, machine learning, data mining, statistics, and hacking"

Here''s how we define Data Science at **Altamira** (my current employer):

![data science diagram][1]

The bottom four rows are the **table stakes** -- the cost of admission just to play the game. These are foundational skills that all aspiring data scientists must obtain. Every data scientist must be a **competent programmer**. He or she must also have a solid grasp of math, statistics, and **analytic methodology**. Data science and "**big data**" go hand-in-hand, so all data scientists need to be familiar with frameworks for distributed computing. Finally, data scientists must have a basic understanding of the domains in which they operate, as well as excellent communications skills and the ability to **tell a good story with data**.

With these basics covered, the next step is to develop **deep expertise** in one or more of the vertical areas. "Data Science" is really an umbrella term for a collection of interrelated techniques and approaches taken from a variety of disciplines, including mathematics, statistics, computer science, and software engineering. The goal of these diverse methods is to **extract actionable intelligence** from data of all kinds, enabling clients to make better **data-driven decisions**. No one person can ever possibly master all aspects of data science; doing so would require multiple lifetimes of training and experience. The best data scientists are therefore "**T-shaped**" individuals -- that is, they possess a breadth of knowledge across all areas of data science, along with deep expertise in at least one. Accordingly, the best **data science teams** bring together a set of individuals with complementary skillsets spanning the **entire spectrum**.

  [1]: http://i.stack.imgur.com/YAUZy.png', 819, '2014-12-06 13:02:19.373', '6037119c-e439-4185-9dc2-5407334a9939', 2630, 5228, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-12-06 13:29:40.867', '89691dfc-9e6b-4d93-a721-5bf0d0f1de85', 2629, '105', 5229, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-12-06 13:29:54.687', 'b989cc4b-6614-4ee8-81c7-a8563adc0beb', 2609, '105', 5230, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For simplicity let''s assume the feature space is the XY plane.', 5306, '2014-12-06 15:04:03.823', 'adc20b3f-09d8-4214-95d9-6cee608a6ca0', 2631, 5231, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What would be a good way to use clustering for outlier detection?', 5306, '2014-12-06 15:04:03.823', 'adc20b3f-09d8-4214-95d9-6cee608a6ca0', 2631, 5232, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><clustering>', 5306, '2014-12-06 15:04:03.823', 'adc20b3f-09d8-4214-95d9-6cee608a6ca0', 2631, 5233, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am running a test on mapReduce algorithm in different environments, like hadoop and mongoDB using different types of data. What are the different methods or techniques to find out the execution time of a query.
If i am inserting a huge amount of data, consider it to be 2-3GB, what are the methods to find out the time for the process to be completed.', 5310, '2014-12-06 17:56:53.157', '2199bf94-94e9-465e-9929-16f2bbb8b447', 2632, 5234, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('timing sequence in mapreduce', 5310, '2014-12-06 17:56:53.157', '2199bf94-94e9-465e-9929-16f2bbb8b447', 2632, 5235, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<map-reduce>', 5310, '2014-12-06 17:56:53.157', '2199bf94-94e9-465e-9929-16f2bbb8b447', 2632, 5236, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to set up a cluster (1 namenode, 1datanode) on AWS.
I''m using free one year trail period of AWS, but the challenge is, instance is created with 1GB of RAM.

As I''m a student I cannot afford much. Can anyone please suggest me?

Also, It would be great if you could provide any links for setting up multi cluster hadoop with spark on AWS.

Thanks in Advance.

note: I cannot try in GCE as my trail period is exhausted. ', 5172, '2014-12-07 04:40:53.677', 'f2143e10-e01a-4211-95c4-63ba40947c65', 2633, 5237, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can hadoop with Spark be configured with 1GB RAM', 5172, '2014-12-07 04:40:53.677', 'f2143e10-e01a-4211-95c4-63ba40947c65', 2633, 5238, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop><aws>', 5172, '2014-12-07 04:40:53.677', 'f2143e10-e01a-4211-95c4-63ba40947c65', 2633, 5239, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Having:

- a set of soft fuzzy classifiers (classification onto overlapping sets) C_i(x) -> [0,1]
- a corresponding set of weak estimators R_i(z) of the form R_i(z) = EX(y|z).

The estimators R_i are just some kind of regression, kalman or particle filters. The classifiers C_i are fixed and static.

How to make a strong estimator out of a weighted combination of the form L(x, z) = SUM_i: C_i(x)R_i(z)Q_i ?

In other words how to choose the weights Q_i?

Is there some kind of online approach to this problem?

Brief description of a practical application:

When an event E is registered, multiple measurements are made. Based on these measurements, the classifiers C_i make a soft assignment of the event to multiple overlapping categories. What we get is fit ratios for the soft clusters.

Now there is some chance that event E may trigger a subsequent event D, depending on another variable z - independent from the event E. We know that all the soft cluster "memberships" may influence the probability of event D being triggered.

We want to estimate the probability that E triggers D, given the C_i fitness ratios and value of z.', 5314, '2014-12-07 10:43:01.697', 'f3ce5336-8922-451d-8b45-80e3c99d9d13', 2634, 5240, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Linear combination of weak estimators over fuzzy classifiers?', 5314, '2014-12-07 10:43:01.697', 'f3ce5336-8922-451d-8b45-80e3c99d9d13', 2634, 5241, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><regression><online-learning>', 5314, '2014-12-07 10:43:01.697', 'f3ce5336-8922-451d-8b45-80e3c99d9d13', 2634, 5242, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am not 100% if a message queue library will be the right tool for this job but so far it looks to me so.

With a messaging library like:

- [nsq](http://nsq.io)
- [zeromq](http://zeromq.org)
- [mqtt](http://mqtt.org) (?)

You can connect different processes operating on different environment through a TCP based protocol. As these systems run distributed it is possible to connect multiple nodes.

For **nsq** we even have a library in Python **and** Go!', 5266, '2014-12-07 11:29:14.057', '55406790-fc31-4a5c-840f-ef226b5dbdb6', 2635, 5243, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Perhaps you could cluster the items, then those items with the furthest distance from the midpoint of any cluster would be candidates for outliers.', 819, '2014-12-07 12:16:10.207', 'b7bcba3e-3503-4d73-a82b-9cabc17dfa3a', 2636, 5244, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><hadoop><nosql><aws>', 5172, '2014-12-07 13:00:19.960', 'a4183f5d-8ae0-4313-8afc-f8eb7164d5d1', 2633, 'edited tags', 5245, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('So if 4GB of RAM isn''t sufficient, 1GB isn''t going to be. That is really too little to run an HDFS namenode, a datanode, YARN, Spark driver alone, let alone leaving room for your workers.

Much more reasonable is to simply run Spark locally on that instance without Hadoop at all.

But I would question whether Spark is the right choice if you are definitely limited to such a small machine.', 21, '2014-12-07 13:37:40.373', 'd902e61f-9cfc-435b-8976-3136256a5c05', 2637, 5246, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have implemented Q-Learning as described in,

http://web.cs.swarthmore.edu/~meeden/cs81/s12/papers/MarkStevePaper.pdf

In order to approx. Q(S,A) I use a neural network structure like the following,

 - Activation sigmoid
 - Inputs, number of inputs + 1 for Action neurons (All Inputs Scaled 0-1)
 - Outputs, single output. Q-Value
 - N number of M Hidden Layers.
 - Exploration method random 0 < rand() < propExplore

At each learning iteration using the following formula,

![enter image description here][1]

I calculate a Q-Target value then calculate an error using,

    error = QTarget - LastQValueReturnedFromNN

and back propagate the error through the neural network.

Q1, Am I on the right track? I have seen some papers that implement a NN with one output neuron for each action.

Q2, My reward function returns a number between -1 and 1. Is it ok to return a number between -1 and 1 when the activation function is sigmoid (0 1)

Q3, From my understanding of this method given enough training instances it should be quarantined to find an optimal policy wight? When training for XOR sometimes it learns it after 2k iterations sometimes it won''t learn even after 40k 50k iterations.

[1]: http://i.stack.imgur.com/e3hgc.png', 5318, '2014-12-07 15:05:31.213', '51a5cb3e-8227-490f-9ec3-787d102100de', 2638, 5247, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Questions about Q-Learning using Neural Networks', 5318, '2014-12-07 15:05:31.213', '51a5cb3e-8227-490f-9ec3-787d102100de', 2638, 5248, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork>', 5318, '2014-12-07 15:05:31.213', '51a5cb3e-8227-490f-9ec3-787d102100de', 2638, 5249, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You might find a solution for this by checking out Viola & Jones face detection algorithm (and object detection in general) http://www.cs.ubc.ca/~lowe/425/slides/13-ViolaJones.pdf. Particularly the AdaBoost algorithm for building a strong classifier from weak classifiers. https://www.cs.princeton.edu/~schapire/papers/explaining-adaboost.pdf

In this algorithm this is used particularly for feature selection, where it is wanted to select just the features that grouped together classify better, discarding the other as noisy features.

The approach to obtain this strong classifier is having a set of examples X, a set of weights W and a set of expected results (classifications) Y. For a two-class example (e.g. face o no face image) the first weak classifier is selected and then the classification error is found between this classifier and the expected output. For the samples that were misclassified their weights are incremented, so the next weak classifier to find is more biased to neglect this wrong classification.

The algorithm (AdaBoost) converges when the sign of the sum of classifications for each sample outputs the correct classification.

For example:

    Y = +1 -1 +1 +1

    WC1=+1 +1 +1 +1
    WC2=-1 -1 -1 -1
    WC3=+1 -1 +1 +1

So the strong classifier is WC1+WC2+WC3:

    SC=+1 -1 +1 +1 == Y

Hope this solves your question.


', 5143, '2014-12-07 15:39:00.947', '77bd0fc5-8680-4e8f-98d6-da4f42f5c257', 2639, 5250, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A very robust clustering algorithm against outliers is PFCM from Bezdek http://www.comp.ita.br/~forster/CC-222/material/fuzzyclust/fuzzy01492404.pdf.

In this paper Bezdek proposes Possibilistic-Fuzzy-C-Means which is an improvement of the different variations of fuzzy posibilistic clustering. This algorithm is particularly good at detecting outliers and avoiding them to influence the clusterization. So using PFCM you could find which points are identified as outliers and at the same time have a very robust fuzzy clustering of your data.', 5143, '2014-12-07 15:46:40.670', '369fe71d-09db-4267-b28c-3d8ccee066fc', 2640, 5251, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Following link contains a list of positive and negative polarised emotions on the scale of [-5, 5]. Just try to count up the scores based on the word matches and you can get overall movie review score.

http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010', 1131, '2014-12-07 16:17:20.613', 'b6d69196-62c5-4b07-8bc1-5aa2fae57364', 2641, 5252, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are there any algorithms which were developed using partial differential equations for tackling some of the machine learning problems? Most works I see online are in the field of computer vision and a few bizarre ones in topic modelling. But just curious if someone has used or seen it being used for some decision making process or classification problems?', 1131, '2014-12-07 16:24:43.997', 'f2c53cac-5d16-45d3-88d8-8561cc9a743b', 2642, 5253, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine Learning & Partial Differential Equations', 1131, '2014-12-07 16:24:43.997', 'f2c53cac-5d16-45d3-88d8-8561cc9a743b', 2642, 5254, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><algorithms>', 1131, '2014-12-07 16:24:43.997', 'f2c53cac-5d16-45d3-88d8-8561cc9a743b', 2642, 5255, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m new to apache spark.

Is it possible to configure multi cluster spark without hadoop?

If so, can you please provide the steps.
I would like to create clusters on Google Compute Engine  (1-master, 1-worker)', 5172, '2014-12-07 16:31:57.913', 'd6a5e93c-28dd-4c6f-b3d1-daaf50cdf04a', 2643, 5256, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to set up multi cluster spark without hadoop on Google Compute engine', 5172, '2014-12-07 16:31:57.913', 'd6a5e93c-28dd-4c6f-b3d1-daaf50cdf04a', 2643, 5257, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop><scala>', 5172, '2014-12-07 16:31:57.913', 'd6a5e93c-28dd-4c6f-b3d1-daaf50cdf04a', 2643, 5258, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What would you put on a CTQ or Voice of Customer diagram for the following hypothetical scenario?

Sinclair Corporation
THE COMPANY
Sinclair Corporation is based in Traverse City, MI and
acquired Premium Photo Products seven months ago.
Premium makes several types of film. The Premium
1000 series is made with a high quality, supersensitive
emulsion. It is nationally marketed in roll form for
amateurs and sheet and film-pack forms for
professionals.
Roll film is similar to most film commonly used by the
general public.
Sheet film is handled in total darkness, negatives being
withdrawn from between interleaf papers by the
photographer and inserted into a light-tight holder
which is then placed in the camera.
Film-pack is a self-contained unit ready to install in the
camera. The photographer merely pulls a paper tab to
shift the exposed negative out of position and a new
one into position.
The procedure for using and developing all of this film
has not changed since introduction. Premium 1000 is
produced in small lots at the Denver plant. After
packaging, the film is shipped to dealers across the
country for display and sale; a remarkably stable film
with a shelf life of eighteen months, Premium 1000
has an excellent reputation in the trade.
THE SITUATION
During the last 48 hours Sales, Manufacturing and
Research have been working to resolve this sudden and
serious problem with the Premium 1000 film.
Customer and dealer complaints about fogged sheet
film are mounting. This crisis now requires immediate
action before Premium 1000 has to be removed from
the market entirely.
During the past ten days, dealers in several sales
districts have had customer complaints of fogged sheet
film. In some cases, dealers have accumulated
complaints for several days before they reported them.
Complaints, mounting every day, have since spread to
all sales districts in the country. Right now there are
complaints from 682 of our 7000 dealers. Although
Sales is replacing each box of fogged film with two new
boxes, this has not placated anyone. Two commercial
studios, for example, threaten total cancellation of
further use of our products because our film, which
they used to photograph unique events, was fogged and
the pictures consequently ruined.
Sales has been very concerned by several
manufacturing problems which have resulted in failures
to meet delivery requirements. For example: a serious
fire two months ago in the Premium 1000 film
emulsion department resulted in the discharge of
several employees for carelessness and the intensive
training of the new emulsion mixers; delivery quantity
difficulties on interleaf paper for over a month after the
acquisition; delay in arrival of the new bulk raw
ingredient storage containers. Though the interleaf
delivery difficulties cleared up by acquiring a new
suppler, all of the new bulk storage containers have not
arrived and so the old ones are still being used.
Sales of sheet film, as well as film-pack, have quickly
dropped and some professionals have begun using
competitive brands. So far, replacing the fogged film
and Premiums good reputation has kept most dealers
from defecting.
Sales found that all Premium 1000 film is shipped,
distributed, stocked and sold by dealers the same way.
They also found that all complaints so far are coming
from small outlets where turnover is very slow, but they
expect to hear complaints from the larger outlets soon.
None of the film was outdated and the fogging was
seen as soon as it was developed rather than later.
Sales asks that we stop production for a couple of days
until Manufacturing thoroughly checks out their
operation (i.e., effects of the fire, effectiveness of
emulsion mixer training, and possible contamination by
the new storage containers). The expense involved in
doing this would be about $30,000.
Manufacturing believes the new zip-top sheet film
package is responsible. Because of substantial
customer complaints that our package was hard to
open, Sales rushed the introduction of the easier to
open zip-top package five weeks ago. There was no Page 2 of 2
time for thorough testing. The problem started since
then. Manufacturing recommends going back to the
former package. It was used for three years without
difficulty. This will gain time to further investigate the
zip-top package. They urge an immediate hold on
expansion of the zip-top package to other Premium
1000 film. This proposal would require a commitment
of only $20,000, but could invite an added source of
known customer dissatisfaction at this critical time.
Research has conducted tests on sample returns. Their
findings are below.
THE LAB REPORT
Memo from the Research Director (delivered 4 days
after the first reports started coming in)
We have studied the samples of returned fogged film
provided by Sales and have attempted to reproduce the
fogging exactly as it appears on the returns.
We analyzed fifty returned boxes of film and found two
things they all had in common: all negatives in a box are
fogged; fogging is even across the entire surface of each
negative.
We then took thirty-six samples of good sheet film from
the plant warehouse and carefully subjected these to a
variety of tests. The only way we were able to
reproduce exactly the unique fogging effect was to
expose the sheet film to a source of radiation (such as
an x-ray machine).
Recognizing that this type of film is supersensitive and is
more susceptible to contamination, and having clearly
demonstrated that radiation produces the exact
characteristics of the returned fogged film, we believe
that a source of radioactivity is the root of the problem.
We know that traces of radioactive fallout have
contaminated various products in the past. With the
current high incidence of Strontium 90 in th', 5328, '2014-12-08 08:15:51.563', 'e38ec8c3-0b46-4f95-b7ef-22dc6c06f65a', 2644, 5259, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('CTQ diagram for a corporation, problem solving, six sigma, lean manufacturing question', 5328, '2014-12-08 08:15:51.563', 'e38ec8c3-0b46-4f95-b7ef-22dc6c06f65a', 2644, 5260, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><dataset>', 5328, '2014-12-08 08:15:51.563', 'e38ec8c3-0b46-4f95-b7ef-22dc6c06f65a', 2644, 5261, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Say I used spectral clustering to cluster a data-set D of points X0 - Xn into a number C of clusters.

How can I efficiently assign a new single point Xn+1 to his convenient cluster, Do I have to do the classification from the beginning (destroy all the clusters and apply the algorithm to the data-set X0 - Xn+1) or is there an optimized way to extend to the point Xn+1.

Thanks in advance.', 5332, '2014-12-08 10:47:52.340', '5dd44128-eb5d-4033-a0b6-fead0605ebe8', 2645, 5262, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Assign new point to a class using spectral clustering', 5332, '2014-12-08 10:47:52.340', '5dd44128-eb5d-4033-a0b6-fead0605ebe8', 2645, 5263, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><clustering>', 5332, '2014-12-08 10:47:52.340', '5dd44128-eb5d-4033-a0b6-fead0605ebe8', 2645, 5264, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In this [wiki page](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF) there is a function `corr()` that calculates the Pearson coefficient of correlation, but my question is that: is there any function in Hive that enables to calculate the [Kendall coefficient][1] of correlation of a pair of a numeric columns in the group?


  [1]: http://en.wikipedia.org/wiki/Kendall%27s_W', 21, '2014-12-08 14:22:40.260', 'a180e33c-8737-471c-b797-65e7e3f99f31', 2575, 'added 60 characters in body; edited tags', 5265, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<hadoop><correlation><hive>', 21, '2014-12-08 14:22:40.260', 'a180e33c-8737-471c-b797-65e7e3f99f31', 2575, 'added 60 characters in body; edited tags', 5266, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are there any articles or discussions about extracting part of text that holds the most of information about current document.

For example, I have a large corpus of documents from the same domain. There are parts of text that hold the key information what single document talks about. I want to extract some of those parts and use them as kind of a summary of the text. Is there any useful documentation about how to achieve something like this.

It would be really helpful if someone could point me into the right direction what I should search for or read to get some insight in work that might have already been done in this field of Natural language processing.', 2750, '2014-12-08 14:51:27.613', 'ecf98bba-9cc5-4786-9b78-413f7df84156', 2646, 5267, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Extract most informative parts of text from documents', 2750, '2014-12-08 14:51:27.613', 'ecf98bba-9cc5-4786-9b78-413f7df84156', 2646, 5268, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><text-mining>', 2750, '2014-12-08 14:51:27.613', 'ecf98bba-9cc5-4786-9b78-413f7df84156', 2646, 5269, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What you''re describing is often achieved using a simple combination of [TF-IDF](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) and [extractive summarization](http://en.wikipedia.org/wiki/Automatic_summarization#Extraction-based_summarization).

In a nutshell, TF-IDF tells you the relative importance of each word in each document, in comparison to the rest of your corpus. At this point, you have a score for each word in each document approximating its "importance." Then you can use these individual word scores to compute a composite score for each sentence by summing the scores of each word in each sentence. Finally, simply take the top-N scoring sentences from each document as its summary.

Earlier this year, I put together an iPython Notebook that culminates with an implementation of this in Python using NLTK and Scikit-learn: [A Smattering of NLP in Python](https://github.com/charlieg/A-Smattering-of-NLP-in-Python).', 819, '2014-12-08 15:48:25.647', 'f0c4ad5b-7881-402b-8975-921d1cb05e97', 2648, 5273, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think you do not, since I was able to find several papers that proposed algorithms for the same:

* [Incremental spectral clustering by efficiently updating the eigen-system](http://www.ifp.illinois.edu/~hning2/papers/incremental_spectral_clustering.pdf)
* [Incremental kernel spectral clustering for online learning of non-stationary data](http://www.sciencedirect.com/science/article/pii/S0925231214004433)
* [A Fast Incremental Spectral Clustering for Large Data Sets](http://dx.doi.org/10.1109/PDCAT.2011.4)

Such algorithms are called _sequential_, _incremental_, _streaming_, or _online_. Armed with this knowledge, you can find more papers on your own.', 381, '2014-12-08 20:55:55.677', '2eeca5ae-093a-44e5-be12-f7f93c238dc6', 2649, 5275, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Neil is correct. There are partial derivatives evwrywhere in gradient computation for machine learning models training.', 5342, '2014-12-08 22:32:05.817', '9c7facc2-dee6-40b5-a5cf-45777bac85f5', 2650, 5276, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am looking for a paper detailing the very basics of deep learning. Ideally like the Andrew Ng course for deep learning. Do you know where I can find this ?', 5342, '2014-12-08 22:37:32.777', '09b67a7e-0f30-4669-9c16-7c18af416542', 2651, 5277, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Deep learning basics', 5342, '2014-12-08 22:37:32.777', '09b67a7e-0f30-4669-9c16-7c18af416542', 2651, 5278, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><deep-learning>', 5342, '2014-12-08 22:37:32.777', '09b67a7e-0f30-4669-9c16-7c18af416542', 2651, 5279, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<br>
Im traying to integrate Hadoop and R, I was install the pachages rJava and Rhipe in R, I do this steps to start Hadoop and R:<br>
-starting Hadoop services.,<br>
-loading rJava and Rhipe packages by library function.<br>
-Calling rhinit() to initialize Rhipe.<br>
the problem here is when I call rhinit() funtion, it show this error:<br>

>>Initializing Rhipe v0.73<br>
Error in .jnew("org/godhuli/rhipe/PersonalServer") :
 java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream''

please some helps to fixe this problem.

', 4705, '2014-12-09 00:58:03.057', '3d32d186-6498-4a82-a47a-a0b0c9822676', 2652, 5280, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream', 4705, '2014-12-09 00:58:03.057', '3d32d186-6498-4a82-a47a-a0b0c9822676', 2652, 5281, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><hadoop>', 4705, '2014-12-09 00:58:03.057', '3d32d186-6498-4a82-a47a-a0b0c9822676', 2652, 5282, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The subject is new so most of the wisdom is scattered in papers, but here are two recent books:

* [Deep Learning](http://www.iro.umontreal.ca/~bengioy/dlbook/), Yoshua Bengio, Ian J. Goodfellow, Aaron Courville.
* [Deep Learning: Methods and Applications](http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf), Li Deng and Dong Yu.

And some practical material: http://deeplearning.net/tutorial/', 381, '2014-12-09 01:19:32.290', '7dd1930c-1aa5-42fc-9e2b-e8f36d5178c5', 2653, 5283, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m wondering if there is a web framework well suited for placing recommendations on content.

In most cases, a data scientist goes through after the fact and builds (or uses) a completely different tool to create recommendations. This involves analyzing traffic logs, a history of shopping cart data, ratings, and so forth. It usually comes from multiples sources (the web server, the application''s database, Google Analytics, etc) and then has to be cleaned up and processed, THEN delivered back to the application in way it understands.

Is there a web framework on the market which handles collecting this data up front, as to minimize the retrospective data wrangling?', 3466, '2014-12-09 02:28:51.430', 'fb03df80-14f8-4e37-a3b0-2af63146fedc', 2654, 5284, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Web Framework Built for Recommendations', 3466, '2014-12-09 02:28:51.430', 'fb03df80-14f8-4e37-a3b0-2af63146fedc', 2654, 5285, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<predictive-modeling><data-cleaning>', 3466, '2014-12-09 02:28:51.430', 'fb03df80-14f8-4e37-a3b0-2af63146fedc', 2654, 5286, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Neural Networks and Deep Learning by Michael Nielsen. The book is still in progress, but it looks quite interesting and promising. And it''s free! Here''s the link: http://neuralnetworksanddeeplearning.com/

There are only 5 chapters so far, and the most of them talk about usual neural networks, but it''s still worth having a look. ', 816, '2014-12-09 08:26:57.997', '58979203-0bba-4ab6-97b7-6072634ac568', 2655, 5288, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Found it myself.

- Go to context menu right clicking to the dimension field.
- Go to **Aliases...** and change the labels.

![enter image description here][1]


  [1]: http://i.stack.imgur.com/qKq73.png', 97, '2014-12-09 08:30:28.000', '5d5934d7-b9a7-494f-9c2d-2b344e1acf87', 2656, 5289, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a huge file of customer complaints about the products my company owns and I would like to do a data analysis on those descriptions and tag a category to each of them.

For example: I need to figure out the number of complaints on ***Software*** and ***Hardware*** side of my product from the customer complaints. Currently, I am using excel to do the data analysis which do seek a significant amount of manual work to get a tag name to the complaints.

Is there a way in NLP to build and train a model to automate this process? I have been reading stuffs about NLP for the past couple of days and it looks like NLP has a lot of good features to get a head start in addressing this issue. Could someone please guide me with the way I should use NLP to address this issue?

  ', 5043, '2014-12-09 20:49:37.093', 'b9a575fa-e11b-41d4-8a89-d3fbcf2f25b5', 2658, 5293, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using NLP to automate the categorization of user description', 5043, '2014-12-09 20:49:37.093', 'b9a575fa-e11b-41d4-8a89-d3fbcf2f25b5', 2658, 5294, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><classification><nlp><categorical-data>', 5043, '2014-12-09 20:49:37.093', 'b9a575fa-e11b-41d4-8a89-d3fbcf2f25b5', 2658, 5295, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was looking at K-means in Matlab today and something struck me as odd or - to be more precise - something that I was unable to find information about namely the following situation:

Imagine that we pick K-means for some dataset (I want to remain as general as possible thus K will not be specified here). As you know K-means goes through multiple steps of refinement and assignment until it has accomplished the goal of having each observation assigned to a cluster (aka a neighbourhood) defined by a centroid.

What happens if in a neighbourhood with some centroid that defines a cluster we have more observations that **should be** (as in fulfill the distance to the centroid requirement) in that neighbourhood than the number K? Which will be left out and why? Also is this a problem that occurs often or is it something of an anomaly?

Thanks!', 5356, '2014-12-09 21:13:10.797', '67a060f7-7b7b-4057-b8b1-3bfc33835b00', 2659, 5296, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('K-means - what happens if more than K observation have the same distance to the centroid of the cluster', 5356, '2014-12-09 21:13:10.797', '67a060f7-7b7b-4057-b8b1-3bfc33835b00', 2659, 5297, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering><k-means>', 5356, '2014-12-09 21:13:10.797', '67a060f7-7b7b-4057-b8b1-3bfc33835b00', 2659, 5298, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One way to handle this is to use ''supervised classification''. In this model, you manually classify a subset of the data and use it to train your algorithm. Then, you feed the remaining data into your software to classify it.

This is accomplished with NLTK for Python (nltk.org).

If you are simply looking for strings like "hardware" and "software", this is a simple use case, and you will likely get decent results using a ''feature extractor'', which informs your classifier which phrases in the document are relevant.

While it''s possible to implement an automated method for finding the keywords, it sounds like you have a list in mind already, so you can skip that step and just use the tags you are aware of. (If your results aren''t satisfactory the first time, this is something you might try later on).

That''s an overview for getting started. If you are unhappy with the initial results, you can refine your classifier by introducing more complex methods, such as sentence segmentation, identification of dialogue act types, and decision trees. The sky is the limit (or more likely, your time is the limit)!

More info at:

http://www.nltk.org/book/ch06.html

', 3466, '2014-12-09 21:19:23.747', '17e1a305-f9fb-4de8-ba79-c229917fcf3b', 2660, 5299, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('K-means does not make an assumption regarding how many observations **should be** assigned to each cluster. `K` is simply the number of clusters one chooses to generate. During each iteration, each observation is assigned to the cluster having the nearest mean. There is no assumption that all clusters should have a comparable number of observations assigned (i.e., for `N` observations, there is no expectation that each cluster should have ~ `N/K` observations assigned).

It is quite possible that the numbers of observations in the various clusters are highly imbalanced. This can be due to the distribution of the data, the number of clusters chosen (`K`), or even how the cluster means are initialized.', 964, '2014-12-09 21:33:31.200', 'fc4a83f2-808f-4735-a70a-c9b2418b07f8', 2661, 5300, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You are mixing up kNN classification and k-means.

There is nothing wrong with having more than k observations near a center in k-means. In fact, this it the usual case; you shouldn''t choose k too large. If you have 1 million points, a k of 100 may be okay. *K-means does not guarantee clusters of a particular size*. Worst case, clusters in k-means can have only one element (outliers) or even disappear.

What you probably meant to write, but got mixed up, is **what to do if a point is at the same distance to two centers**.

From a statistical point of view, it doesn''t matter. Both have the same squared error.

From an implementation point of view, choose any *deterministic* rule, so that your algorithm converges and doesn''t go into an infinite loop of reassignment.', 924, '2014-12-09 22:32:22.660', 'e412af2c-d644-40f5-b282-1785a0e8ec1d', 2662, 5301, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Gaussian mixture modeling can - if your data is nicely gaussian-like - be used for outlier detection. Points with a low density in *every* cluster are likely to be outliers.

Works well in idealistic scenarios.', 924, '2014-12-09 22:33:38.640', '9d4a21d1-5797-4193-8d79-a9fcfc428a77', 2663, 5302, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I haven''t seen anything like that and very much doubt that such frameworks exist, at least, as complete frameworks. The reason for this is IMHO the fact that data *transformation* and *cleaning* is very **domain-** and **project-specific**. Having said that, there are multiple tools that can help with these activities in terms of *partial automation* and *integration* with and between existing statistical and Web frameworks.

For example, for **Python**, the use of *data manipulation* library `pandas` as well as *machine learning* library `scikit-learn` can be easily integrated with Web frameworks (especially Python-based, but not necessarily), as these libraries are also Python-based. These and other Python data science tools that might be of interest can be found here: http://pydata.org/downloads. Specifically, for cleaning and pre-processing tasks, which you asked about, `pandas` seem to be the first tool to explore. Again, for Python, the following discussion on StackOverflow on methods and approaches might be helpful: http://stackoverflow.com/q/14262433/2872891.

Consider an example of **another platform**. The use of `pandas` for data transformation and cleaning is rather *low-level*. The platform that I like very much and currently use as the platform of choice for data science tasks is `R`. Rich *ecosystem of R packages* especially shines in the area of data transformation and cleaning. This is because, in addition to very flexible low-level methods of performing these tasks, there are some R packages, which take a *higher-level approach* to the problem, which may potentially improve developer''s productivity and decrease the amount of defects. In particular, I''m talking about two packages, which I find very promising: `editrules` and `deducorrect`. You can find more detailed information about these and other R packages for data transformation and cleaning in my another answer here on *Data Science StackExchange* (paper that I reference in the last link there could be especially useful, as it presents an *approach* to data transformation and cleaning that is *generic* enough, so that could be used as a **framework** for this on any decent platform): http://datascience.stackexchange.com/a/722/2452.', 2452, '2014-12-10 01:59:48.700', '045bc26f-0e69-4f29-8fec-48357edd2365', 2664, 5303, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I haven''t seen anything like that and very much doubt that such frameworks exist, at least, as complete frameworks. The reason for this is IMHO the fact that data *transformation* and *cleaning* is very **domain-** and **project-specific**. Having said that, there are multiple tools that can help with these activities in terms of *partial automation* and *integration* with and between existing statistical and Web frameworks.

For example, for **Python**, the use of *data manipulation* library `pandas` as well as *machine learning* library `scikit-learn` can be easily integrated with Web frameworks (especially Python-based, but not necessarily), as these libraries are also Python-based. These and other Python data science tools that might be of interest can be found here: http://pydata.org/downloads. Specifically, for cleaning and pre-processing tasks, which you asked about, `pandas` seem to be the first tool to explore. Again, for Python, the following discussion on StackOverflow on methods and approaches might be helpful: http://stackoverflow.com/q/14262433/2872891.

Consider an example of **another platform**. The use of `pandas` for data transformation and cleaning is rather *low-level*. The platform that I like very much and currently use as the platform of choice for data science tasks is `R`. Rich *ecosystem of R packages* especially shines in the area of data transformation and cleaning. This is because, in addition to very flexible low-level methods of performing these tasks, there are some R packages, which take a *higher-level approach* to the problem, which may potentially improve developer''s productivity and decrease the amount of defects. In particular, I''m talking about two packages, which I find very promising: `editrules` and `deducorrect`. You can find more detailed information about these and other R packages for data transformation and cleaning in my another answer here on *Data Science StackExchange* (paper that I reference in the last link there could be especially useful, as it presents an *approach* to data transformation and cleaning that is *generic* enough, so that could be used as a **framework** for this on any decent platform): http://datascience.stackexchange.com/a/722/2452.

**UPDATE**: On the topic of *recommender systems* and their integration with data wrangling tools and Web frameworks, you may find my other answer here on DS SE useful: http://datascience.stackexchange.com/a/836/2452.', 2452, '2014-12-10 02:05:33.430', 'b8d1738d-c0f1-4b2a-9e5c-dd75de5da82f', 2664, 'Added link to my answer on recommender systems.', 5304, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The subject is new so most of the wisdom is scattered in papers, but here are two recent books:

* [Deep Learning](http://www.iro.umontreal.ca/~bengioy/dlbook/), Yoshua Bengio, Ian J. Goodfellow, Aaron Courville.
* [Deep Learning: Methods and Applications](http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf), Li Deng and Dong Yu.

And some practical material: http://deeplearning.net/tutorial/

*  ACL 2012 + NAACL 2013 Tutorial: [Deep Learning for NLP (without Magic)](http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial)', 381, '2014-12-10 04:17:13.757', '1a573bd2-1dea-46d3-8672-b3dfc01f1a0f', 2653, 'http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial', 5309, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('
How can I connect to TitanDB from Python ?

Is there any good documentation/tutorial available ?

', 5091, '2014-12-10 04:33:31.350', 'a660a591-fe7d-4e2a-957b-c3721502c6e2', 2667, 5310, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Python interface to TitanDB', 5091, '2014-12-10 04:33:31.350', 'a660a591-fe7d-4e2a-957b-c3721502c6e2', 2667, 5311, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><databases>', 5091, '2014-12-10 04:33:31.350', 'a660a591-fe7d-4e2a-957b-c3721502c6e2', 2667, 5312, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How can I connect to Titan database from Python ?

Is there any good documentation/tutorial available ?

', 5091, '2014-12-10 05:35:18.940', 'e8ce6515-5a78-4a96-b821-ee72b5e17978', 2667, 'deleted 2 characters in body; edited title', 5313, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Python interface to Titan Database', 5091, '2014-12-10 05:35:18.940', 'e8ce6515-5a78-4a96-b821-ee72b5e17978', 2667, 'deleted 2 characters in body; edited title', 5314, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I know that Spark is fully integrated with Scala. It''s use case is specifically for large data sets. Which other tools have good Scala support? Is Scala best suited for larger data sets? Or is it also suited for smaller data sets? ', 3466, '2014-12-10 06:37:17.193', 'b49b323b-2b14-4e17-a5e2-a8c1748da86a', 2668, 5315, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data Science Tools Using Scala', 3466, '2014-12-10 06:37:17.193', 'b49b323b-2b14-4e17-a5e2-a8c1748da86a', 2668, 5316, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<scalability><scala>', 3466, '2014-12-10 06:37:17.193', 'b49b323b-2b14-4e17-a5e2-a8c1748da86a', 2668, 5317, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It is important to see all the rules if one of the states of target column is more important then others. For example, if you are predicting fraudulent transactions, you might want to flag something as fraud even if is has 5% probability.', 5366, '2014-12-10 07:12:33.640', 'a979d187-6f30-4069-b724-24bd1f79768c', 2669, 5318, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to find an equivalent of Hinton Diagrams for multilayer networks to plot the weights during training.

The trained network is somewhat similar to a Deep SRN, i.e. it has a high number of multiple weight matrices which would make the simultaneous plot of several Hinton Diagrams visually confusing.

Does anyone know of a good way to visualize the weight update process for recurrent networks with multiple layers?

I haven''t found much papers on the topic. I was thinking to display time-related information on the weights per layer instead if I can''t come up with something. E.g. the weight-delta over time for each layer (omitting the use of every single connection). PCA is another possibility, though I''d like to not produce much additional computations, since the visualization is done online during training.', 5316, '2014-12-10 10:15:00.940', 'f058d5e7-ee82-493d-9ecf-467d994e8423', 2670, 5321, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Visualizing deep neural network training', 5316, '2014-12-10 10:15:00.940', 'f058d5e7-ee82-493d-9ecf-467d994e8423', 2670, 5322, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork><visualization><deep-learning>', 5316, '2014-12-10 10:15:00.940', 'f058d5e7-ee82-493d-9ecf-467d994e8423', 2670, 5323, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**EDIT** It was pointed out in the Answers-section that I am confusing k-means and kNN. Indeed I was thinking about kNN but wrote k-means since I''m still new to this topic and confuse the terms quite often. So here is the changed question:

I was looking at kNN today and something struck me as odd or - to be more precise - something that I was unable to find information about namely the following situation:

Imagine that we pick kNN for some dataset (I want to remain as general as possible thus K will not be specified here). As you know kNN goes through multiple steps of refinement and assignment until it has accomplished the goal of having each observation assigned to a cluster (aka a neighbourhood) defined by a centroid.

What happens if in a neighbourhood with some centroid that defines a cluster we have more observations that **should be** (as in fulfill the distance to the centroid requirement) in that neighbourhood than the number K? Which will be left out and why? Also is this a problem that occurs often or is it something of an anomaly?

Thanks!', 5356, '2014-12-10 12:28:20.713', 'ff412178-018d-4413-a6ad-f9e1111d5204', 2659, 'Incorrectly asked question!', 5324, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('kNN - what happens if more than K observation have the same distance to the centroid of the cluster', 5356, '2014-12-10 12:28:20.713', 'ff412178-018d-4413-a6ad-f9e1111d5204', 2659, 'Incorrectly asked question!', 5325, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<clustering>', 5356, '2014-12-10 12:28:20.713', 'ff412178-018d-4413-a6ad-f9e1111d5204', 2659, 'Incorrectly asked question!', 5326, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Since you''ve updated your question to refer to a different algorithm (changed k-means to kNN), I''m adding this as a separate answer specifically for kNN.]

It appears you may still be confusing `kNN` with `k-means`. `kNN` does not have multiple stages of refinement, nor does it require computing centroids. It is a [lazy learner](http://en.wikipedia.org/wiki/Lazy_learning) that classifies a new observation by examining the `k` training observations that are closest to the new observation and picks whichever class is most prevalent among those `k` training observations. Note that the distances are *not* relative to a cluster centroid. You don''t have to worry about there being more than `k` training observations in the neighborhood of the new observation because the neighborhood isn''t based on a threshold distance - it is defined simply by the `k` points nearest to the new observation (i.e., it changes for each new observation being evaluated).

A possible pathological case is when multiple training observations lie at *exactly* the same distance from the new observation, which would require you to evaluate more than `k` neighbors. But you would still simply pick the class that is most prevalent among that group of observations.', 964, '2014-12-10 13:54:43.770', '05b8e945-aaea-408b-9e0b-1279ddcc1115', 2671, 5327, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m currently going into the world of machine learning and Neural Networks, thanks to [synaptic (js)][1] that interests me a lot.

So I read a lot, wikipedia links and [synaptic''s NN 101][2], but there''s a lot of basics questions that I don''t understand (but I''d like to) in the use of machine learning (NN) and the point of these technologies.

Let''s say, I wan''t my network to (kind of) learn (something like) gravity, so to train it I set in input 10 objects with a mass, and a position x, y (and z) and I set output the new x, y (and z) of each objects.
I guess I should give it several configurations and everything but **here is the question; can it, then, be able to compute the interactions between 10000, 100000 objects?**

At this stage in my learning, what I don''t clearly get is what is the point of teaching/training neurons to compute XOR like it''s shown in [synaptic''s documentation][3]:

    var trainingSet = [
        {
            input: [0,0],
            output: [0]
        },
        {
            input: [0,1],
            output: [1]
        },
        {
            input: [1,0],
            output: [1]
        },
        {
            input: [1,1],
            output: [0]
        },
    ];

    var trainer = new Trainer(myNetwork);
    trainer.train(trainingSet);

Were we just give it all the possible inputs and outputs to a XOR.


Well, as I''m all new to the technologies I think my questions are full of non-sense and everything but thanks for reading and help you might bring :)



  [1]: https://github.com/cazala/synaptic
  [2]: https://github.com/cazala/synaptic/wiki/Neural-Networks-101
  [3]: https://github.com/cazala/synaptic#documentation', 5369, '2014-12-10 14:18:36.933', '6d079cf6-ead5-4df7-a073-1509b5d4bf71', 2673, 5329, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Training Neural Networks with unknown length of input', 5369, '2014-12-10 14:18:36.933', '6d079cf6-ead5-4df7-a073-1509b5d4bf71', 2673, 5330, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork>', 5369, '2014-12-10 14:18:36.933', '6d079cf6-ead5-4df7-a073-1509b5d4bf71', 2673, 5331, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Sheldon is correct, this sounds like a fairly typical use case for supervised classification. If all of your customer complaints are either software or hardware (i.e., zero individual complaints cover both categories, and zero are outside these two classes), then all you need is a binary classifier, which makes things simpler than they otherwise could be.

If you''re looking for a Java-based NLP toolkit that supports something like this, you should check out the Stanford Classifier: http://nlp.stanford.edu/software/classifier.shtml -- it''s licensed as open source software under the GPL.

Their wiki page should help you get started using the classifier: http://www-nlp.stanford.edu/wiki/Software/Classifier -- keep in mind that you''ll need to manually annotate a large sample of your data as a training set, as Sheldon mentioned.', 819, '2014-12-10 15:37:41.853', 'd19929e6-d8d9-4b0c-88c5-15c61407aa59', 2675, 5335, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Gravity**

Basically you''d like to find a function that maps an input (an object with a mass and a location) to an output (new location). It''s not necessary to have one set of input neurons for each different object. It''s sufficient to encode the input variables generically for all objects.

Like a function  $f( x, y, z, mass ) -> ( x_n, y_n, z_n )$

**XOR**

Training the XOR function is special as the early neural networks, the perceptrons, were unable to learn the XOR function (because it is not linear separable). Multi-Layer Perceptrons however are able to learn the XOR function. This is just to demonstrate that the NN implementation of Synaptic is capable of learning problems that are not linear separable
', 3132, '2014-12-10 15:41:03.987', '0cb2f172-4f71-463d-bdff-deb4f94f6bb9', 2676, 5336, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**Gravity**

Basically you''d like to find a function that maps an input (an object with a mass and a location) to an output (new location). It''s not necessary to have one set of input neurons for each different object. It''s sufficient to encode the input variables generically for all objects.

Like a function

    f( x, y, z, mass ) -> ( x_n, y_n, z_n )

**XOR**

Training the XOR function is special as the early neural networks, the perceptrons, were unable to learn the XOR function (because it is not linear separable). Multi-Layer Perceptrons however are able to learn the XOR function. This is just to demonstrate that the NN implementation of Synaptic is capable of learning problems that are not linear separable
', 3132, '2014-12-10 15:58:35.137', 'a625a2e1-01f0-4f76-8246-e932ae802924', 2676, 'added 3 characters in body', 5337, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a document classification project where I am getting site content and then assigning one of numerous labels to the website according to content.

I found out that [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) could be very useful for this. However, I was unsure as to *when* exactly to use it.

Assumming a website that is concerned with a specific topic makes repeated mention of it, this was my current process:

1. Retrieve site content, parse for plain text
2. Normalize and stem content
3. Tokenize into unigrams (maybe bigrams too)
4. Retrieve a count of each unigram for the given document, filtering low length and low occurrence words
5. Train a classifier such as NaiveBayes on the resulting set

My question is the following: **Where would tf-idf fit in here**? Before normalizing/stemming? After normalizing but before tokenizing? After tokenizing?

Any insight would be greatly appreciated.', 5199, '2014-12-10 16:08:03.537', '72acf9a5-c7be-476a-8ede-389c227b0e52', 2677, 5338, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Document classification: tf-idf prior to or after feature filtering?', 5199, '2014-12-10 16:08:03.537', '72acf9a5-c7be-476a-8ede-389c227b0e52', 2677, 5339, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><feature-selection><feature-extraction>', 5199, '2014-12-10 16:08:03.537', '72acf9a5-c7be-476a-8ede-389c227b0e52', 2677, 5340, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**EDIT** It was pointed out in the Answers-section that I am confusing k-means and kNN. Indeed I was thinking about kNN but wrote k-means since I''m still new to this topic and confuse the terms quite often. So here is the changed question:

I was looking at kNN today and something struck me as odd or - to be more precise - something that I was unable to find information about namely the following situation:

Imagine that we pick kNN for some dataset (I want to remain as general as possible thus K will not be specified here). Further we select at some point an observation where the number of neighbours that fulfill the requirement to be in the neighbourhood of the selected observation are actually more than the specified K (for example for O1,O2,O3,...,ON where O=obervation, N>K all have the exact same distance (cityblock, euclidean or something else that is used)). What criterion/criteria should be applied here if we are restricted to use the specific K and thus cannot alter the structure of the neighbourhood (number of neighbours). Which observations will be left out and why? Also is this a problem that occurs often or is it something of an anomaly?

Thanks!', 5356, '2014-12-10 16:20:01.197', '57a002a7-3768-4430-ba4d-26fd45312013', 2659, 'Improved answer to fit the goal', 5341, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You are mixing up kNN classification and k-means.

There is nothing wrong with having more than k observations near a center in k-means. In fact, this it the usual case; you shouldn''t choose k too large. If you have 1 million points, a k of 100 may be okay. *K-means does not guarantee clusters of a particular size*. Worst case, clusters in k-means can have only one element (outliers) or even disappear.

What you probably meant to write, but got mixed up, is **what to do if a point is at the same distance to two centers**.

From a statistical point of view, it doesn''t matter. Both have the same squared error.

From an implementation point of view, choose any *deterministic* rule, so that your algorithm converges and doesn''t go into an infinite loop of reassignment.

**Update:** with respect to kNN classification:

There are many ways to resolve this, that will surprisingly often work just as good as the other, without a clear advantage of one over the other:

1. randomly choose a winner from the tied objects
2. take all into account with equal weighting
3. if you have m objects at the same distance where you expected only r, then put a weight of r/k on each of them.

E.g. k=5.

    distance   label   weight
        0        A       1
        1        B       1
        1        A       1
        2        A      2/3
        2        B      2/3
        2        B      2/3

yields A=2.66, B=2.33

The reason that randomly choosing works just as good as the others is that usually, the majority decision in kNN will not be changed by contributions with a weight of less than 1; in particular when k is larger than say 10.', 924, '2014-12-10 16:21:24.247', 'a222798e-d763-4d71-ab5b-1c190ffdb399', 2662, 'added 886 characters in body', 5342, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As you''ve described it, Step 4 is where you want to use TF-IDF. Essentially, TD-IDF will count each term in each document, and assign a score given the relative frequency across the collection of documents.

There''s one big step missing from your process, however: annotating a training set. Before you train your classifier, you''ll need to manually annotate a sample of your data with the labels you want to be able to apply automatically using the classifier.

To make all of this easier, you might want to consider using the [Stanford Classifier](http://nlp.stanford.edu/software/classifier.shtml). It will perform the feature extraction and build the classifier model (supporting several different machine learning algorithms), but you''ll still need to annotate the training data by hand.', 819, '2014-12-10 17:36:20.550', '5af9e52b-2487-416e-8094-3fcf7c9334f4', 2678, 5343, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The closes thing I know is [ConvNetJS](http://cs.stanford.edu/people/karpathy/convnetjs/):

> ConvNetJS is a Javascript library for training Deep Learning models (mainly Neural Networks) entirely in your browser. Open a tab and you''re training. No software requirements, no compilers, no installations, no GPUs, no sweat.

Demos on this site plot weighs and how do they change with time (bear in mind, its many parameters, as practical networks do have a lot of neurons). Moreover, if you are not satisfied with their plotting, there is access to networks parameters and you can plot as you wish (since it is JavaScript).', 289, '2014-12-10 19:50:17.797', 'e655a9bf-afd9-4127-a4e6-ccde1d10736a', 2679, 5344, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have used Neo4J to implement a content recommendation engine. I like Cypher, and find graph databases to be intuitive.

Looking at scaling to a larger data set, I am not confident No4J + Cypher will be performant. Spark has the GraphX project, which I have not used in the past.

Has anybody switched from Neo4J to Spark GraphX? Do the use cases overlap, aside from scalability? Or, does GraphX address a completely different problem set than Neo4J?', 3466, '2014-12-10 22:47:49.880', 'a91464bf-de8f-4b2d-ae74-210155870669', 2682, 5351, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Use Cases of Neo4J and Spark GraphX', 3466, '2014-12-10 22:47:49.880', 'a91464bf-de8f-4b2d-ae74-210155870669', 2682, 5352, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<scalability><graphs><neo4j>', 3466, '2014-12-10 22:47:49.880', 'a91464bf-de8f-4b2d-ae74-210155870669', 2682, 5353, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How can I connect to Titan database from Python ?

What I understand is that Titan (Graph database) is an interface (Blueprint) to Cassandra (Column Store) and
bulb is a python interface to graph DB.

Now how can I start programming in python to connect with titan DB?
Is there any good documentation/tutorial available ?

', 5091, '2014-12-11 05:02:21.350', 'fbac0c6b-7e63-4dd1-b158-acf02a27cb1a', 2667, 'added 223 characters in body; edited tags', 5357, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<python><databases><nosql>', 5091, '2014-12-11 05:02:21.350', 'fbac0c6b-7e63-4dd1-b158-acf02a27cb1a', 2667, 'added 223 characters in body; edited tags', 5358, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can anyone please help me in resolving below issue,

I''m trying to install spark on centos7, steps that i followed are,

1) gctuil for ssh
2) installed java 1.7
3) installed scala 2.10.1
4) installed spark 1.1.1
5) executing - sbt/sbt assembly/

results in below error


[warn] Multiple resolvers having different access mechanism configured with same name ''sbt-plugin-releases''. To avoid conflict, Remove duplicate project resolvers (`res
olvers`) or rename publishing resolver (`publishTo`).

[info] Set current project to spark-parent (in build file:


[error] Expected key
[error] assembly/
[error]          ^
', 5172, '2014-12-11 06:33:37.140', '613a3a2f-7607-4383-a26a-4f25d3cefd87', 3684, 6355, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Error in Spark installation on GCE', 5172, '2014-12-11 06:33:37.140', '613a3a2f-7607-4383-a26a-4f25d3cefd87', 3684, 6356, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><hadoop><scala>', 5172, '2014-12-11 06:33:37.140', '613a3a2f-7607-4383-a26a-4f25d3cefd87', 3684, 6357, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Based on my cursory understanding of the topics, associated with your question, I think that **Gephi** (https://gephi.github.io; the original gephi.org link redirects there) should be able to handle *neural network dynamic visualization*. It seems that, in order to achieve your goal, you need to **stream** your graph(s) with corresponding weights (https://forum.gephi.org/viewtopic.php?t=1875). For *streaming*, you most likely will need this *plug-in*: https://marketplace.gephi.org/plugin/graph-streaming.', 2452, '2014-12-11 07:08:00.540', '57471d49-b8d4-419d-9882-968cc2e41fb1', 3685, 6358, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How can I connect to Titan database from Python ?

What I understand is that Titan (Graph database) provides an interface (Blueprint) to Cassandra (Column Store) and
bulb is a python interface to graph DB.

Now how can I start programming in python to connect with titan DB?
Is there any good documentation/tutorial available ?

', 5091, '2014-12-11 09:03:16.463', 'be869d7f-739d-4c2f-b665-5443debd5d18', 2667, 'added 6 characters in body', 6359, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Based on my cursory understanding of the topics, associated with your question, I think that **Gephi** (https://gephi.github.io; the original gephi.org link redirects there) should be able to handle *neural network dynamic visualization*. It seems that, in order to achieve your goal, you need to **stream** your graph(s) with corresponding weights (https://forum.gephi.org/viewtopic.php?t=1875). For *streaming*, you most likely will need this *plug-in*: https://marketplace.gephi.org/plugin/graph-streaming.

**UPDATE**: You may also find useful SoNIA software: http://web.stanford.edu/group/sonia.', 2452, '2014-12-11 12:45:09.910', '3114e0e9-2fe8-46cb-a3f9-4d14f89777ec', 3685, 'Updated with reference to SoNIA software.', 6361, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to connect with cassandra in R using RJDBC. When I execute "casscon <- dbConnect(cassdrv, "jdbc:cassandra://ipaddrs:9160/demodb")" , I am getting "Error in .jcall(drv@jdrv, "Ljava/sql/Connection;", "connect", as.character(url)[1],  :
  java.lang.StringIndexOutOfBoundsException: String index out of range: -1".

 I can''t figure out the problem. I need a solution for this. Thanks in advance.', 5161, '2014-12-11 13:37:29.430', '2d885638-2345-4291-9d51-9f5c329bd29b', 3687, 6362, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R-Cassandra connection by RJDBC', 5161, '2014-12-11 13:37:29.430', '2d885638-2345-4291-9d51-9f5c329bd29b', 3687, 6363, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><databases>', 5161, '2014-12-11 13:37:29.430', '2d885638-2345-4291-9d51-9f5c329bd29b', 3687, 6364, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a document classification project where I am getting site content and then assigning one of numerous labels to the website according to content.

I found out that [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) could be very useful for this. However, I was unsure as to *when* exactly to use it.

Assumming a website that is concerned with a specific topic makes repeated mention of it, this was my current process:

1. Retrieve site content, parse for plain text
2. Normalize and stem content
3. Tokenize into unigrams (maybe bigrams too)
4. Retrieve a count of each unigram for the given document, filtering low length and low occurrence words
5. Train a classifier such as NaiveBayes on the resulting set

My question is the following: **Where would tf-idf fit in here**? Before normalizing/stemming? After normalizing but before tokenizing? After tokenizing?

Any insight would be greatly appreciated.

----------

**Edit:**

Upon closer inspection, I think I may have run into a misunderstanding at to how TF-IDF operates. At the above step 4 that I describe, would I have to feed the *entirety* of my data into TF-IDF at once? If, for example, my data is as follows:

    [({tokenized_content_site1}, category_string_site1),
     ({tokenized_content_site2}, category_string_site2),
    ...
     ({tokenized_content_siten}, category_string_siten)}]

Would I have to feed the *entirety* of that data into the TF-IDF calculator at once to achieve the desired effect? Specifically, I have been looking at the [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) TfidfVectorizer to do this, but I am a bit unsure as to its use as examples are pretty sparse.

', 5199, '2014-12-11 16:27:28.457', '2068837d-fefa-4e07-9572-43c67369ecf8', 2677, 'added follow-up', 6365, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a document classification project where I am getting site content and then assigning one of numerous labels to the website according to content.

I found out that [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) could be very useful for this. However, I was unsure as to *when* exactly to use it.

Assumming a website that is concerned with a specific topic makes repeated mention of it, this was my current process:

1. Retrieve site content, parse for plain text
2. Normalize and stem content
3. Tokenize into unigrams (maybe bigrams too)
4. Retrieve a count of each unigram for the given document, filtering low length and low occurrence words
5. Train a classifier such as NaiveBayes on the resulting set

My question is the following: **Where would tf-idf fit in here**? Before normalizing/stemming? After normalizing but before tokenizing? After tokenizing?

Any insight would be greatly appreciated.

----------

**Edit:**

Upon closer inspection, I think I may have run into a misunderstanding at to how TF-IDF operates. At the above step 4 that I describe, would I have to feed the *entirety* of my data into TF-IDF at once? If, for example, my data is as follows:

    [({tokenized_content_site1}, category_string_site1),
     ({tokenized_content_site2}, category_string_site2),
    ...
     ({tokenized_content_siten}, category_string_siten)}]

Here, the outermost structure is a list, containing tuples, containing a dictionary (or hashmap) and a string.

Would I have to feed the *entirety* of that data into the TF-IDF calculator at once to achieve the desired effect? Specifically, I have been looking at the [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) TfidfVectorizer to do this, but I am a bit unsure as to its use as examples are pretty sparse.

', 5199, '2014-12-11 16:38:53.367', 'ac187940-62a1-4802-8c80-aaf8728ed208', 2677, 'added 114 characters in body', 6366, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a corpus of text with a corresponding topics. For example `"A rapper Tupac was shot in LA"` and it was labelled as `["celebrity", "murder"]`. So basically each vector of features can have many labels (not the same amount. The first feature vector can have 3 labels, second 1, third 5).

If I would have just one label corresponded to each text, I would try a [Naive Bayes classifier][1], but I do not really know how should I proceed if I can have many labels.

Is there any way to transform Naive Bayes into multi label classification problem (if there is a better approach - please let me know)?

**P.S.** few things about the data I have.

- approximately 10.000 elements in the dataset
- text is approximately 2-3 sentences
- maximum 7 labels per text

  [1]: http://en.wikipedia.org/wiki/Naive_Bayes_classifier', 6387, '2014-12-11 19:23:55.907', '544d5c56-a611-43ba-b9ee-e96fb5722db5', 3688, 6367, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Multiple labels in supervised learning algorithm', 6387, '2014-12-11 19:23:55.907', '544d5c56-a611-43ba-b9ee-e96fb5722db5', 3688, 6368, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><text-mining>', 6387, '2014-12-11 19:23:55.907', '544d5c56-a611-43ba-b9ee-e96fb5722db5', 3688, 6369, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From listening to presentations by Martin Odersky, the creator of Scala, it is especially well suited for building highly scalable systems by leveraging functional programming constructs in conjuction with object orientation and flelxible syntax. It is also useful for development of small systems and rapid prototyping because it takes less lines of code than some other languages and it has an interactive mode for fast feedback. One notable Scala framework is Akka which uses the actor model of concurrent computation. Many of Odersky''s presentations are on YouTube and there is a list of tools implemented with Scala on wiki.scala-lang.org.', 5280, '2014-12-11 20:27:27.847', '5dc1d3c0-e2bd-4b72-a450-e17b4949611b', 3689, 6370, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m wonder if it''s possible to export a model trained in R, to OpenCV''s Machine Learning (ML) library format? The latter appears to save/read models in [XML/YAML][1], whereas the former might be exportable via [PMML][2]. Specifically, I''m working with Random Forests, which are classifiers available both in R and OpenCV''s ML library.

Any advice on how I can get the two to share models would be greatly appreciated.


  [1]: http://docs.opencv.org/modules/ml/doc/statistical_models.html#cvstatmodel-load
  [2]: http://cran.r-project.org/web/packages/pmml/index.html', 6390, '2014-12-11 23:56:03.023', '1c069643-2e27-4389-9aa3-af4d69472429', 3690, 6371, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Exporting R model to OpenCV''s Machine Learning Library', 6390, '2014-12-11 23:56:03.023', '1c069643-2e27-4389-9aa3-af4d69472429', 3690, 6372, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><open-source>', 6390, '2014-12-11 23:56:03.023', '1c069643-2e27-4389-9aa3-af4d69472429', 3690, 6373, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For starters, Naive Bayes is probably not appropriate here.  It assumes independence among the inputs (hence the "Naive") and words in a sentence are very dependent.

But, assuming you really want to run with NB as an algorithm to start your experimentation, there are two options I''d consider:

## Ungraceful: Lots of NB classifiers
  This would be an alternative approach.  Make a corupus of all the words observed as your vector of inputs.  Make a corpus off all the tags that are observed as your vector of outputs.  An NB classifier with multiple outputs is the equivalent of having multiple NB classifiers with one output each (so do whichever is easier to implement in whatever software framework you''re using).  Treat each element as a training sample where a given input (a word) is a `1` if that word is present and a `0` if that word isn''t.  Use the same binary scheme for the output.

  This brute forces the application of the NB Classifier to your data, and leaves you to find meaning by still haivng to mine the huge set of classifiers you''ll be left with.

## More Graceful: Process your data
  This is the approach I''d recommend if you want to run with one multiple-class NB Classifier.

Your goal here is to figure out how to map each set of tags to a single class.  I''m sure there is some sort of clustering scheme or network analysis (perhaps ["celebrity"] linked to ["murder"] could become a the segment ["debauchery"]) that will sensibly map your tags to one single cluster.  If you treat tags as nodes and two given tags together as links, then you''ll want to look into community detection algorithms (which is where I''d start).  But, if you just want something working, then some sort of hack on the tags that converts a list of tags to only the tag that''s most commonly seen in your dataset would be enough.

  This method front-loads the work of cleaning your data and would make the NB Classifier''s output easier to understand.


', 6391, '2014-12-12 00:13:10.680', 'ee4f3705-2423-417a-bc23-ca81fd34cf0a', 3691, 6374, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('1. Apply your clustering algorithm
2. Calculate distance from all data points to its assigned cluster
3. Label the data points furthest from a center as an outlier

Randomly generating 100 data points from three gaussians, clustering them with k-means, and marking the 10 ''furthest from a center'' data points gave the following graph:
![enter image description here][1]

see [this notebook](http://nbviewer.ipython.org/github/TheGrimmScientist/Snippets/blob/master/OutlierDetectionWithClustering/OutlierDetectionWithClustering.ipynb) for the full example

The burden of solving what "distance" means will already have to be solved for you to run a clustering algorithm.  It will still be up to you to pick off what distance means an outlier.  In this example, I just picked the N most distant data point, though you''ll probably want to pick any number of data points over a certain number of standard deviations from a center.

  [1]: http://i.stack.imgur.com/TrSDZ.png', 6391, '2014-12-12 01:30:26.723', '9d0a0759-79e9-4a91-82e9-0b20802fbf2a', 3692, 6375, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m working on a project which asks fellow students to share their original text data for further analysis using data mining techniques, and, I think it would be appropriate to anonymize student names with their submissions.

Setting aside the better solutions of a url where students submit their work and a backend script inserts the anonymized ID, **What sort of solutions could I direct students to implement on their own to anonymized their own names?**

I''m still a noob in this area. I don''t know what are the norms. I was thinking the solution could be a hashing algorithm. That sounds like a better solution than making up a fake name as two people could pick the same fake name.possible people could pick the same fake name. **What are some of the concerns I should be aware of?** ', 2742, '2014-12-12 03:00:57.507', '0d9579f7-27f3-4032-908f-ad85f012e85c', 3693, 6376, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the best practices to anonymize user names in data?', 2742, '2014-12-12 03:00:57.507', '0d9579f7-27f3-4032-908f-ad85f012e85c', 3693, 6377, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-cleaning>', 2742, '2014-12-12 03:00:57.507', '0d9579f7-27f3-4032-908f-ad85f012e85c', 3693, 6378, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am trying to connect with cassandra in R using [RJDBC][1]. When I execute

    casscon <- dbConnect(cassdrv, "jdbc:cassandra://ipaddrs:9160/demodb")

I am getting

    Error in .jcall(drv@jdrv, "Ljava/sql/Connection;", "connect", as.character(url)[1],
    : java.lang.StringIndexOutOfBoundsException: String index out of range: -1

 I can''t figure out the problem. I need a solution for this.


  [1]: http://cran.r-project.org/web/packages/RJDBC/index.html', 2961, '2014-12-12 04:13:53.157', 'd55727d6-63f4-47c9-a1c5-f0dec55feadd', 3687, 'improved formatting', 6379, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('R-Cassandra connection with RJDBC', 2961, '2014-12-12 04:13:53.157', 'd55727d6-63f4-47c9-a1c5-f0dec55feadd', 3687, 'improved formatting', 6380, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-12-12 04:13:53.157', 'd55727d6-63f4-47c9-a1c5-f0dec55feadd', 3687, 'Proposed by 2961 approved by 5161 edit id of 194', 6381, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A standard practice in psychology (where you want to code participants in order to link different measurements together) is to have participants choose their mother''s maiden name initials and birthdate, e.g., in the format XX-YYMMDD.

This if course can still run into conflicts. Then again, I don''t think there is *any* surefire conflict-free anonymization algorithm your students could do *without knowing all the other students*. Mothers'' names and birthdates could be identical, own birthdates could be identical, shoe sizes could be, favorite superhero characters... The only thing I could think of would be (US) Social Security numbers, but you [*really* don''t want to use them](http://www.ssa.gov/pubs/EN-05-10064.pdf).

Bottom line: anonymize on the backend. Or, as [@Emre suggests](http://datascience.stackexchange.com/questions/3693/what-are-the-best-practices-to-anonymize-user-names-in-data#comment3919_3693), think about whether you really need an identifier at all. Maybe the DB-generated index is enough?', 2853, '2014-12-12 09:02:16.350', 'a0188f10-dc0d-4fb0-a4e7-db4adc2ee64d', 3694, 6382, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('It is important to see all the rules if one of the states of target column is more important than others. For example, if you are predicting fraudulent transactions, you might want to flag something as fraud even if is has 5% probability.', 84, '2014-12-12 20:00:23.620', '13f80e88-1962-4aad-9457-b1cfd166c2d7', 2669, 'Minor spelling error.', 6384, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Say I used spectral clustering to cluster a data-set D of points X_0 - X_n into a number C of clusters.

How can I efficiently assign a new single point X_{n+1} to his convenient cluster, Do I have to do the classification from the beginning (destroy all the clusters and apply the algorithm to the data-set X_0 - X_{n+1}) or is there an optimized way to extend to the point X_{n+1}.

Thanks in advance.', 847, '2014-12-12 21:01:42.370', '7a4908d5-a1ff-4b7a-a1be-acb34768ed6d', 2645, 'fixed formatting', 6385, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-12-12 21:01:42.370', '7a4908d5-a1ff-4b7a-a1be-acb34768ed6d', 2645, 'Proposed by 847 approved by 2452, -1 edit id of 191', 6386, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Say I used spectral clustering to cluster a data-set $D$ of points $X_0 - X_n$ into a number $C$ of clusters. How can I efficiently assign a new single point $X_{n+1}$ to his convenient cluster?

Do I have to do the classification from the beginning (destroy all the clusters and apply the algorithm to the data-set $X_0 - X_{n+1}$), or is there an optimized way to extend to the point $X_{n+1}$?', 84, '2014-12-12 21:01:42.370', 'f8df50e9-b2a2-4909-8572-abbe55eae5ca', 2645, 'fixed formatting', 6387, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**EDIT** It was pointed out in the Answers-section that I am confusing k-means and kNN. Indeed I was thinking about kNN but wrote k-means since I''m still new to this topic and confuse the terms quite often. So here is the changed question:

I was looking at kNN today and something struck me as odd or - to be more precise - something that I was unable to find information about namely the following situation:

Imagine that we pick kNN for some dataset (I want to remain as general as possible thus K will not be specified here). Further we select at some point an observation where the number of neighbours that fulfill the requirement to be in the neighbourhood are actually more than the specified K. What criterion/criteria should be applied here if we are restricted to use the specific K and thus cannot alter the structure of the neighbourhood (number of neighbours). Which observations will be left out and why? Also is this a problem that occurs often or is it something of an anomaly?
', 924, '2014-12-12 21:05:30.020', 'ab3245fc-47d1-417b-b3a5-b0c77cfab5a2', 2659, 'Fix tags, too.', 6388, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><classification>', 924, '2014-12-12 21:05:30.020', 'ab3245fc-47d1-417b-b3a5-b0c77cfab5a2', 2659, 'Fix tags, too.', 6389, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-12-12 21:05:30.020', 'ab3245fc-47d1-417b-b3a5-b0c77cfab5a2', 2659, 'Proposed by 924 approved by -1 edit id of 192', 6390, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**EDIT** It was pointed out in the Answers-section that I am confusing k-means and kNN. Indeed I was thinking about kNN but wrote k-means since I''m still new to this topic and confuse the terms quite often. So here is the changed question.

I was looking at kNN today and something struck me as odd or - to be more precise - something that I was unable to find information about namely the following situation.

Imagine that we pick kNN for some dataset. I want to remain as general as possible, thus $k$ will not be specified here. Further we select, at some point, an observation where the number of neighbors that fulfill the requirement to be in the neighbourhood are actually more than the specified $k$.

What criterion/criteria should be applied here if we are restricted to use the specific K and thus cannot alter the structure of the neighborhood (number of neighbors). Which observations will be left out and why? Also is this a problem that occurs often, or is it something of an anomaly?', 84, '2014-12-12 21:05:30.020', '60924e86-2f55-41f6-8e52-c5a21be6bebb', 2659, 'Fix tags, too.', 6391, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('
## Re: size of data

### The short answer

Scala works for both small and large data, but its creation and development is motivated by needing something scalable.  [Scala is an acronym for Scalable Language](http://www.scala-lang.org/what-is-scala.html).

### The long answer
Scala is a [functional programming language](http://en.wikipedia.org/wiki/Functional_programming) that runs on the [jvm](http://en.wikipedia.org/wiki/Java_virtual_machine).  The ''functional'' part of this is a fundamental difference in the language that makes you think differently about programming.  If you like that way of thinking, it lets you quickly work with small data.  Whether you like it or not, functional languages are fundamentally easier to massively scale.  The jvm piece is also important because the jvm is basically everywhere and, thus, Scala code can run basically everywhere.  (Note there are plenty of other [languages written on the jvm](http://en.wikipedia.org/wiki/List_of_JVM_languages) and plenty of other [functional programming languages](http://en.wikipedia.org/wiki/List_of_programming_languages_by_type#Functional_languages), and languages beyond Scala do appear in both lists.)

[This talk](https://www.youtube.com/watch?v=3jg1AheF4n0) give a good overview of the motivation behind Scala.


## Re: other tools that have good Scala support:
As you mentioned, Spark (distributable batch processing better at iteratative algorithms than its counterpart) is a big one.  With Spark comes its libraries [Mllib](https://spark.apache.org/mllib/) for machine learning and [GraphX](https://spark.apache.org/graphx/) for graphs.  As mentioned by  Erik Allik and Tris Nefzger, [Akka](http://akka.io) and [Factorie](https://github.com/factorie/factorie) exist.  There is also [Play](https://www.playframework.com/).


Generally,  I can''t tell if there is a specific use case you''re digging for (if so, make that a part of your question), or just want a survey of big data tools and happen to know Scala a bit and want to start there.', 6391, '2014-12-12 22:39:58.470', '7b2bbf72-dac1-40ab-983c-2822f4bfa26d', 3696, 6395, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data becomes "big" when a single [commodity computer](http://en.wikipedia.org/wiki/Commodity_computing) can no longer handle the amount of data you have.  It denotes the point at which you need to start thinking about building supercomputers or using clusters to process your data.', 6391, '2014-12-12 22:48:41.970', '943da83f-da49-409d-82b1-eea2dcd1dd36', 3697, 6396, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What would you put on a CTQ or Voice of Customer diagram for the following hypothetical scenario?

> Sinclair CorporationTHE COMPANY<br> Sinclair Corporation is based
> in Traverse City, MI and acquired Premium Photo Products seven months
> ago. Premium makes several types of film. The Premium 1000 series is
> made with a high quality, supersensitive emulsion. It is nationally
> marketed in roll form for amateurs and sheet and film-pack forms for
> professionals.<br> Roll film is similar to most film commonly used by
> the general public.<br> Sheet film is handled in total darkness,
> negatives being withdrawn from between interleaf papers by the
> photographer and inserted into a light-tight holder which is then
> placed in the camera. Film-pack is a self-contained unit ready to
> install in the camera. The photographer merely pulls a paper tab to
> shift the exposed negative out of position and a new one into
> position. The procedure for using and developing all of this film has
> not changed since introduction. Premium 1000 is produced in small
> lots at the Denver plant. After packaging, the film is shipped to
> dealers across the country for display and sale; a remarkably stable
> film with a shelf life of eighteen months, Premium 1000 has an
> excellent reputation in the trade. THE SITUATION During the last 48
> hours Sales, Manufacturing and Research have been working to resolve
> this sudden and serious problem with the Premium 1000 film. Customer
> and dealer complaints about fogged sheet film are mounting. This
> crisis now requires immediate action before Premium 1000 has to be
> removed from the market entirely. During the past ten days, dealers in
> several sales districts have had customer complaints of fogged sheet
> film. In some cases, dealers have accumulated complaints for several
> days before they reported them. Complaints, mounting every day, have
> since spread to all sales districts in the country. Right now there
> are complaints from 682 of our 7000 dealers. Although Sales is
> replacing each box of fogged film with two new boxes, this has not
> placated anyone. Two commercial studios, for example, threaten total
> cancellation of further use of our products because our film, which
> they used to photograph unique events, was fogged and the pictures
> consequently ruined. Sales has been very concerned by several
> manufacturing problems which have resulted in failures to meet
> delivery requirements. For example: a serious fire two months ago in
> the Premium 1000 film emulsion department resulted in the discharge
> of several employees for carelessness and the intensive training of
> the new emulsion mixers; delivery quantity difficulties on interleaf
> paper for over a month after the acquisition; delay in arrival of the
> new bulk raw ingredient storage containers. Though the interleaf
> delivery difficulties cleared up by acquiring a new suppler, all of
> the new bulk storage containers have not arrived and so the old ones
> are still being used. Sales of sheet film, as well as film-pack, have
> quickly dropped and some professionals have begun using competitive
> brands. So far, replacing the fogged film and Premiums good
> reputation has kept most dealers from defecting. Sales found that all
> Premium 1000 film is shipped, distributed, stocked and sold by
> dealers the same way. They also found that all complaints so far are
> coming from small outlets where turnover is very slow, but they expect
> to hear complaints from the larger outlets soon. None of the film was
> outdated and the fogging was seen as soon as it was developed rather
> than later. Sales asks that we stop production for a couple of days
> until Manufacturing thoroughly checks out their operation (i.e.,
> effects of the fire, effectiveness of emulsion mixer training, and
> possible contamination by the new storage containers). The expense
> involved in doing this would be about \$30,000. Manufacturing believes
> the new zip-top sheet film package is responsible. Because of
> substantial customer complaints that our package was hard to open,
> Sales rushed the introduction of the easier to open zip-top package
> five weeks ago. There was no Page 2 of 2 time for thorough testing.
> The problem started since then. Manufacturing recommends going back to
> the former package. It was used for three years without difficulty.
> This will gain time to further investigate the zip-top package. They
> urge an immediate hold on expansion of the zip-top package to other
> Premium 1000 film. This proposal would require a commitment of only
> \$20,000, but could invite an added source of known customer
> dissatisfaction at this critical time. Research has conducted tests on
> sample returns. Their findings are below. THE LAB REPORT Memo from the
> Research Director (delivered 4 days after the first reports started
> coming in) We have studied the samples of returned fogged film
> provided by Sales and have attempted to reproduce the fogging exactly
> as it appears on the returns. We analyzed fifty returned boxes of film
> and found two things they all had in common: all negatives in a box
> are fogged; fogging is even across the entire surface of each
> negative. We then took thirty-six samples of good sheet film from the
> plant warehouse and carefully subjected these to a variety of tests.
> The only way we were able to reproduce exactly the unique fogging
> effect was to expose the sheet film to a source of radiation (such as
> an x-ray machine).<br> Recognizing that this type of film is
> supersensitive and is more susceptible to contamination, and having
> clearly demonstrated that radiation produces the exact characteristics
> of the returned fogged film, we believe that a source of radioactivity
> is the root of the problem. We know that traces of radioactive fallout
> have contaminated various products in the past. With the current high
> incidence of Strontium 90 in th', 84, '2014-12-12 23:31:47.657', 'cbfc647e-dcd8-4d92-b76f-7876196dcb78', 2644, 'added 154 characters in body; edited title', 6397, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Critical To Quality (CQT) diagram for a corporation, problem solving, six sigma, lean manufacturing question', 84, '2014-12-12 23:31:47.657', 'cbfc647e-dcd8-4d92-b76f-7876196dcb78', 2644, 'added 154 characters in body; edited title', 6398, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What would you put on a CTQ or Voice of Customer diagram for the following hypothetical scenario?

> Sinclair Corporation <br>
THE COMPANY <br>
Sinclair Corporation is based in Traverse City, MI and
acquired Premium Photo Products seven months ago.
Premium makes several types of film. The Premium
1000 series is made with a high quality, supersensitive
emulsion. It is nationally marketed in roll form for
amateurs and sheet and film-pack forms for
professionals. <br>
Roll film is similar to most film commonly used by the
general public. <br>
Sheet film is handled in total darkness, negatives being
withdrawn from between interleaf papers by the
photographer and inserted into a light-tight holder
which is then placed in the camera. <br>
Film-pack is a self-contained unit ready to install in the
camera. The photographer merely pulls a paper tab to
shift the exposed negative out of position and a new
one into position. <br>
The procedure for using and developing all of this film
has not changed since introduction. Premium 1000 is
produced in small lots at the Denver plant. After
packaging, the film is shipped to dealers across the
country for display and sale; a remarkably stable film
with a shelf life of eighteen months, Premium 1000
has an excellent reputation in the trade. <br>
THE SITUATION <br>
During the last 48 hours Sales, Manufacturing and
Research have been working to resolve this sudden and
serious problem with the Premium 1000 film. <br>
Customer and dealer complaints about fogged sheet
film are mounting. This crisis now requires immediate
action before Premium 1000 has to be removed from
the market entirely. <br>
During the past ten days, dealers in several sales
districts have had customer complaints of fogged sheet
film. In some cases, dealers have accumulated
complaints for several days before they reported them. <br>
Complaints, mounting every day, have since spread to
all sales districts in the country. Right now there are
complaints from 682 of our 7000 dealers. Although
Sales is replacing each box of fogged film with two new
boxes, this has not placated anyone. Two commercial
studios, for example, threaten total cancellation of
further use of our products because our film, which
they used to photograph unique events, was fogged and
the pictures consequently ruined. <br>
Sales has been very concerned by several
manufacturing problems which have resulted in failures
to meet delivery requirements. For example: a serious
fire two months ago in the Premium 1000 film
emulsion department resulted in the discharge of
several employees for carelessness and the intensive
training of the new emulsion mixers; delivery quantity
difficulties on interleaf paper for over a month after the
acquisition; delay in arrival of the new bulk raw
ingredient storage containers. Though the interleaf
delivery difficulties cleared up by acquiring a new
suppler, all of the new bulk storage containers have not
arrived and so the old ones are still being used. <br>
Sales of sheet film, as well as film-pack, have quickly
dropped and some professionals have begun using
competitive brands. So far, replacing the fogged film
and Premiums good reputation has kept most dealers
from defecting. <br>
Sales found that all Premium 1000 film is shipped,
distributed, stocked and sold by dealers the same way.
They also found that all complaints so far are coming
from small outlets where turnover is very slow, but they
expect to hear complaints from the larger outlets soon. <br>
None of the film was outdated and the fogging was
seen as soon as it was developed rather than later.
Sales asks that we stop production for a couple of days
until Manufacturing thoroughly checks out their
operation (i.e., effects of the fire, effectiveness of
emulsion mixer training, and possible contamination by
the new storage containers). The expense involved in
doing this would be about \$30,000. <br>
Manufacturing believes the new zip-top sheet film
package is responsible. Because of substantial
customer complaints that our package was hard to
open, Sales rushed the introduction of the easier to
open zip-top package five weeks ago. There was no Page 2 of 2
time for thorough testing. The problem started since
then. Manufacturing recommends going back to the
former package. It was used for three years without
difficulty. This will gain time to further investigate the
zip-top package. They urge an immediate hold on
expansion of the zip-top package to other Premium
1000 film. This proposal would require a commitment
of only \$20,000, but could invite an added source of
known customer dissatisfaction at this critical time.
Research has conducted tests on sample returns. Their
findings are below. <br>
THE LAB REPORT <br>
Memo from the Research Director (delivered 4 days
after the first reports started coming in)
We have studied the samples of returned fogged film
provided by Sales and have attempted to reproduce the
fogging exactly as it appears on the returns. <br>
We analyzed fifty returned boxes of film and found two
things they all had in common: all negatives in a box are
fogged; fogging is even across the entire surface of each
negative. <br>
We then took thirty-six samples of good sheet film from
the plant warehouse and carefully subjected these to a
variety of tests. The only way we were able to
reproduce exactly the unique fogging effect was to
expose the sheet film to a source of radiation (such as
an x-ray machine). <br>
Recognizing that this type of film is supersensitive and is
more susceptible to contamination, and having clearly
demonstrated that radiation produces the exact
characteristics of the returned fogged film, we believe
that a source of radioactivity is the root of the problem. <br>
We know that traces of radioactive fallout have
contaminated various products in the past. With the
current high incidence of Strontium 90 in th', 84, '2014-12-12 23:39:37.523', '79be9cff-5b92-48fe-ae0e-d45a469f61c7', 2644, 'added 154 characters in body; edited title', 6399, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Critical To Quality (CQT) diagram for a corporation, problem solving, six sigma, lean manufacturing question', 84, '2014-12-12 23:39:37.523', '79be9cff-5b92-48fe-ae0e-d45a469f61c7', 2644, 'added 154 characters in body; edited title', 6400, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m very passionate about how computers can be made able to think intelligently and independently (in our favour, of course!). I''m currently studying Bachelors science of information technology at UTS (University of Technology:Sydney). I have two months before I start my second year, and have not yet been able to decide on which major should I select that can lead myself towards dedicated study of Artificial Intelligence (which I love with my life).

I have the following majors available:

- Internetworking and Applications
- Data Analytics
- (there are other two as well, but business oriented).

[Here](http://uts.edu.au) is the link to my subjects. I believe that being able to play with data is a sign of intelligence (I may be wrong too!). Will one of these subjects form me a good foundation for my further study in A.I.? Or should I jump into Engineering? Or Pure Science?', 84, '2014-12-12 23:48:02.230', '574b605a-034a-4e5f-81ca-fdbc9d0e8b8f', 2586, 'improving formatting', 6401, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Can data analytics be a basis for artificial intelligence?', 84, '2014-12-12 23:48:02.230', '574b605a-034a-4e5f-81ca-fdbc9d0e8b8f', 2586, 'improving formatting', 6402, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><career>', 84, '2014-12-12 23:48:02.230', '574b605a-034a-4e5f-81ca-fdbc9d0e8b8f', 2586, 'improving formatting', 6403, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**Gravity**

Basically you''d like to find a function that maps an input (an object with a mass and a location) to an output (new location). It''s not necessary to have one set of input neurons for each different object. It''s sufficient to encode the input variables generically for all objects.

Like a function:

$$f( x, y, z, mass ) \to ( x_n, y_n, z_n )$$

**XOR**

Training the XOR function is special as the early neural networks, the perceptrons, were unable to learn the XOR function (because it is not linear separable). Multi-Layer Perceptrons however are able to learn the XOR function. This is just to demonstrate that the NN implementation of Synaptic is capable of learning problems that are not linear separable
', 84, '2014-12-12 23:53:01.607', 'f74fdd15-39c2-454a-b5d5-c503f80ce009', 2676, 'added 2 characters in body', 6405, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have implemented Q-Learning as described [here](http://web.cs.swarthmore.edu/~meeden/cs81/s12/papers/MarkStevePaper.pdf). In order to approximate $Q(S,A)$, I use a neural network structure like the following:

 - Activation sigmoid;
 - Inputs, number of inputs +1 for Action neurons (All Inputs Scaled 0-1);
 - Outputs, single output. Q-Value;
 - N number of M Hidden Layers;
 - Exploration method random 0 < $rand()$ < propExplore;

At each learning iteration using the following formula,

![enter image description here][1]

I calculate a Q-Target value then calculate an error using,

$$\mathit{error} = \text{QTarget} - \text{LastQValueReturnedFromNN}$$

and back propagate the error through the neural network.

Questions:

1. Am I on the right track? I have seen some papers that implement a NN with one output neuron for each action.

2. My reward function returns a number between -1 and 1. Is it ok to return a number between -1 and 1 when the activation function is sigmoid (0 1).

3. From my understanding of this method given enough training instances it should be quarantined to find an optimal policy wight? When training for XOR sometimes it learns it after 2k iterations sometimes it won''t learn even after 40k 50k iterations.

[1]: http://i.stack.imgur.com/e3hgc.png', 84, '2014-12-13 00:06:11.770', 'fb548682-f908-45aa-84de-306ae369f205', 2638, 'added 46 characters in body', 6417, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Having:

- a set of soft fuzzy classifiers (classification onto overlapping sets) $C_i(x) \to [0,1]$;
- a corresponding set of weak estimators $R_i(z)$ of the form $R_i(z) = \mathit{EX}(y\mid z)$.

The estimators $R_i$ are just some kind of regression, Kalman or particle filters. The classifiers $C_i$ are fixed and static. How to make a strong estimator out of a weighted combination of the form:

$$L(x, z) = \sum_{i}C_i(x)R_i(z)Q_i$$

In other words how to choose the weights $Q_i$? Is there some kind of online approach to this problem?

Here is brief description of a practical application. When an event $E$ is registered, multiple measurements are made. Based on these measurements, the classifiers $C_i$ make a soft assignment of the event to multiple overlapping categories. What we get is fit ratios for the soft clusters.

Now there is some chance that event $E$ may trigger a subsequent event $D$, depending on another variable $z$ -- independent from the event $E$. We know that all the soft cluster "memberships" may influence the probability of event $D$ being triggered.

We want to estimate the probability that $E$ triggers $D$, given the $C_i$ fitness ratios and value of $z$.', 84, '2014-12-13 00:11:04.017', '2b021992-3923-4a9f-a1ee-ce92191c91c1', 2634, 'Improving formatting', 6418, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Neil is correct. There are partial derivatives evwrywhere in gradient computation for machine learning models training.

For instance you can look at the gradient descent method used in the backpropagation method for a neural network. The course from AndrewNg on coursera describes it very well.', 5342, '2014-12-13 00:14:11.770', '1d6eb88a-ed00-4ec2-a521-47286d6424f7', 2650, 'added 176 characters in body', 6421, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m running a test on MapReduce algorithm in different environments, like Hadoop and MongoDB, and using different types of data. What are the different methods or techniques to find out the execution time of a query.

If I''m inserting a huge amount of data, consider it to be 2-3GB, what are the methods to find out the time for the process to be completed.', 84, '2014-12-13 00:14:33.460', '5faf2722-36cb-49e8-8167-9f134cc37ef4', 2632, 'added 5 characters in body; edited tags; edited title', 6422, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Timing sequence in MapReduce', 84, '2014-12-13 00:14:33.460', '5faf2722-36cb-49e8-8167-9f134cc37ef4', 2632, 'added 5 characters in body; edited tags; edited title', 6423, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<efficiency><map-reduce><performance><experiments>', 84, '2014-12-13 00:14:33.460', '5faf2722-36cb-49e8-8167-9f134cc37ef4', 2632, 'added 5 characters in body; edited tags; edited title', 6424, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If your Data points are dense and noise points are away from the dense region, you can try DBSCAN algorithm.

http://en.wikipedia.org/wiki/DBSCAN

![enter image description here][1]

Tweak its parameters until u get a best fit.


  [1]: http://i.stack.imgur.com/gdFlG.png', 6412, '2014-12-13 05:28:04.013', '4aa1d978-fd16-476b-b1ea-265d1fe34a66', 3698, 6431, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<h5>
For an imbalanced set of data is it better to choose an L1 or L2 regularization?<br>
Is there any special preprocessing to improve the model score (log_loss in particular)?<br><br>
<b>Note</b>: I''m a beginner in Machine Learning and I''m learning through examples and competitions on Kaggle.
</h5>', 6418, '2014-12-13 17:42:54.927', '318bdefb-7930-4a2c-8518-ac8907307d79', 3699, 6432, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to optimize the parameters of a classifier on scikit-learn?', 6418, '2014-12-13 17:42:54.927', '318bdefb-7930-4a2c-8518-ac8907307d79', 3699, 6433, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><logistic-regression><scikit>', 6418, '2014-12-13 17:42:54.927', '318bdefb-7930-4a2c-8518-ac8907307d79', 3699, 6434, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In [his thesis][1] (section 2.3.3) Belkin uses the heat equation to derive an approximation for $\mathcal{L}f$:

$$\mathcal{L}f(x_i)\approx \frac{1}{t}\Big(f(x_i)-\alpha \sum_{x_j, ||x_i-x_j||<\epsilon}e^{-\frac{||x_i-x_j||^2}{4t}}f(x_j)\Big)$$
where $$\alpha=\Big(\sum_{x_j, ||x_i-x_j||<\epsilon}e^{-\frac{||x_i-x_j||^2}{4t}}\Big)^{-1}$$.

However, I''m not sure how these considerations lead to this choice of weights for the weight matrix (which will be used to construct the Laplacian):

$$W_{ij} =
\begin{cases}
e^{-\frac{||x_i-x_j||^2}{4t}} & if\ ||x_i-x_j||<\epsilon \\
0 & otherwise
\end{cases}$$

A very vague idea of mine was that the factors $\alpha$ and $\frac{1}{t}$ don''t change for a given $x_i$ so if one choses the weights like above the resulting discrete Laplacian would (let aside those two constants) converge to the continuous version.

Any ideas or tips what I''d have to read up to in order to get a better understanding?


  [1]: http://web.cse.ohio-state.edu/~mbelkin/papers/PLM_UCTHESIS_03.pdf "Belkin''s thesis about Laplacian Eigenmaps"', 6415, '2014-12-13 19:27:56.213', '08d2ff01-3676-422b-b357-a56d560925f4', 3700, 6435, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Choice of weights for the Laplacian Eigenmaps algorithm', 6415, '2014-12-13 19:27:56.213', '08d2ff01-3676-422b-b357-a56d560925f4', 3700, 6436, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 6415, '2014-12-13 19:27:56.213', '08d2ff01-3676-422b-b357-a56d560925f4', 3700, 6437, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I suspected you were using the names as identifiers. You shouldn''t; they''re not unique and they raise this privacy issue. Use instead their student numbers, which you can verify from their IDs, stored in hashed form.', 381, '2014-12-13 20:37:07.843', 'e95fda7d-9e00-42ea-9870-3265799b911c', 3701, 6438, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For my Computational Intelligence class, I''m working on classifying short text. One of the papers that I''ve found makes a lot of use of *granular computing*, but I''m struggling to find a decent explanation of what exactly it is.

From what I can gather from the paper, it sounds to me like granular computing is very similar to fuzzy sets. So, what exactly is the difference. I''m asking about rough sets as well, because I''m curious about them and how they relate to fuzzy sets. If at all.


Edit: [Here](http://ijcai.org/papers11/Papers/IJCAI11-298.pdf) is the paper I''m referencing.', 84, '2014-12-13 21:30:55.703', 'e5d956e4-cea6-4812-8c99-1c5c9a156490', 2349, 'added 4 characters in body', 6439, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Rough vs Fuzzy vs Granular Computing', 84, '2014-12-13 21:30:55.703', 'e5d956e4-cea6-4812-8c99-1c5c9a156490', 2349, 'added 4 characters in body', 6440, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I know there is the normal *subtract the mean and divide by the standard deviation* for standardizing your data, but I''m interested to know if there are more appropriate methods for this kind of discrete data. Consider the following case.

I have 5 items that have been ranked by customers. First 2 items were ranked on a 1-10 scale. Others are 1-100 and 1-5. To transform everything to a 1 to 10 scale, is there another method better suited for this case?

If the data has a central tendency, then the standard would work fine, but what about when you have more of a halo effect, or some more exponential distribution?', 84, '2014-12-13 21:41:34.030', '4a22cce1-40c7-400a-be9a-d482f87563d1', 1240, 'added 30 characters in body; edited title', 6441, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Methods for standardizing different rank scales', 84, '2014-12-13 21:41:34.030', '4a22cce1-40c7-400a-be9a-d482f87563d1', 1240, 'added 30 characters in body; edited title', 6442, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For item-ratings type of data with the restriction that an item''s rating should be between 1 and 10 after transformation, I would suggest using a simple re-scaling, such that the item''s transformed rating $x_t$ is given by:

$$x_t = 9\left(\frac{x_i - x_{min}}{x_{max} - x_{min}}\right) + 1$$

where $x_{min}$ and $x_{max}$ are the minimum and maximum possible rating in the specific scale for the item, and $x_i$ is the item rating.

In the case of the above scaling, the transformation applied is independent of the data. However, in the normalization, the transformation applied is dependent on the data (through mean and standard deviation), and might change as more data becomes available.

Section 4.3 on page 30 of [this document][3] shows other ways of normalizing in which your restriction (transforming to the same absolute scale) might not be preserved.

  [3]: http://www.dai-labor.de/fileadmin/files/publications/DiplomaThesisStephanSpiegel.pdf', 84, '2014-12-13 21:46:55.380', '08d9c1f1-8c61-43bb-a1b2-e9282be02e98', 2360, 'deleted 267 characters in body', 6443, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Methods for standardizing / normalizing different rank scales', 84, '2014-12-13 21:47:23.723', 'f0ea00c8-21a2-4ea4-ae36-e3116f1cc4de', 1240, 'edited title', 6444, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I suspected you were using the names as identifiers. You shouldn''t; they''re not unique and they raise this privacy issue. Use instead their student numbers, which you can verify from their IDs, stored in hashed form. Use the student''s last name as a salt, for good measure (form the string to be hashed by concatenating the ID number and the last name).', 381, '2014-12-13 21:47:49.987', '9c7b0e3d-d7f4-41f4-9836-e1ffeb655590', 3701, 'added 137 characters in body', 6445, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is the k-nearest neighbour algorithm a discriminative or a generative classifier? My first thought on this was that it was generative, because it actually used Bayes''s theorem to compute the posterior. Searching further for this, it seems like it is a discriminative model, but I couldn''t find the explanation.

So is KNN discriminative first of all? And if it is, is that because it doesn''t model the the priors or the likelihood?', 6419, '2014-12-13 23:08:53.930', '41cd7fbb-711e-4faf-a1b3-722059ae4903', 3702, 6446, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('K nearest neighbour', 6419, '2014-12-13 23:08:53.930', '41cd7fbb-711e-4faf-a1b3-722059ae4903', 3702, 6447, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 6419, '2014-12-13 23:08:53.930', '41cd7fbb-711e-4faf-a1b3-722059ae4903', 3702, 6448, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Instead of *exporting* your models, consider creating an R-based **interoperable environment** for your *modeling* needs. Such environment would consists of *R environment* proper as well as *integration layers* for your third-party libraries. In particular, for the *OpenCV* project, consider either using `r-opencv` open source project (https://code.google.com/p/r-opencv), or integration via *OpenCV* C++ APIs and *R* `Rcpp` package (http://dirk.eddelbuettel.com/code/rcpp.html). Finally, if you want to add *PMML* support to the mix and create a **deployable-to-cloud solution**, take a look at the following excellent blog post with relevant examples: http://things-about-r.tumblr.com/post/37861967022/predictive-modeling-using-r-and-the.', 2452, '2014-12-14 03:42:25.213', 'a14391d6-e943-4579-bb16-425c2255febf', 3703, 6449, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to get to grips with sci-kit learn for some simple machine learning projects but I''m coming unstuck with Pipelines and wonder what I''ve done wrong...

I''m trying to work through a [tutorial on Kaggle][1]

Here''s my code:

    > import pandas as pd
    >
    > train = pd.read_csv(local path to training data) train_labels =
    > pd.read_csv(local path to labels)
    >
    >
    > from sklearn.decomposition import PCA from sklearn.svm import
    > LinearSVC from sklearn.grid_search import GridSearchCV
    >
    > pca = PCA() clf = LinearSVC()
    >
    > n_components = arange(1, 39) loss =[''l1'',''l2''] penalty =[''l1'',''l2''] C
    > = arange(0, 1, .1) whiten = [True, False] from sklearn.pipeline import Pipeline
    >
    > #set up pipeline pipe = Pipeline(steps=[(''pca'', pca), (''clf'', clf)])
    >
    >
    >
    > #set up GridsearchCV estimator = GridSearchCV(pipe, dict(pca__n_components = n_components, pca__whiten = whiten,
    >                                     clf__loss = loss, clf__penalty = penalty, clf__C = C)) estimator

Returns:

    GridSearchCV(cv=None,
           estimator=Pipeline(steps=[(''pca'', PCA(copy=True, n_components=None, whiten=False)), (''clf'', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
         intercept_scaling=1, loss=''l2'', multi_class=''ovr'', penalty=''l2'',
         random_state=None, tol=0.0001, verbose=0))]),
           fit_params={}, iid=True, loss_func=None, n_jobs=1,
           param_grid={''clf__penalty'': [''l1'', ''l2''], ''clf__loss'': [''l1'', ''l2''], ''clf__C'': array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9]), ''pca__n_components'': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,
           35, 36, 37, 38]), ''pca__whiten'': [True, False]},
           pre_dispatch=''2*n_jobs'', refit=True, score_func=None, scoring=None,
           verbose=0)

But when I try to train data:

    estimator.fit(train, train_labels)
    The error is:

        428         for test_fold_idx, per_label_splits in enumerate(zip(*per_label_cvs)):
        429             for label, (_, test_split) in zip(unique_labels, per_label_splits):
    --> 430                 label_test_folds = test_folds[y == label]
        431                 # the test split can be too big because we used
        432                 # KFold(max(c, self.n_folds), self.n_folds) instead of

    IndexError: too many indices for array

Can anyone point me in the right direction?


  [1]: http://www.kaggle.com/c/data-science-london-scikit-learn/data
', 974, '2014-12-16 01:19:12.477', 'c98a0896-4f02-4a5c-b865-3200d2f7a325', 3705, 6454, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Sci-kit Pipeline and GridsearchCV returns indexError: too many indices for array', 974, '2014-12-16 01:19:12.477', 'c98a0896-4f02-4a5c-b865-3200d2f7a325', 3705, 6455, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<sklearn>', 974, '2014-12-16 01:19:12.477', 'c98a0896-4f02-4a5c-b865-3200d2f7a325', 3705, 6456, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It turns out that the Pandas dataframe is the wrong shape.

    estimator.fit(train.values, train_labels[0].values)

works, although I also had to drop the penalty term.', 974, '2014-12-16 02:44:28.950', '9f39f657-48a7-40cf-8558-edb05c9a70c5', 3706, 6457, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''s a lot of online tutorial. Especially in youtube, but if you will want accurate website you can see from here http://ttic.uchicago.edu/~shai/icml08tutorial/ or http://cs229.stanford.edu/materials.html. you can visit them now.', 6444, '2014-12-16 06:12:35.423', 'ca23eb89-e248-4d62-9392-0beb9ea0ff8e', 3707, 6458, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can see if you can mix Spark streaming (https://spark.apache.org/docs/1.1.0/streaming-programming-guide.html) and Spark ML Library (https://spark.apache.org/docs/1.1.0/mllib-guide.html).

Spark Streaming permits to process live data streams and Spark ML Library is a Machine Learning Library for Spark. So maybe you can do something good!

But this is a very interesting subject, I am working on it. It can be good to create a Google Community on it (https://plus.google.com/communities)?', 5165, '2014-12-16 08:51:30.987', 'ceec1f94-bfa0-4c80-be89-264d832d856d', 3708, 6459, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the common applications of Data Science techniques used by public transport operators?

So far, what i can think of is:

+ Analyzing passengers'' information (travel time, itinerary, frequency of use, etc.) to better adapt the transport offer to passengers'' needs.
+ Finding patterns in road traffic data for optimizing transport schedule.
+ Predicting a probability of a given line''s tardiness/accidents, based on the data of each vehicle''s technical state  (say, engine temperature, tire pressure, etc.)
+ Analyzing geographical/demographic data of a city/country for predicting each station''s attendance, deciding where to build a new station/line.
+ Offering a better information to passengers in real time?

My vision of the domain is far from being clear, need some guide advice. Thanks.', 6447, '2014-12-16 10:13:07.037', '0189955c-ed33-4541-a63c-56b8c513ad30', 3709, 6460, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data Science use cases for Public Transport domain', 6447, '2014-12-16 10:13:07.037', '0189955c-ed33-4541-a63c-56b8c513ad30', 3709, 6461, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<usecase>', 6447, '2014-12-16 10:13:07.037', '0189955c-ed33-4541-a63c-56b8c513ad30', 3709, 6462, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Naive Bayes apparently handles missing data differently, depending on whether they exist in training or testing/classification instances.

When classifying instances, the attribute with the missing value is simply not included in the probability calculation (http://www.inf.ed.ac.uk/teaching/courses/iaml/slides/naive-2x2.pdf)

In training, "the instance [with the missing data] is not included in frequency count for attribute value-class combination." (http://www.csee.wvu.edu/~timm/cs591o/old/BasicMethods.html)

Does that mean that particular training record simply isn''t included in the training phase? Or does it mean something else?', 6451, '2014-12-16 13:07:55.063', '06a5bfaf-f135-4b9f-a810-78db8fa3684d', 3711, 6466, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How does the naive Bayes classifier handle missing data in training?', 6451, '2014-12-16 13:07:55.063', '06a5bfaf-f135-4b9f-a810-78db8fa3684d', 3711, 6467, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><classification>', 6451, '2014-12-16 13:07:55.063', '06a5bfaf-f135-4b9f-a810-78db8fa3684d', 3711, 6468, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('See the answer here [here][1]. To clarify, k nearest neighbor is a discriminative classifier.

The difference between a generative and a discriminative classifier is that the former models the joint probability where as the latter models the conditional probability (the posterior) starting from the prior.

In the case of nearest neighbors, the conditional probability of a class given a data point is modeled. To do this, one starts with the prior probability on the classes.

  [1]: http://stats.stackexchange.com/questions/105979/is-knn-a-discriminative-learning-algorithm', 847, '2014-12-17 02:58:06.057', '0ae9a702-5103-43fc-b82d-1110d9c3a7b6', 3713, 6470, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This seems to be a standard regression problem in which there are two goals:

 1. Obtain a predictive model that can be used for prediction.
 2. Which variables seem to be the most important ones to be used.

For both the above problems use an ensemble model. Consider both a random forest and a gradient boosted machine. Both these models will use the independent variables and predict the Hospital time. Additionally, through variable importances, you can obtain which variables are the most important ones and have the most impact in predicting the output. ', 847, '2014-12-17 09:07:11.320', '90df0511-1d52-42ac-8b41-b945f9957389', 3715, 6474, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In the way that you''ve defined or set up the problem, i.e.

    sales = alpha*quality + beta*position + epsilon

We can easily quantify `beta` given that your model is correct. You just need to run it through linear regression and it will give you the coefficient for `beta`*.

If you would like to model click through rates, you would have to train a classifier. So you would have to fit a logistic model that models:

    clicks ~ alpha*quality + beta*position + epsilon

*I believe you would have to restrict the training set to contain results where all impressions were obtained on the first page otherwise your model will not hold (I would guess that `beta` is going to be strongly dependent on the page). ', 847, '2014-12-17 09:32:53.703', '668f85fc-9eef-4632-8b74-96ae91176c6f', 3716, 6475, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('See a similar answer [here][1]. To clarify, k nearest neighbor is a discriminative classifier.

The difference between a generative and a discriminative classifier is that the former models the joint probability where as the latter models the conditional probability (the posterior) starting from the prior.

In the case of nearest neighbors, the conditional probability of a class given a data point is modeled. To do this, one starts with the prior probability on the classes.

  [1]: http://stats.stackexchange.com/questions/105979/is-knn-a-discriminative-learning-algorithm', 847, '2014-12-17 09:34:11.103', 'ae4957c8-130a-4b46-b6fa-7a7d484fc44d', 3713, 'added 1 character in body', 6476, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In general, you have a choice when handling missing values hen training a naive Bayes classifier. You can either choose to either

 1. Omit records with any missing values,
 2. Omit only the missing attributes.

I''ll use the example linked to above to demonstrate these two approaches.  Suppose we add one more training record to that example.

    Outlook  Temperature  Humidity   Windy   Play
    -------  -----------  --------   -----   ----
    rainy    cool        normal    TRUE    no
    rainy    mild        high      TRUE    no
    sunny    hot         high      FALSE   no
    sunny    hot         high      TRUE    no
    sunny    mild        high      FALSE   no
    overcast cool        normal    TRUE    yes
    overcast hot         high      FALSE   yes
    overcast hot         normal    FALSE   yes
    overcast mild        high      TRUE    yes
    rainy    cool        normal    FALSE   yes
    rainy    mild        high      FALSE   yes
    rainy    mild        normal    FALSE   yes
    sunny    cool        normal    FALSE   yes
    sunny    mild        normal    TRUE    yes
    NA       hot         normal    FALSE   yes

 1. If we decide to omit the last record due to the missing `outlook` value, we would have the exact same trained model as discussed in the link.

 2. We could also choose to use all of the information available from this record.  We could choose to simply omit the attribute `outlook` from this record. This would yield the following updated table.

<pre>
           Outlook            Temperature           Humidity
====================   =================   =================
          Yes    No            Yes   No            Yes    No
Sunny       2     3     Hot     3     2    High      3     4
Overcast    4     0     Mild    4     2    Normal    7     1
Rainy       3     2     Cool    3     1
          -----------         ---------            ----------
Sunny     2/9   3/5     Hot   3/10   2/5    High    3/10   4/5
Overcast  4/9   0/5     Mild  4/10   2/5    Normal  7/10   1/5
Rainy     3/9   2/5     Cool  3/10   1/5


            Windy        Play
=================    ========
      Yes     No     Yes   No
False 7      2       10     5
True  3      3
      ----------   ----------
False  7/10    2/5   10/15  5/15
True   3/10    3/5
</pre>

Notice there are 15 observations for each attribute **except** `Outlook`, which has only 14. This is since that value was unavailable for the last record. All further development would continue as discussed in the linked article.

For example in the R package `e1071` naiveBayes implementation has the option `na.action` which can be set to na.omit or na.pass.
    ', 4724, '2014-12-17 18:15:42.133', '65a27cbd-e13d-402f-b9b0-bd58fc2b37a2', 3717, 6484, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m looking to graph and interactively explore live/continuously measured data. There are quite a few options out there, plot.ly being the most userfriendly. Plot.ly has a fantastic and easy to use UI (easily scalable, pannable, easily zoomable/fit to screen) but cannot handle the large sets of data i''m collecting. Does anyone know of any alternatives?

I have matlab but don''t have enough licenses to simultaneously run this and do development at the same time. I know LABView would be a great option but it is currently cost-prohibitive.

Thanks in advanced!', 6469, '2014-12-17 21:17:13.340', 'b5576f68-7b16-4633-bf80-dd4d9b3ce566', 3718, 6485, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Interactive Graphing while logging data', 6469, '2014-12-17 21:17:13.340', 'b5576f68-7b16-4633-bf80-dd4d9b3ce566', 3718, 6486, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><data-visualization>', 6469, '2014-12-17 21:17:13.340', 'b5576f68-7b16-4633-bf80-dd4d9b3ce566', 3718, 6487, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m looking to graph and interactively explore live/continuously measured data. There are quite a few options out there, with plot.ly being the most user-friendly. Plot.ly has a fantastic and easy to use UI (easily scalable, pannable, easily zoomable/fit to screen), but cannot handle the large sets of data I''m collecting. Does anyone know of any alternatives?

I have MATLAB, but don''t have enough licenses to simultaneously run this and do development at the same time. I know that LabVIEW would be a great option, but it is currently cost-prohibitive.

Thanks in advance!', 2452, '2014-12-18 02:39:23.860', '9e73fe50-1136-4d75-81ed-05de45ab1948', 3718, 'Fixed spelling and grammar.', 6488, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am working on a data-science project related on social relationship mining and need to store data in some graph databases. Initially I chose Neo4j as the database.  But it seams Neo4j doesn''t scale well. The alternative I found out are Titan  and oriebtDB. So Could some one help me in choosing the best one. Mainly I would like to compare performance, scaling, on line documentation/tutorials available, Python library support, query language complexity and graph algorithm support. Also is there any other good database options ?', 5091, '2014-12-18 04:36:06.107', 'f6c1d605-955f-4946-9090-9a9dac1147c0', 3719, 6491, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Neo4j vs OrientDB vs Titan', 5091, '2014-12-18 04:36:06.107', 'f6c1d605-955f-4946-9090-9a9dac1147c0', 3719, 6492, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><graphs><databases><social-network-analysis>', 5091, '2014-12-18 04:36:06.107', 'f6c1d605-955f-4946-9090-9a9dac1147c0', 3719, 6493, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Visualizing large datasets is a long standing problem. One of the issues is to understand how we can show over a million points on a screen that has only about ~ million pixels.

Having said that, here are a few tools that can handle big data:

1. Tableau: you could use their free desktop tool.
2. Tabplot: built on top of ggplot2 in R to handle larger datasets.
3. See [this][1] review for 5 other products that can help you do your job.


  [1]: http://www.techrepublic.com/blog/five-apps/five-web-based-apps-to-help-you-visualize-big-data/', 847, '2014-12-18 04:41:33.493', 'b4324d81-285f-46e5-9cf2-a64066747c45', 3720, 6494, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am working on a data-science project related on social relationship mining and need to store data in some graph databases. Initially I chose Neo4j as the database.  But it seams Neo4j doesn''t scale well. The alternative I found out are Titan  and oriebtDB. I have gone through [this][1] comparison on these three Databases. But along technical aspect I would also like to get more details on some ''hands on'' experience with these databases. So Could some one help me in choosing the best one. Mainly I would like to compare performance, scaling, on line documentation/tutorials available, Python library support, query language complexity and graph algorithm support. Also is there any other good database options ?


  [1]: http://db-engines.com/en/system/Neo4j%3BOrientDB%3BTitan', 5091, '2014-12-18 04:48:15.780', '8b6e7e0e-0e5b-481b-98c9-38ab1fba4be0', 3719, 'added 198 characters in body', 6495, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('With increasingly sophisticated methods that work on large scale datasets, financial applications are obvious. I am aware of machine learning being employed on financial services to detect fraud and flag fraudulent activities but I have a lesser understanding of how it helps to predict the price of the stock the next day and how many stocks of a particular company to buy.

Do the hedge funds still employ portfolio optimization techniques that are right out of the mathematical finance literature or have they started to use machine learning to hedge their bets? More importantly, what are the features that are used by these hedge funds and what is a representative problem set up? ', 847, '2014-12-18 04:48:49.820', '332edf75-6bbf-4fb7-86c2-267d32474286', 3721, 6496, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine Learning for hedging/ portfolio optimization?', 847, '2014-12-18 04:48:49.820', '332edf75-6bbf-4fb7-86c2-267d32474286', 3721, 6497, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><feature-selection><optimization>', 847, '2014-12-18 04:48:49.820', '332edf75-6bbf-4fb7-86c2-267d32474286', 3721, 6498, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('That is a rather broad question, and there is tons of literature about quantitative analysis and stock market prediction using machine learning.

The most classical example of predicting the stock market is employing neural networks; you can use whatever feature you think might be relevant for your prediction, for example the unemployment rate, the oil price, the gold price, the interest rates, and the timeseries itself, i. e. the volatility, the change in the last 2,3,7,..., days etc. - a more classical approach is the input-output-analysis in econometrics, or the autoregression analysis, but all of it can be modeled using neural networks or any other function approximator / regression in a very natural way.

But, as said, there are tons of other possibilities to model the market, to name a few: Ant Colony Optimization (ACO), Classical regression analysis, genetic algorithms, etc. you name it, almost EVERYTHING has probably been applied to the stock market prediction problem.

There are different fond manager types on the markets. There are still the Quants which are doing a quantitative analysis using classical financial maths and maths borrowed from the physics to describe the market movements. There are still the most conservative ones which do a long-term, fundamental analysis of the corporation, that is, looking in how the corporation earns money and where it spends money. Or the tactical analysts who just look for immediate signals to buy / sell a stock in the short term.', 3132, '2014-12-18 08:10:00.630', 'd468802d-a14c-411e-86f2-f981425194a7', 3722, 6499, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('That is a rather broad question, and there is tons of literature about quantitative analysis and stock market prediction using machine learning.

The most classical example of predicting the stock market is employing neural networks; you can use whatever feature you think might be relevant for your prediction, for example the unemployment rate, the oil price, the gold price, the interest rates, and the timeseries itself, i. e. the volatility, the change in the last 2,3,7,..., days etc. - a more classical approach is the input-output-analysis in econometrics, or the autoregression analysis, but all of it can be modeled using neural networks or any other function approximator / regression in a very natural way.

But, as said, there are tons of other possibilities to model the market, to name a few: Ant Colony Optimization (ACO), Classical regression analysis, genetic algorithms, decision trees, reinforcement learning etc. you name it, almost EVERYTHING has probably been applied to the stock market prediction problem.

There are different fond manager types on the markets. There are still the Quants which are doing a quantitative analysis using classical financial maths and maths borrowed from the physics to describe the market movements. There are still the most conservative ones which do a long-term, fundamental analysis of the corporation, that is, looking in how the corporation earns money and where it spends money. Or the tactical analysts who just look for immediate signals to buy / sell a stock in the short term. And those quantitative guys who employ machine learning amongst other methods.', 3132, '2014-12-18 08:25:09.517', 'ecf01ff8-a4d0-44c2-8cf3-871ddcea1c8d', 3722, 'added 118 characters in body', 6500, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For this answer, I have assumed that you prefer **open source solutions** to *big data visualization*. This assumption is based on budgetary details from your question. However, there is one *exclusion* to this - below I will add a reference to one commercial product, which I believe might be beneficial in your case (provided that you could afford that). I also assume that *browser-based solutions are acceptable* (I would even prefer them, unless you have specific contradictory requirements).

Naturally, the first candidate as a solution to your problem I would consider **D3.js JavaScript library**: http://d3js.org. However, despite *flexibility* and other *benefits*, I think that this solution is *too low-level*.

Therefore, I would recommend you to take a look at the following **open source projects for big data visualization**, which are *powerful* and *flexible* enough, but operate at a *higher level of abstraction* (some of them are based on D3.js foundation and sometimes are referred to as D3.js [visualization stack](http://schoolofdata.org/2013/08/12/climbing-the-d3-js-visualisation-stack)).

- **Bokeh** - Python-based interactive visualization library, which supports big data and streaming data: http://bokeh.pydata.org
- **Flot** - JavaScript-based interactive visualization library, focused on jQuery: http://www.flotcharts.org
- **NodeBox** - unique rapid data visualization system (not browser-based, but multi-language and multi-platform), based on generative design and visual functional programming: https://www.nodebox.net
- **Processing** - complete software development system with its own programming language, libraries, plug-ins, etc., oriented to visual content: https://www.processing.org (allows executing Processing programs in a browser via http://processingjs.org)
- **Crossfilter** - JavaScript-based interactive visualization library for big data by Square (very fast visualization of large multivariate data sets): http://square.github.io/crossfilter
- **bigvis** - an R package for big data exploratory analysis (not a visualization library per se, but could be useful to process large data sets /aggregating, smoothing/ prior to visualization, using various R graphics options): https://github.com/hadley/bigvis
- **prefuse** - Java-based interactive visualization library: http://prefuse.org
- **Lumify** - big data integration, analysis and visualization platform (interesting feature: supports Semantic Web): http://lumify.io

Separately, I''d like to mention two open source *big data analysis and visualization projects*, focused on **graph/network data** (with some support for *streaming data* of that type): [Cytoscape](http://www.cytoscape.org) and [Gephi](https://gephi.github.io). If you are interested in some other, *more specific* (*maps* support, etc.) or *commercial* (basic free tiers), projects and products, please see this **awesome compilation**, which I thoroughly *curated* to come up with the main list above and *analyzed*: http://blog.profitbricks.com/39-data-visualization-tools-for-big-data.

Finally, as I promised in the beginning, **Zoomdata** - a commercial product, which I thought you might want to take a look at: http://www.zoomdata.com. The reason I made an exclusion for it from my open source software compilation is due to its **built-in support for big data platforms**. In particular, Zoomdata provides *data connectors* for Cloudera Impala, Amazon Redshift, MongoDB, Spark and Hadoop, plus search engines, major database engines and streaming data.

**Disclaimer:** I have no affiliation with Zoomdata whatsoever - I was just impressed by their *range of connectivity options* (which might **cost** you dearly, but that''s another *aspect* of this topic''s analysis).', 2452, '2014-12-18 13:30:54.210', 'e0b64ea4-234c-464b-b169-267a34a0aaca', 3723, 6501, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('ScalaNLP is a suite of machine learning and numerical computing libraries with support for common natural language processing tasks. [http://www.scalanlp.org/][1]


  [1]: http://www.scalanlp.org/', 6478, '2014-12-18 18:07:18.170', '09e96584-f23c-4ada-bc6e-6e83b354b614', 3724, 6502, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Recall the definition he makes for the graph Laplacian earlier, $L = D -W $. Now consider the map in the RHS parentheses which I''ll call $L^*$,
$$ L^*f(x_i) := f(x_i) - \alpha \sum_{x_j, ||x_i-x_j||<\epsilon}e^{-\frac{||x_i-x_j||^2}{4t}}f(x_j).$$
The suggested weight matrix definition is natural because it lets us write
$$ L^* := I - D^{-1}W. $$ Here''s [a reference] [1] to a related paper with some easy to read exposition. Hope this helps!


   [1]: http://www.cs.yale.edu/homes/singer/publications/laplacian_ACHA.pdf', 4724, '2014-12-19 01:20:39.907', '4de48760-7164-45d6-a1bc-791679eca946', 3725, 6504, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is the fundamental challenge to all data modeling.  We don''t just want to memorize the the link between a given input and a given output (otherwise you wouldn''t be modeling data, you''d be memorizing 1:1 connections with a dict / hash / relational database table / etc).  We want to capture the underlying pattern in the data from only looking at the training data.

Let''s expand a little on your gravity example.  You have your 10 training samples showing the start and ending position of an object dropped.  For consistency, let''s say the object was dropped the moment the object''s location was initially recorded and the ending location was recorded at some precise time interval later (but before the object hit the ground).  Let''s also say the model (neural network in this case) managed to precisely learn the expected change in location since it just comes down to subtraction in one axis.  You can show it another 10, 100, 1000 examples that all leverage the connection found and your model will keep performing well.

Why not keep going to 10k, 100k, or even more samples?  Theoretically, if you managed to isolate the connection and run the experiment the same way each time, your model will always work.  But realistically, something is going to eventually change in the system.  You hire a new lab assistant who tends to press the ''record location'' button well after having dropped the object (giving the object more initial velocity, which you won''t notice having only recorded location).  Maybe you lost your initial ball and had to use something else which is lighter and catches the wind more (so it goes slower).  .... the longer you run the experiment, the more small changes will creep into your system.  Eventually these changes will alter the connection enough to make your initial model wrong.


When modeling data, we want to capture the underlying patterns and acknowledge that the model only matters as long as those underlying patterns stay relevant.  It''s not really about the number of samples.  It''s about the connections / the model itself.  The number of samples just happens to be one of the better proxies we have - the more samples you use, the more confident you have some underlying pattern.  ''Statistical validity'' is one stab at solving this, though it''s validity is still up for question in the era of big data.  There is [plenty of work](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5726612&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5726612) done trying to solve for how to gain confidence in good generalization in neural networks specifically, but it''s still very much an open question.


For a different example, if you''re looking at user behavior, you''ll see differences between day and night; weekdays and weekends; summer and winter; year of a person''s life; culture a person grew up in... even if you prove you found a pattern in your initial sample, the system will eventually change and it''s up to luck whether the connection(s) you found are a part of the system that changed or a part of the system that didn''t.
', 6391, '2014-12-19 01:42:36.780', 'afde28c0-9af2-41ab-8e8d-893f4d1892cc', 3726, 6505, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":381,"DisplayName":"Emre"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-12-19 09:40:10.057', '0cfd7747-591e-44ff-b6c4-04913c2d47fa', 2343, '104', 6507, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was wondering if anyone knew which piece of software is being used in this video? It is an image recognition system that makes the training process very simple.

http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn#t-775098

The example is with car images, though the video should start at the right spot.
', 5175, '2014-12-19 11:42:04.547', '5a9a43b9-4ed7-46bf-afa1-834101e330f1', 3728, 6509, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What software is being used in this image recognition system?', 5175, '2014-12-19 11:42:04.547', '5a9a43b9-4ed7-46bf-afa1-834101e330f1', 3728, 6510, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification>', 5175, '2014-12-19 11:42:04.547', '5a9a43b9-4ed7-46bf-afa1-834101e330f1', 3728, 6511, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m pretty sure that the software you''re referring to is a some kind of **internal** *research project* software, developed by Enlitic (http://www.enlitic.com), where Jeremy Howard works as a founder and CEO. By "internal research project software" I mean either a *proof-of-concept* software, or a *prototype* software.', 2452, '2014-12-19 16:02:06.330', '2f6b07b2-88fc-490c-b1d2-89b888e65369', 3729, 6512, '2');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-12-19 16:44:35.113', 'f469a22b-a978-493d-8395-789019aa4ca2', 3730, 'from http://stats.stackexchange.com/questions/129759/r-random-forest-on-amazon-ec2-error-cannot-allocate-vector-of-size-5-4-gb', 6513, '36');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-12-19 16:44:35.113', '84e1c562-3c3b-4cdf-8d67-50c4f60e94b4', 3731, 'from http://stats.stackexchange.com/questions/129759/r-random-forest-on-amazon-ec2-error-cannot-allocate-vector-of-size-5-4-gb/129764#129764', 6514, '36');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here''s some advice (use at your own risk!):

- make sure that your R environment on EC2 is identical to the one on your laptop
- make sure you''re using 64-bit images for your virtual machine instance
- try to create/enable swap space: http://stackoverflow.com/a/22247782/2872891
- see this discussion: http://stackoverflow.com/q/5171593/2872891
- see this discussion: http://stackoverflow.com/q/12322959/2872891
- see this discussion: http://stackoverflow.com/q/1358003/2872891

If the above-mentioned simpler measures don''t help OR you want to achieve more **scalability** and/or **performance**, including an ability to *parallelize* the process on a single machine or across multiple machines, consider using `bigrf` R package: http://cran.r-project.org/web/packages/bigrf. Also see this discussion: http://stackoverflow.com/q/1358003/2872891.', 2452, '2014-12-19 16:33:44.423', '3c8c2bae-2afa-492a-b76e-c405bdc3c0c6', 3731, 6515, '2');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('SOUser', 'I''m training random forest models in R using randomForest() with 1000 trees and data.frames with about 20 predictors and 600K rows. On my laptop everything works fine but when I move to amazon ec2 to run the same thing i get:

    Error: cannot allocate vector of size 5.4 Gb
    Execution halted

I''m using the c3.4xlarge instance type so it''s pretty beefy... does anyone know a workaround for this to get it to run on this instance? I would love to know the memory nuances that causes this problem only on the ec2 instance and not on my laptop (OS X 10.9.5 Processor  2.7 GHz Intel Core i7; Memory  16 GB 1600 MHz DDR3)

Thanks!', '2014-12-19 16:02:48.693', 'fe4d3eed-b61f-4fb0-a204-be5b3ba13d40', 3730, 6516, '2');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('SOUser', 'R random forest on Amazon ec2 Error: cannot allocate vector of size 5.4 Gb', '2014-12-19 16:02:48.693', 'fe4d3eed-b61f-4fb0-a204-be5b3ba13d40', 3730, 6517, '1');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('SOUser', '<r><random-forest>', '2014-12-19 16:02:48.693', 'fe4d3eed-b61f-4fb0-a204-be5b3ba13d40', 3730, 6518, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Additional to other ideas: reduce your data until you figure out what you **can** run on the Amazon instance. If it can''t do 100k rows then something is very wrong, if it fails at 590k rows then its marginal.

The c3.4xlarge instance has 30Gb of RAM, so yes it should be enough.', 471, '2014-12-19 23:24:55.017', 'a199d43c-c6b5-434a-91bf-f6a892220231', 3732, 6519, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a non-function (not in closed form) that takes in a few parameters (about 20) and returns a real value. A few of these parameters are discrete while others are continuous. Some of these parameters can only be chosen from a finite space of values.

Since I don''t have the function in closed form, I cannot use any gradient based methods. However, the discrete nature and the boxed constraints on a few of those parameters restrict even the number of derivative free optimization techniques at my disposal. I am wondering what are the options in terms of optimization methods that I can use. ', 847, '2014-12-20 03:37:21.820', '2cbf95dc-ed5b-4641-a605-bfe1208e2589', 3733, 6522, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which Optimization method to use?', 847, '2014-12-20 03:37:21.820', '2cbf95dc-ed5b-4641-a605-bfe1208e2589', 3733, 6523, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<optimization>', 847, '2014-12-20 03:37:21.820', '2cbf95dc-ed5b-4641-a605-bfe1208e2589', 3733, 6524, '3');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-12-20 13:11:17.463', 'c9988a23-397a-44ef-a653-cb83787c62fb', 3734, 'from http://stats.stackexchange.com/questions/129838/approaches-to-high-dimension-pattern-matching-problem', 6525, '36');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2014-12-20 13:11:17.463', '43be8d6f-5817-47a2-8bd5-69e2a723ffaf', 3735, 'from http://stats.stackexchange.com/questions/129838/approaches-to-high-dimension-pattern-matching-problem/129841#129841', 6526, '36');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('According to the following discussion on *StackOverflow*, a situation like that you''ve described can occur, when one of the variables in a data set is of **unexpected type** (for example, a `factor` instead of a `character`): http://stackoverflow.com/q/7246412/2872891.

Also, consider using package `bigmemory`, recommended in the accepted answer, or similar packages for *big data* analysis. For the latter, please see section **"Large memory and out-of-memory data"** in *CRAN Task View* [High-Performance and Parallel Computing with R](http://cran.r-project.org/web/views/HighPerformanceComputing.html).

Finally, an additional note. There is an **ecosystem** of R packages, built around the `arules` package, which includes supporting packages for algorithms (`arulesNBMiner`), applications (`arulesSequences`, `arulesClassify`) and visualization (`arulesViz`). You are likely aware of [that](http://www.jmlr.org/papers/volume12/hahsler11a/hahsler11a.pdf), but I have decided to include this good-to-know fact just in case and for the sake of completeness.', 2452, '2014-12-20 08:06:13.077', 'cefd42a7-2c72-4fa7-895b-0ec6992d7cbf', 3735, 6527, '2');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My apologies in advance as I am new to this. I have searched the internet and tried various processes and nothing seems to work or address this situation.

I have a dataset of 30,000 transactions and 500,000 items. Average item size for a transaction is 50. The dataset is sparse, so the support number must be set quite low. Furthermore, the rules become more valuable the larger the number of items in the rule.

I have tried running this in arules and the tests fail after exceeding 64 gb of RAM (the limit of the machine). I have tried reducing items and transactions to smaller subsets, but still hit this memory limit.

Ultimately, I am looking for ways to cluster large groups of similar accounts by selection of items and generate confidence and lift of various next items selected from those clusters.

My question: are there alternative, more efficient ways to do this, or other approaches to consider?

Thank you.', 'Chris', 6506, '2014-12-20 07:03:57.837', 'fd27be5e-0fcb-4e35-a502-fbb66292d2f0', 3734, 6528, '2');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Approaches to high dimension pattern matching problem', 'Chris', 6506, '2014-12-20 07:03:57.837', 'fd27be5e-0fcb-4e35-a502-fbb66292d2f0', 3734, 6529, '1');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r>', 'Chris', 6506, '2014-12-20 07:03:57.837', 'fd27be5e-0fcb-4e35-a502-fbb66292d2f0', 3734, 6530, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Bayesian optimization](http://en.wikipedia.org/wiki/Bayesian_optimization) is a principled way of sequentially finding the extremum of black-box functions. What''s more, there a numerous software packages that make it easy, such as [BayesOpt](http://rmcantin.bitbucket.org/html/) and [MOE](http://engineeringblog.yelp.com/2014/07/introducing-moe-metric-optimization-engine-a-new-open-source-machine-learning-service-for-optimal-ex.html). Another flexible Bayesian framework that you can use for optimization is [Gaussian processes](http://en.wikipedia.org/wiki/Gaussian_process): [Global Optimisation with Gaussian Processes](http://ml.dcs.shef.ac.uk/gpss/gpss13/talks/Sheffield-GPSS2013-Osborne.pdf)', 381, '2014-12-21 00:40:12.337', 'ef0f62f8-2b7b-4f1b-8afb-f9f9037e13d1', 3736, 6531, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is a **tricky** question, because it''s easy and difficult at the same time. *Easy*, because there is a lot of resources that potentially can help you make a decision on the topic. *Difficult*, because the situation is very different for a particular person (not to mention that their interest might change at any time), which makes extremely difficult for other people to give you a **good** advice and for you to make the **right** decision.

As for **data science career** options, you can certainly consider a **degree path** (MS or MS + PhD), but you need to be aware of **other options**. For a comprehensive resource, dedicated to **data science and related degree programs** (both *traditional* and *online*), please visit this page: http://www.kdnuggets.com/education/index.html. A **comprehensive** review of **all** these offerings is IMHO an enormous task and is **far** beyond an answer here on Stack Exchange or, even, a lengthy blog post.

However, nowadays one is not limited to *traditional educational options* and I think it''s important to be aware of **other educational options**. One of the other options include **certifications** (linked at the above-mentioned page, but, in my opinion, the only certification worth considering is the [Certified Analytics Professional](https://www.informs.org/Certification-Continuing-Ed/Analytics-Certification) as a *solid* and *vendor-neutral* certification from a reputable INFORMS). Another option is recently booming **data science intensive educational offerings**, from *short-term* (and often too commercial, to put it lightly) *bootcamps* to *more solid offerings*, including free, but competitive, ones, such as [Insight Data Science Fellows Program](http://insightdatascience.com), where one needs to be a PhD to apply, or its sister program [Insight Data Engineering Fellows Program](http://www.insightdataengineering.com), which doesn''t have such requirement. Finally, there is yet another option: **self-study**. It partially intersects with the certificate option, if one uses *massive open online courses (MOOC)* (a review of which deserves a separate comprehensive post), but there are **open curricula** that might suit one better, such as the *Open Source Data Science Masters* curriculum, linked in my [earlier relevant answer](http://datascience.stackexchange.com/a/742/2452).

**P.S.** While your answer focused on data science, I think that it may be wise to at least consider another career path, given your *math background*. I''m talking about **operations research** field, which is not that far away from data science (and even somewhat intersects with it). While similar, data science is IMHO more statistics-focused, whereas operations research is more math-focused, at least that''s how I see it. Despite all the popularity and "gold rush" of data science, operations research career is a solid one, just not as hot. Of course, if you''re excited about things like artificial intelligence, machine learning and, especially, deep learning, data science career is the way to go. Whatever you will choose, the good thing is that with your math background it will be easy to change focus, should you decide to. Hope this helps. Good luck!', 2452, '2014-12-21 11:43:35.100', '107e7637-f04e-45a9-a7bd-28389a9ee88c', 3737, 6532, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First off, I''m quite new to data science: I don''t have any real grounding in statistics, or quite often in what might seem like obvious terms or steps in carrying out work on a data set. I''ve picked up what I know over the course of the last year or so largely from academic papers and sites like this, but I''ve not found anything that answers one area I''m really unsure about:

I''m working on a time series data set of energy meter readings. The length of the series varies by meter - for some I have several years, others only a few months, etc. Many display significant seasonality, and often multiple layers - higher use at certain times of day for instance, or at certain points in the week (weekend vs. weekday, usually), or over the course of the year.

To give some background, one of the things I''ve been working on is clustering of these time series. I did some initial work where I calculated various features (percentage used on weekends vs. weekday, percentage used in different time blocks, etc.). I then moved on to looking at using DTW to obtain the distance, and clustering based on that.

My concern is this: the time series can change in seasonality, especially in the longer series where I have years'' worth of readings for a meter. Some cases might be quite extreme (like a meter no longer consuming energy), but other cases could simply be a meter showing a different day/night pattern or a move towards consuming in the same pattern all week instead of showing a regular pattern on weekdays followed by a couple of low, "flat" days on the weekend. If I''m clustering in either of the above ways, it strikes me that I could end up with some incorrect distances, and with time series which behave very differently from one another clustered together.

How do I avoid or deal with this? I''ve been wondering whether I need to split the series where things like this happen, but I''m not sure how I''d detect instances of it in order to do so (the data set is too big to do that manually), and I''m honestly just not sure if that''s a suitable thing to do, anyway.

Apologies if this is a silly question and easily answered via some online resource, I''m struggling to find a way to search for any guides or academic papers which discuss the topic.


----------


P.S. - I can''t post in Meta or even a comment yet, so sorry for the off-topic bit, but: I love this site and hope it picks up steam! I was so happy when I found it, I immediately found topics which weren''t covered at all or as helpfully at SO and CV. I feel a lot more comfortable asking this question here because it''s not a pure statistics question or a programming question. I would feel like I was asking something potentially off-topic at either of those sites, and I feel like I''m more likely to get an answer which really covers my question and which I can understand by asking here.', 5246, '2014-12-22 03:30:45.673', '5ae23cae-9f42-42d8-9ddd-77e1ba169821', 3738, 6535, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to deal with time series which change in seasonality or other patterns?', 5246, '2014-12-22 03:30:45.673', '5ae23cae-9f42-42d8-9ddd-77e1ba169821', 3738, 6536, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><clustering><time-series><beginner>', 5246, '2014-12-22 03:30:45.673', '5ae23cae-9f42-42d8-9ddd-77e1ba169821', 3738, 6537, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you want to just mine for seasonal patterns, then look into [autocorrelation](http://en.wikipedia.org/wiki/Autocorrelation).  If you''re looking for a model that can learn seasonal patterns and make forecasts from it, then [Holt-Winters](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/HoltWinters.html) is a good start, and [ARIMA](http://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) would be a good thing to follow up with.  [Here](http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/)[[pdf]](https://media.readthedocs.org/pdf/a-little-book-of-r-for-time-series/latest/a-little-book-of-r-for-time-series.pdf) is the tutorial that got me off the ground.', 6391, '2014-12-22 05:14:16.130', '1f4f145b-eb08-4f46-99f2-6acbbca822eb', 3739, 6538, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am performing document (text) classification on the category of websites, and use the website content (tokenized, stemmed and lowercased).

My problem is that I have an over-represented category which has vastly more data points than any other (roughly 70% or 4000~ of my data points are of his one category, while about 20 other categories make up the last 30%, some of which have fewer than 50 data points).

**My first question:**

What could I do to improve the accuracy of my classifier in this case of sparse data for some of the labels? Should I simply discard a certain proportion of the data points in the category which is over-represented? Should I use something other than Gaussian Naive Bayes with tf-idf?

**My second question:**

After I perform the classification, I save the tfidf vector as well as the classifier to disk. However, when I re-rerun the classification on the same data, I sometimes get different results from what I initially got (for example, if previously a data point was classified as "Entertainment", it might receive "News" now). Is this indicative of an error in my implementation, or expected?', 5199, '2014-12-22 09:31:31.747', '4516b419-dd89-408f-b401-3976a0e87ef7', 3740, 6539, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Improving Naive Bayes accuracy for text classification', 5199, '2014-12-22 09:31:31.747', '4516b419-dd89-408f-b401-3976a0e87ef7', 3740, 6540, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><accuracy>', 5199, '2014-12-22 09:31:31.747', '4516b419-dd89-408f-b401-3976a0e87ef7', 3740, 6541, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m now trying to figure out a strange phenomenon, when I use matrix factorization(the Netflix Prize solution) in a rating matrix.

R = P^T * Q + B_u + B_i

It''s rating range from 1 to 10.

Then I evaluate the model by each label''s absolute mean average error in test set, the first column is origin_score, the second(we don''t transform the data, then train and its  prediction error), the third(we transform the data all by dividing 2, train, and when I use this model to make prediction, firstly reconstruct the matrix and then just multiply 2 and make it back to the same scale)

As you see, in grade 3-4 (most samples are label from 3-4), it''s more precise while in high score range(like 9 and 10, just 2% of the whole traiing set), it''s worse.

````
+----------------------+--------------------+--------------------+
| rounded_origin_score | abs_mean_avg_error | abs_mean_avg_error  |
+----------------------+--------------------+---------------------+
| 1.0                  | 2.185225396100167  |  2.559125413626183  |
| 2.0                  | 1.4072212825108161 |  1.5290497332538155 |
| 3.0                  | 0.7606073396581479 |  0.6285151230269825 |
| 4.0                  | 0.7823491986435621 |  0.6419077576969795 |
| 5.0                  | 1.2734369551159568 |  1.256590210555053  |
| 6.0                  | 1.9546560495715863 |  2.0461809588933835 |
| 7.0                  | 2.707229888048017  |  2.8866856489147494 |
| 8.0                  | 3.5084244741417137 |  3.7212155956153796 |
| 9.0                  | 4.357185793060213  |  4.590550124054919  |
| 10.0                 | 5.180752400467891  |  5.468600926567884  |
+----------------------+--------------------+---------------------+
````

I''ve re-train the model several times, and got same result, so I think it''s not effect by randomness.', 1048, '2014-12-22 13:04:38.577', 'a5dda33d-0c53-45ec-864d-74fc7bc8a4f3', 3741, 6544, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Stochastic gradient descent in matrix factorization, sensitive to label''s scale?', 1048, '2014-12-22 13:04:38.577', 'a5dda33d-0c53-45ec-864d-74fc7bc8a4f3', 3741, 6545, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<gradient-descent>', 1048, '2014-12-22 13:04:38.577', 'a5dda33d-0c53-45ec-864d-74fc7bc8a4f3', 3741, 6546, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve been working in SAS for a few years but as my time as a student with a no-cost-to-me license comes to an end, I want to learn R.

Is it possible to transpose a data set so that all the observations for a single ID are on the same line?  (I have 2-8 observations per unique individual but they are currently arranged vertically rather than horizontally.)  In SAS, I had been using PROC SQL and PROC TRANSPOSE depending on my analysis aims.', 6491, '2014-12-22 14:06:45.610', '8e42d9fa-088d-4f5c-948f-c667a1b1e18a', 3742, 6547, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data transposition code in R', 6491, '2014-12-22 14:06:45.610', '8e42d9fa-088d-4f5c-948f-c667a1b1e18a', 3742, 6548, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><r><dataset>', 6491, '2014-12-22 14:06:45.610', '8e42d9fa-088d-4f5c-948f-c667a1b1e18a', 3742, 6549, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m trying to figure out a strange phenomenon, when I use matrix factorization (the Netflix Prize solution) for a rating matrix:

$R = P^T * Q + B_u + B_i$

with ratings ranging from 1 to 10.

Then I evaluate the model by each label''s absolute mean average error in test set, the first column is origin_score, the second(we don''t transform the data, then train and its  prediction error), the third(we transform the data all by dividing 2, train, and when I use this model to make prediction, firstly reconstruct the matrix and then just multiply 2 and make it back to the same scale)

As you see, in grade 3-4 (most samples are label from 3-4), it''s more precise while in high score range(like 9 and 10, just 2% of the whole traiing set), it''s worse.

````
+----------------------+--------------------+--------------------+
| rounded_origin_score | abs_mean_avg_error | abs_mean_avg_error  |
+----------------------+--------------------+---------------------+
| 1.0                  | 2.185225396100167  |  2.559125413626183  |
| 2.0                  | 1.4072212825108161 |  1.5290497332538155 |
| 3.0                  | 0.7606073396581479 |  0.6285151230269825 |
| 4.0                  | 0.7823491986435621 |  0.6419077576969795 |
| 5.0                  | 1.2734369551159568 |  1.256590210555053  |
| 6.0                  | 1.9546560495715863 |  2.0461809588933835 |
| 7.0                  | 2.707229888048017  |  2.8866856489147494 |
| 8.0                  | 3.5084244741417137 |  3.7212155956153796 |
| 9.0                  | 4.357185793060213  |  4.590550124054919  |
| 10.0                 | 5.180752400467891  |  5.468600926567884  |
+----------------------+--------------------+---------------------+
````

I''ve re-train the model several times, and got same result, so I think it''s not effect by randomness.', 1131, '2014-12-22 15:41:29.910', '797a174f-90a6-48ca-989c-80298e1fb03f', 3741, 'math grammar', 6550, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-12-22 15:41:29.910', '797a174f-90a6-48ca-989c-80298e1fb03f', 3741, 'Proposed by 1131 approved by 21 edit id of 195', 6551, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-mining><r><dataset><beginner>', 97, '2014-12-22 15:41:37.473', 'dad3c02a-eb1f-4456-9ed5-934d4b102e94', 3742, 'Additional tag.', 6552, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-12-22 15:41:37.473', 'dad3c02a-eb1f-4456-9ed5-934d4b102e94', 3742, 'Proposed by 97 approved by 21 edit id of 196', 6553, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I recently read a lot about the **n-armed bandit problem** and its solution with various algorithms, for example for webscale content optimization. Some discussions were referring to ''**contextual bandits**'', I couldn''t find a clear definition what the word ''contextual'' should mean here. Does anyone know what is meant by that, in contrast to ''usual'' bandits?

', 3132, '2014-12-22 15:52:12.133', '5da237b0-ed19-4e94-b05f-42d387cbefac', 3743, 6554, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What does ''contextual'' mean in ''contextual bandits''?', 3132, '2014-12-22 15:52:12.133', '5da237b0-ed19-4e94-b05f-42d387cbefac', 3743, 6555, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 3132, '2014-12-22 15:52:12.133', '5da237b0-ed19-4e94-b05f-42d387cbefac', 3743, 6556, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve been working in SAS for a few years but as my time as a student with a no-cost-to-me license comes to an end, I want to learn R.

Is it possible to transpose a data set so that all the observations for a single ID are on the same line?  (I have 2-8 observations per unique individual but they are currently arranged vertically rather than horizontally.)  In SAS, I had been using PROC SQL and PROC TRANSPOSE depending on my analysis aims.


    ID    date        timeframe  fruit_amt   veg_amt <br/>
    4352  05/23/2013  before     0.25        0.75 <br/>
    5002  05/24/2014  after      0.06        0.25 <br/>
    4352  04/16/2014  after      0           0 <br/>
    4352  05/23/2013  after      0.06        0.25 <br/>
    5002  05/24/2014  before     0.75        0.25 <br/>', 6491, '2014-12-22 16:15:40.573', '764144e4-58c2-4abe-bf47-648738ce91aa', 3742, 'add example data set with proper formatting', 6557, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve been working in SAS for a few years but as my time as a student with a no-cost-to-me license comes to an end, I want to learn R.

Is it possible to transpose a data set so that all the observations for a single ID are on the same line?  (I have 2-8 observations per unique individual but they are currently arranged vertically rather than horizontally.)  In SAS, I had been using PROC SQL and PROC TRANSPOSE depending on my analysis aims.

Example:

    ID    date        timeframe  fruit_amt   veg_amt <br/>
    4352  05/23/2013  before     0.25        0.75 <br/>
    5002  05/24/2014  after      0.06        0.25 <br/>
    4352  04/16/2014  after      0           0 <br/>
    4352  05/23/2013  after      0.06        0.25 <br/>
    5002  05/24/2014  before     0.75        0.25 <br/>

Desired:

    ID    B_fr05/23/2013   B_veg05/23/2013  A_fr05/23/2013  A_veg05/23/2013   B_fr05/24/2014   B_veg05/24/2014   (etc)  <br/>
    4352  0.25             0.75             0.06            0.25              .                .  <br/>
    5002  .                .                .               .                 0.75             0.25 <br/>', 6491, '2014-12-22 16:23:45.540', '37353b13-c161-4611-954e-605859ef46af', 3742, 'add example& desired data set with proper formatting', 6559, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was looking to learn about Bayesian theory in decision tree and how it avoids overfitting but couldn''t find any tutorials for someone just starting. Do you know any resources to learn about it?', 6523, '2014-12-22 18:33:35.363', 'ff6f1727-3387-409f-98ca-bb9f7024cb7e', 3745, 6560, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Bayesian Decision Tree', 6523, '2014-12-22 18:33:35.363', 'ff6f1727-3387-409f-98ca-bb9f7024cb7e', 3745, 6561, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 6523, '2014-12-22 18:33:35.363', 'ff6f1727-3387-409f-98ca-bb9f7024cb7e', 3745, 6562, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A contextual bandit algorithm not only adapts to the user-click feedback as the algorithm progresses, it also utilizes pre-existing information about the user''s (and similar users) browsing patterns to select which content to display.

So, rather than starting with no prediction (cold start) with what the user will click (traditional bandit and also traditional A/B testing), it takes other data into account (warm start) to help predict which content to display during the bandit test.

See: http://www.research.rutgers.edu/~lihong/pub/Li10Contextual.pdf', 3466, '2014-12-22 18:44:18.983', '6de9a8c2-3573-45d0-9b5e-6c5f8964cce2', 3746, 6563, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Regarding your first question...

Do you anticipate the majority category to be similarly over-represented in real-world data as it is in your training data? If so, perhaps you could perform two-step classification:

 1. Train a binary classifier (on all your training data) to predict membership (yes/no) in the majority class.
 2. Train a multi-class classifier (on the rest of the training data) to predict membership in the remaining minority classes.', 819, '2014-12-22 19:12:53.943', '0f9c18bb-32b6-497f-9f31-b7ecd54d24ab', 3747, 6564, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There''s also Richard Socher''s recent PhD dissertation on intersection of NLP and deep learning: [Recursive Deep Learning for Natural Language Processing and Computer Vision](http://nlp.stanford.edu/~socherr/thesis.pdf)', 819, '2014-12-22 19:14:26.313', '7c77e587-1cdc-415c-9270-1efab5a7a726', 3748, 6565, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try ''arrange(Data.frame.name, ID)'' function from package ''dplyr''', 5224, '2014-12-22 23:55:11.037', '2cf7ca52-9642-4885-9ce7-6eb6622dc75a', 3749, 6566, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a list of several organision names(1,000,000+),but some of then actually refer to the same organisions , for example **309th hospital** and **309 hospital PLA** and **309 hospital Chinese**.I am now using TDA(a software which is used to cluster organision names and combine different names(standard and non-standard names) of the same organision) to do organision names cleaning. To make it simple, below is the output of TDA:
![enter image description here][1]


  [1]: http://i.stack.imgur.com/gbqdi.png

For organision 309th hospital, it find one standard name and several non-standard names .But this software is not omnipotentit also make some mistakes ,for example, 88th hospital ,150th hospital is also in it. So , my question is ,is there a solution to eliminate/reduce these errors instead of doing it manually?', 6531, '2014-12-23 02:33:54.430', '8218a06b-35ad-4d86-bd09-94b5710b196a', 3750, 6567, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to elimilate wrong organision names?', 6531, '2014-12-23 02:33:54.430', '8218a06b-35ad-4d86-bd09-94b5710b196a', 3750, 6568, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp>', 6531, '2014-12-23 02:33:54.430', '8218a06b-35ad-4d86-bd09-94b5710b196a', 3750, 6569, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a list of several organization names (1,000,000+), but some of then actually refer to the same organizations , for example **309th hospital** and **309 hospital PLA** and **309 hospital Chinese**. I am now using TDA (a software which is used to cluster organization names and combine different names(standard and non-standard names) of the same organization) to do organization names cleaning. To make it simple, below is the output from TDA:
![enter image description here][1]


  [1]: http://i.stack.imgur.com/gbqdi.png

For organization "309th hospital", it find one standard name and several non-standard names. But this software is not omnipotentit also makes some mistakes, for example, 88th hospital and 150th hospital are also in it. So, my question is: "Is there a solution to eliminate/reduce these errors instead of doing it manually?".', 2452, '2014-12-23 05:01:05.583', '8b5f3259-f37f-4129-82f8-60d53e24f55c', 3750, 'Fixed spelling and grammar.', 6570, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to eliminate wrong organization names?', 2452, '2014-12-23 05:01:05.583', '8b5f3259-f37f-4129-82f8-60d53e24f55c', 3750, 'Fixed spelling and grammar.', 6571, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I will like to pick up on the topic of deep learning. Should I begin from the topic of AI before working my way into Deep learning?

Thanks', 6536, '2014-12-23 07:15:34.500', 'edce2891-7b58-4fce-a361-d5f297bae0df', 3751, 6572, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Route to picking up Deep learning', 6536, '2014-12-23 07:15:34.500', 'edce2891-7b58-4fce-a361-d5f297bae0df', 3751, 6573, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<beginner>', 6536, '2014-12-23 07:15:34.500', 'edce2891-7b58-4fce-a361-d5f297bae0df', 3751, 6574, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"OriginalQuestionIds":[1021],"Voters":[{"Id":381,"DisplayName":"Emre"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-12-23 15:42:50.883', '5e24b251-a741-4c9f-b6f4-bf84e6097a95', 3750, '101', 6575, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Regarding the being new to decision trees and wanting to get off the ground, I wrote [a tutorial on decision trees](http://thegrimmscientist.com/2014/10/23/tutorial-decision-trees/) that will help.

Regarding methods to avoid overfitting:  The game for any model is to limit its complexity to what is reasonable given the data you have.  Complexity in decision trees is manifested as adding new decision boundaries, so any limit in complexity is a limit in the decision boundaries it can draw.  Two common ways to do this is to place constraints on when a new decision can be created (a minimum of data in a leaf, significant increase in information, etc) or more simply to limit the max depth of the tree.', 6391, '2014-12-23 17:10:10.777', '4d9fcdde-e866-4b51-b62d-73210820bce7', 3752, 6576, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can use the `reshape2` package for this task.

First, transform the data to the long format with `melt`:

    library(reshape2)
    dat_m <- melt(dat, measure.vars = c("fruit_amt", "veg_amt"))

Second, cast to the wide format:

    dcast(dat_m, ID ~ timeframe + variable + date)

The result:

        ID after_fruit_amt_04/16/2014 after_fruit_amt_05/23/2013 after_fruit_amt_05/24/2014 after_veg_amt_04/16/2014
    1 4352                          0                       0.06                         NA                        0
    2 5002                         NA                         NA                       0.06                       NA
      after_veg_amt_05/23/2013 after_veg_amt_05/24/2014 before_fruit_amt_05/23/2013 before_fruit_amt_05/24/2014
    1                     0.25                       NA                        0.25                          NA
    2                       NA                     0.25                          NA                        0.75
      before_veg_amt_05/23/2013 before_veg_amt_05/24/2014
    1                      0.75                        NA
    2                        NA                      0.25
    > ', 106, '2014-12-23 18:11:24.117', '86a814fe-33c8-4684-80a2-f47ba31a97ef', 3753, 6577, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am working on a project with two data sets. A time vs. speed data set (let''s call it traffic), and a time vs. weather data set (called weather).

I am looking to find a correlation between these two sets using Pig. However the traffic data set has the time field, D/M/Y hr:min:sec, and the weather data set has the time field, D/M/Y.

Due to this I would like to average the speed per day and put it into a single D/M/Y value inside the traffic file.

I then plan to use:

    data = JOIN speed BY day, JOIN weather BY day with ''merge''
I will then find the correlation using: (I am borrowing this code from elsewhere)

    set = LOAD ''data.txt'' AS (speed:double, weather:double)
    rel = GROUP set ALL
    cor = FOREACH rel GENERATE COR(set.speed, set.weather)
    dump cor;

This is my first experience with Pig (I''ve never even used SQL), so I would like to know a few things:

    1. How can I merge the rows of my traffic file (ie. average D/M/Y hr:min:sec into D/M/Y)?
    2. Is there a better way to find a correlation between the fields of different datasets?
    3. Are the JOIN BY and the COR() functions used appropriately in my above code?
', 2614, '2014-12-23 19:46:57.267', '8218602f-94ce-4773-bd07-9635ff15b053', 3754, 6578, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hadoop/Pig Aggregate Data', 2614, '2014-12-23 19:46:57.267', '8218602f-94ce-4773-bd07-9635ff15b053', 3754, 6579, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop><beginner><correlation><pig>', 2614, '2014-12-23 19:46:57.267', '8218602f-94ce-4773-bd07-9635ff15b053', 3754, 6580, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You can use the `reshape2` package for this task.

First, transform the data to the long format with `melt`:

    library(reshape2)
    dat_m <- melt(dat, measure.vars = c("fruit_amt", "veg_amt"))

where `dat` is the name of your data frame.

Second, cast to the wide format:

    dcast(dat_m, ID ~ timeframe + variable + date)

The result:

        ID after_fruit_amt_04/16/2014 after_fruit_amt_05/23/2013 after_fruit_amt_05/24/2014 after_veg_amt_04/16/2014
    1 4352                          0                       0.06                         NA                        0
    2 5002                         NA                         NA                       0.06                       NA
      after_veg_amt_05/23/2013 after_veg_amt_05/24/2014 before_fruit_amt_05/23/2013 before_fruit_amt_05/24/2014
    1                     0.25                       NA                        0.25                          NA
    2                       NA                     0.25                          NA                        0.75
      before_veg_amt_05/23/2013 before_veg_amt_05/24/2014
    1                      0.75                        NA
    2                        NA                      0.25
    > ', 106, '2014-12-23 20:36:06.047', '6e149843-1cfa-424f-a3bc-bb70d611d08e', 3753, 'added 47 characters in body', 6584, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Apart from the fancier methods you could try the Bayes formula

P(I | p1 ... pn) =   P(p1 ... pn | I) P(I) / sum_i (P(p1 ... pn | i) P(i))

P(I | p1 ... pn) is the probability that a user belongs to age group I if he visited p1, .., pn

P(i) is the probability that a user belongs to age group i

P(p1 .. pn | i) is the probability that a user visited p1, .., pn if he belongs to age group i.

- You already have the estimates for P(i) from your data: this is just the proportion of users in age group I.
- To estimate P(p1 ... pn |i), for each age group i estimate the probability (frequency) p_ij of visiting a page j within a given period from your data.
To have p_ij non-zero for all j, you can mix in the frequency for the whole population with a small weight.

- Then log P(p1...pn| i) = sum(log p_ij, i = p1, .., pn), the sum over all pages that a new user has visited. This formula would be approximately true assuming that a user visits pages in his age group independently.
- Theoretically, you should also add log (1-p_ij) for all i that he hasn''t visited, but in practice you should find that the sum of log (1-p_ij) will be irrelevantly small, so you won''t need too much memory.

If you or someone else has tried this, please comment about the result.
', 6550, '2014-12-24 13:46:47.367', 'a081acab-78b6-4fa4-b504-8882b648259b', 3757, 6589, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Apart from the fancier methods you could try the Bayes formula

P(I | p1 ... pn) =   P(p1 ... pn | I) P(I) / sum_i (P(p1 ... pn | i) P(i))

P(I | p1 ... pn) is the probability that a user belongs to age group I if he liked p1, .., pn

P(i) is the probability that a user belongs to age group i

P(p1 .. pn | i) is the probability that a user liked p1, .., pn if he belongs to age group i.

- You already have the estimates for P(i) from your data: this is just the proportion of users in age group I.
- To estimate P(p1 ... pn |i), for each age group i estimate the probability (frequency) p_ij to like a page j. To have p_ij non-zero for all j, you can mix in the frequency for the whole population with a small weight.

- Then log P(p1...pn| i) = sum(log p_ij, i = p1, .., pn), the sum over all pages that a new user likes. This formula would be approximately true assuming that a user likes the pages in his age group independently.
- Theoretically, you should also add log (1-p_ij) for all i that he hasn''t liked, but in practice you should find that the sum of log (1-p_ij) will be irrelevantly small, so you won''t need too much memory.

If you or someone else has tried this, please comment about the result.
', 6550, '2014-12-24 13:54:07.417', '88337f62-032e-499a-ac88-187fa66ce2eb', 3757, 'deleted 52 characters in body', 6590, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The larger your target scores, the larger latent variables should be (well, it''s not only a magnitude that matters, but also a variance, but it still applies to your case). There''s no problem with larger coefficients of latent vectors unless you use regularization (and, likely, you do). In case of regularization your optimal solution will tend towards smaller values, and''d sometimes prefer to sacrifice some accuracy for lower regularization penalty.

Gradient Descent doesn''t suffer from problem of large coefficients (unless you run into some sort of numerical issues): if the learning rate is tuned properly (there are lots of stuff on it, google), it should arrive to equivalent parameters. Otherwise nobody guarantees you convergence :-)

The common rule of thumb when doing regression (and your instance of matrix factorization is a kind of regression) is to standardize your data: make it having zero mean and unit variance.', 811, '2014-12-24 20:02:42.213', 'bd178635-1ba7-4177-8002-1efb09ca919b', 3758, 6593, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('From listening to presentations by Martin Odersky, the creator of Scala, it is especially well suited for building highly scalable systems by leveraging functional programming constructs in conjuction with object orientation and flelxible syntax. It is also useful for development of small systems and rapid prototyping because it takes less lines of code than some other languages and it has an interactive mode for fast feedback. One notable Scala framework is Akka which uses the actor model of concurrent computation. Many of Odersky''s presentations are on YouTube and there is a list of tools implemented with Scala on wiki.scala-lang.org.

An implicit point is that tools and frameworks written in Scala inherently have Scala integration and usually a Scala API. Then other APIs may be added to support other languages beginning with Java since Scala is already integrated and in fact critically depends on Java.  If a tool or framework is not written in Scala, it is unlikely that it offers any support for Scala.  That is why in answer to your question I have pointed towards tools and frameworks written in Scala and Spark is one example.  However, Scala currently has a minor share of the market but its adoption rate is growing and the high growth rate of Spark will enhance that. The reason I use Scala is because Spark''s API for Scala is richer than the Java and Python APIs.

The main reasons I prefer Scala generally is because it is much more expressive than Java because it allows and facilitates the use of functions as objects and values while retaining object oriented modularity, which enables development of complex and correct programs with far less code than Java which I had preferred because of widespread use, clarity and excellent documentation.', 5280, '2014-12-24 20:10:01.487', '66bb1922-b415-4a6c-849f-3962b7678cd5', 3689, 'added futher explanation and reasons for preferring scala', 6594, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am new to data science/ machine learning world. I know that in Statistics we assume that a certain event/ process has some particular distribution and the samples of that random process are part of some sampling distribution. The findings from the data could then be generalized by using confidence intervals and significance levels.

How do we generalize our findings once we "learn" the patterns in the data set? What is the alternative to confidence levels here?', 4933, '2014-12-25 05:24:55.590', 'fa96def8-207f-44d1-862d-dec315f4fb01', 3759, 6596, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How are the findings learnt from the data set are generalized compared to Statistics?', 4933, '2014-12-25 05:24:55.590', 'fa96def8-207f-44d1-862d-dec315f4fb01', 3759, 6597, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><statistics>', 4933, '2014-12-25 05:24:55.590', 'fa96def8-207f-44d1-862d-dec315f4fb01', 3759, 6598, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to determine what is the best number of hidden neurons for my MATLAB neural network. I was thinking to adopt the following strategy:

 - Loop for some values of hidden neurons, e.g. 1 to 40;
 - For each NN with a fixed number of hidden neurons, perform a certain number of   training (e.g. 40, limiting the number of epoch for time reasons: I was thinking to doing this because the network seems to be hard to train, the MSE after some epochs is very high)
 - Store the MSE obtained with all the nets with different number of hidden neurons
 - Perform the previous procedure more than 1 time, e.g. 4, to take into account the initial random weight, and take the average of the MSEs
 - Select and perform the "real" training on a NN with a number of hidden neurons such that the MSE previously calculated is minimized

The MSE that I''m referring is the validation MSE: my samples splitting in trainining, testing and validation to avoid overfitting is 70%, 15% and 15% respectively)

Other informations related to my problem are:
fitting problem
9 input neurons
2 output neurons
1630 samples

This strategy could be work? Is there any better criterion to adopt? Thank you
', 6559, '2014-12-25 10:13:16.273', 'bba935dd-c5a3-425c-a93a-c87ffe6ce509', 3760, 6599, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Neural Network Hidden Neuron Selection Strategy', 6559, '2014-12-25 10:13:16.273', 'bba935dd-c5a3-425c-a93a-c87ffe6ce509', 3760, 6600, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork>', 6559, '2014-12-25 10:13:16.273', 'bba935dd-c5a3-425c-a93a-c87ffe6ce509', 3760, 6601, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('This is a **tricky** question, because it''s easy and difficult at the same time. *Easy*, because there is a lot of resources that potentially can help you make a decision on the topic. *Difficult*, because the situation is very different for a particular person (not to mention that their interest might change at any time), which makes extremely difficult for other people to give you a **good** advice and for you to make the **right** decision.

As for **data science career** options, you can certainly consider a **degree path** (MS or MS + PhD), but you need to be aware of **other options**. For a comprehensive resource, dedicated to **data science and related degree programs** (both *traditional* and *online*), please visit this page: http://www.kdnuggets.com/education/index.html. A **comprehensive** review of **all** these offerings is IMHO an enormous task and is **far** beyond an answer here on Stack Exchange or, even, a lengthy blog post.

However, nowadays one is not limited to *traditional educational options* and I think it''s important to be aware of **other educational options**. One of the other options include **certifications** (linked at the above-mentioned page, but, in my opinion, the only certification worth considering is the [Certified Analytics Professional](https://www.informs.org/Certification-Continuing-Ed/Analytics-Certification) as a *solid* and *vendor-neutral* certification from a reputable INFORMS). Another option is recently booming **data science intensive educational offerings**, from *short-term* (and often too commercial, to put it lightly) *bootcamps* to *more solid offerings*, including free, but competitive, ones, such as [Insight Data Science Fellows Program](http://insightdatascience.com), where one needs to be a PhD to apply, or its sister program [Insight Data Engineering Fellows Program](http://www.insightdataengineering.com), which doesn''t have such requirement. Finally, there is yet another option: **self-study**. It partially intersects with the certificate option, if one uses *massive open online courses (MOOC)* (a review of which deserves a separate comprehensive post), but there are **open curricula** that might suit one better, such as the *Open Source Data Science Masters* curriculum, linked in my [earlier relevant answer](http://datascience.stackexchange.com/a/742/2452).

**P.S.** While your question focuses on data science, I think that it may be wise to at least consider another career path, given your *math background*. I''m talking about **operations research** field, which is not that far away from data science (and even somewhat intersects with it). While similar, data science is IMHO more statistics-focused, whereas operations research is more math-focused, at least that''s how I see it. Despite all the popularity and "gold rush" of data science, operations research career is a solid one, just not as hot. Of course, if you''re excited about things like artificial intelligence, machine learning and, especially, deep learning, data science career is the way to go. Whatever you will choose, the good thing is that with your math background it will be easy to change focus, should you decide to. Hope this helps. Good luck!', 2452, '2014-12-25 10:41:26.943', 'd74d5627-a273-4db8-930d-d455bbef800d', 3737, 'Fixed and improved wording.', 6602, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A rule of thumb approach is:

 - start with a number of hidden neurons equal (or little higher) that the number of features.
 - In your case it would be 9. My suggestion is to start with 9*2 = 18 to cover a wider range of possibilities.
 - Be sure your test and validation sets are selected "fairly": a random selection and varying the seed some number of times to test different configurations would be ok.

In general, a number of neurons equal to the number of features will tend to make each hidden neuron try to learn *that special thing* that each feature is adding, so will could say it is "learning each feature" separately. Although this sounds good it might tend to overfitting.

Since your number of inputs and your dataset size is small its ok to start with a hidden layer size of the double (18) and start lowering down. When the training error and test error stabilize in a difference lower than a threshold then you could have found a better generalizing model.

Neural networks are very good at finding local optima by exploring deeply a solution from a starting point. However, the starting point it is also very important. If you are not getting a good generalization you might try to find good initial starting points with methods of Hybrid Neural Networks. A common one, for example, is using genetic algorithms to find an initial combination of weights and then start the neural from that point. Given that your search space would be better covered (in case your problem actually needs that).

As for every problem in machine learning is very important to clean your data before introducing it to the NN. Try to be very detailed in order to avoid the NN to learn things you already know. For example if you know how two features are correlated improve the input data by making this correlation explicit so less workload is given to the NN (that might actually get you in trouble).', 5143, '2014-12-25 15:40:43.283', 'b9caf02e-d0c9-4036-8650-63b8441e23ce', 3761, 6603, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('## Top level:
The rule is to chose the most simple network that can perform satisfactorily.  See [this publication](http://ieeexplore.ieee.org/xpl/login.jsp?reload=true&tp=&arnumber=5726612&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5726612) and it''s [pdf](http://www.pdx.edu/sites/www.pdx.edu.sysc/files/EvaluatingANNs.pdf).

## The Methodology:
So do your proposed test (training many networks at each number of hidden nodes) and plot the results.  At the minimum number of nodes, you''ll see the worst performance.  As you increase the number of nodes, you''ll see an increase in performance (reduction of error).  At some point N, you''ll see the performance seems to hit an upper limit and increasing nodes beyond this will stop giving significant performance gains.  Further increases may start to hurt performance a little as training gets more difficult).  That point N is the number of nodes you want.

# How it worked for me:
The first time I used this methodology, it created a beautiful almost-sigmoid-looking function with a very clear number of nodes that were needed to achieve good results.  I hope this works for you as well as it worked for me.
', 6391, '2014-12-25 18:10:13.447', '15b5f35b-99a4-48b8-a10e-e3ad8b917707', 3762, 6604, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Background
==========

I''m working on a time series data set of energy meter readings. The length of the series varies by meter - for some I have several years, others only a few months, etc. Many display significant seasonality, and often multiple layers - within the day, week, or year.

One of the things I''ve been working on is clustering of these time series. My work is academic for the moment, and while I''m doing other analysis of the data as well, I have a specific goal to carry out some clustering.

I did some initial work where I calculated various features (percentage used on weekends vs. weekday, percentage used in different time blocks, etc.). I then moved on to looking at using Dynamic Time Warping (DTW) to obtain the distance between different series, and clustering based on the difference values, and I''ve found several papers related to this.



Question
========

Will the seasonality in a specific series changing cause my clustering to be incorrect? And if so, how do I deal with it?

In case the above is unclear, consider these examples:

Example 1
---------
A meter has low readings from midnight until 8AM, the readings then increase sharply for the next hour and stay high from 9AM until 5PM, then decrease sharply over the next hour and then stay low from 6PM until midnight. The meter continues this pattern consistently every day for several months, but then changes to a pattern where readings simply stay at a consistent level throughout the day.

Example 2
---------
A meter shows approximately the same amount of energy being consumed each month. After several years, it changes to a pattern where energy usage is higher during the summer months before returning to the usual amount.

I''ve been wondering whether I need to split the series where things like this happen, but I''m not sure how I''d detect instances of it in order to do so (the data set is too big to do that manually), and I''m honestly just not sure if that''s a suitable thing to do, anyway.

----------


P.S. - I can''t post in Meta or even a comment yet, so sorry for the off-topic bit, but: I love this site and hope it picks up steam! I was so happy when I found it, I immediately found topics which weren''t covered at all or as helpfully at SO and CV. I feel a lot more comfortable asking this question here because it''s not a pure statistics question or a programming question. I would feel like I was asking something potentially off-topic at either of those sites, and I feel like I''m more likely to get an answer which really covers my question and which I can understand by asking here.', 5246, '2014-12-25 23:32:13.547', '8e90b94a-8b91-45f8-b7df-17206ef59104', 3738, 'Re-formatting and re-writing my question to enhance the clarity.', 6605, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Background
==========

I''m working on a time series data set of energy meter readings. The length of the series varies by meter - for some I have several years, others only a few months, etc. Many display significant seasonality, and often multiple layers - within the day, week, or year.

One of the things I''ve been working on is clustering of these time series. My work is academic for the moment, and while I''m doing other analysis of the data as well, I have a specific goal to carry out some clustering.

I did some initial work where I calculated various features (percentage used on weekends vs. weekday, percentage used in different time blocks, etc.). I then moved on to looking at using Dynamic Time Warping (DTW) to obtain the distance between different series, and clustering based on the difference values, and I''ve found several papers related to this.



Question
========

Will the seasonality in a specific series changing cause my clustering to be incorrect? And if so, how do I deal with it?

My concern is that the distances obtained by DTW could be misleading in the cases where the pattern in a time series has changed.

In case the above is unclear, consider these examples:

Example 1
---------
A meter has low readings from midnight until 8AM, the readings then increase sharply for the next hour and stay high from 9AM until 5PM, then decrease sharply over the next hour and then stay low from 6PM until midnight. The meter continues this pattern consistently every day for several months, but then changes to a pattern where readings simply stay at a consistent level throughout the day.

Example 2
---------
A meter shows approximately the same amount of energy being consumed each month. After several years, it changes to a pattern where energy usage is higher during the summer months before returning to the usual amount.

I''ve been wondering whether I need to split the series where things like this happen, but I''m not sure how I''d detect instances of it in order to do so (the data set is too big to do that manually), and I''m honestly just not sure if that''s a suitable thing to do, anyway.

----------


P.S. - I can''t post in Meta or even a comment yet, so sorry for the off-topic bit, but: I love this site and hope it picks up steam! I was so happy when I found it, I immediately found topics which weren''t covered at all or as helpfully at SO and CV. I feel a lot more comfortable asking this question here because it''s not a pure statistics question or a programming question. I would feel like I was asking something potentially off-topic at either of those sites, and I feel like I''m more likely to get an answer which really covers my question and which I can understand by asking here.', 5246, '2014-12-25 23:39:32.033', '5e2bb1e4-3b84-42c0-b230-c832ae00260b', 3738, 'added 132 characters in body', 6606, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In [this](http://www.wired.com/2014/01/how-to-hack-okcupid/all/) article, Chris McKinlay says he used AdaBoost to choose the proper "importances" of questions he answered on okcupid.

If you haven''t read and don''t want to read the article, or are unfamiliar with okcupid and the question system, here''s the data and problem he had:
The goal is to "match" as highly as possible with as many users as possible, each of whom may have answered an arbitrary number of questions. These questions may have between 2 and 4 answers each, and for the sake of simplicity, let''s pretend that the formula for a match% $\ M $ between you and another user is given by

$\ M = Q_a/Q_c $

Where $\ Q_c $ is the number of questions you and the other user have in common, and

$\ Q_a $ is the number of questions you both answered with the same value.

The real formula is slightly more complex, but the approach would be the same regarding "picking" a correct answer (he actually used boosting to find the ideal "importance" to place on a given question, rather than the right answer).
In any case, the point is you want to pick a certain value for each question, such that you maximize your match% with as many users as possible - something you might quantify by the sum of $\ M $ over all users.

Now I''ve watched the MIT course on AI up to and including the lecture on boosting, but I don''t understand how you would apply it to a problem like this. Honestly I don''t even know where to begin with choosing rules for the weak learners. I don''t have any "rules" about what values to choose for each question (if the user is under 5''5, choose A, etc) - I''m just trying to fit the data I have.

Is this not the way boosting is supposed to be used? Is there likely some other optimization left out of how he figured this out?', 6568, '2014-12-26 06:53:29.670', 'f8a8eda3-004e-42fa-97f7-9c455b459395', 3763, 6609, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to apply AdaBoost to more "complex" (non-binary) classifications/data fitting?', 6568, '2014-12-26 06:53:29.670', 'f8a8eda3-004e-42fa-97f7-9c455b459395', 3763, 6610, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><optimization>', 6568, '2014-12-26 06:53:29.670', 'f8a8eda3-004e-42fa-97f7-9c455b459395', 3763, 6611, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('After reading your question, I became curious about the topic of *time series clustering* and *dynamic time warping (DTW)*. So, I have performed a limited search and came up with basic understanding (for me) and the following set of IMHO *relevant references* (for you). I hope that you''ll find this useful, but keep in mind that I have intentionally skipped research papers, as I was more interested in *practical aspects* of the topic.

**Resources:**

- **UCR Time Series Classification/Clustering**: [main page](http://www.cs.ucr.edu/~eamonn/time_series_data), [software page](http://www.cs.ucr.edu/~eamonn/UCRsuite.html) and [corresponding paper](http://www.cs.ucr.edu/~eamonn/SIGKDD_trillion.pdf)
- **Time Series Classification and Clustering with Python**: [a blog post](http://alexminnaar.com/2014/04/06/time-series-classification-and-clustering-with-python)
- **Capital Bikeshare: Time Series Clustering**: [another blog post](http://ofdataandscience.blogspot.com/2013/03/capital-bikeshare-time-series-clustering.html)
- **Time Series Classification and Clustering**: [ipython notebook](http://nbviewer.ipython.org/github/alexminnaar/time-series-classification-and-clustering/blob/master/Time%20Series%20Classification%20and%20Clustering.ipynb)
- **Dynamic Time Warping using rpy and Python**: [another blog post](https://nipunbatra.wordpress.com/2013/06/09/dynamic-time-warping-using-rpy-and-python)
- **Mining Time-series with Trillions of Points: Dynamic Time Warping at Scale**: [another blog post](http://practicalquant.blogspot.com/2012/10/mining-time-series-with-trillions-of.html)
- **Time Series Analysis and Mining in R** (to add R to the mix): [yet another blog post](http://rdatamining.wordpress.com/2011/08/23/time-series-analysis-and-mining-with-r)
- And, finally, two **tools implementing/supporting DTW**, to top it off: [R package](http://rdatamining.wordpress.com/2011/08/23/time-series-analysis-and-mining-with-r) and [Python module](http://mlpy.sourceforge.net)', 2452, '2014-12-26 10:01:46.500', '69f566ec-006d-41f7-94dd-ae503ea5b803', 3764, 6612, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I execute the following code I have no problem:
require(foreign)
require(nnet)
require(ggplot2)
require(reshape2)

ml <- read.dta("http://www.ats.ucla.edu/stat/data/hsbdemo.dta")
ml$prog2 <- relevel(ml$prog, ref = "academic")
test <- multinom(prog2 ~ ses + write, data = ml)
predict(test, newdata = dses, "probs")

but if I try:

require(caret)
ml <- read.dta("http://www.ats.ucla.edu/stat/data/hsbdemo.dta")
ml$prog2 <- relevel(ml$prog, ref = "academic")
test <- train(prog2 ~ ses + write,method="multinom" ,data = ml)
predict(test$finalModel, newdata = dses, "probs")

it returns "Error in eval(expr, envir, enclos) : object ''sesmiddle'' not found", why?

Thanks in advance
', 6572, '2014-12-26 16:25:27.373', '07ca7866-3a90-432a-b127-bbec125c595c', 3766, 6616, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('predict with Multinomial Logistic Regression', 6572, '2014-12-26 16:25:27.373', '07ca7866-3a90-432a-b127-bbec125c595c', 3766, 6617, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><logistic-regression><predictive-modeling>', 6572, '2014-12-26 16:25:27.373', '07ca7866-3a90-432a-b127-bbec125c595c', 3766, 6618, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('That''s not the error I get and I''m thinking that you left out some code. I get:

<pre>
predict(test$finalModel, newdata = dses, "probs")
Error in as.data.frame(newdata) : object ''dses'' not found
</pre>

I don''t know why I see this so much, but you should avoid using the `finalModel` object for prediction. `train` is doing some things that the model from `multinom` sin''t aware of (such as expanding dummy variables). Try using

<pre>
predict(test, newdata = dses, "probs")
</pre>

and never use `predict(test$finalModel)` again...

Max
', 810, '2014-12-26 17:26:02.033', '2f440d42-504b-4897-a66b-73bb630c6599', 3767, 6619, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m trying to determine what is the best number of hidden neurons for my MATLAB neural network. I was thinking to adopt the following strategy:

 - Loop for some values of hidden neurons, e.g. 1 to 40;
 - For each NN with a fixed number of hidden neurons, perform a certain number of   training (e.g. 40, limiting the number of epoch for time reasons: I was thinking to doing this because the network seems to be hard to train, the MSE after some epochs is very high)
 - Store the MSE obtained with all the nets with different number of hidden neurons
 - Perform the previous procedure more than 1 time, e.g. 4, to take into account the initial random weight, and take the average of the MSEs
 - Select and perform the "real" training on a NN with a number of hidden neurons such that the MSE previously calculated is minimized

The MSE that I''m referring is the validation MSE: my samples splitting in trainining, testing and validation to avoid overfitting is 70%, 15% and 15% respectively)

Other informations related to my problem are:
fitting problem
9 input neurons
2 output neurons
1630 samples

This strategy could be work? Is there any better criterion to adopt? Thank you

Edit: Test done, so the result suggest me to adopt 12 neurons? (low validation MSE and  number of neurons lower than 2*numberOfInputNeurons? but also 18 could be good...
![enter image description here][1]


  [1]: http://i.stack.imgur.com/qxOSV.png', 6559, '2014-12-26 20:37:37.720', '2e69b438-18f4-446a-81f1-23b4c4dce4f5', 3760, 'added 225 characters in body', 6620, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Background
==========

I''m working on a time series data set of energy meter readings. The length of the series varies by meter - for some I have several years, others only a few months, etc. Many display significant seasonality, and often multiple layers - within the day, week, or year.

One of the things I''ve been working on is clustering of these time series. My work is academic for the moment, and while I''m doing other analysis of the data as well, I have a specific goal to carry out some clustering.

I did some initial work where I calculated various features (percentage used on weekends vs. weekday, percentage used in different time blocks, etc.). I then moved on to looking at using Dynamic Time Warping (DTW) to obtain the distance between different series, and clustering based on the difference values, and I''ve found several papers related to this.



Question
========

Will the seasonality in a specific series changing cause my clustering to be incorrect? And if so, how do I deal with it?

My concern is that the distances obtained by DTW could be misleading in the cases where the pattern in a time series has changed. This could lead to incorrect clustering.

In case the above is unclear, consider these examples:

Example 1
---------
A meter has low readings from midnight until 8AM, the readings then increase sharply for the next hour and stay high from 9AM until 5PM, then decrease sharply over the next hour and then stay low from 6PM until midnight. The meter continues this pattern consistently every day for several months, but then changes to a pattern where readings simply stay at a consistent level throughout the day.

Example 2
---------
A meter shows approximately the same amount of energy being consumed each month. After several years, it changes to a pattern where energy usage is higher during the summer months before returning to the usual amount.

Possible Directions
===================

 - I''ve wondered whether I can continue to compare whole time series, but split them and consider them as a separate series if the pattern changes considerably. However, to do this I''d need to be able to detect such changes. Also, I just don''t know if this is a suitable way or working with the data.
 - I''ve also considered splitting the data and considering it as many separate time series. For instance, I could consider every day/meter combination as a separate series. However, I''d then need to do similarly if I wanted to consider the weekly/monthly/yearly patterns. I *think* this would work, but it''s potentially quite onerous and I''d hate to go down this path if there''s a better way that I''m missing.

Further Notes
=============
These are things that have come up in comments, or things I''ve thought of due to comments, which might be relevant. I''m putting them here so people don''t have to read through everything to get relevant information.

 - I''m working in Python, but have rpy for those places where R is more suitable. I''m not necessarily looking for a Python answer though - if someone has a practical answer of what should be done I''m happy to figure out implementation details myself.
 - I have a lot of working "rough draft" code - I''ve done some DTW runs, I''ve done a couple of different types of clustering, etc. I think I largely understand the direction I''m taking, and what I''m really looking for is related to how I process my data before finding distances, running clustering, etc. Given this, I suspect the answer would be the same whether the distances between series are calculated via DTW or a simpler Euclidean Distance (ED).
 - I have found these papers especially informative on time series and DTW and they may be helpful if some background is needed to the topic area: http://www.cs.ucr.edu/~eamonn/selected_publications.htm

----------


P.S. - I can''t post in Meta or even a comment yet, so sorry for the off-topic bit, but: I love this site and hope it picks up steam! I was so happy when I found it, I immediately found topics which weren''t covered at all or as helpfully at SO and CV. I feel a lot more comfortable asking this question here because it''s not a pure statistics question or a programming question. I would feel like I was asking something potentially off-topic at either of those sites, and I feel like I''m more likely to get an answer which really covers my question and which I can understand by asking here.', 5246, '2014-12-27 21:53:29.410', '1913e9d7-63e6-4b8c-a28a-3b24e1b9216e', 3738, 'Including some info discussed in comments, and elucidating on possible solutions.', 6631, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Google Trends returns weekly data so I have to find a way to merge them with my daily/monthly data.

What I have done so far is to break each serie into daily data, for exemple:

from:

2013-03-03 - 2013-03-09 37

to:

2013-03-03 37
2013-03-04 37
2013-03-05 37
2013-03-06 37
2013-03-07 37
2013-03-08 37
2013-03-09 37

But this is adding a lot of complexity to my problem. I was trying to predict google searchs from the last 6 months values, or 6 values in monthly data. Daily data would imply a work on 180 past values. (I have 10 years of data so 120 points in monthly data / 500+ in weekly data/ 3500+ in daily data)

The other approach would be to "merge" daily data in weekly/monthly data. But some questions arise from this process. Some data can be averaged because their sum represent something. Rainfall for example, the amount of rain in a given week will be the sum of the amounts for each days composing the weeks.

In my case I am dealing with prices, financial rates and other things. For the prices it is common in my field to take volume exchanged into account, so the weekly data would be a weighted average. For financial rates it is a bit more complex a some formulas are involved to build weekly rates from daily rates.  For the other things i don''t know the underlying properties. I think those properties are important to avoid meaningless indicators (an average of fiancial rates would be a non-sense for example).

So three questions:

**For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?**

I feel like breaking weekly/monthly data into daily data like i''ve done is somewhat wrong because I am introducing quantities that have no sense in real life. So almost the same question:

**For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?**

Last but not least: **when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?** I think this is a compromise between the number of data and the complexity of the model but I can''t see any strong argument to choose between those options.

edit: I tought I was on cross-validated. I don''t know If it is ok to post this here. So here''s is a fourth question: where is the boundary between data science.SE and cross-validated.SE ?', 303, '2014-12-28 11:29:14.240', '4d3b1d4d-3a89-4b80-a52a-65529067897f', 3770, 6632, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to merge monthly, daily and weekly data?', 303, '2014-12-28 11:29:14.240', '4d3b1d4d-3a89-4b80-a52a-65529067897f', 3770, 6633, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<time-series>', 303, '2014-12-28 11:29:14.240', '4d3b1d4d-3a89-4b80-a52a-65529067897f', 3770, 6634, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Google Trends returns weekly data so I have to find a way to merge them with my daily/monthly data.

What I have done so far is to break each serie into daily data, for exemple:

from:

2013-03-03 - 2013-03-09 37

to:

2013-03-03 37
2013-03-04 37
2013-03-05 37
2013-03-06 37
2013-03-07 37
2013-03-08 37
2013-03-09 37

But this is adding a lot of complexity to my problem. I was trying to predict google searchs from the last 6 months values, or 6 values in monthly data. Daily data would imply a work on 180 past values. (I have 10 years of data so 120 points in monthly data / 500+ in weekly data/ 3500+ in daily data)

The other approach would be to "merge" daily data in weekly/monthly data. But some questions arise from this process. Some data can be averaged because their sum represent something. Rainfall for example, the amount of rain in a given week will be the sum of the amounts for each days composing the weeks.

In my case I am dealing with prices, financial rates and other things. For the prices it is common in my field to take volume exchanged into account, so the weekly data would be a weighted average. For financial rates it is a bit more complex a some formulas are involved to build weekly rates from daily rates.  For the other things i don''t know the underlying properties. I think those properties are important to avoid meaningless indicators (an average of fiancial rates would be a non-sense for example).

So three questions:

**For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?**

I feel like breaking weekly/monthly data into daily data like i''ve done is somewhat wrong because I am introducing quantities that have no sense in real life. So almost the same question:

**For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?**

Last but not least: **when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?** I think this is a compromise between the number of data and the complexity of the model but I can''t see any strong argument to choose between those options. ', 303, '2014-12-28 11:40:19.360', '216aad7f-42e7-4876-ab9f-9b34a14f78ea', 3770, 'deleted 191 characters in body', 6635, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Google Trends returns weekly data so I have to find a way to merge them with my daily/monthly data.

What I have done so far is to break each serie into daily data, for exemple:

from:

2013-03-03 - 2013-03-09 37

to:

2013-03-03 37
2013-03-04 37
2013-03-05 37
2013-03-06 37
2013-03-07 37
2013-03-08 37
2013-03-09 37

But this is adding a lot of complexity to my problem. I was trying to predict google searchs from the last 6 months values, or 6 values in monthly data. Daily data would imply a work on 180 past values. (I have 10 years of data so 120 points in monthly data / 500+ in weekly data/ 3500+ in daily data)

The other approach would be to "merge" daily data in weekly/monthly data. But some questions arise from this process. Some data can be averaged because their sum represent something. Rainfall for example, the amount of rain in a given week will be the sum of the amounts for each days composing the weeks.

In my case I am dealing with prices, financial rates and other things. For the prices it is common in my field to take volume exchanged into account, so the weekly data would be a weighted average. For financial rates it is a bit more complex a some formulas are involved to build weekly rates from daily rates.  For the other things i don''t know the underlying properties. I think those properties are important to avoid meaningless indicators (an average of fiancial rates would be a non-sense for example).

So three questions:

**For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?**

I feel like breaking weekly/monthly data into daily data like i''ve done is somewhat wrong because I am introducing quantities that have no sense in real life. So almost the same question:

**For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?**

Last but not least: **when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?** I think this is a compromise between the number of data and the complexity of the model but I can''t see any strong argument to choose between those options.

Edit: if you know a tool (in R Python even Excel) to do it easily it would be very appreciated.', 303, '2014-12-28 11:50:12.840', '464eacf2-3ad0-467f-913e-f41e0b4fe142', 3770, 'added 99 characters in body', 6636, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to predict clients comportement from market rates.

The value of the products depends on the actual rate but this is not enough. The comportement of the client also depends on their awareness wich depends on the evolution of rates. I''ve added this in model using past 6 month rates as features. I am currently fitting polynomials objectives functions.

In fact media coverage of rate mostly depends on rate variations and I wanted to add that in my model.  The idea would be to add a derivative/variation of rate as a feature. But I anticipated something wrong, example with only two month , my variation will be of the form $x_1 - x_{2}$ that is a simple linear combination of $x_1$, my actual rate and $x_{2}$ the past month rate. I feel like this variation does not add anything to the objective function i am trying to fit to my data.

I was thinking about a better interpolation of my curve to have a better idea of my derivative. For example, a cubic spline interpolation of my curve so that the derivative will be a bit more than a difference. The splines will have a polynomial expression of x and my features, wich give a polynomial expression of my feature when i calculate the derivative in one point. In the end, adding these features will only change the order of the objective function.

Is this a general idea ? all information is somewhat directly contained in data and modifications of features will result in higher order objective function ?

After fitting my function how do I look for the influence of variation ? With the 2 month example, if I end up with the objective function $a x_1 + b x_2$ and I rewrite it as $ \frac{a+b}{2} (x_1 + x_2) + \frac{a-b}{2} (x_1 - x_2)$, $\frac{a-b}{2}$ would be the influence of variation ? What would be the interpretation of $ \frac{a+b}{2}$ ? Is this generalisable to higher number of feature ? higher order of objective function ? (Is this just a question of polynomial factorization ?)
', 303, '2014-12-28 14:42:52.403', '2b03c302-17dc-4c20-9a0c-f76a376adf5b', 3771, 6637, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Time series: variations as a feature', 303, '2014-12-28 14:42:52.403', '2b03c302-17dc-4c20-9a0c-f76a376adf5b', 3771, 6638, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<time-series><feature-selection><predictive-modeling><optimization>', 303, '2014-12-28 14:42:52.403', '2b03c302-17dc-4c20-9a0c-f76a376adf5b', 3771, 6639, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to predict a time serie from another one. My approach is based on a moving windows. I predict the output value of the serie from the following features: the previous value and the 6 past values of the source serie.

Is it usefull to add the previous value of the time serie ?

I feel like I don''t use all the information contained in the curve to predict futures values. But I don''t see how it would be possible to use all previous data to predict a value (first, the number of features would be growing trough time...).

What are the caveats of a 6 month time-window approach ?

Is there any paper about differents method of feature selection for time-series ? ', 303, '2014-12-28 14:58:45.320', 'f56f5d1c-7b9c-4cc4-83aa-8e5e05aa3de4', 3772, 6640, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Time series prediction', 303, '2014-12-28 14:58:45.320', 'f56f5d1c-7b9c-4cc4-83aa-8e5e05aa3de4', 3772, 6641, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<time-series><feature-selection>', 303, '2014-12-28 14:58:45.320', 'f56f5d1c-7b9c-4cc4-83aa-8e5e05aa3de4', 3772, 6642, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m fairly new to this myself, but have spent a lot of time recently learning about time series and hope that I can help fellow learners. If I had the reputation to comment I''d ask you a few things first, but I can''t. I''ll happily do further work and edit this response if you respond or make edits to your question. With those caveats out of the way:

Is it useful to use the previous value as a feature?
====================================================
One of the first things I would say is that the correct aspects to be looking at in your data very much depends on the nature of the data, as well as what you''re trying to do with it:

 - It sounds like you have monthly values, but it''s not clear how far into the future you''re wanting to predict, or how much historic data you have access to.
 - We also don''t know what these two series represent, or why one time series is being used to predict the other - and without that, I don''t think anyone will be able to tell you whether the previous value of the series to be predicted is valuable information or not.

Any caveats to using a 6 month time window?
===========================================
One obvious caveat to only using the last 6 months is that if there''s any seasonality over the year-long period then you''re going to miss it.

 - If you''re not sure: if you have multiple years of information, try plotting the series you want to predict over multiple years. You may well be able to see whether the series generally increases or decreases at certain times of year. If you can share this plot here, it might help people answer your questions in more depth.

As far as caveats about this time-window approach, I''m not too clear from your post what algorithm you''re using to predict values. More information on that would be helpful; it''s possible that rather than questioning what features to select, you should be questioning what methodology to use for forecasting.

Helpful further reading?
========================
Once you''ve provided more information I''ll be happy to tackle your last question on suitable reading if I''m able to. For now, I will say that there''s a *lot* of information available, but a lot of it is academic in nature. Quite frequently papers aren''t easy to digest, seem to contradict one another, or are only relevant to specific situations. This is a rapidly growing and changing field, so it''s sometimes difficult to find a clear best practice or consensus opinion.

That said, it might be worth looking at some of the free, online courses available to see if any would help you understand the area you''re interested in:

 - [Coursera "data science" related courses][1]


  [1]: https://www.coursera.org/courses?query=data%20science', 5246, '2014-12-28 20:10:11.300', 'efb3c94b-73cb-4285-a8e0-871d3c568404', 3773, 6643, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Let me give you a few simple approaches in time series analysis.

The first approach consists in using previous values of your time series $Y_{t}$ as in $Y_{t} = \phi_{1}Y_{t-1} + ... + \phi_{n}Y_{t-n}$. In case you don''t know, these models are called autoregressive (AR) models. This answers your first question. Of course it is useful to include the previous value of your time series. There is a whole set of models based on that idea.

The second approach is taking a window and extracting some features to describe the time series at each point in time. Then you use a conventional machine learning technique to predict future values as typically done. This is more common in a classification or regression setting but future values can be thought of as classifying future values. This technique has the advantage of dramatically reducing the number of features, although you usually lose characteristics associated with time. This addresses your second concern.

Another model that could be helpful in your case is the vector autoregressive model (VAR) (using [Wikipedia](http://en.wikipedia.org/wiki/Vector_autoregression)''s notation):

$$\left( \begin{array}{ccc}
y_{1,t} \\
y_{2,t}
\end{array}\right) = \left( \begin{array}{ccc}c_{1} \\ c_{2}\end{array}\right) + \left( \begin{array}{ccc}A_{1,1} & A_{1,2} \\ A_{2,1} & A_{2,2}\end{array}\right)\left( \begin{array}{ccc}
y_{1,t-1} \\
y_{2,t-1}
\end{array}\right) + \left( \begin{array}{ccc}
e_{1,t} \\
e_{2,t}
\end{array}\right)$$

Here you can see that $y_{1,t}$ has a contribution from its previous value $t_{1,t-1}$ but also includes the value of the other series $y_{2,t-1}$ in a linear combination. As usual, the purpose is to find the elements of $A_{i,j}$ that minimize some measure of error between observed values and estimated values.

A general suggestion: The first thing you need to do is to test the autocorrelation of your first series in order to confirm that an autoregressive approach is suitable and then test the cross correlation between both series to support the idea that using the second series to improve your predictions is appropriate.', 4621, '2014-12-29 06:53:32.200', '5cf5252c-4f16-49f6-9a8c-cb99eec580e5', 3774, 6644, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This does not seem a Data Science problem. However there are very nice tools to do exactly that, checkout: logstash, flume and fluentd. Actually if you want to be able to filter in fast and "smart" way checkout Kibana from the guys of ElastichSearch (http://www.elasticsearch.org/overview/kibana). Those tools are enough to solve your problem in a very efficient way.


', 5143, '2014-12-29 08:38:44.673', 'c8ac2ff7-3808-4265-bcb9-bd16b6f0c32e', 3775, 6645, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve heard it said that Bias-Variance decomposition is one way, and that the V-C dimension/bound is another...

Both of these are metrics you can use to get a feeling of how confident you should be that your training results will generalize to out-of-sample.', 6597, '2014-12-29 09:03:08.853', '697e771d-623b-4bb3-b058-821f6cdd6cbd', 3776, 6646, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Bias-Variance decomposition is one way, and V-C dimension/bound is another...

Both of these are metrics you can use to get a feeling of how confident you should be that your training results will generalize to out-of-sample.

V-C dimension focuses on the results of *this* learning algorithm outcome.
Bias-Variance focuses on the expected outcome of the algorithm itself.

Pick your poison - I sincerely hope that this was helpful.', 6597, '2014-12-29 09:08:32.210', 'e002f347-4c6b-4c78-a522-d4a9a355cd73', 3776, 'deleted 33 characters in body', 6647, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Essentially machine learning uses non parametric methods...The assumption is that you have enough data and (computation)  time
 You Identify best model by cross validation (rather than eg  assessing significance of coefficients) ,  and estimate prediction error by using test set.  Confidence intervals can also be generated by bootstrapping. ', 1256, '2014-12-29 09:21:26.780', 'da4e28be-2fd1-4bb3-aa98-387f09672f88', 3777, 6648, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('If I execute the following code I have no problem:

    require(foreign)
    require(nnet)
    require(ggplot2)
    require(reshape2)

    ml <- read.dta("http://www.ats.ucla.edu/stat/data/hsbdemo.dta")
    ml$prog2 <- relevel(ml$prog, ref = "academic")
    test <- multinom(prog2 ~ ses + write, data = ml)
    predict(test, newdata = dses, "probs")

but if I try:

    require(caret)
    ml <- read.dta("http://www.ats.ucla.edu/stat/data/hsbdemo.dta")
    ml$prog2 <- relevel(ml$prog, ref = "academic")
    test <- train(prog2 ~ ses + write,method="multinom" ,data = ml)
    predict(test$finalModel, newdata = dses, "probs")

it returns `Error in eval(expr, envir, enclos) : object ''sesmiddle'' not found`, why?


', 106, '2014-12-29 09:29:58.913', '1d768ea3-b74c-440b-9e51-68abab1c4a8f', 3766, 'improved formatting', 6650, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2014-12-29 09:29:58.913', '1d768ea3-b74c-440b-9e51-68abab1c4a8f', 3766, 'Proposed by 106 approved by 2452, 21 edit id of 197', 6651, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Let''s assume I''m building a content recommendation engine for online content. I have web log data which I can import into a graph, containing a user ID, the page they viewed, and a timestamp for when they viewed it. Essentially, it''s a history of which pages each user has viewed.

I''ve used Neo4J and Cypher to write a simple traversal algorithm. For each page (node) I want to build recommendations for, I find which pages are most popular amongst other users who have also visited this page. That seems to give decent results. But, I''d like to explore alternatives to see which method gives the most relevant recommendations.

In addition to my simple traversal, I''m curious if there are graph-level properties I can utilize to build another set of recommendations with this data set. I''ve looked at [SNAP][1], it has a good library for algorithms like Page Rank, Clauset-Newman-Moore community detection, Girvan-Newman community detection, betweenness centrality, K core, and so on.

There are many algorithms to choose from. Which algorithms have you had success with? Which would you consider trying?

  [1]: http://snap.stanford.edu/', 3466, '2014-12-29 17:27:59.763', '869a19e4-cc6f-4a5d-98c4-6beb570ff320', 3779, 6656, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which Graph Properties are Useful for Predictive Analytics?', 3466, '2014-12-29 17:27:59.763', '869a19e4-cc6f-4a5d-98c4-6beb570ff320', 3779, 6657, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation><predictive-modeling><graphs>', 3466, '2014-12-29 17:27:59.763', '869a19e4-cc6f-4a5d-98c4-6beb570ff320', 3779, 6658, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Recently, I studied a paper called "What Does Your Chair Know About Your Stress Level?

It can be download at the link below.

> http://www.barnrich.ch/wiki/data/media/pub/2010_what_does_your_chair_know_about_your_stress_level.pdf

at page 210, Fig.5 (picture 5) mentioned that

> Spectra of the norm of the CoP vector during the experiment (stages 37) for the same subject used in Fig. 4.

How did they do to transform Fig.4 to Fig.5? And what do x-axis and y-axis mean in Fig.5?

Fig4
![Fig4][1]

Fig5
![Fig5][2]


  [1]: http://i.stack.imgur.com/Iq7qc.png
  [2]: http://i.stack.imgur.com/7HPxu.png', 6602, '2014-12-30 06:43:24.340', 'ea4697e0-5c2d-4572-8946-bfd9d3ae0c11', 3780, 6659, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to transform one graph to a spectrum?', 6602, '2014-12-30 06:43:24.340', 'ea4697e0-5c2d-4572-8946-bfd9d3ae0c11', 3780, 6660, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 6602, '2014-12-30 06:43:24.340', 'ea4697e0-5c2d-4572-8946-bfd9d3ae0c11', 3780, 6661, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('#I have a dataset of xyz coordinates with a date component

ex:


 - date1: [x1,y1,z1],
 - date2: [x2,y2,z2],
 - date3: [x3,y3,z3],
..

##I would like to classify a sample of object positions over the period of a week
(using indexes to re-map the classification label back to the date)

like so:

 - [x1,y1,z1], [x2,y2,z2], [x3,y3,z3], [x4,y4,z4], [x5,y5,z5], [x6,y6,z6], [x7,y7,z7],
 - [x8,y8,z8],[x9,y9,z9],[x10,y10,z10],[x11,y11,z11],[x12,y12,z12],[x13,y13,z13],[x14,y14,z14],

When I try to run KMeans it returns

    k_means = KMeans(n_clusters=cclasses,compute_labels=True)
    k_means.fit(process_set.hpc)
    date_classes = k_means.labels_

    ValueError: Found array with dim 3. Expected <= 2

Questions:

 - Do I have to run it through Principal Component Analysis (PCA) first? if so, how do I maintain date mapping to the classification created?
 - Are there any other methods I could use?
 - Am I doing everything completely backwards and should consider a different approach, if so, please direct me to somewhere where I can read', 6606, '2014-12-30 10:33:08.240', 'db863ece-d825-44ff-bbc5-1147888669c4', 3781, 6662, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Scikit Learn: KMeans Clustering 3D data over a time period (dimentionality reduction?)', 6606, '2014-12-30 10:33:08.240', 'db863ece-d825-44ff-bbc5-1147888669c4', 3781, 6663, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><scikit><k-means><dimensionality-reduction>', 6606, '2014-12-30 10:33:08.240', 'db863ece-d825-44ff-bbc5-1147888669c4', 3781, 6664, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('#I have a dataset of xyz coordinates with a date component

ex:


 - date1: [x1,y1,z1],
 - date2: [x2,y2,z2],
 - date3: [x3,y3,z3],
..

##I would like to classify a sample of object positions over the period of a week
(using indexes to re-map the classification label back to the date)

like so:

 - [x1,y1,z1], [x2,y2,z2], [x3,y3,z3], [x4,y4,z4], [x5,y5,z5], [x6,y6,z6], [x7,y7,z7],
 - [x8,y8,z8],[x9,y9,z9],[x10,y10,z10],[x11,y11,z11],[x12,y12,z12],[x13,y13,z13],[x14,y14,z14],

When I try to run KMeans it returns

    k_means = KMeans(n_clusters=cclasses,compute_labels=True)
    k_means.fit(process_set.hpc)
    date_classes = k_means.labels_

    ValueError: Found array with dim 3. Expected <= 2

Questions:

 - Do I have to run it through Principal Component Analysis (PCA) first? if so, how do I maintain date mapping to the classification created?
 - Are there any other methods I could use?
 - Am I doing everything completely backwards and should consider a different approach, any thoughts?

Thanks!', 6606, '2014-12-30 10:40:59.650', '74231e19-9b1b-4594-b2af-cdc94210c4fb', 3781, 'deleted 29 characters in body', 6665, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Imagine modeling the "*input(plaintext) - output(ciphertext)"* pairs of an encryption algorithm as a data science problem. Very informally the strength of an encryption scheme is measured by the randomness (unpredictable) of the output.
This is counter intuitive to classic regression problem which is used for prediction. Say Informally, the strength of the encryption scheme is determined by the number of such input-output pairs needed beyond which it becomes predictable ( the more the stronger).

How do we model this as data science problem ? Given all the pairs of two different encryption schemes can we determine which is stronger just by using the input-output pairs of both the schemes?

Is there any other way apart from regression ? to solve this problem ?', 4695, '2014-12-30 12:55:06.247', 'dee57b5c-c565-436c-91a2-317051809438', 3783, 6669, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to model this "un predicatability" problem?', 4695, '2014-12-30 12:55:06.247', 'dee57b5c-c565-436c-91a2-317051809438', 3783, 6670, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><regression>', 4695, '2014-12-30 12:55:06.247', 'dee57b5c-c565-436c-91a2-317051809438', 3783, 6671, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('while comparing two different algorithms to feature selection I stumbled upon the follwing question:

For a given dataset with a discrete class variable we want to train a naive bayes classifier. We decide to conduct feature selection during preprocessing using naive bayes in a wrapper approach.
Does this method of feature selection consider the size of the used feature subsets?

![Naive Bayes Classification][1]

When considering how NB classifies a given instance, the size of the feature subset being used for classification only influences the number of parts that the product of the conditional dependencies has but that does not make a difference, or does it?

It''d be great if someone could offer a solid explanation since for me it''s more of a gut feeling at the moment.

  [1]: http://i.stack.imgur.com/2Vh2Q.png', 6608, '2014-12-30 14:23:58.323', '896d0404-3cae-443c-87e6-139778e1b89e', 3784, 6673, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Does a NB wrapper consider feature subset size?', 6608, '2014-12-30 14:23:58.323', '896d0404-3cae-443c-87e6-139778e1b89e', 3784, 6674, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><feature-selection>', 6608, '2014-12-30 14:23:58.323', '896d0404-3cae-443c-87e6-139778e1b89e', 3784, 6675, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The correct term for what you''re describing here is ''class imbalance'' or ''class imbalance problem'' . It''d be great if you could include that in the title to have a more meaningful title.

**Concerning your first question:**

Have you plotted a confusion matrix of the resulting classifications? Is the reason why the accuracy is not satisfying really that instances are wrongly classified as the most common class?

Given your context of application it seems that you could use a certain degree of oversampling. To what extent this can be applied should depend on the frequency of each underrepresented class.

If there is a reasonably high variation in the value distributions on the underrepresented class instances then one could argue that this should be considered when applying the oversampling.

Also, the approach Charlie suggested in his answer could be considered, given that the instances of the underrepresented classes would form a dataset that is suitable for classification.

**Regarding your second question:**

Are you splitting the dataset in any way? Normally, given that the classifier has been trained on the same data, the outcome for the same instance should also be the same.


', 6608, '2014-12-30 19:45:57.283', '49f02503-62a8-463e-9b6c-0b5e940e406b', 3785, 6676, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The correct term for what you''re describing here is ''class imbalance'' or ''class imbalance problem'' . It''d be great if you could include that in the title to have a more meaningful title.

**Concerning your first question:**

Have you plotted a confusion matrix of the resulting classifications? Is the reason why the accuracy is not satisfying really that instances are wrongly classified as the most common class?

Given your context of application it seems that you could use a certain degree of oversampling. To what extent this can be applied should depend on the frequency of each underrepresented class.

If there is a reasonably high variation in the value distributions on the underrepresented class instances then one could argue that this should be considered when applying the oversampling.

Also, the approach Charlie suggested in his answer could be considered, given that the instances of the underrepresented classes would form a dataset that is suitable for classification.

*Edit:* I haven''t used naive bayes for text classification yet, so I''m not too sure how your attributes look like exactly. Do you just use the frequency of the terms that scored best with tfidf? More general, do you have discrete or continously valued attributes?

If the latter is the case you should consider using some kind of discretisation.

**Regarding your second question:**

Are you splitting the dataset in any way? Normally, given that the classifier has been trained on the same data, the outcome for the same instance should also be the same.


', 6608, '2014-12-30 19:56:36.333', '124fa3a1-795a-4316-9f82-07b8879115e1', 3785, 'added 352 characters in body', 6677, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have an excel file containing lot of columns. I want to create a graph database in Neo4J with all these columns as a separate node so that I can establish relationship between them and play around with the cypher query to get to know my data quite well. I did write a cypher query to make each column as a node, but when I run the relationship query it shows me the name of one node and gives a random number for all the related nodes despite changing the **captions in the properties** section.

Sample Create statement:

    LOAD CSV WITH HEADERS FROM "File Location" AS row CREATE (:NodeName {DATE_OCCURED: row.Date});

Query to visualize the relationship between nodes:

    MATCH (a)-[:`REL NAME`]->(b) RETURN a,b LIMIT 25;

This gives me the values the node "a" and random numbers for all the node "b". I don''t know where I am going wrong. I thought it has something to do with the way the data has been set up in my file. So I created a file for each column of my main file and wrote the create statements to get the nodes. I still don''t get the actual values of the related nodes.  ', 5043, '2014-12-30 21:45:13.760', 'f00b2a93-f597-4876-a258-9ddc5332ca43', 3786, 6679, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to create separate nodes for each column in a spreadsheet using Neo4J?', 5043, '2014-12-30 21:45:13.760', 'f00b2a93-f597-4876-a258-9ddc5332ca43', 3786, 6680, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neo4j><csv>', 5043, '2014-12-30 21:45:13.760', 'f00b2a93-f597-4876-a258-9ddc5332ca43', 3786, 6681, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('# Short Version
The difference is in the trade-off between complexity of model and how hard it will be to train.  The more states your variable can take, the much more data you''ll need to train it.  It sounds like you''re reducing the feature space through some front-end processing.

# Example

### Context:
Say you''re using NBC to model if you go outside given the high temperature that day.  Let''s say temperature is given to you as an int in Celsius which just happened to range between 0 and 40 degrees.  Over one year of running the experiment - recording the temperature and if you went outside - you''ll have 365 data points.

### Internal Representation:
The internal structure the NBC uses will be a matrix with 82 values (41 values for your one input variable * 2 because you have two states in your output class).  That means each bin will have an average of `365/82 = 8.9ish` samples.  In practice, you''ll probably see a few bins with lots more samples than this and a many bins with 0 samples.

### A Pitfall
Say you saw 8 cases at a temperature of 5 C (all of which you stayed inside) and nothing at a temp of 3 or 4 C.  If, after the model is built, you ''ask it'' what class a temp of 4 C should be in.  It won''t know.  Intuitively, we''d say "stay inside", but the model won''t.  One way to fix this is to bin the classes 0,1,2,... into larger groups of temperatures (i.e., class 0 for temp 0-3, class 1 for temp 4-7, etc).  The extreme case of this would be two temperature states "high" and "low".  The actual cut-off should depend more on the data observed, but one such scheme is converting temperatures within `0-20` to "low" and `21-40` is "high".

### Discussion

This front end processing seems to be what you''re talking about as the "wrapper" around the NBC.  The result of our "high" and "low" binning scheme will result in a much smaller model (2 input states and 2 output classes gives you a `2*2 = 4` number of classes.  This will be way easier to process and will take way less data to confidently train at the expense of complexity.  One example of a drawback of such a scheme is:  Say the actual pattern is that you love spring and fall weather and only go outside when it''s not too hot or not too cold.  So your outside adventures are evenly split across the upper part of the "low" class and the lower part of the "high" class giving you a really useless model.  If this were the case, a 3-class bin scheme of "high", "medium", and "low" would be best.', 6391, '2014-12-31 00:09:35.943', '9e12a9a3-21cf-47d8-9299-889bf9fd9384', 3787, 6682, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What''s the best / easiest way to create a graph from address data?  For example, if I have 100 houses from all across a city is there any easy way to determine the shortest distance between two houses and all that good stuff?  Would this require changing the data into coordinates and using GIS software or can I get away with using Python or R?', 6611, '2014-12-31 02:50:32.863', 'e2bb151e-eed6-4d74-bb3a-915e9a336658', 3788, 6683, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Creating a Graph from Address Data', 6611, '2014-12-31 02:50:32.863', 'e2bb151e-eed6-4d74-bb3a-915e9a336658', 3788, 6684, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><graphs>', 6611, '2014-12-31 02:50:32.863', 'e2bb151e-eed6-4d74-bb3a-915e9a336658', 3788, 6685, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2014-12-31 12:39:30.103', '21d4ccfa-66c3-4985-a29b-a145fa5df21e', 2382, '102', 6687, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have an excel file containing lot of columns. I want to create a graph database in Neo4J with all these columns as a separate node so that I can establish relationship between them and play around with the cypher query to get to know my data quite well. I did write a cypher query to make each column as a node, but when I run the relationship query it shows me the name of one node and gives a random number for all the related nodes despite changing the **captions in the properties** section.

Sample Create statement:

    LOAD CSV WITH HEADERS FROM "File Location" AS row CREATE (:NodeName {DATE_OCCURED: row.Date});

Query to visualize the relationship between nodes:

    MATCH (a)-[:`REL NAME`]->(b) RETURN a,b LIMIT 25;

This gives me the values the node "a" and random numbers for all the node "b". I don''t know where I am going wrong. I thought it has something to do with the way the data has been set up in my file. So I created a file for each column of my main file and wrote the create statements to get the nodes. I still don''t get the actual values of the related nodes.

![Sample output for the relationship query][1]


  [1]: http://i.stack.imgur.com/xxKVi.png', 5043, '2014-12-31 14:48:00.310', 'a9147463-2e28-4a2b-8c5d-46f96d8ae6d0', 3786, 'added 97 characters in body', 6688, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-12-31 15:34:17.397', '1c7143a2-7ecc-4907-99f6-b74c6eb8ca07', 3789, 6689, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2014-12-31 15:34:17.397', '34b8e766-0db5-43c3-98d1-a0d3e9f46236', 3790, 6690, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you have latitude/longitude coordinate data, there should be no problem accomplishing this using great circle calculations which could indeed be accomplished in Python, R, or essentially any other language.

Here is an article on several methods and calculations for this:

[Calculate distance, bearing and more between Latitude/Longitude points][1]

The main issue with street address data alone is of course the lack of contextual information regarding the physical layout of the streets. Given a sufficiently complete scale map of the relevant area, you could also calculate a distance. That said, the haversine formula discussed in the article above would have a greater accuracy unless the scale map mentioned was also plotted as the surface of a sphere.

  [1]: http://www.movable-type.co.uk/scripts/latlong.html', 2932, '2014-12-31 16:18:21.237', '9e281ec4-3704-4275-9382-30372ee3f9ff', 3791, 6691, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m wondering if e-commerce companies where products are offered by users, such as EBay, are using Object Recognition to ensure that an uploaded image corresponds to an specific type of object (clothing, shoes, glasses, etc) either to classify automatically or more importantly to filter undesired images (such as non related or even illegal types).

If so, which algorithms and/or open platforms could be use for doing so? From what I''ve looked it seems that HOG+Exemplar SVM might be one of the most accurate methods developed so far (http://www.cs.cmu.edu/~efros/exemplarsvm-iccv11.pdf), even having couple of public repo''s with Matlab implementations (https://github.com/quantombone/exemplarsvm), but I''m still wondering if this is being used in industry.', 5143, '2014-12-31 17:57:48.243', '11179028-252e-48c2-8456-6dc17e18826a', 3792, 6692, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Object Recognition for classification, is it being used in industry?', 5143, '2014-12-31 17:57:48.243', '11179028-252e-48c2-8456-6dc17e18826a', 3792, 6693, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><object-recognition><hog><computer-vision>', 5143, '2014-12-31 17:57:48.243', '11179028-252e-48c2-8456-6dc17e18826a', 3792, 6694, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here''s a simple solution using the R package `ggmap`.

    start <- ''95 clark st, new haven, ct''
    end <- ''maison mathis, new haven, ct''
    legs <- route(start,end, alternatives = TRUE)

Will find routes from `start` to `end`.

          m    km     miles seconds   minutes       hours  startLon startLat    endLon   endLat leg route
    1    60 0.060 0.0372840       7 0.1166667 0.001944444 -72.91617 41.31511 -72.91678 41.31541   1     A
    2   718 0.718 0.4461652      95 1.5833333 0.026388889 -72.91678 41.31541 -72.92100 41.30979   2     A
    3   436 0.436 0.2709304      64 1.0666667 0.017777778 -72.92100 41.30979 -72.92555 41.31171   3     A
    4   431 0.431 0.2678234      68 1.1333333 0.018888889 -72.92555 41.31171 -72.92792 41.30829   4     A
    5    60 0.060 0.0372840       7 0.1166667 0.001944444 -72.91617 41.31511 -72.91678 41.31541   1     B
    6  1276 1.276 0.7929064     179 2.9833333 0.049722222 -72.91678 41.31541 -72.92430 41.30543   2     B
    7   421 0.421 0.2616094      62 1.0333333 0.017222222 -72.92430 41.30543 -72.92869 41.30729   3     B
    8   129 0.129 0.0801606      28 0.4666667 0.007777778 -72.92869 41.30729 -72.92792 41.30829   4     B
    9    60 0.060 0.0372840       7 0.1166667 0.001944444 -72.91617 41.31511 -72.91678 41.31541   1     C
    10  421 0.421 0.2616094      58 0.9666667 0.016111111 -72.91678 41.31541 -72.91924 41.31211   2     C
    11  522 0.522 0.3243708     101 1.6833333 0.028055556 -72.91924 41.31211 -72.92502 41.31382   3     C
    12  240 0.240 0.1491360      33 0.5500000 0.009166667 -72.92502 41.31382 -72.92555 41.31171   4     C
    13  431 0.431 0.2678234      68 1.1333333 0.018888889 -72.92555 41.31171 -72.92792 41.30829   5     C

And then we can find the length of time estimated to walk there with something like this.

    tapply(legs$minutes, legs$route, max)

Hope this helps!', 4724, '2014-12-31 21:47:37.490', 'ff772342-d252-43ce-b29a-04fdab6b5235', 3793, 6695, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This refers to a system described in a book by Nick Lawrence titled "[Correlithm Object Technology][1]". The author coined the term "correlithm" as a combination of "correlation" and "algorithm".

The Correlithm Objects ("COs") described in the book are, in the author''s view, "primary data tokens" in biological neural systems and are central to "all high-level data representation, storage, and manipulation". In addition to this the author describes the COs as "important mathematical objects of the statistics of bounded, high-dimensional spaces".

Considering these descriptions it seems like these COs could be useful for AI. What I was curious about is whether they are actually used anywhere in the industry, and if so, in what kind of situations?

  [1]: http://www.amazon.com/Correlithm-Object-Technology-Nick-Lawrence/dp/0975276107/ref=si3_rdr_bb_product', 6625, '2015-01-02 05:36:54.783', '31f9b01e-4867-4717-9ea4-d2ff2625e9a0', 3795, 6699, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are Correlithm Objects used for anything in the industry?', 6625, '2015-01-02 05:36:54.783', '31f9b01e-4867-4717-9ea4-d2ff2625e9a0', 3795, 6700, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><algorithms><correlation>', 6625, '2015-01-02 05:36:54.783', '31f9b01e-4867-4717-9ea4-d2ff2625e9a0', 3795, 6701, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As far as I am aware, object recognition is not extensively used in industry yet. Google''s image search, for instance, is based on exploiting the text on the web pages rather than the images themselves. I have seen several start-up companies that market prototypes based on object recognition, but these products are either prototypes or not widely used.

Regarding state-of-the-art techniques, you should take a look to recent literature on deep learning and deep neural networks applied to object recognition, along with more specific techniques like convolutional networks. ', 2576, '2015-01-02 10:58:38.187', '7b419f12-d647-466f-a7a7-2368fc447553', 3796, 6702, '2');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (303, '2015-01-02 14:48:57.567', '00bc2110-a495-4ff5-8390-cc6015380340', 3770, '9', 6703, '33');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am reading up about lambda architecture.

It makes sense. we have queue based data ingestion. we have an in-memory store for data which is very new and we have HDFS for old data.

So we have our entire data set. in our system. very good.

but the architecture diagram shows that the merge layer is able to query both the batch layer and the speed layer in one shot.

How to do that?

Your batch layer is probably a map reduce job or a HIVE query. The speed layer query is probably a scala program which is execution on the spark.

Now how will you merge these?

Is there any guidance.
', 6644, '2015-01-02 20:03:59.950', 'e8970e18-7371-4f6c-954c-868d25de23c7', 3797, 6704, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Lambda Architecture - How to implement the Merge Layer / Query Layer', 6644, '2015-01-02 20:03:59.950', 'e8970e18-7371-4f6c-954c-868d25de23c7', 3797, 6705, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop>', 6644, '2015-01-02 20:03:59.950', 'e8970e18-7371-4f6c-954c-868d25de23c7', 3797, 6706, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try exploring the rich field of "Anomaly Detection in Time Series". Control charts and CUSUMs (or cumulative sum control charts) might help you.

Simple [Bullet Graphs][1] might be all you need. Based on historical data and domain knowledge, define normal variance. Then make it clear to stakeholders when the current value is outside of predefined ranges.

Stephen Few is an expert in business dashboards. Any of his books will help you.

If you are open to R, try [Shiny][2] to create simple interactive web applications (It is very straightforward).

Create quick prototypes and get feedback!


  [1]: http://en.wikipedia.org/wiki/Bullet_graph
  [2]: http://shiny.rstudio.com/', 1330, '2015-01-03 01:07:15.080', 'aa6c0eac-6f22-4bea-9180-78143738c782', 3798, 6707, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have shared a number of resources on *time series classification and clustering* in one of my recent answers here on *Data Science StackExchange*: http://datascience.stackexchange.com/a/3764/2452. Hopefully, you will find them relevant to this question and useful.', 2452, '2015-01-03 01:17:47.957', '8b874b81-df26-4bd1-989e-5a455f52b0f7', 3799, 6708, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This won''t be a very satisfying answer, but here''s my take...

> **For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?**

> **For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?**

Same answer for both: you can''t do this for unknown properties, and for known properties it will depend on how the values were computed.

As you alluded to:

> (an average of fiancial rates would be a non-sense for example)

There is no single transformation that will be appropriate in all cases, whether the properties/values are known or unknown. Even with known properties, you''ll likely need a unique transformation for each type: mean, median, mode, min, max, boolean, etc.

> **when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?**

Whenever possible, try to preserve the full granularity of the smallest possible step. Assuming you know how to transform the values, you can always roll-up the steps (e.g., day to month, month to year)... but you won''t necessarily be able to reconstruct smaller steps from larger ones following a lossy conversion.', 819, '2015-01-03 03:15:08.960', 'fbbaaef8-0599-48c3-86d5-41a3460163cd', 3800, 6711, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('>**when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?**

For your timeseries analysis you should do both: get to the highest granularity possible with the daily dataset, and also repeat the analysis with the monthly dataset. With the monthly dataset you have 120 data points, which is sufficient to get a timeseries model even with seasonality in your data.

>**For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?**

To obtain say weekly or monthly data from daily data, you can use smoothing functions. For financial data, you can use moving average or exponential smoothing, but if those do not work for your data, then you can use the spline smoothing function "smooth.spline" in R: https://stat.ethz.ch/R-manual/R-patched/library/stats/html/smooth.spline.html

The model returned will have less noise than the original daily dataset, and you can get values for the desired time points. Finally, these data points can be used in your timeseries analysis.


>**For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?**

To obtain daily data when you have monthly or weekly data, you can use interpolation. First, you should find an equation to describe the data. In order to do this you should plot the data (e.g. price over time). When factors are known to you, this equation should be influenced by those factors. When factors are unknown, you can use a best fit equation. The simplest would be a linear function or piecewise linear function, but for financial data this won''t work well. In that case, you should consider piecewise cubic spline interpolation. This link goes into more detail on possible interpolation functions: http://people.math.gatech.edu/~meyer/MA6635/chap2.pdf.

In R, there is a method for doing interpolation of timeseries data. Here you would create a vector with say weekly values and NAs in the gaps for the daily values, and then use the "interpNA" function to get the interpolated values for the NAs. However, this function uses the "approx" function to get the interpolated values, which applies either a linear or constant interpolation. To perform cubic spline interpolation in R, you should use the "splinefun" function instead.

Something to be aware of is that timeseries models typically do some sort of averaging to forecast future values whether you are looking at exponential smoothing or Auto-Regressive Integrated Moving Average (ARIMA) methods amongst others. So a timeseries model to forecast daily values may not be the best choice, but the weekly or monthly models may be better. ', 6648, '2015-01-03 07:58:10.663', '9412ed5f-9ab8-48ae-9947-c78838871533', 3801, 6712, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m not an expert in this area, but I believe that your question is concerned with *time series aggregation and disaggregation*. If that is the case, here are some hopefully relevant resources, which might be helpful in solving your problem (first four items are main, but representative, and last two are supplementary):

- [Temporal Aggregation and Economic Time Series](http://www4.ncsu.edu/~jjseater/tempaggecontimeseries.pdf)
- [Temporal Disaggregation of Time Series](http://journal.r-project.org/archive/2013-2/sax-steiner.pdf) (IMHO, an excellent overview paper)
- [CRAN Task View: Time Series Analysis](http://cran.r-project.org/web/views/TimeSeries.html) (R-focused)
- [Working with Financial Time Series Data in R](http://faculty.washington.edu/ezivot/econ424/Working%20with%20Time%20Series%20Data%20in%20R.pdf)
- [Notes on chapters contents for the book "Time Series Analysis and Forecasting"](http://www.scausa.com/tsfbook.php)
- [Discussion on Cross Validated](http://stats.stackexchange.com/q/121599/31372) on daily to monthly data conversion (Python-focused)', 2452, '2015-01-03 08:48:51.070', '9b9cf791-36d6-418f-96da-b66d4d89977e', 3802, 6713, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m not an expert in this area, but I believe that your question is concerned with *time series aggregation and disaggregation*. If that is the case, here are some hopefully relevant resources, which might be helpful in solving your problem (first five items are main, but representative, and last two are supplementary):

- [Temporal Aggregation and Economic Time Series](http://www4.ncsu.edu/~jjseater/tempaggecontimeseries.pdf)
- [Temporal Disaggregation of Time Series](http://journal.r-project.org/archive/2013-2/sax-steiner.pdf) (IMHO, an excellent overview paper)
- [CRAN Task View: Time Series Analysis](http://cran.r-project.org/web/views/TimeSeries.html) (R-focused)
- [Introduction to R''s Time Series Facilities](http://people.su.se/~lundh/reproduce/introduction_ts.pdf)
- [Working with Financial Time Series Data in R](http://faculty.washington.edu/ezivot/econ424/Working%20with%20Time%20Series%20Data%20in%20R.pdf)
- [Notes on chapters contents for the book "Time Series Analysis and Forecasting"](http://www.scausa.com/tsfbook.php)
- [Discussion on Cross Validated](http://stats.stackexchange.com/q/121599/31372) on daily to monthly data conversion (Python-focused)', 2452, '2015-01-03 09:00:28.947', 'f57772aa-996f-490b-aabe-a61536f10727', 3802, 'Added reference to introductory paper on time series in R.', 6714, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For an upcoming project, I''m mining textual posts from an online forum, using Scrapy. What is the best way to store this text data? I''m thinking of simply exporting it into a jsonline file, but is there a better format? Or does it not matter?', 6660, '2015-01-03 22:38:39.097', '26ed7980-c266-466a-b0ff-433a10d463e0', 3804, 6718, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Format for Storing Mined Textual Data', 6660, '2015-01-03 22:38:39.097', '26ed7980-c266-466a-b0ff-433a10d463e0', 3804, 6719, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining><crawling>', 6660, '2015-01-03 22:38:39.097', '26ed7980-c266-466a-b0ff-433a10d463e0', 3804, 6720, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('for python, the standard tool is pandas. It was specifically designed to deal with financial data timeseries.
[pandas timeseries][1]


  [1]: http://pandas.pydata.org/pandas-docs/stable/timeseries.html', 1256, '2015-01-03 23:08:17.697', '8accfb60-2c21-40dd-8804-9c948bc42b5b', 3805, 6721, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The *optimal* solution very much depends on multiple factors, including your (current and future) business and IT processes, stakeholders'' needs and preferences, and, in general, business and IT architectures. So, IMHO it''s difficult to answer this wide and not well-defined question. Having said that, I hope that you will find the following earlier **answers** of mine here on *Data Science StackExchange* relevant and useful.

- On **data analysis workflow** (introductory): http://datascience.stackexchange.com/a/1006/2452
- On **data analysis workflow** (more detailed): http://datascience.stackexchange.com/a/759/2452
- On **dashboard visualization**: http://datascience.stackexchange.com/a/907/2452
- On **big data visualization**: http://datascience.stackexchange.com/a/3723/2452', 2452, '2015-01-04 05:22:48.670', 'dfd4597d-f937-42d2-9473-f39199b40301', 3806, 6722, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Im doing my academic project. im having the base paper for reference  the paper is IEEE paper "effective and efficient clustering methods for correlated probabilistic graph". i wish to do this in R tool. in this paper two algorithm are implemented. i like to implement the peedr algorithm in the paper. how can i give the input for that algorithm.? suggest the packgages in R tool
 the paper can be found here
http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6570474', 6664, '2015-01-04 10:59:42.973', 'b1d52658-ba4e-45c4-adfe-2e15b4039e12', 3807, 6723, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How i can generate the probabilistic graph for my dataset?', 6664, '2015-01-04 10:59:42.973', 'b1d52658-ba4e-45c4-adfe-2e15b4039e12', 3807, 6724, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<graphs>', 6664, '2015-01-04 10:59:42.973', 'b1d52658-ba4e-45c4-adfe-2e15b4039e12', 3807, 6725, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to develop my neural network with both early stopping and bayesian regularization (matlab implementation, lm algorithm is used for both).
Since in bayesian regularization I have not the validation set, how can I compare the generalization capability of the networks obtained with the two methodologies?
Thanks', 6559, '2015-01-04 11:22:28.817', 'b276d171-dead-488a-92a8-7883af027b9d', 3808, 6726, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Compare Neural Network generalization results', 6559, '2015-01-04 11:22:28.817', 'b276d171-dead-488a-92a8-7883af027b9d', 3808, 6727, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork>', 6559, '2015-01-04 11:22:28.817', 'b276d171-dead-488a-92a8-7883af027b9d', 3808, 6728, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Søren Højsgaard has many quality resources for graphical models in R. He has a tutorial ["Graphical Models with R"][1] and a [list of CRAN packages][2].

Additionally, [mclust][3] is one of the best clustering packages in R.


  [1]: http://people.math.aau.dk/~sorenh/misc/2012-Oslo-GMwR/GMwR-notes.pdf
  [2]: http://cran.r-project.org/web/views/gR.html
  [3]: http://cran.r-project.org/web/packages/mclust/index.html', 1330, '2015-01-04 19:29:48.930', '4110c1ff-0bf3-47cb-b1a1-56727fd6da22', 3810, 6733, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m building a neural network to analyze a business'' sales. I''m normalizing all input values to the range `{0,1}`.

I''m struggling with the day of the week column. Business days are identified by a number ranging `{1-5}` (1=Monday). Normalizing these values to the range `{0,1}` is straightforward, but results in a major bias in the final output.

The reason is that the full range of normalized values for the business day column is explored with every week worth of data, whereas other price-related column explore their full range of normalized values infrequently.

The business day column ends up being the largest contributor to the final output.

How can I normalize it to make its contribution more in tune with the rest of the inputs?', 6669, '2015-01-04 21:52:38.977', '5cc6d768-862f-425a-84df-10d2cb9872c2', 3811, 6735, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('normalize identification values properly', 6669, '2015-01-04 21:52:38.977', '5cc6d768-862f-425a-84df-10d2cb9872c2', 3811, 6736, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork>', 6669, '2015-01-04 21:52:38.977', '5cc6d768-862f-425a-84df-10d2cb9872c2', 3811, 6737, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Don''t forget about MLlib (the hot kid on the block), cloudera-ml, cloudera oryx, cloudera oryx2, and a myriad of other efforts to make old algorithms like k-means solve nonexistant problems on wannabe big data...

# There is no one size fits all.

Thus, there is no answer to your question, in particular no concise answer. ', 924, '2015-01-04 22:29:14.980', 'e5bd68a3-7dd8-4c3b-90ac-c599a9dbbbf3', 3812, 6738, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In general, use a storage method that allows you to quickly query it. If your collection is huge, you might need something Lucene-based, like ElasticSearch. If you are a SQL crack and your favorite DB supports it, a full-text index might do the trick. For small sizes like the 5000 documents, even Linux'' LocateDb+grep or OSX'' spotlight could be enough.

The important point is to be able to quickly verify assumptions about the content of your data - how many documents contain X and Y, does any document contain W but not V, etc.

This will be useful at both the whole set level as well as for analyzing your topic clusters. Finally, a few GNU tools or SQL mastery can also help you profile your document sets more efficiently (n-gram counts/ranks, collocations, concordances, etc)

Edit: that means, for the above reasons and given your collection size, good old plain text (in a file system or a database) might be more efficient than any "fancy" format.', 6672, '2015-01-04 23:57:20.150', 'd31b3a88-2cee-45fa-9308-fe174ef3f4c4', 3813, 6739, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Statistics is a scientific approach to inductive inference based on probabilistic models associated with the data at hand. By extension, it covers the design of experiments and surveys to gather data to this purpose.', 6622, '2015-01-05 11:25:52.770', '20d43604-c8e2-44b6-b6b4-ad0601e34074', 3790, 'added 216 characters in body', 6742, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-05 11:25:52.770', '20d43604-c8e2-44b6-b6b4-ad0601e34074', 3790, 'Proposed by 6622 approved by 2452, 21 edit id of 198', 6743, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It is often pointed out that _sample_ is an overloaded term in statistics and the sciences being supported by statistics.  In my field (geological sciences) as in most other sciences, the process of collecting meaningful data is critical and discussions about the traps and pitfalls in that process talk about _sampling_.  Not far down the road from that, particularly when lab results are back, conversations involving statisticians, data scientists, geomathematicians, GIS analysts and even _normal_ geologists are likely to attempt to include multiple meanings of _sample_ in the same sentence!

Q: Have any data scientists (or statisticians) found practical ways to communicate these different meanings?

One way is to always add soil, rock, statistical and so on before _sample_.  But I was curious if there are any other approaches to effective communication that are in use.', 6646, '2015-01-05 18:34:55.920', 'bf4f591b-791c-4e57-a31f-4be287639e54', 3814, 6744, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Communicating clearly about "samples"', 6646, '2015-01-05 18:34:55.920', 'bf4f591b-791c-4e57-a31f-4be287639e54', 3814, 6745, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics><sampling>', 6646, '2015-01-05 18:34:55.920', 'bf4f591b-791c-4e57-a31f-4be287639e54', 3814, 6746, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('"Sample" as a noun usually refers to a single data point.  "Sample" as a verb is the act of extracting a subset of data points from some larger body (reality or a larger dataset).  The only way to be less ambiguous is use more specific words than "data" or "sample".


## Example:

Say you collect 1MM data points from four different sensors in the field giving you four sets of 250k data points.  Say this data is too big for some demo of a model you''re testing or an analysis you''re running, so you pick 100k data points evenly split across the four sensors (giving four sets of 25k data points).

In this example, you''re sampling twice.  First, to gather your 1MM data points sampling from reality.  Second, you sample again to decrease the size of your data set to something more manageable.  ''Data'' or ''sample'' could refer to reality, the 1MM dataset, the 100k dataset, or any of the sensor-specific subsets.  To make it less ambiguous, establish a unique name as soon as possible for each possible definition you''ll be working with.  (''reality'' for the set of all possibly observed samples.  ''the complete dataset'', something derived from the source of the dataset, or even X for the full 1MM dataset.  ''our trial dataset'', or even Y for the small 100k dataset.

What you actually do comes down to context and what''s appropriate for your intended audience, but the general answer is to use more specific words.', 6391, '2015-01-05 22:16:06.093', '1e2b5c37-c8a9-4d5d-8769-c1100094bec1', 3815, 6747, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As Neil said in the comments, split out a [test set](http://en.wikipedia.org/wiki/Test_set) (data you don''t use in training either model) and see how each trained model performs on the test set.', 6391, '2015-01-05 22:33:02.010', '30f591a7-df10-4e20-80d9-e0550f18cb81', 3816, 6748, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('> For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?

Aggregation.

For example, you have the number of time people searched for ''widgets'' every day.  Add up the daily totals for a month to get monthly totals.  I would need to see more specifics about the actual data collected at each granularity to give you a more complete version.

> For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?

You can''t.

In physics, a comparable idea is the [Nyquist frequency](http://en.wikipedia.org/wiki/Nyquist_frequency).  The general idea is that you can''t add more information than what you already have present in your data without bringing in more data.  Given only the day someone ran a query, how can you tell what time of day that query was ran?  You may be able to make some inferences, but the only way to answer the question is to directly or indirectly bring in more information to the system.  There are things you can do to make informed guesses at the daily state of monthly variables (as
gchaks mentioned, interpolation), but your data is still fundamentally monthly data stretched to look daily.

> When given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?

That totally depends on what you''re trying to answer.

The smaller granularity will be more sensitive to noise and other anomalies.  The lager granularity will be able to answer questions more confidently, but loose some of it''s usefulness.  For example, if you''re trying to see when people start looking up venues to weekend plans to know when to launch marketing campaigns for a new night club, you''ll want to be looking at daily data, if not smaller.  If you''re looking at the general trending of night clubs to figure out who you want to invest in, then monthly would probably be better.

', 6391, '2015-01-05 22:49:27.293', 'ca0340b2-b344-4c29-b7c1-076d107463b3', 3817, 6749, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It is possible that the other variables you''re feeding into the NN are simply bad at predicting sales.  Sell prediction is a notoriously hard problem.

Specifically the addressing of mapping a multi-state categorical variable to the NN''s {0,1} input range:  Another idea is to change that one, 5-state variable into five boolean variables.  Rather than {0,0.25,0.5,0.75,1.0} on your one variable, make each of the five boolean variables represent a single day and make [1,0,0,0,0] equal Monday, [0,1,0,0,0] equal Tuesday, etc.  I''ve personally had more success both with training good networks and introspecting the network itself when spreading out states of classes like that.

Other hacks you can try:
 * Take out the the ''day'' column all together and see if any of the other variables get used.
 * Plot the distribution of spend as a function of day.  Even if nothing else comes of this current model, it sounds like you''ve found one interesting insight already.
 * Consider also trying different models.', 6391, '2015-01-05 23:09:25.800', '8b40de71-b290-46a9-80a1-04d18d53d905', 3818, 6750, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Being able to aggregate data based on date segments is a piece of cake in Tableau software. You just plug your data in and you can drag and drop both the metric and date dimensions onto a report body space, and Tableau can whip up aggregate visualizations and/or detail data tables on the fly. You can group/sum by year, quarter, month, week, day, hour, etc. and it''s all standard, built in, out of the box functionality. If you wanted to incorporate additional data later on (which I assume is what you meant by ''unknown properties''), you can import another data set onto the first one, as long as it also offers dates.

I recommend checking out the free version, which I believe is called Tableau Public. ', 6682, '2015-01-06 01:00:54.457', '4847d819-dcec-4955-95e9-a44c148980cf', 3819, 6751, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Being able to aggregate data based on date segments is a piece of cake using Tableau software. You would simply plug your data into the tool, and then you can drag and drop both the metric and date dimensions onto a report body space. Tableau will instantaneously whip up aggregate visualizations and/or detail data tables, on the fly. You can group/sum by year, quarter, month, week, day, hour, etc. (standard, built in, out of the box functionality offered by the tool)

Also, if you wanted to incorporate additional data later on (which I assume is what you meant by ''unknown properties''), you can import another data set and easily append it onto the first one, as long as it also offers dates.

I would recommend checking out the free version, which I believe is called Tableau Public. ', 6682, '2015-01-06 01:06:26.827', 'cb008c5e-5c75-4f9b-afb8-463e3ded8bbd', 3819, 'added 86 characters in body', 6752, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You don''t need a tool and I don''t recommend you use one.

Convert the html to well-formed XML (XHTML) - I recommend the tagsoup.

Once you''ve done that the data is just another XML feed and you can write an XSLT transformation (or XQuery) to access and pull out the data you want in the format you want.

That might mean learning  XSLT/XQuery if you don''t already know it but you will be learning skills that (unlike scraping tools) have multiple rather than just than one useful application.

', 498, '2015-01-06 01:26:41.587', '648575a3-2be1-4d80-be41-2afe8d517674', 3820, 6753, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am interested in finding a statistic that tracks the unpredictability of a time series. For simplicity sake, assume that each value in the time series is either 1 or 0. So for example, the following two time series are entirely predictable
TS1: 1 1 1 1 1 1 1 1
TS2: 0 1 0 1 0 1 0 1 0 1 0 1

However, the following time series is not that predictable:
TS3: 1 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1

I am looking for a statistic that given a time series, would return a number between 0 and 1 with 0 indicating that the series is completely predictable and 1 indicating the series in completely unpredictable.

I looked at some entropy measures like Kolmogorov Complexity and Shannon entropy, but neither seem to fit my requirement. In Kolmogorov complexity, the statistic value changes depending on the length of the time series (as in "1 0 1 0 1" and "1 0 1 0" have different complexities, so its not possible to compare predictability of two time series with differing number of observations). In Shannon entropy, the order of observations didn''t seem to matter.

Any pointers on what would be a good statistic for my requirement?
', 6685, '2015-01-06 04:34:09.967', '5222b30f-d51b-4f18-859e-db769965aea1', 3821, 6754, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Finding unpredictability or uncertainty in a time series', 6685, '2015-01-06 04:34:09.967', '5222b30f-d51b-4f18-859e-db769965aea1', 3821, 6755, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<time-series>', 6685, '2015-01-06 04:34:09.967', '5222b30f-d51b-4f18-859e-db769965aea1', 3821, 6756, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Since you have looked at Kolmogorov-Smirnov and Shannon entropy measures, I would like to suggest some other hopefully relevant options. First of all, you could take a look at the so-called [approximate entropy $ApEn$](http://en.wikipedia.org/wiki/Approximate_entropy). Other potential statistics include *block entropy*, *T-complexity* (*T-entropy*) as well as *Tsallis entropy*: http://members.noa.gr/anastasi/papers/B29.pdf

In addition to the above-mentioned potential measures, I would like to suggest to have a look at available statistics in *Bayesian inference-based* model of *stochastic volatility* in time series, implemented in `R` package `stochvol`: http://cran.r-project.org/web/packages/stochvol (see detailed [vignette](http://cran.r-project.org/web/packages/stochvol/vignettes/article.pdf)). Such statistics of uncertainty include *overall level of volatility* $\mu$, *persistence* $\phi$ and *volatility of volatility* $\sigma$: http://simpsonm.public.iastate.edu/BlogPosts/btcvol/KastnerFruwhirthSchnatterASISstochvol.pdf. A **comprehensive** *example* of using stochastic volatility model approach and `stochvol` package can be found in the excellent blog post ["Exactly how volatile is bitcoin?"](http://www.themattsimpson.com/2014/02/22/exactly-how-volatile-is-bitcoin) by Matt Simpson.', 2452, '2015-01-06 07:40:12.223', '7a79d35d-e3d8-49af-86aa-bce18f960406', 3823, 6760, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a data set, in excel format, with **account names, reported symptoms, a determined root cause and a date in month year format** for each row. I am trying to implement a mahout like system with a purpose of determining the likelihood symptoms an account can report by doing a user based similarity kind of a thing. Technically, I am just hoping to tweak the recommendation system into a deterministic system to spot out the probable symptoms an account can report on. Instead of ratings, I can get the frequency of symptoms by accounts. Is it possible to use a programming language or any other software to build such system?

Here is an example:

Account : *X* Symptoms : *AB, AD, AB, AB*

Account : *Y*  Symptoms : *AE, AE, AB, AB, EA*

For the sake of this example, let''s assume that all the dates are this month.

O/P: Account : X Symptom: AE

Here both of them have reported *AB* 2 or more times. I could fix such number as a threshold to look for probable symptoms. ', 5043, '2015-01-06 14:08:35.533', '5c6e92d2-1da8-4e41-a94c-b867213e32a4', 3825, 6765, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Mimic a Mahout like system', 5043, '2015-01-06 14:08:35.533', '5c6e92d2-1da8-4e41-a94c-b867213e32a4', 3825, 6766, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation><similarity><apache-mahout>', 5043, '2015-01-06 14:08:35.533', '5c6e92d2-1da8-4e41-a94c-b867213e32a4', 3825, 6767, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to do a correlation analysis between inputs and outputs inspecting the data in order to understand which input variables to include. What could be a threshold in the correlation value to consider a variable eligible to be an input for my Neural Network?', 6559, '2015-01-06 20:43:35.480', '97ea1fa2-732d-4349-a3a1-22d570adbf12', 3826, 6768, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Correlation threshold for Neural Network features selection', 6559, '2015-01-06 20:43:35.480', '97ea1fa2-732d-4349-a3a1-22d570adbf12', 3826, 6769, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 6559, '2015-01-06 20:43:35.480', '97ea1fa2-732d-4349-a3a1-22d570adbf12', 3826, 6770, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Given non-linearity of neural networks, I believe correlation analysis isn''t a good way to estimate importance of variables. For example, imagine that you have 2 input variables - `x1` and `x2` - and following conditions hold:

 - `cor(x2, y) = 1` if `x1 = 1`
 - `cor(x2, y) = 0` otherwise
 - `x1 = 1` in 10% of cases

That is, `x2` is a very good predictor for `y`, but only given that `x1 = 1`, which is the case only in 10% of data. Taking into account correlations of `x1` and `x2` separately won''t expose this dependency, and you will most likely drop out both variables.

There are other ways to perform feature selection, however. Simplest one is to train your model with all possible sets of variables and check the best subset. This is pretty inefficient with many variables, though, so many ways to improve it exist. For a good introduction in best subset selection see chapter 6.1 of [Introduction to Statistical Learning][1].


[1]: http://www-bcf.usc.edu/~gareth/ISL/', 1279, '2015-01-07 01:26:17.560', 'a3c19bd4-4609-4b19-af16-e4771d6cc751', 3827, 6771, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Suppose that a forum post has, on average, 2000 characters, which is more or less the equivalent of a page of text, than the total memory needed is 10MB if text is ASCII. Even if the text is Unicode encoded in an Asian language it will take 40MB.

This is far too little for modern computers, so a simple text format is the best since it can be parsed in the fastest way, and loaded into RAM all at once.', 108, '2015-01-07 07:39:04.027', '0ce587c3-8a99-4b1d-8818-38a5d03d5105', 4826, 7768, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is there any way to use package "dplyr" on RStudio having R base 3.0.2 ?  I am not interested  in "plyr" package.

 Thanks
Navin', 7702, '2015-01-07 16:50:18.803', '893f35a2-6bc3-4720-8121-af16409c2f15', 4827, 7769, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can we use package "dplyr" on R base 3.0.2?', 7702, '2015-01-07 16:50:18.803', '893f35a2-6bc3-4720-8121-af16409c2f15', 4827, 7770, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 7702, '2015-01-07 16:50:18.803', '893f35a2-6bc3-4720-8121-af16409c2f15', 4827, 7771, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('dplyr 0.3 requires R 3.1+. If you''re stuck on R 3.0.x, you have to use dplyr 0.2.x. ', 404, '2015-01-07 22:03:37.357', 'e6e7665b-768e-44b6-8269-ab5db954feba', 4828, 7772, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('We faced this problem and analysed the issue.
Cloudera, Hue or Hive didn''t have any error reported but the users via Beeswax were unable to run queries.
It was good to analyze below:
Duplicate PID for Hive (check for all hive servers. If you do have a tool to check the duplicate PID then use that or use the kinit to login to cloudera admin node and analyze it.)
We killed the duplicate PID
AND
restarted hive server. It fixed the issue.', 7707, '2015-01-08 00:25:05.277', '98a8e884-1d25-4fca-bfd0-7c8858bfa10f', 4829, 7773, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Try exploring the rich field of "Anomaly Detection in Time Series". Control charts and CUSUMs (or cumulative sum control charts) might help you.

Simple [Bullet Graphs][1] might be all you need. Based on historical data and domain knowledge, define normal variance. Then make it clear to stakeholders when the current value is outside of predefined ranges.

Stephen Few is an expert in business dashboards. Any of his books will help you.

If you are open to R, try [Shiny][2] to create simple interactive web applications (It is very straightforward). There is also an open source package for [anomaly detection][3], both local and global.

Create quick prototypes and get feedback!


  [1]: http://en.wikipedia.org/wiki/Bullet_graph
  [2]: http://shiny.rstudio.com/
  [3]: https://github.com/twitter/AnomalyDetection', 1330, '2015-01-08 03:12:03.630', 'e2f8cfc2-e0d1-4436-9c20-f118869feb41', 3798, 'add R package suggestion', 7775, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think one has to be very careful when storing textual data. If they are user comments then, for security concerns it''s better if it is encoded in some format before storage.
A protobuf object can then be defined to resolve the encoding.

Depending on the query pattern, and accepted latency in retrieval of the data, DB should be decided. Just a recommendation, if the idea is to store comments over a period of time for each user, consider HBase or Cassandra. They are optimized for time range queries.

**Recommend read:** http://info.mapr.com/rs/mapr/images/Time_Series_Databases.pdf', 5179, '2015-01-08 07:55:31.067', '5e48dacb-66e7-4092-8d7e-5a142f7a3f5e', 4830, 7776, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Could anyone recommend a good similarity measure for objects which have multiple classes, where each class is part of a hierarchy?

For example, let''s say the classes look like:

    1 Produce
      1.1 Eggs
        1.1.1 Duck eggs
        1.1.2 Chicken eggs
      1.2 Milk
        1.2.1 Cow milk
        1.2.2 Goat milk
    2 Baked goods
      2.1 Cakes
        2.1.1 Cheesecake
        2.1.2 Chocolate

An object might be tagged with items from the above at any level, e.g.:

    Omelette: eggs, milk (1.1, 1.2)
    Duck egg omelette: duck eggs, milk (1.1.1, 1.2)
    Goat milk chocolate cheesecake: goat milk, cheesecake, chocolate (1.2.2, 2.1.1, 2.1.2)
    Beef: produce (1)

If the classes weren''t part of a hierarchy, I''d probably I''d look at cosine similarity (or equivalent) between classes assigned to an object, but I''d like to use the fact that different classes with the same parents also have some similarity value (e.g. in the example above, beef has some small similarity to omelette, since they both have items from the class ''1 produce'').

If it helps, the hierarchy has ~200k classes, with a maximum depth of 5.', 474, '2015-01-08 10:09:32.010', '1fc874fe-0c0a-42be-afdc-206e37faa5e8', 4831, 7777, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Similarity measure based on multiple classes from a hierarchical taxonomy?', 474, '2015-01-08 10:09:32.010', '1fc874fe-0c0a-42be-afdc-206e37faa5e8', 4831, 7778, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<similarity>', 474, '2015-01-08 10:09:32.010', '1fc874fe-0c0a-42be-afdc-206e37faa5e8', 4831, 7779, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('While I don''t have enough expertise to advise you on selection of the best *similarity measure*, I''ve seen a number of them in various papers. The following **collection** of research papers hopefully will be useful to you in determining the optimal measure for your research. Please note that I intentionally included papers, using both *frequentist* and *Bayesian* approaches to hierarchical classification, including class information, for the sake of more *comprehensive* coverage.

**Frequentist approach:**

 - [Semantic similarity based on corpus statistics and lexical taxonomy](http://arxiv.org/pdf/cmp-lg/9709008.pdf)
 - [Cant see the forest for the leaves: Similarity and distance measures for hierarchical taxonomies with a patent classification example](http://www.sciencedirect.com/science/article/pii/S0048733313000115) (also see [additional results and data](http://mcnameephd.pbworks.com/w/page/11389167/Research%20Policy%20-%20Similarity%20and%20distance%20measures%20for%20hierarchical%20taxonomies))
 - [Learning hierarchical similarity metrics](http://research.microsoft.com/pubs/192155/hier_embd.pdf)
 - [A new similarity measure for taxonomy based on edge counting](http://arxiv.org/ftp/arxiv/papers/1211/1211.4709.pdf)
 - [Hierarchical classification of real life documents](http://www.siam.org/meetings/sdm01/pdf/sdm01_22.pdf)
 - [Hierarchical document classification using automatically generated hierarchy](http://users.cis.fiu.edu/~taoli/pub/Li-jiis.pdf)
 - [Split-Order distance for clustering and classification hierarchies](http://www.cs.ucla.edu/~weiwang/paper/SSDBM09.pdf)
 - [A hierarchical k-NN classifier for textual data](http://ccis2k.org/iajit/PDF/vol.8,no.3/791.pdf)

**Bayesian approach:**

 - [Improving classification when a class hierarchy is available using a hierarchy-based prior](http://ba.stat.cmu.edu/journal/2007/vol02/issue01/shahbaba.pdf)
 - [Bayesian aggregation for hierarchical genre classification](http://gfx.cs.princeton.edu/pubs/DeCoro_2007_BAF/bayesgenre.pdf)
 - [Hierarchical classification for multiple, distributed web databases](http://oro.open.ac.uk/12991/1/Hierarchical_Classification.pdf)', 2452, '2015-01-08 12:08:43.113', '033500dc-b457-45d1-a8f5-e72fccff309a', 4833, 7781, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am working on a project where we would like to take the ratio of two measurements B/A and subject these ratios to a ranking algorithm , the ratio is normalized prior to ranking (though the ranking/normalization are not that import to my question).  In most cases measurement A (the starting measurement) is a count with values greater than 1000.  We expect an increase for measurement B for positive effects and a decrease in measurement B for negative effects.  Here is the issue, some of our starting counts are nearly zero which we believe is an artifact of experimental preparation.  This of course leads to some really high ratios/scaling issues for these data points.  What is the best way to adjust these values in order to better understand the real role in our experiment?  One suggestion we received was to add 1000 to all counts (from measurement A and B) to scale the values and remove the bias of such a low starting count,  is this a viable option?  Thank you in advance for your assistance, let me know if I am not being clear enough.
', 7713, '2015-01-08 13:44:20.160', '9f7db265-a07a-48c8-a4f1-526594a9ae47', 4834, 7782, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Correcting Datasets with artificially low starting values', 7713, '2015-01-08 13:44:20.160', '9f7db265-a07a-48c8-a4f1-526594a9ae47', 4834, 7783, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics><data-cleaning>', 7713, '2015-01-08 13:44:20.160', '9f7db265-a07a-48c8-a4f1-526594a9ae47', 4834, 7784, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-01-08 14:01:37.343', 'c9e6bb23-e442-45da-abd0-0efc6fc48835', 2578, '104', 7785, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes, the general idea is to add a baseline small count to every category. The name for this is Laplace smoothing (http://en.wikipedia.org/wiki/Additive_smoothing). Really it''s not so much of a hack, as encoding the idea that you think there is some (uniform?) prior distribution of the events occurring.', 21, '2015-01-08 14:22:33.347', 'ea3a4cab-f809-43ae-b835-cd1ac8476b09', 4835, 7787, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to know if you people have some good tutorials (fast and straightforward) about topic models and LDA, teaching intuitively how to set some parameters, what they mean and if possible, with some real examples.', 7715, '2015-01-08 15:47:34.103', 'a340270d-0125-44dc-9daa-b09111975174', 4836, 7788, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Tutorials on topic models and LDA', 7715, '2015-01-08 15:47:34.103', 'a340270d-0125-44dc-9daa-b09111975174', 4836, 7789, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<topic-model><lda>', 7715, '2015-01-08 15:47:34.103', 'a340270d-0125-44dc-9daa-b09111975174', 4836, 7790, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('(Me: Never learned calculus or advance maths and I started stanford openclasses for machine learning. I know basic matrix calculations.)
One chapter is about cost function. I have been trying find any example calculation of it with numbers. googling only finds the same forumla everytime, and also on octave. But I want to do the same thing first with pen+paper and without it, i cannot understand. Please give me a very simple example of using the formula with numbers. Thanks a lot.', 7712, '2015-01-08 16:13:36.613', '598cdb35-715d-4aba-a0a2-3411c3be568d', 4837, 7791, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('learning cost function for linear regression', 7712, '2015-01-08 16:13:36.613', '598cdb35-715d-4aba-a0a2-3411c3be568d', 4837, 7792, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 7712, '2015-01-08 16:13:36.613', '598cdb35-715d-4aba-a0a2-3411c3be568d', 4837, 7793, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I highly recommend this tutorial: **[Getting Started with Topic Modeling and MALLET](http://programminghistorian.org/lessons/topic-modeling-and-mallet)**

Here are some additional links to help you get started...

Good introductory materials (including links to research papers): http://www.cs.princeton.edu/~blei/topicmodeling.html

Software:

 - MALLET (Java): http://mallet.cs.umass.edu/topics.php
    - topic modeling developer''s guide: http://mallet.cs.umass.edu/topics-devel.php
 - gensim (Python): http://radimrehurek.com/gensim/
 - topicmodels (R): http://cran.r-project.org/web/packages/topicmodels/index.html
 - Stanford Topic Modeling Toolbox (designed for use by social scientists): http://www-nlp.stanford.edu/software/tmt/tmt-0.4/
 - Mr.LDA (scalable topic modeling using MapReduce): http://lintool.github.io/Mr.LDA/
    - If you''re working with *massive* amounts of input text, you might want to consider using Mr.LDA to build your topics models -- its MapReduce-based approach might be more efficient when working with lots of data.

Even more here on the Biased Estimates blog: [Topic Models Reading List](http://www.biasedestimates.com/p/topic-models-reading-list.html)', 819, '2015-01-08 20:58:44.547', 'e5e2f377-fc03-488b-86bf-3a4e22620ee9', 4838, 7796, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Let me assume you intend to use Python libraries to analyze the data, since you are using Scrapy to gather the data.

If this is true, then a factor to consider for storage would be compatibility with other Python libraries.  Of course, plain text is compatible with anything.  But e.g. Pandas has a [host of IO tools](http://pandas.pydata.org/pandas-docs/stable/io.html) that simplifies reading from certain formats.  If you intend to use `scikit-learn` for modeling, then Pandas can still read the data in for you, if you then cast it from a DataFrame to a Numpy array as an intermediate step.

These tools allow you to read CSV and JSON, but also HDF5 ... particularly, I would draw your attention to the [experimental support for msgpack](http://pandas.pydata.org/pandas-docs/stable/io.html#io-msgpack), which seems to be a [binary version of JSON](http://msgpack.org).  Binary means here that the stored files will be smaller and therefore faster to read and write.  A somewhat similar alternative is [BSON](http://bsonspec.org), which has a Python implementation  &mdash; no Pandas or Numpy involved.

Considering these formats only makes sense if you intend to give *at least some formatting* to the stored text, e.g. storing the post title separately from the post content, or storing all posts in a thread in order, or storing the timestamp ... If you considered JSON at all, then I suppose this is what you intended.  If you just intend to store the plain post contents, then use plain text. ', 1367, '2015-01-08 21:07:57.643', 'fe76e595-ba86-4731-9972-f2333ff0cd8f', 4839, 7797, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To learn machine learning, I''d like to develop an app similar to TableDrum (link [here][1]). Although I''ve been coding for a few years, I''ve never done machine learning before. I think what I''m supposed to do is:

 - Choose a bunch of "features" that describe my sounds
 - Provide training data to create some algorithm
 - Generate features in real time for recorded sounds

Are there machine learning libraries out there (ideally that would work on mobile platforms) that can get me started? Thanks!


  [1]: http://www.gizmag.com/tabledrum-transforms-tapping-into-drum-sounds/19538/', 7717, '2015-01-08 22:10:29.767', '020235a8-4f5d-4b5b-a9ff-6315793e5f76', 4840, 7798, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where to start for transient sound classification?', 7717, '2015-01-08 22:10:29.767', '020235a8-4f5d-4b5b-a9ff-6315793e5f76', 4840, 7799, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 7717, '2015-01-08 22:10:29.767', '020235a8-4f5d-4b5b-a9ff-6315793e5f76', 4840, 7800, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('For an upcoming project, I''m mining textual posts from an online forum, using Scrapy. What is the best way to store this text data? I''m thinking of simply exporting it into a JSON file, but is there a better format? Or does it not matter?', 1367, '2015-01-08 23:25:32.430', '7f1830db-2f4e-4bdc-9fb4-c75793729fcb', 3804, 'Capitalize acronym', 7805, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Format for storing textual data', 1367, '2015-01-08 23:25:32.430', '7f1830db-2f4e-4bdc-9fb4-c75793729fcb', 3804, 'Capitalize acronym', 7806, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-08 23:25:32.430', '7f1830db-2f4e-4bdc-9fb4-c75793729fcb', 3804, 'Proposed by 1367 approved by 21 edit id of 201', 7807, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''ve looked at all the other sections on stackexchange and haven''t found a better fit for this question. Sorry if it''s misplaced. If it is, feel free to downvote, but could you please also drop a recommendation where the question would be better suited? Thanks!

I work in an analytical role at a a large financial services firm. We do a ton of daily reporting over metrics that rarely change in a meaningful way from day to day. From this daily reporting, our management is required to extract what was important yesterday and what important trends have developed / are developing over time.

I want to change this to a model of daily exception reporting and weekly trend reporting.

Features might include:

- User report consolidation (so there''s only one daily email)
- report ordering based upon level of variance from past performance (see the most important stuff first)
- HTML email support (with my audience, pretty counts)
- web interface to allow preference changes, including LDAP support (make administration easier)
- Unsubscribe feature at the report level

Here''s what I''d like to know

- What are the practical problems I might run into?
- What is the best way to display the new reports?
- How should  I define an "exception"? How can I know if my definition is a good one?
- I assume I''d be using a mix of Python, SQL, and powershell. Anything else I should consider, e.g. R? What are some good resources?

Thanks,
Brandon
', 1367, '2015-01-08 23:25:37.657', 'd9d0fa78-f430-4039-8dfe-4d5317cb115e', 2346, 'Format questions as list and fix typo', 7808, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-08 23:25:37.657', 'd9d0fa78-f430-4039-8dfe-4d5317cb115e', 2346, 'Proposed by 1367 approved by 21 edit id of 202', 7809, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Yes, the general idea is to add a baseline small count to every category. The technical term for this is [Laplace smoothing](http://en.wikipedia.org/wiki/Additive_smoothing). Really it''s not so much of a hack, as encoding the idea that you think there is some (uniform?) prior distribution of the events occurring.', 1367, '2015-01-08 23:25:48.103', '4aa24b1c-c44b-4d86-8954-e3afaa0cf2ba', 4835, 'Use Markdown link', 7810, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-08 23:25:48.103', '4aa24b1c-c44b-4d86-8954-e3afaa0cf2ba', 4835, 'Proposed by 1367 approved by 21 edit id of 199', 7811, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am working on a project where we would like to take the ratio of two measurements A/B and subject these ratios to a ranking algorithm. The ratio is normalized prior to ranking (though the ranking/normalization are not that import to my question).

In most cases measurement A (the starting measurement) is a count with values greater than 1000.  We expect an increase for measurement B for positive effects and a decrease in measurement B for negative effects.

Here is the issue, some of our starting counts are nearly zero which we believe is an artifact of experimental preparation.  This of course leads to some really high ratios/scaling issues for these data points.

What is the best way to adjust these values in order to better understand the real role in our experiment?

One suggestion we received was to add 1000 to all counts (from measurement A and B) to scale the values and remove the bias of such a low starting count,  is this a viable option?  Thank you in advance for your assistance, let me know if I am not being clear enough.
', 1367, '2015-01-08 23:25:55.630', 'b578bbdb-8d59-416d-869b-911978c6a2e2', 4834, 'Divide in paragraphs for clarity', 7812, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-08 23:25:55.630', 'b578bbdb-8d59-416d-869b-911978c6a2e2', 4834, 'Proposed by 1367 approved by 21 edit id of 200', 7813, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-01-08 23:42:30.970', '451d18a9-f7a3-41d3-9ee7-37f8fd623db0', 4837, '103', 7814, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You have a few problems here.  The first is cleaning your data.  That''s a whole separate issue form anonymization and belongs in another question if you''re still having problems with it.

The second is your anonymization.  After you have some sort of identifier you''re satisfied with (sounds like you''re using people''s real names), try hashing their names to generate a new id.  This id is useful because you''ll always be able to take the original name and figure out what id it is but won''t be able to derive the real names from just the hashed id (providing your hashing algorithm is good).

Further reading:

 * http://security.stackexchange.com/a/61878
 * http://stackoverflow.com/a/21563966/4435034', 7719, '2015-01-09 00:18:05.037', 'c0686741-576f-4c64-9804-2451e0469c4c', 4842, 7815, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Looks like lots of marketing around a specific case of what everyone was already doing to sound bigger than it is.  So, are people implementing mathematical frameworks in software to extract and use the structure within data?  Absolutely.  Are people using the word ''Correlithm'' when doing it?  Not as far as I''ve seen.', 7719, '2015-01-09 00:25:16.067', 'a647badf-122d-4a0e-abc7-4061a4ae4056', 4843, 7816, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('(Me: Never learned calculus or advance maths and I started stanford openclasses for machine learning. I know basic matrix calculations.)
One chapter is about cost function. I have been trying find any example calculation of it with numbers. googling only finds the same forumla everytime, and also on octave. But I want to do the same thing first with pen+paper and without it, i cannot understand. Please give me a very simple example of using the formula with numbers. Thanks a lot.

I require a cost function calculation example for following sample dataset:

    #Rooms = Rent
    1 = 4000
    2 = 10000
    3 = 22000
    4 = 30000', 7712, '2015-01-09 01:01:07.867', '8939c317-1264-402e-8b79-0b2eb54c4664', 4837, 'added 159 characters in body', 7817, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to build a home server/workstation to run my R projects. Based on what I have gathered, it should probably be Linux based. I want to buy the hardware now, but I am confused with the many available options for processors/ram/motherboards. I want to be able to use parallel processing, at least 64GB? of memory and enough storage space (~10TB?). Software wise, Ubuntu?, R, RStudio, PostgreSQL, some NOSQL database, probably Hadoop. I do a lot of text/geospatial/network analytics that are resource intensive. Budget ~$3000US.

**My Questions:**<BR>
What could an ideal configuration look like? (Hardware + Software)<br>
What type of processor?<BR>

**Notes:**<BR>
No, I don''t want to use a cloud solution.<BR>
I know it is a vague question, but any thoughts will help, please?<BR>
If it is off-topic or too vague, I will gladly delete.<BR>

Cheers B', 7722, '2015-01-09 03:46:04.837', 'b46394de-91d6-420a-9359-608b8c1781e9', 4844, 7818, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hardware requirements for Linux server to run R & RStudio', 7722, '2015-01-09 03:46:04.837', 'b46394de-91d6-420a-9359-608b8c1781e9', 4844, 7819, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r>', 7722, '2015-01-09 03:46:04.837', 'b46394de-91d6-420a-9359-608b8c1781e9', 4844, 7820, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m interested in an overview of the modern/state-of-the-art approaches to bag-of-words information retrieval, where you have a single query $q$ and a set of documents which you hope to rank by relevance $d_1,...,d_n$.

I''m specifically interested in approaches which require absolutely no linguistic knowledge and rely on no outside linguistic or lexical resources for boosting performance (such as thesauri or prebuilt word-nets and the like).  Thus where a ranking is produced entirely by evaluating query-document similarity and where the problems of synonymy and polysemy are overcome by exploiting inter-document word co-occurence.

I''ve spent a some amount of time in the literature (reading papers/tutorials), however there is so much information out there it''s hard to get a bird''s eye view.  The best I can make out is that modern approaches involve a combination of some vector space model or generalization thereof (such as a generalized vector space model, LSI, or a topic-based vector space model using LDA), in conjunction with pseudo-relevance feedback (using either Rocchio or some more advanced approach).

Is this basically where we currently are in attacking this particular type of problem? ', 7723, '2015-01-09 05:42:54.587', '13697fff-01ef-441a-97ac-039790f25810', 4845, 7821, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Approaches to Bag-Of-Words Information Retrieval', 7723, '2015-01-09 05:42:54.587', '13697fff-01ef-441a-97ac-039790f25810', 4845, 7822, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<information-retrieval>', 7723, '2015-01-09 05:42:54.587', '13697fff-01ef-441a-97ac-039790f25810', 4845, 7823, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m interested in an overview of the modern/state-of-the-art approaches to bag-of-words information retrieval, where you have a single query $q$ and a set of documents which you hope to rank by relevance $d_1,...,d_n$.

I''m specifically interested in approaches which require absolutely no linguistic knowledge and rely on no outside linguistic or lexical resources for boosting performance (such as thesauri or prebuilt word-nets and the like).  Thus where a ranking is produced entirely by evaluating query-document similarity and where the problems of synonymy and polysemy are overcome by exploiting inter-document word co-occurence.

I''ve spent some amount of time in the literature (reading papers/tutorials), however there is so much information out there it''s hard to get a bird''s eye view.  The best I can make out is that modern approaches involve some combination of a weighted vector space model (such as a generalized vector space model, LSI, or a topic-based vector space model using LDA), in conjunction with pseudo-relevance feedback (using either Rocchio or some more advanced approach).

Is this basically where we currently are in attacking this particular type of problem? ', 7723, '2015-01-09 05:48:44.127', '11568722-06f4-43ed-865a-1c5a7a8597bf', 4845, 'added 9 characters in body', 7824, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m interested in an overview of the modern/state-of-the-art approaches to bag-of-words information retrieval, where you have a single query $q$ and a set of documents which you hope to rank by relevance $d_1,...,d_n$.

I''m specifically interested in approaches which require absolutely no linguistic knowledge and rely on no outside linguistic or lexical resources for boosting performance (such as thesauri or prebuilt word-nets and the like).  Thus where a ranking is produced entirely by evaluating query-document similarity and where the problems of synonymy and polysemy are overcome by exploiting inter-document word co-occurence.

I''ve spent some amount of time in the literature (reading papers/tutorials), however there is so much information out there it''s hard to get a bird''s eye view.  The best I can make out is that modern approaches involve some combination of a weighted vector space model (such as a generalized vector space model, LSI, or a topic-based vector space model using LDA), in conjunction with pseudo-relevance feedback (using either Rocchio or some more advanced approach).

All these vector space models tend to use cosine-similarity as the metric, however I have seen some literature discussing semi-metrics, as it''s certainly not clear that semantic similarity between words ought to satisfy the triangle inequality.

Is this basically where we currently are in attacking this particular type of problem? ', 7723, '2015-01-09 05:54:32.867', 'e981f572-8893-4a97-b7da-35e2cdf03e54', 4845, 'added 9 characters in body', 7825, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m interested in an overview of the modern/state-of-the-art approaches to bag-of-words information retrieval, where you have a single query $q$ and a set of documents which you hope to rank by relevance $d_1,...,d_n$.

I''m specifically interested in approaches which require absolutely no linguistic knowledge and rely on no outside linguistic or lexical resources for boosting performance (such as thesauri or prebuilt word-nets and the like).  Thus where a ranking is produced entirely by evaluating query-document similarity and where the problems of synonymy and polysemy are overcome by exploiting inter-document word co-occurence.

I''ve spent some amount of time in the literature (reading papers/tutorials), however there is so much information out there it''s hard to get a bird''s eye view.  The best I can make out is that modern approaches involve some combination of a weighted vector space model (such as a generalized vector space model, LSI, or a topic-based vector space model using LDA), in conjunction with pseudo-relevance feedback (using either Rocchio or some more advanced approach).

All these vector space models tend to use cosine-similarity as the similarity metric, however I have seen some literature discussing similarity metrics which are more general.

Is this basically where we currently are in attacking this particular type of problem? ', 7723, '2015-01-09 06:23:07.133', 'a2f412ec-0098-432a-9516-9723ce57058b', 4845, 'added 71 characters in body', 7826, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m interested in an overview of the modern/state-of-the-art approaches to bag-of-words information retrieval, where you have a single query $q$ and a set of documents which you hope to rank by relevance $d_1,...,d_n$.

I''m specifically interested in approaches which require absolutely no linguistic knowledge and rely on no outside linguistic or lexical resources for boosting performance (such as thesauri or prebuilt word-nets and the like).  Thus where a ranking is produced entirely by evaluating query-document similarity and where the problems of synonymy and polysemy are overcome by exploiting inter-document word co-occurence.

I''ve spent some amount of time in the literature (reading papers/tutorials), however there is so much information out there it''s hard to get a bird''s eye view.  The best I can make out is that modern approaches involve some combination of a weighted vector space model (such as a generalized vector space model, LSI, or a topic-based vector space model using LDA), in conjunction with pseudo-relevance feedback (using either Rocchio or some more advanced approach).

All these vector space models tend to use cosine-similarity as the similarity function, however I have seen some literature discussing similarity functions which are more exotic.

Is this basically where we currently are in attacking this particular type of problem? ', 7723, '2015-01-09 06:30:31.983', '5edc4359-b698-4c6a-851b-412314f0c1cd', 4845, 'added 3 characters in body', 7827, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is no ideal configuration, for `R` or in general - product selection is always a difficult task and many factors are at play. I think that the solution is rather simple - get the best computer that your budget allows.

Having said that, since you want to focus on `R` development and one of `R`''s pressing issues is its [critical dependence](http://stackoverflow.com/q/5171593/2872891) on the amount of available *physical memory* (RAM), I would suggest favoring more RAM to other parameters. The second most important parameter, in my opinion, would be *number of cores* (or *processors* - see details below), due to your potential *multiprocessing* focus. Finally, the two next most important criteria I''d pay attention to would be *compatibility* with Linux and system/manufacturer''s *quality*.

As far as the *storage* goes, I suggest considering *solid state drives (SSD)*, if you''d rather prefer to have a bit more more speed than more space (however, if your work will involve intensive disk operations, you might want to investigate the issue of SSD *reliability* or consult with people, knowledgeable in this matter). However, I think that for R-focused work, disk operations are much less critical than memory ones, as I''ve mentioned above.

When choosing a specific *Linux distribution*, I suggest using a well-supported one, such as Debian or, even better, Ubuntu (if you care more about support, choose their LTS version). I''d rather not buy parts and assemble custom box, but some people would definitely prefer that route - for that you really need to know hardware well, but potential compatibility could still be an issue. The next paragraph provides some examples for both *commercial-off-the-shelf (COTS)* and *custom* solutions.

Should you be interested in the *custom system* route, [this discussion](http://www.tomshardware.com/forum/295552-28-multiprocessor-multicore-system-rendering-workstation) might be worth reading, as it contains some interesting pricing numbers (just to get an idea of potential savings) and also sheds some light on *multiprocessor vs. multi-core* alternatives (obviously, the *context* is different, but nevertheless could be useful). As I said, I would go the COTS route, mainly due to reliability and compatibility issues. In terms of single-processor multi-core systems, your budget is more than enough. However, when we go to multiprocessor workstations (I''m not even talking about servers), even two-processor configurations can go over your budget easily. Some, not far away, such as [HP Z820 Workstation](http://www8.hp.com/us/en/campaigns/workstations/z820.html). It starts from 2439 USD, but in minimal configuration. When you upgrade it to match your desired specs (if it''s even possible), I''m sure that we''ll be talking about 5K USD price range (extrapolating from the series'' higher-level models). What I like about HP Z820, though, is the fact that this system is [Ubuntu certified](http://www.ubuntu.com/certification/hardware/201011-6754). Considering system compatibility and assuming your desire to run Ubuntu, the best way to approach your problem is to go through [Ubuntu-certified hardware lists](http://www.ubuntu.com/certification) and shortlist systems that you like. Just for the sake of completeness, take a look at this [interesting multiprocessor system](http://www.mediaworkstations.net/i-x2.html), which in compatible configuration might cost less than from HP or other major vendors. However, it''s multimedia-oriented as well as it''s reliability and compatibility are unknown, not to mention that it''s way over your specified budget.

In terms of `R` and `R`-focused software, I highly recommend you to use *RStudio Server* instead of *RStudio*, as that will provide you with an opportunity to be able to work from any Internet-enabled location (provided you computer will be running, obviously). Another advice that I have is to keep an eye on *alternative `R` distributions*. I''m not talking about commercial expensive ones, but about emerging open source projects, such as `pqR`: http://www.pqr-project.org. Will update as needed. I hope this is helpful.', 2452, '2015-01-09 08:22:40.920', 'b01ba298-89c2-495e-91ad-9cf20ae0dd91', 4846, 7828, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-01-09 08:32:49.777', '7ae9e56c-dc18-45cb-9fe2-c59ec744259d', 4837, 7829, '11');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is a worked out example [in the Wikipedia page for "simple linear regression"](http://en.wikipedia.org/wiki/Simple_linear_regression#Numerical_example)', 1367, '2015-01-09 10:02:08.437', 'c13e4134-12d8-4636-aa4e-45ac7b8e574b', 4847, 7830, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There is a worked out example [in the Wikipedia page for "simple linear regression"](http://en.wikipedia.org/wiki/Simple_linear_regression#Numerical_example)

Just for the sake of it, let me plug in your example into the formulas:

The fitted model should be a straight line with parameters $\alpha$ (value at $x = 0$) and $\beta$ (the slope):

$$f(x) = \alpha + \beta x$$

The values for these parameters that minimize the distance between line and data points are called $\hat{\alpha}$ and $\hat{\beta}$.  They can be computed out of the data point values by using these formulae, [derived here](http://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line):

$$\begin{align}
\hat{\beta} & = \frac{ \overline{xy} - \bar{x}\bar{y} }{ \overline{x^2} - \bar{x}^2 } ,
\\
\\
\hat{\alpha} & = \bar{y} - \hat{\beta}\bar{x}
\end{align}
$$

where an expression with an overline $\overline{xy}$ means the sample average of that expression: $\overline{xy} = \tfrac{1}{n} \sum_{i=1}^n{x_iy_i}$.

Here are the values I find for the datapoints you have listed in your question:

$$
\begin{align}
\overline{xy} &= \frac{1}{4} \sum{<(1 \times 4000), (2 \times 10000), (3 \times 22000), (4 \times 30000)>} \\
              &= \frac{1}{4}(4000 + 20000 + 66000 + 120000) = 52500, \\
\overline{x} &= \frac{1}{4} \sum{<1, 2, 3, 4>} = 2.5 ,\\
\overline{y} &= \frac{1}{4} \sum{<4000, 10000, 22000, 30000>} = 16500 , \\
\overline{x^2} &= \frac{1}{4} \sum{<1^2, 2^2, 3^2, 4^2>} = 7.5 , \\
\overline{x}^2 &= 2.5^2 = 6.25
\end{align}
$$

and the fitted line should be:

$$
\begin{align}\
\hat{\beta} &= \frac{52500 - 2.5 \times 16500}{7.5 - 6.25} = \frac{41250}{1.25} = 33000 , \\
\hat{\alpha} &= 16500 - 33000 \times 2.5 = - 66000 , \\
\Rightarrow f(x) &= - 66000 + 33000 x
\end{align}
$$

Therefore, the model would predict, for a house with 10 rooms, a rent of:

$$ f(x) = -66000 + 33000 \times 10 = 264000 $$', 1367, '2015-01-09 10:55:33.910', '3aa01a8e-d4ac-430b-bb96-d33c0b0606da', 4847, 'Work out the OP example', 7831, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What you are asking about is, in my view, the main problem of implementing a lambda architecture.  Here are some suggestions on how to solve it.

The combination of Spark and [Spark Streaming](https://spark.apache.org/docs/1.1.1/streaming-programming-guide.html) largely supersedes the original lambda architecture (which usually involved Hadoop and Storm).  [Read here](http://blog.cloudera.com/blog/2014/08/building-lambda-architecture-with-spark-streaming/) an example of how to use a `SparkContext` and a separate `StreamingContext` to produce *different* `RDD`s, one for batch processed results and another for real-time results.

Once you have replicated that in your system, you still have to think about *how to query* both kind of `RDD`s.  The trivial case would be to just `union` both of them:

    scala> rdd1.union(rdd2).collect

Or maybe you can create a new `DStream`, similar to `stateStream` in the linked example, where some keys are kept for real-time results, and others for batch results. ', 1367, '2015-01-09 11:25:11.623', '9104902f-f6b2-49f3-8a2b-e0fe4ad606ca', 4848, 7832, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am trying to predict clients comportement from market rates.

The value of the products depends on the actual rate but this is not enough. The comportement of the client also depends on their awareness wich depends on the evolution of rates. I''ve added this in model using past 6 month rates as features in polynomial regression.

In fact media coverage of rate mostly depends on rate variations and I wanted to add that in my model.  The idea would be to add a derivative/variation of rate as a feature. But I anticipated something wrong, example with only two month , my variation will be of the form $x_n - x_{n-1}$ that is a simple linear combination of actual and past rates. So for a 1d polynomial regression i will have:

$$ x_{n+1} = a * x_{n} +  b * x_{n-1} + c * (x_{n} - x_{n-1})$$

instead of:

$$ x_{n+1} = a_0 * x_{n} +  b_0 * x_{n-1}$$

wich is strictly equivalent with $ a + c = a_0 $ and $b-c= b_0$. Higher polynomial degree results in a more or less equivalent result.

I am thinking about a way to include derivative information but it seems not possible. So I am wondering if all the information is included in my curve. Is this a general idea ? all information is somewhat directly contained in data and modifications of features will result in higher order objective function ?
', 303, '2015-01-09 15:28:36.703', '6c83c7c0-724f-45b8-9fe5-dc28ff9d600e', 3771, 'deleted 654 characters in body', 7836, '5');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (303, '2015-01-09 15:29:04.720', '546fe369-5364-4e67-9bed-b5550f60dcd9', 3771, '10', 7837, '33');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (303, '2015-01-09 15:38:47.323', '22642d62-904a-4489-ab64-a456125849a4', 3770, '9', 7838, '34');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think this point is the core of your question:

> - How should I define an "exception"? How can I know if my definition is a good one?

I would go about it as follows:

- Go back to the managers who receive the current reporting and ask them to give you examples of reported exceptions that were actually exceptions, i.e. they were acted upon and the action yielded some benefit.
- Ask them also if there were any features in that report that were not plainly viewable, but led them to think it was an exception, e.g. the month-on-month difference was not reported, but it could be computed from the week-on-week difference.

Treat the examples from the first as labels for training; you want to learn "what is an exception" and you need to have an expert answer that question for you.

Treat the suggested features as new features for your classification.

If you cannot get the experts to answer the questions you make, then try to infer them from their interaction with your reports: how many of those reports were downloaded, how many times are they mentioned, how are they ever used in decision-making ...  Try to separate the important ones from the uninteresting ones, and there you have your labels.', 1367, '2015-01-09 18:26:08.933', '3baf89b1-b7d8-48e2-a1bf-a22c310a68c5', 4850, 7842, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I don''t have enough reputation to ask questions in comment for clarification before answering it, so I''m going to do both here.

###Here are the things that would help answering this question for now:

Can you post part of the `process_set.hpc`?

What''s its format? Is it a numpy array? Is it a Pandas dataframe?

What''s the value of `cclasses`?

###And now the answer:

First of all, `k-means` algorithm is able to find clusters in any n-dimensional data. If `n` is too big, it is better to use PCA but for `n=3` that wouldn''t necessarily add any value.

The second thing that looks suspicious to me is that in the documentation for `kmeans` in `scikit-learn`, there is no `compute_labels` option, as seen [here](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). However, that option exists for `MiniBatchKMeans` as seen [here](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html).

Also, if you make your data in the form of a pandas dataframe (if it is not already so), things would be much easier to track and you won''t have to reattach the timing information to your data afterwards.

I may be able to give you a more thorough answer if I know a bit more about the format of the data.

Good luck!
', 7735, '2015-01-09 19:05:38.040', '76027eac-808a-417c-90e6-881f27db85d7', 4851, 7843, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a mysql database with the following format:

    id      string
    1        foo1...
    2        foo2...
    ..       ...

There are >100k entries in this db.

What I want to do is for each string, compare it to each other string and store some metric of the comparison.  Doing this will essentially yield a 2D matrix of size `NxN` where `N` is the number of row in the db.

My initial thought was creating another db where each index corresponds to the string of the index in the first db and each column is the value from comparing the two strings. For example, id 1 column 2 in the second db would be the value outputted from comparing id1 and id2 in the first db.

The format of the second db:

    id    col1    col2    col3    ....
    1       1      0.4     0.5    .....
    ...    ...     ...      ...

This way of creating the second db would result in 100k rows x 100k columns, which is the issue at hand.  What is the best way to handle large data sets like this?  Is storing the data in a text file more efficient (say each text file corresponds to one row in the second db.)', 7739, '2015-01-10 00:15:45.990', '9154822b-887b-46d8-980b-ca2a781bece2', 4852, 7846, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Storing Big Matrix in DataBase', 7739, '2015-01-10 00:15:45.990', '9154822b-887b-46d8-980b-ca2a781bece2', 4852, 7847, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><relational-dbms>', 7739, '2015-01-10 00:15:45.990', '9154822b-887b-46d8-980b-ca2a781bece2', 4852, 7848, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It is usually more practical to compute the distances on the fly rather than storing $N^2$ values. There are techniques you can use to avoid comparing an item against *all* others at query-time such as building a spatial index or using a heuristic that selects only a subset of items that are not too far away. PostgreSQL can create an [index on character trigrams](http://www.postgresql.org/docs/9.1/static/pgtrgm.html) in strings that enables efficient querying for strings that are sufficiently similar (measured by the Jaccard similarity coefficient).

Storing an $N$ column wide table is not possible because the maximum number of columns allowed in MySQL is 4096. Even if that were not the case, it would be an abuse of the relational model and you would have difficulty working with such a table.

 If for some reason you must store a dense distance matrix, a format such as [HDF5](http://www.hdfgroup.org/HDF5/) would be more efficient than an RDBMS.', 7740, '2015-01-10 04:33:10.407', '52f3b369-a728-4504-abff-ece9ff03da88', 4853, 7849, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<career>', 97, '2015-01-10 11:10:53.493', 'af247a05-4100-4ca8-a12c-86fff65698ca', 1216, 'Retagging to more relevant ', 7850, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-10 11:10:53.493', 'af247a05-4100-4ca8-a12c-86fff65698ca', 1216, 'Proposed by 97 approved by 2452, 21 edit id of 204', 7851, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('(Me: Never learned calculus or advanced math and I started Stanford openclasses for machine learning. I know basic matrix calculations.)

One chapter of my course is about cost function. I have been trying to find any example calculation of it with numbers. Googling only finds the same formula everytime, and also on Octave. But I want to do the same thing first with pen+paper and without it, I cannot understand. Please give me a very simple example of using the formula with numbers. Thanks a lot.

I require a cost function calculation example for following sample dataset:

    #Rooms = Rent
    1 = 4000
    2 = 10000
    3 = 22000
    4 = 30000', 1367, '2015-01-10 11:11:10.110', 'b2037f3c-4265-4637-89a3-95eb5ff13162', 4837, 'Capitalize some words', 7852, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Learning cost function for linear regression', 1367, '2015-01-10 11:11:10.110', 'b2037f3c-4265-4637-89a3-95eb5ff13162', 4837, 'Capitalize some words', 7853, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-10 11:11:10.110', 'b2037f3c-4265-4637-89a3-95eb5ff13162', 4837, 'Proposed by 1367 approved by 2452, 21 edit id of 203', 7854, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-01-10 11:13:44.367', '869f7d0d-1fd1-4d1e-aec3-bb888c3c11c3', 4844, '104', 7855, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am currently collecting second by second data regarding buyer vs seller initiated trades for different financial instruments (securities mostly). If there are more buyer initiated trades in a given second, then that second''s data point would contain a positive value in the pertinent feature. If there are more seller initiated trades, then there would be a negative value. And if either there is an equal amount of buy vs seller initiated trades OR if there are simply not any trades in a given second, there will be a 0 for the feature in that data point. Along with this feature, there are several other features that are based on what occurred in the preceding seconds (eg if the value discussed above was 12 for the data point immediately preceding the current point, then the second feature for the current data point would be 12 - please let me know if this is not clear) After much troubleshooting, I have concluded that if there are too many data points with too many 0''s for features, the classifier simply wont work. When I print out the probabilities of evaluation data points falling into different classes, I simply get

    0:NaN,1:NaN

for all model evaluation points I try to classify. (I am using logistic regression from apache-mahout. In total have 183 features, but over 40million data points. There are three categories to which the data point can be classified)

I have found that if I set the default value to 1, then I no longer encounter this error, e.g. if there are no trades, the value will be a 1, if there is one seller initiated trade, the value will be 0.

So with all this in mind, I have two related questions:

1) Has anyone else encountered this issue? e.g. if you have a vector with x features, and for a majority of the data points, a majority of the features contain 0''s, is this know to give issues?

2) Is shifting all values up by a constant (such as 1) a valid fix to this issue? I assume that if this constant is applied to all values, then it shouldn''t skew the data, but I figure it won''t hurt to check with the experts.

***
Also, I''m new to this, so if you believe that my question could use more info please let me know, and if you could give me ideas of what information to include, it would be greatly appreciated.

thanks in advance', 6636, '2015-01-10 22:53:14.353', '1a2f51a4-5d29-4240-8c47-1084f3b6efae', 4855, 7859, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Shifting dataPoints up by a constant (Is there an issue with too many 0''s for features?)', 6636, '2015-01-10 22:53:14.353', '1a2f51a4-5d29-4240-8c47-1084f3b6efae', 4855, 7860, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<feature-selection><logistic-regression>', 6636, '2015-01-10 22:53:14.353', '1a2f51a4-5d29-4240-8c47-1084f3b6efae', 4855, 7861, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I believe  you do not need to include $(x_n - x_{n-1})$ as a feature because linear models obey [superposition][1], and it is just a linear addition to the model.

You can think of the independent variables $x_n$ and $x_{n-1}$ as forming a [standard basis][2] for the two dimensions of your model''s input, that is $\langle 1, 0\rangle$ and $\langle 0, 1\rangle$.
Neither $a\langle 1, 0 \rangle$ or $b \langle 0, 1 \rangle$ are orthogonal to  c$\langle1, -1\rangle$.

Therefore, **if you want to use $c(x_n - x_{n-1})$ as a feature, I would also use the orthogonal feature $d(x_n + x_{n-1})$**. That is,

$$ x_{n+1} = c(x_n - x_{n-1}) + d(x_n + x_{n-1}) $$.



  [1]: https://en.wikipedia.org/wiki/Superposition_principle
  [2]: https://en.wikipedia.org/wiki/Standard_basis', 7738, '2015-01-11 01:57:49.490', 'f46661ae-58a7-4412-babd-b2a4d5418ee2', 4856, 7862, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You can think of the independent variables $x_n$ and $x_{n-1}$ as forming a [standard basis][2] for the two dimensions of your model''s input, that is $\langle 1, 0\rangle$ and $\langle 0, 1\rangle$.
Neither $a\langle 1, 0 \rangle$ or $b \langle 0, 1 \rangle$ are orthogonal to  c$\langle1, -1\rangle$.

Therefore, **if you want to use $c(x_n - x_{n-1})$ as a feature, I would also use the orthogonal feature $d(x_n + x_{n-1})$**. That is,

$$ x_{n+1} = c(x_n - x_{n-1}) + d(x_n + x_{n-1}) $$.

You could think of this model as the derivative plus the integral.

The same logic applies to the choice of scaling function for the Haar wavelet.

  [1]: https://en.wikipedia.org/wiki/Superposition_principle
  [2]: https://en.wikipedia.org/wiki/Standard_basis', 7738, '2015-01-11 02:42:59.650', '58ac1b13-0c81-4f51-be3a-660ead890c52', 4856, 'deleted 17 characters in body', 7867, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am very new in machine learning. I have annotated data with category, aspect, opinion word and sentiment. for example, for the bellow text

"The apple was really tasty"

I have category->food, aspect-> apple, opinion word ->tasty and sentiment->positive. I have training data like this format.

How can I train a SVM classifier using this format of training set?
How to extract features like n-gram, POS and sentiment word to train the classifier?
Could you please suggest any beginning step for this aspect based sentiment analysis using machine learning algorithms?
', 7750, '2015-01-11 11:43:38.253', 'd28f2adf-d48b-41ea-9d06-c7d056bec6af', 4859, 7869, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Aspect based sentiment analysis using machine learning approach', 7750, '2015-01-11 11:43:38.253', 'd28f2adf-d48b-41ea-9d06-c7d056bec6af', 4859, 7870, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><svm><feature-extraction>', 7750, '2015-01-11 11:43:38.253', 'd28f2adf-d48b-41ea-9d06-c7d056bec6af', 4859, 7871, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would recommend you to start from reading the draft of the introductory book ["Sentiment analysis and opinion mining"](http://www.amazon.com/dp/1608458849) by Bing Liu. The draft in a PDF document format is available for free [here](http://www.cs.uic.edu/~liub/FBS/SentimentAnalysis-and-OpinionMining.pdf).

More details about the new upcoming book of this author, as well as comprehensive information on the topic of aspect-based sentiment analysis, with references and links to data sets, are available at this page: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html.

Another interesting resource is a survey book ["Opinion mining and sentiment analysis](http://www.cs.cornell.edu/home/llee/opinion-mining-sentiment-analysis-survey.html) by Bo Pang and Lillian Lee. The book is available in print and as a downloadable PDF e-book in a [published version](http://www.cs.cornell.edu/home/llee/omsa/omsa-published.pdf) or an [author-formatted version](http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf), which are almost identical in terms of contents.', 2452, '2015-01-11 13:45:04.360', 'aa85e4b4-47fc-4b45-9e1f-cbc0fb76b7dc', 4860, 7872, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('While reading blogs and papers is helpful to identify the latest and greatest, having a solid foundation helps a lot, too. But I assume you already have gone over Manning''s great (and free in e-book form) book on IR, right?

http://nlp.stanford.edu/IR-book/

It contains information on creating your own thesaurus from your document collection to solve synonymy problems, LSA for polysemy, etc..

As for similarity measures, you will see there that Okapi BM25 (Robertson et al.) is considered superior to cosine similarity (but more expensive to implement and run).
Regarding the current state of the art, there was a small emergence of Bayesian Network-based classifiers in the early nineties (starting with Turtle & Croft), but that went quiet for a while.
However, right now, using BNs for IR is again finding some revival, particularly in  biomedical IR.
In that respect, I think most ongoing work is directed towards using Bayesian models incl. topic models and deep learning for word-sense disambiguation (WSD) and semantic similarity.
Here is a pointer to a recent paper with good references on the topic.

http://arxiv.org/abs/1412.6629
', 6672, '2015-01-11 17:38:19.187', '5deb3649-6a4c-44b2-a52f-6e0db9a087ec', 4861, 7873, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you can provide more details about the processing you''re doing to the data, I think the responses would be a little more helpful.

Here are a few things to consider:


- What''s the shape of your data going into k-means? Are you aggregating up to the week level? If so, then your data won''t look like the example you posted, since that seems like daily data.

- Try using .[shape][1] on your output data set. Does the # of rows match the data set you put into it? (This ties in to question #1. You''ll be unable to join back directly if your original data is daily and the k-means data is weekly.

- I''m not sure if this is what you''re attempting, but just declaring `data_classes` as the labels isn''t going to add them back in to your original data. Like oxtay suggested, using pandas is a good choice because it will allow you to [join][2].


  [1]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html#numpy.ndarray.shape
  [2]: http://pandas.pydata.org/pandas-docs/stable/merging.html', 7761, '2015-01-12 05:47:37.127', '1538d458-241c-4d35-b60a-4d3dacbf49a5', 4862, 7880, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to recommend to check the following **open data** *repositories* and *meta-repositories* (they are not focused on *categorical data*, but I''m sure that many data sets, listed there, contain such data):

 - http://www.kdnuggets.com/datasets
 - http://www.data.gov
 - http://www.datasciencecentral.com/profiles/blogs/big-data-sets-available-for-free

Also check built-in data sets in the open source software *Parallel Sets*, which is focused on the categorical data visualization: https://eagereyes.org/parallel-sets.', 2452, '2015-01-12 07:58:39.780', 'aaf993ce-bcbd-48ea-a910-8970fdb34fc2', 4863, 7881, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have 11 lottery tickets(used) and I have discovered that in each ticket, the 3rd digit''s value is +1 of the value of the 6th digit.I have 11 tickets, each ticket is composed of 16 digits.Would someone(anyone) help me find the logic used here, in creating the other digits and their interrelation?I am completely a noob in data analysis.And any help would be greatly appreciated.For those who would like to know, I incidentally discovered the certain pattern, and made me realize these numbers are not totally random and if I could find the underlying pattern i''d be able to predict.The excel file is here [Excel file][1]


  [1]: http://s000.tinyupload.com/index.php?file_id=39010250377074241779', 7762, '2015-01-12 09:27:15.250', '5761cf84-7120-4db6-9305-7927ba7a17a7', 4864, 7882, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the underlying pattern, or algorithm(s) used in these numbers?', 7762, '2015-01-12 09:27:15.250', '5761cf84-7120-4db6-9305-7927ba7a17a7', 4864, 7883, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><algorithms>', 7762, '2015-01-12 09:27:15.250', '5761cf84-7120-4db6-9305-7927ba7a17a7', 4864, 7884, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This article shows some details on scalability, special advantage for Titan.
https://groups.google.com/forum/#!topic/orient-database/CpPh42ukfH4
', 7764, '2015-01-12 10:31:34.573', '305c28fe-4291-47bc-903f-fcfa77bc330b', 4865, 7885, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have 11 lottery tickets (used) and I have discovered that in each ticket, the 3rd digit''s value is +1 of the value of the 6th digit. I have 11 tickets, each ticket is composed of 16 digits. Would someone (anyone) help me find the logic used here, in creating the other digits and their interrelation? I am a complete noob in data analysis, and any help would be greatly appreciated. For those who would like to know, I incidentally discovered the certain pattern, and made me realize these numbers are not totally random and if I could find the underlying pattern I''d be able to predict. The excel file is here [Excel file][1]


  [1]: http://s000.tinyupload.com/index.php?file_id=39010250377074241779', 6496, '2015-01-12 10:33:17.913', 'd718e211-4e5f-4bfe-a733-fce0d43401ea', 4864, 'Fixed Grammar', 7886, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-12 10:33:17.913', 'd718e211-4e5f-4bfe-a733-fce0d43401ea', 4864, 'Proposed by 6496 approved by 7762 edit id of 205', 7887, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('1- 11 instances of data is insufficient for discovering the behavior (even finding patterns) for such presumably complex systems. You need at least thousands of them.

2- These numbers are encoded versions of numbers generated by Serial Key Generation Algorithms, which in my experience are usually implemented by Python. So it''s not at all strange that there may be a correlation between some digits of them. As it is necessary for the system to recognize its created numbers from the numbers created by other lotteries.
3- Even if there was a correlation or correlation between all the instances of lottery numbers. The thing is,*Correlation doesn''t imply Causation* and these patterns, if found any, couldn''t really give any information about the key generation system itself.
', 6496, '2015-01-12 10:37:27.507', '58149e65-7658-4de1-9c7b-da38cf255474', 4866, 7888, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From your question I am not sure how you are using logistic regression. The "vanilla" version of logistic regression gives a binary classifier, but your target has three values: a positive value, a negative value, and zero. The Apache site, on the page for logistic regression refers to the thesis of Paul Komarek, in which "logistic regression" refers to the binary classifier. So your problem might be that there your target variable has 3 values.

Assuming that that is the case, you have many options, for example:
 - a) use "multi-nomial logistic",
 - b) use ordinary logistic regression 3 times to fit 3 models, one per level, to distinguish it from the other two combined

', 7763, '2015-01-12 10:41:57.830', '7d05cc31-dbdb-4afd-a902-1e93fd19b945', 4867, 7889, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('1- 11 instances of data is insufficient for discovering the behavior (even finding patterns) for such presumably complex systems. You need at least thousands of them.

2- These numbers are encoded versions of numbers generated by Serial Key Generation Algorithms, which in my experience are usually implemented by Python. So it''s not at all strange that there may be a correlation between some digits of them. As it is necessary for the system to recognize its created numbers from the numbers created by other lotteries.
3- Even if there was a correlation or correlation between all the instances of lottery numbers. The thing is,*Correlation doesn''t imply Causation* and these patterns, if found any, couldn''t really give any information about the key generation system itself.
4- If you are really serious about it. you could go to this page and enter your data as a sequence it would show you the patterns, if there''s any.
<http://oeis.org/>[The On-Line Encyclopedia of Integer Sequences](http://oeis.org/)', 6496, '2015-01-12 11:07:23.467', '235af37c-ed94-4d6a-8733-0ad2f0684394', 4866, 'added to my own answer', 7890, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('1- 11 instances of data is insufficient for discovering the behavior (even finding patterns) for such presumably complex systems. You need at least thousands of them.

2- These numbers are encoded versions of numbers generated by Serial Key Generation Algorithms, which in my experience are usually implemented by Python. So it''s not at all strange that there may be a correlation between some digits of them. As it is necessary for the system to recognize its created numbers from the numbers created by other lotteries.
3- Even if there was a correlation or correlation between all the instances of lottery numbers. The thing is,*Correlation doesn''t imply Causation* and these patterns, if found any, couldn''t really give any information about the key generation system itself.
4- If you are really serious about it. you could go to this page and enter your data as a sequence it would show you the patterns, if there''s any.
[The On-Line Encyclopedia of Integer Sequences](http://oeis.org/)', 6496, '2015-01-12 11:14:14.357', '9b11f626-e70a-401e-8f4e-4ae1895b331f', 4866, 'deleted 18 characters in body', 7891, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('1- 11 instances of data is insufficient for discovering the behavior (even finding patterns) for such presumably complex systems. You need at least thousands of them.

2- These numbers are encoded versions of numbers generated by Serial Key Generation Algorithms, which in my experience are usually implemented by Python. So it''s not at all strange that there may be a correlation between some digits of them. As it is necessary for the system to recognize its created numbers from the numbers created by other lotteries.

3- Even if there was a correlation or correlation between all the instances of lottery numbers. The thing is,*Correlation doesn''t imply Causation* and these patterns, if found any, couldn''t really give any information about the key generation system itself.

4- If you are really serious about it. you could go to this page and enter your data as a sequence it would show you the patterns, if there''s any.
[The On-Line Encyclopedia of Integer Sequences](http://oeis.org/)', 6496, '2015-01-12 14:21:17.067', '3fb25f5d-0896-46ab-a837-fede7d33e578', 4866, 'added 2 characters in body', 7892, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('#I have a dataset of xyz coordinates with a date component in a pandas dataframe

ex:


 - date1: [x1,y1,z1],
 - date2: [x2,y2,z2],
 - date3: [x3,y3,z3],
..

##I would like to classify a sample of object positions over the period of a week
(using indexes to re-map the classification label back to the date)

like so:

 - [x1,y1,z1], [x2,y2,z2], [x3,y3,z3], [x4,y4,z4], [x5,y5,z5], [x6,y6,z6], [x7,y7,z7],
 - [x8,y8,z8],[x9,y9,z9],[x10,y10,z10],[x11,y11,z11],[x12,y12,z12],[x13,y13,z13],[x14,y14,z14],

When I try to run KMeans it returns

    k_means = KMeans(n_clusters=cclasses)
    k_means.fit(process_set.hpc)
    date_classes = k_means.labels_

    ValueError: Found array with dim 3. Expected <= 2

Questions:

 - Do I have to run it through Principal Component Analysis (PCA) first? if so, how do I maintain date mapping to the classification created?
 - Are there any other methods I could use?
 - Am I doing everything completely backwards and should consider a different approach, any thoughts?

Thanks!', 6606, '2015-01-12 14:23:15.343', '50b141de-2c73-4328-ae2d-b0192f221164', 3781, 'Pandas', 7893, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The similarity function at the core of the method will define all the values for your distances $d_1,d_2, \ldots, d_n$. The initial query should have some words as a reference point to compare to the words in the document. Not knowing whether the query is a sentence or arbitrary list, you are restricted to a method that does some kind of histogram comparison of the frequency of the words matching in the documents. You can perform naive summations of keyword mappings counts, look at keyword likelihoods in the normalized distributions, or give a distribution of weighting based on the strongest matches. More exotic functions will be based on your prior belief of how the words should be compared. Working within a Bayesian Framework you can see your prior assumptions explicitly. Cosine similarity or any other vector based measure will be slightly arbitrary without knowing the desired nature of comparison between query and document.

There is not much more you can do without looking at some type of features, or attempt to cross compare the documents together, or use the initial query''s structure. In short, my answer is to use normalized frequency similarities of the document to the queries and produce a ranking, and with more specific goals in mind to apply measures like cosine similarity on test datasets to search for the best measure. ', 34, '2015-01-12 16:39:40.640', '6aadbd62-3acf-42ea-b915-d4d8a611d8c5', 4868, 7894, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('It is usually more practical to compute the distances on the fly rather than storing $N^2$ values. If possible, you will want to use a heuristic to select only the items could be sufficiently similar so you don''t waste time computing distance to irrelevant items. PostgreSQL can create an [index on character trigrams](http://www.postgresql.org/docs/9.1/static/pgtrgm.html) in strings that enables efficient querying for other strings that are sufficiently similar (measured by the Jaccard similarity coefficient).

Storing an $N$ column wide table is not possible because the maximum number of columns allowed in MySQL is 4096. Even if that were not the case, using an RDBMS in such a way is considered bad practice and such a table would be difficult to work with.

 If for some reason you must store a dense distance matrix, a format such as [HDF5](http://www.hdfgroup.org/HDF5/) would be more efficient than an RDBMS.', 7740, '2015-01-13 04:21:38.920', '99f5da0f-a2e2-4a8c-a746-044f4a062b43', 4853, 'Edits for clarity', 7895, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Aleksandar Blekh has given some really nice links about the big picture of how to do sentiment analysis; I''ll try to provide some links to software and talk about the nitty-gritty of how to make it work. I''ll point you to the example using scikit-learn (http://scikit-learn.org/stable/ ), a machine learning library in Python.

You would first want to take your dataset and load it into scikit-learn in a sparse format. This link (http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html ) gives examples of how to load text in a bag-of-words representation, and the same module (scikit-learn.feature_extraction.text) can also count n-grams. It then describes how to run Naive Bayes and SVM on that dataset. You can take that example and start playing with it. ', 7786, '2015-01-13 17:00:04.647', 'b4130c5d-358e-40e9-9f83-78ba14986130', 4870, 7899, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The derivative is a linear transform, and you''re using a linear model. As you''ve demonstrated, nothing is gained by adding linear combination of other features (like the derivative) as a new feature in a linear model.

You may want a derivative coefficient to help interpret the model. You could use the normal model $x_{n+1} = a * x_n + b * n_{n-1}$ and then report the value $a-b$ to the client. A more general approach is to examine the [frequency response][1] of the model (the derivative is a high-pass filter, your model is a FIR filter).

Alternately you can rotate the basis of the input such that the derivative is a feature and the dimensions are still independent as follows:

You can think of the independent variables $x_n$ and $x_{n-1}$ as forming a [standard basis][2] for the two dimensions of your model''s input, that is $\langle 1, 0\rangle$ and $\langle 0, 1\rangle$.
Neither $a\langle 1, 0 \rangle$ or $b \langle 0, 1 \rangle$ are orthogonal to  c$\langle1, -1\rangle$.

Therefore, **if you want to use $c(x_n - x_{n-1})$ as a feature, I would also use the orthogonal feature $d(x_n + x_{n-1})$**. That is,

$$ x_{n+1} = c(x_n - x_{n-1}) + d(x_n + x_{n-1}) $$.

You could think of this model as the derivative plus the integral.

The same logic applies to the choice of scaling function for the Haar wavelet.


  [1]: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.signal.freqz.html
  [2]: https://en.wikipedia.org/wiki/Standard_basis', 7738, '2015-01-13 22:55:35.117', 'fc84d079-f0e7-4dc9-b9d1-aafd1bbafdae', 4856, 'added 734 characters in body', 7904, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Using derivatives as features is **almost the same** as using past values, as both reconstruct phase or state space for dynamic system behind the time series. but they differ in some points, like noise amplification and how they carry information.
(see **Ref**: State space reconstruction in the presence of noise; Martin Casdagli; Physica D - 1991 - section 2)

Notice all information is embedded in time series, but using derivatives is going to reinterpret this information, which may be useful or useless.

In your case, if you use all parameters and terms, i believe there is no use in it. but in case of using some algorithms like orthogonal forward regression (OFR) it may be beneficial. (see **Ref**: Orthogonal least squares methods and their application to non-linear
system identification; S. CHEN, S. A. BILLINGS; INT. J. CONTROL, 1989)', 5200, '2015-01-14 01:08:04.997', '2aa41876-b7b5-4ab3-a329-7222e36afab2', 4872, 7905, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How can I get information about an entity from DBpedia using Python?

Eg: I need to get all DBpedia information about USA [http://dbpedia.org/page/United_States].  So I need to write the query from python (SPARQL) and need to get all attributes on USA as result.

How can I do this and which all are the plugins/api available for python to connect with DBpedia ?

Also what will be the SPARQL query for generating the above problem result?', 5091, '2015-01-14 09:08:00.483', 'd92413be-c927-49d6-8239-7b2b0596761a', 4873, 7906, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Querying DBPedia from Python', 5091, '2015-01-14 09:08:00.483', 'd92413be-c927-49d6-8239-7b2b0596761a', 4873, 7907, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python>', 5091, '2015-01-14 09:08:00.483', 'd92413be-c927-49d6-8239-7b2b0596761a', 4873, 7908, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Querying DBpedia from Python', 5091, '2015-01-14 09:13:54.820', 'bcded365-c256-4ed8-9bc3-a2962b18c229', 4873, 'edited title', 7909, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (' 1. 11 instances of data is insufficient for discovering the behavior (even finding patterns) for such presumably complex systems. You need at least thousands of them.

 2. These numbers are encoded versions of numbers generated by Serial Key Generation Algorithms, which in my experience are usually implemented by Python. So it''s not at all strange that there may be a correlation between some digits of them. As it is necessary for the system to recognize its created numbers from the numbers created by other lotteries.

 3. Even if there was a correlation or correlation between all the instances of lottery numbers, *correlation doesn''t imply causation* and these patterns, if found any, couldn''t really give any information about the key generation system itself.

 4. If you are really serious about it. you could go to this page and enter your data as a sequence it would show you the pattern, if there is any:
[The On-Line Encyclopedia of Integer Sequences](http://oeis.org/)', 2853, '2015-01-14 09:59:34.743', '36fafe16-d52f-4902-a2be-d4ed514a4628', 4866, 'formatting', 7910, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-14 09:59:34.743', '36fafe16-d52f-4902-a2be-d4ed514a4628', 4866, 'Proposed by 2853 approved by 2452, 6496 edit id of 207', 7911, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How can I get information about an entity from DBpedia using Python?

Eg: I need to get all DBpedia information about USA  ([http://dbpedia.org/page/United_States][1]) .  So I need to write the query from python (SPARQL) and need to get all attributes on USA as result.

How can I do this and which all are the plugins/api available for python to connect with DBpedia ?

Also what will be the SPARQL query for generating the above problem result?


  [1]: http://dbpedia.org/page/United_States', 471, '2015-01-14 12:27:03.637', 'e9d7e871-9541-4f0f-8268-002755018e65', 4873, 'link code was malformed/broken', 7912, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-14 12:27:03.637', 'e9d7e871-9541-4f0f-8268-002755018e65', 4873, 'Proposed by 471 approved by 2452, 5091 edit id of 208', 7913, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How can I get information about an entity from DBpedia using Python?

Eg: I need to get all DBpedia information about USA  ([http://dbpedia.org/page/United_States][1]) .  So I need to write the query from python (SPARQL) and need to get all attributes on USA as result.

How can I do this and which all are the possible plugins/api available for python to connect with DBpedia ?

Also what will be the SPARQL query for generating the above problem result?


  [1]: http://dbpedia.org/page/United_States', 5091, '2015-01-14 12:31:54.430', '85e2381e-e755-493c-9778-2a050092904d', 4873, 'added 9 characters in body', 7914, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You do not need a wrapper for DBPedia, you need a library that can issue a SPARQL query to its SPARQL endpoint.  Here is [an option for the library](https://pypi.python.org/pypi/SPARQLWrapper/1.6.4) and here is the URL to point it to: http://dbpedia.org/sparql

An example query to retrieve all countries in Europe, based [on this other example](http://jcastellssala.wordpress.com/2012/06/19/europe-countries-dbpediasparqlpython/).  It is not exactly what you need, but I am sure it can be modified to fit your case with some extra work:

    PREFIX yago: <http://dbpedia.org/class/yago/>
    PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>

    SELECT DISTINCT ?place WHERE {
        ?place a yago:EuropeanCountries.
        ?place a dbpedia-owl:Country.
    }

Here is how you would issue the query and process the results into a normal Python list.  Note this is summarized from a longer module, it may not work by just copy and paste:

<!-- language: python -->

    from SPARQLWrapper import SPARQLWrapper, JSON

    def get_european_country_names():
        sparql = SPARQLWrapper("http://dbpedia.org/sparql")
        sparql.setReturnFormat(JSON)

        sparql.setQuery("""
        PREFIX yago: <http://dbpedia.org/class/yago/>
        PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>

        SELECT DISTINCT ?place WHERE {
            ?place a yago:EuropeanCountries.
            ?place a dbpedia-owl:Country.
        }
        """)

        converted_query = sparql.query().convert()
        names = []
        for result in converted_query[''results''][''bindings'']:
            path = urlparse(result[''place''][''value'']).path
            names.append(posixpath.split(path)[-1])
        return sorted(names)', 1367, '2015-01-14 12:46:41.357', 'f9fdef14-331a-47df-a7ed-568dcb5a42c5', 4874, 7915, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Given a dataset that has a binary (0/1) dependent variable and a large collection of continuous and categorical independent variables, is there a process and ideally a R package that can find combinations/subsets/segments of the IVs that are highly correlated with the DV?

Simple example:
DV: college education (0/1), and IVs: age (20 to 120), income (0 to 1 million), race (white, black, hispanic etc), gender (0/1), state, etc.

Then finding correlations combining IVs and subsets of IVs (e.g. women between 30 and 50, with incomes over 100k are highly positively correlated with the DV), and then being able to compare the combinations (e.g. to find out women between 30 and 40, with incomes over 100k have a higher correlation than women between 40 and 50, with incomes over 100k have a higher correlation than)', 102, '2015-01-14 15:42:48.013', '59bd773c-3b75-471d-a93d-db9d007adc0e', 4875, 7916, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Determine highly correlated segments', 102, '2015-01-14 15:42:48.013', '59bd773c-3b75-471d-a93d-db9d007adc0e', 4875, 7917, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><correlation>', 102, '2015-01-14 15:42:48.013', '59bd773c-3b75-471d-a93d-db9d007adc0e', 4875, 7918, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a data, which looks like this:

![Data table][1]

This data is only for one subject. I will have a lot of it.
The data will be analyzed in R.

Now I''m storing like this:

    subject <- rep(1, times = 24)
    measurement <- factor(x = rep(x = 1:3, each = 8),
                          labels = c("Distance", "Frequency", "Energy"))
    speed <- factor(x = rep(x = 1:2, each = 4, times = 3),
                    labels = c("speed1", "speed2"))
    condition <- factor(x = rep(x = 1:2, each = 2, times = 6),
                        labels = c("Control", "Experm"))
    Try <- factor(x = rep(x = 1:2, times = 12),
                  labels = c("Try1", "Try2"))
    result <- c(1:8,
                11:18,
                21:28)

    dt <- data.frame(subject, measurement, speed, condition, Try, result)

What is appropriate way to store this data in R (in data frame)?


  [1]: http://i.stack.imgur.com/EqH9I.jpg

', 3377, '2015-01-14 15:52:10.633', 'e2c094d3-0855-48b6-b80c-0d22a48a4341', 4876, 7919, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Appropriate way to store data in R', 3377, '2015-01-14 15:52:10.633', 'e2c094d3-0855-48b6-b80c-0d22a48a4341', 4876, 7920, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><dataset>', 3377, '2015-01-14 15:52:10.633', 'e2c094d3-0855-48b6-b80c-0d22a48a4341', 4876, 7921, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Given a dataset that has a binary (0/1) dependent variable and a large collection of continuous and categorical independent variables, is there a process and ideally a R package that can find combinations/subsets/segments of the IVs that are highly correlated with the DV?

Simple example:
DV: college education (0/1), and IVs: age (20 to 120), income (0 to 1 million), race (white, black, hispanic etc), gender (0/1), state, etc.

Then finding correlations combining IVs and subsets of IVs (e.g. women between 30 and 50, with incomes over 100k are highly positively correlated with the DV), and then being able to compare the combinations (e.g. to find out women between 30 and 40, with incomes over 100k have a higher correlation than women between 40 and 50, with incomes over 100k)', 102, '2015-01-14 16:15:04.760', '6b11f10e-4cff-48f3-a0ae-93276e2c2317', 4875, 'wording', 7922, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You do not need a wrapper for DBPedia, you need a library that can issue a SPARQL query to its SPARQL endpoint.  Here is [an option for the library](https://pypi.python.org/pypi/SPARQLWrapper/1.6.4) and here is the URL to point it to: http://dbpedia.org/sparql

You need to issue a DESCRIBE query on the United_States resource page:

    PREFIX dbres: <http://dbpedia.org/resource/>

    DESCRIBE dbres:United_States

Please note this is a **huge** download of resulting triplets.

Here is how you would issue the query:

<!-- language: python -->

    from SPARQLWrapper import SPARQLWrapper, JSON

    def get_country_description():
        sparql = SPARQLWrapper("http://dbpedia.org/sparql")
        sparql.setReturnFormat(JSON)

        sparql.setQuery(query)  # the previous query as a literal string

        return sparql.query().convert()', 1367, '2015-01-14 16:38:14.707', 'e0e9e868-8fa0-4a7a-91e2-3281e707e345', 4874, 'Edit code snippet to fit question more precisely', 7923, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am no expert in that particular case, but doing a bit of research, it seems that the measure you want to construct is called "[Point-biserial correlation coefficient](https://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient)", i.e. the inferred correlation between a continuous variable $X$ and a categorical variable $Y$, e.g. $Y\{1,0,1\}$.  See a related question on [Cross Validated SE](http://stats.stackexchange.com/questions/74116/pearson-correlation-between-discrete-variable-thats-mostly-0-and-a-standard-nor).

And yes, [there is an R package for that](http://cran.r-project.org/web/packages/polycor/index.html) :)', 1367, '2015-01-14 17:06:58.750', '71b11fb4-eb70-488b-ab53-3eeec06ae30d', 4877, 7924, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Without more information all I can say is that:

 1. the say you''re storing it is fine in general
 2. you can further transform/store your data depending on your use case

To expand on #2, if I want to study Distance vs Energy across all subjects, then I would format my data like this:

    > library(reshape2)
    > dt2 <- dt[dt$measurement %in% c(''Distance'',''Energy''),]
    > dt_cast <- dcast(dt2, subject+Try~measurement+speed+condition, value.var=''result'')

The transformed data (dt_cast) would then look like:

      subject  Try Distance_speed1_Control Distance_speed1_Experm Distance_speed2_Control
    1       1 Try1                       1                      3                       5
    2       1 Try2                       2                      4                       6
      Distance_speed2_Experm Energy_speed1_Control Energy_speed1_Experm Energy_speed2_Control
    1                      7                    21                   23                    25
    2                      8                    22                   24                    26
      Energy_speed2_Experm
    1                   27
    2                   28

Allowing me to, for example, look at the relationship between the Distance_speed1_Control vs Energy_speed1_Control columns.

Basically subset/aggregate your data and then use the dcast to get the rows and columns the computer needs.', 525, '2015-01-14 17:56:58.563', '1c4852ea-b700-41d0-9508-ea0e25eb96ca', 4878, 7925, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As an example, say the input is an array of numbers representing an audio snippet and the output is a transformed/filtered version of it.

What would be the proper term for that? Which are examples of algorithms for the job? Is there any in this diagram attached? ![enter image description here][1]


  [1]: http://i.stack.imgur.com/ILCFu.png', 7806, '2015-01-15 00:48:48.383', '7963f19e-817e-4ae9-a730-4fa3b833d153', 4879, 7927, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Training a function that maps n-dim to n-dim', 7806, '2015-01-15 00:48:48.383', '7963f19e-817e-4ae9-a730-4fa3b833d153', 4879, 7928, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<algorithms>', 7806, '2015-01-15 00:48:48.383', '7963f19e-817e-4ae9-a730-4fa3b833d153', 4879, 7929, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m wondering if there''s a way to automatically generate a list of tags for live chat transcripts without domain knowledge.  I''ve tried applying NLP chunking to the chat transcripts and keep only the noun phrases as tag candidates.  However, this approach would generate too many useless noun phrases.  I could use some rules to prune out some of them, but it would be hard to generalize the rules.

', 7807, '2015-01-15 01:57:38.387', '362c7e4a-c72f-4965-b31d-1df786e56b77', 4880, 7930, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Generate tags for live chat transcripts', 7807, '2015-01-15 01:57:38.387', '362c7e4a-c72f-4965-b31d-1df786e56b77', 4880, 7931, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 7807, '2015-01-15 01:57:38.387', '362c7e4a-c72f-4965-b31d-1df786e56b77', 4880, 7932, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How can I get information about an entity from DBpedia using Python?

Eg: I need to get all DBpedia information about USA  ([http://dbpedia.org/page/United_States][1]) .  So I need to write the query from python (SPARQL) and need to get all attributes on USA as result.

I tried :

    PREFIX db: <http://dbpedia.org/resource/>
    SELECT ?p ?o
    WHERE { db:United_States ?p ?o }

But here all DBpedia information is not displaying.

How can I do this and which all are the possible plugins/api available for python to connect with DBpedia ?

Also what will be the SPARQL query for generating the above problem result?


  [1]: http://dbpedia.org/page/United_States', 5091, '2015-01-15 11:33:18.910', '28dc2511-e0ac-4e38-96fc-ec5a12eda1a4', 4873, 'added 173 characters in body', 7933, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This question might sound silly. But I have been wondering why do we assume that there is
a hidden probability distribution between input-output pairs in machine learning setup ?

Can anyone please provide a good intuitive explanation for this ?', 7811, '2015-01-15 11:35:08.843', 'f022c7ef-053c-401a-ad97-cd1ff178b13e', 4881, 7934, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Probability distribution in input-output pairs', 7811, '2015-01-15 11:35:08.843', 'f022c7ef-053c-401a-ad97-cd1ff178b13e', 4881, 7935, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><statistics>', 7811, '2015-01-15 11:35:08.843', 'f022c7ef-053c-401a-ad97-cd1ff178b13e', 4881, 7936, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a **dataset of xyz coordinates with a date component** in a pandas dataframe

ex:


 - date1: $[x_1,y_1,z_1]$,
 - date2: $[x_2,y_2,z_2]$,
 - date3: $[x_3,y_3,z_3]$,
..

I would like to **classify a sample of object positions over the period of a week**
(using indexes to re-map the classification label back to the date), like this:

 - Week 1: $[x_1,y_1,z_1], [x_2, y_2, z_2], [x_3,y_3,z_3], [x_4,y_4,z_4], [x_5,y_5,z_5], [x_6,y_6,z_6], [x_7,y_7,z_7]$,
 - Week 2: $[x_8,y_8,z_8],[x_9,y_9,z_9],[x_{10},y_{10},z_{10}],[x_{11},y_{11},z_{11}],[x_{12},y_{12},z_{12}],[x_{13},y_{13},z_{13}],[x_{14},y_{14},z_{14}]$,

When I try to run KMeans it returns

    k_means = KMeans(n_clusters=cclasses)
    k_means.fit(process_set.hpc)
    date_classes = k_means.labels_

    ValueError: Found array with dim 3. Expected <= 2

Questions:

 - Do I have to run it through Principal Component Analysis (PCA) first? if so, how do I maintain date mapping to the classification created?
 - Are there any other methods I could use?
 - Am I doing everything completely backwards and should consider a different approach, any thoughts?

Thanks!', 1367, '2015-01-15 13:41:08.957', 'd155324d-85ef-44bb-b3d5-762b1ad2f40e', 3781, 'Use LaTex subscripts and clarify formulation', 7937, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-15 13:41:08.957', 'd155324d-85ef-44bb-b3d5-762b1ad2f40e', 3781, 'Proposed by 1367 approved by 2452, 21 edit id of 209', 7938, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":471,"DisplayName":"Spacedman"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-01-15 13:41:48.937', '2028f508-37d3-4e08-b3a1-8886f5c8255b', 4873, '104', 7940, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are quite a number of generative models [here][1], e.g.:

    Gaussian mixture model and other types of mixture model
    Hidden Markov model
    Probabilistic context-free grammar
    Naive Bayes
    Averaged one-dependence estimators
    Latent Dirichlet allocation
    Restricted Boltzmann machine

What I want to do is, given a set of strings learnt by each of the generative model, I want to make use of the learnt model to output a set of strings in descending order of their probabilities.

Is there any readily available tools (preferably in Python, other are also ok) that support for generative model that allows me to achieve this?

  [1]: http://en.wikipedia.org/wiki/Generative_model', 7812, '2015-01-15 14:15:51.923', 'fe6f3034-7847-4cfd-92be-a20edf892e27', 4882, 7941, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Tools for generative models', 7812, '2015-01-15 14:15:51.923', 'fe6f3034-7847-4cfd-92be-a20edf892e27', 4882, 7942, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><tools><sklearn>', 7812, '2015-01-15 14:15:51.923', 'fe6f3034-7847-4cfd-92be-a20edf892e27', 4882, 7943, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am currently working on a recommendation system for daily news. At first, I evaluated all the recommender algorithms and their corresponding settings (e.g., similarities, factorizers, ...etc), using reading behavior of the users 2 days ago to do recommendation on the next day. The evaluated RMSE is good, the best recommender is SVD+SGD, so we implemented the recommender on our system for several days of trial run.

However, the result, the actually recommended news, seems to be not very attractive for real users ("not attractive" here means, the users feel like "why you recommend this to me?"). So we decided another approach: use the tags and categories and their relationship to do the main job of recommendation, the result from CF is for just supporting.

This makes me wonder if CF if not appropriate for some kind of content. Because I also worked on movie and music recommendation, CF is a good tool. But for news, it seems not the case.

Can anyone explain why this happening, and also give some guideline about how to choose appropriate recommendation methods? Thanks:)
', 5184, '2015-01-15 15:13:54.713', 'e3a559ff-eecc-41f5-bab9-f75e86ca5a40', 4883, 7944, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What kind of data is not appropriate using CF to do recommendation?', 5184, '2015-01-15 15:13:54.713', 'e3a559ff-eecc-41f5-bab9-f75e86ca5a40', 4883, 7945, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation>', 5184, '2015-01-15 15:13:54.713', 'e3a559ff-eecc-41f5-bab9-f75e86ca5a40', 4883, 7946, '3');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (303, '2015-01-15 16:08:43.723', '0aa20029-b511-4f54-b3cb-48a5fdbc1c08', 3771, '10', 7947, '34');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As an example, say the input is an array of numbers representing an audio snippet and the output is a transformed/filtered version of it.

What would be the proper term for that? Which are examples of algorithms for the job?', 7806, '2015-01-15 16:47:11.953', '23c23da9-c0b6-4cfb-87cc-6b5d7125513c', 4879, 'deleted 121 characters in body', 7948, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As an example, say the input is an array of numbers representing an audio snippet and the output is a transformed/filtered version of it.

What would be the proper term for that? Which are examples of algorithms for the job?

**EDIT:**
More specifically, I want to train audio source separation. The input is a mixed sound (spectrogram) and the output is the sound with some energy removed in certain frequencies. The function needs to recognize some pattern in the input and decide what to remove.', 7806, '2015-01-15 17:03:40.697', 'a8f9047d-e30c-4647-af06-191c458342b1', 4879, 'added 263 characters in body', 7949, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I hope you can help me, as I have some questions on this topic. I''m new in the field of deep learning, and while I did some tutorials, I can''t relate or distinguish concepts from one another.', 3615, '2015-01-15 17:59:37.837', '28fcbde1-715c-4fd8-a97b-212def17422f', 1253, 'deleted 2536 characters in body; edited tags; edited title', 7950, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Why are NLP and Machine Learning communities interested in deep learning?', 3615, '2015-01-15 17:59:37.837', '28fcbde1-715c-4fd8-a97b-212def17422f', 1253, 'deleted 2536 characters in body; edited tags; edited title', 7951, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><nlp><neuralnetwork>', 3615, '2015-01-15 17:59:37.837', '28fcbde1-715c-4fd8-a97b-212def17422f', 1253, 'deleted 2536 characters in body; edited tags; edited title', 7952, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was wondering whether we could list machine learning winning methods to apply in many fields of interest: NLP, image, vision, medical, deep package inspection, etc. I mean, if someone will get started a new ML project, what are the ML methods that cannot be forgotten?', 6560, '2015-01-15 18:32:36.883', '9bb5024a-83b7-40bb-ac56-0747cb440f46', 4884, 7953, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the current killing machine learning methods?', 6560, '2015-01-15 18:32:36.883', '9bb5024a-83b7-40bb-ac56-0747cb440f46', 4884, 7954, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 6560, '2015-01-15 18:32:36.883', '9bb5024a-83b7-40bb-ac56-0747cb440f46', 4884, 7955, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you''re working in R, Carson Sievert''s tutorial on using LDA to model topics in movie reviews is an excellent starting point:

http://cpsievert.github.io/LDAvis/reviews/reviews.html

This tutorial makes use of LDAvis, an interactive visualization of topic and word distributions that can really aid intuition.

Also, although not short, David M. Blei''s lectures on topic models are a great resource for understanding the meaning behind the parameters: http://videolectures.net/mlss09uk_blei_tm/
', 686, '2015-01-15 20:07:51.903', 'cd8f5c6f-f5cd-484f-87a7-885b385c2e54', 4885, 7956, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am just 5 months late but with CRFSuite you can actually use those float features as numbers, not as strings. For this, you just need to invent an unique label for each dimension then add a ":" followed by the value.

For example, a word "jungle" is represented in 5 dimensions:
0.1 0.4 0.8 0.2 0.9

Then CRFSuite would take that word + feature as:

LABEL f1:0.1 f2:0.4 f3:0.8 f4:0.2 f5:0.9

where of course you replace ``LABEL'''' by an actual string and you separate all spaces with tabs (that''s the format for CRFSuite).

Not sure though for other packages.
', 7818, '2015-01-15 23:27:20.597', '8b3f7a33-2fe4-4d1b-88c5-fa4ae7fe7eda', 4886, 7957, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The question is very general. However, there are some studies being conducted to test which algorithms perform relatively well in a broad range of problems (I''ll add link to papers later), concerning regression and classification.

Lately Random Decision Forests, Support Vector Machines and certain variations of Neural Networks are being said to achieve the best results for very broad variety of problems.

This does not mean that these are "the best algorithms" for any problem, that does not exist, and actually is not very realistic to pursue. Also it must be observed that both RDF and SVM are rather-easy methods to initially grasp and obtain good results, so they are becoming really popular. NN have been used intensively since couple of decades (after they revived), so they appear often in implementations.

If you are interested in learning further you should look for an specific area and deal with a problem that can be solved nicely by machine learning to understand the main idea (and why is impossible to find  **the method**).

You will find common the task to try to predict the expected behavior of something given some known or observable characteristics (to learn the function that models the problem given input data), the issues related to dealing with data in high-dimensional spaces, the need for good quality data, the notable improvements that can give data pre-processing, and many others.', 5143, '2015-01-16 01:00:45.890', '5dfb3953-5fa1-4096-a547-d9189b191398', 4887, 7959, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('### Why to use deep networks?

Let''s first try to solve very simple classification task. Say, you moderate a web forum which is sometimes flooded with spam messages. These messages are easily identifiable - most often they contain specific words like "buy", "porn", etc. and a URL to outer resources. You want to create filter that will alert you about such suspecious messages. It turns to be pretty easy - you get list of features (e.g. list of suspicious words and presence of a URL) and train simple logistic regression (a.k.a. perceptron), i.e. model like:

    g(w0 + w1*x1 + w2*x2 + ... + wnxn)

where `x1..xn` are your features (either presence of specific word or a URL), `w0..wn` - learned coefficients and `g()` is a [logistic function](http://en.wikipedia.org/wiki/Logistic_function) to make result be between 0 and 1. It''s very simple classifier, but for this simple task it may give very good results, creating linear decision boundary. Assuming you used only 2 features, this boundary may look something like this:

![linear boundary](http://www.mblondel.org/images/perceptron_linear.png)

Here 2 axes represent features (e.g. number of occurrences of specific word in a message, normalized around zero), red points stay for spam and blue points - for normal messages, while black line shows separation line.

But soon you notice that some good messages contain a lot of occurrences of word "buy", but no URLs, or extended discussion of [porn detection](http://stackoverflow.com/questions/713247/what-is-the-best-way-to-programatically-detect-porn-images), not actually refferring to porn movies. Linear decision boundary simply cannot handle such situations. Instead you need something like this:

![non-linear boundary](http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex8materials/ex8b_100.png)

This new non-linear decision boundary is much more **flexible**, i.e. it can fit the data much closer. There are many ways to achieve this non-linearity - you can use polynomial features (e.g. `x1^2`) or their combination (e.g. `x1*x2`) or project them out to a higher dimension like in [kernel methods](http://en.wikipedia.org/wiki/Kernel_method). But in neural networks it''s common to solve it by **combining perceptrons** or, in other words, by building [**multilayer perceptron**](http://en.wikipedia.org/wiki/Multilayer_perceptron). Non-linearity here comes from logistic function between layers. The more layers, the more sophisticated patterns may be covered by MLP. Single layer (perceptron) can handle simple spam detection, network with 2-3 layers can catch tricky combinations of features, and networks of 5-9 layers, used by large research labs and companies like Google, may model the whole language or detect cats on images.

This is essential reason to have **deep architectures** - they can *model more sophisticated patterns*.

### Why deep networks are hard to train?

With only one feature and linear decision boundary it''s in fact enough to have only 2 training examples - one positive and one negative. With several features and/or non-linear decision boundary you need several orders more examples to cover all possible cases (e.g. you need not only find examples with `word1`, `word2` and `word3`, but also with all possible their combinations). And in real life you need to deal with hundreds and thousands of features (e.g. words in a language or pixels in an image) and at least several layers to have enough non-linearity. Size of a data set, needed to fully train such networks, easily exceeds 10^30 examples, making it totally impossible to get enough data. In other words, with many features and many layers our decision function becomes **too flexible** to be able to learn it **precisely**.

There are, however, ways to learn it _approximately_. For example, if we were working in probabilistic settings, then instead of learning frequencies of all combinations of all features we could assume that they are independent and learn only individual frequencies, reducing full and unconstrained [Bayes classifier](http://en.wikipedia.org/wiki/Bayes_classifier) to a [Naive Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) and thus requiring much, much less data to learn.

In neural networks there were several attempts to (meaningfully) reduce complexity (flexibility) of decision function. For example, convolutional networks, extensively used in image classification, assume only local connections between nearby pixels and thus try only learn combinations of pixels inside small "windows" (say, 16x16 pixels = 256 input neurons) as opposed to full images (say, 100x100 pixels = 10000 input neurons). Other approaches include feature engineering, i.e. searching for specific, human-discovered descriptors of input data.

Manually discovered features are very promising actually. In natural language processing, for example, it''s sometimes helpful to use special dictionaries (like those containing spam-specific words) or catch negation (e.g. "_not_ good"). And in computer vision things like [SURF descriptors](http://en.wikipedia.org/wiki/SURF) or [Haar-like features](http://en.wikipedia.org/wiki/Haar-like_features) are almost irreplaceable.

But the problem with manual feature engineering is that it takes literally years to come up with good descriptors. Moreover, these features are often specific

### Unsupervised pretraining

But it turns out that we can _obtain good features automatically_ right from the data using such algorithms as **autoencoders** and **restricted Boltzmann machines**. I described them in detail in my other [answer](http://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma/117188#117188), but in short they allow to **find repeated patterns in the input data** and transform it into higher-level features. For example, given only row pixel values as an input, these algorithms may identify and pass higher whole edges, then from these edges construct figures and so on, until you get really high-level descriptors like variations in faces.

![deep learning](http://i.stack.imgur.com/oGBRR.jpg)

After such (unsupervised) pretraining network is usually converted into MLP and used for normal supervised training. Note, that pretraining is done layer-wise. This significantly reduces solution space for learning algorithm (and thus number of training examples needed) as it only needs to learn parameters _inside_ each layer without taking into account other layers.

### And beyond...

Unsupervised pretraining have been here for some time now, but recently other algorithms were found to improve learning both - together with pretraining and without it. One notable example of such algorithms is [**dropout**](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) - simple technique, that randomly "drops out" some neurons during training, creatig some distortion and preventing networks of following data too closely. This is still a hot research topic, so I leave this to a reader. ', 1279, '2015-01-16 01:10:21.240', 'a6ea299e-b5d6-4b37-bb42-d5a941a7c387', 4888, 7960, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you have existing properly tagged chat transcripts, you can try treating it as a supervised learning problem. If you''re starting from a blank slate, that won''t work. ', 7786, '2015-01-16 04:44:23.923', '6d59be00-c8df-4ad2-a344-9fb4fae45a48', 4889, 7961, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The key is establishing a proper validation metric.

I notice you talk about how you tried different recommendation algorithms, but at the end of the day you evaluated them all with RMSE. But there''s no particular reason to believe that minimizing RMSE generates a "subjectively better" recommendation experience for the user - it just happens to be convenient, and happens to work well in some industries, but there is no real reason why it must.

RMSE is measuring how well your recommender algorithm is predicting user behavior. But that''s not the same as measuring recommendation quality. Maybe users value something else - familiarity, or serendipity, or some other quality of the item being recommended. Users don''t really care about being predicted.

Given your results, if you want to understand your users further, I''d focus my efforts in coming up with a mathematical metric that more closely matches the target you care about - user satisfaction - rather than RMSE. Once you know what metric you''re trying to optimize, the algorithm to optimize it is much easier to select! ', 7786, '2015-01-16 04:54:40.550', 'eff9f875-60ee-480f-96c6-63f003dceb21', 4890, 7962, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am currently working on a recommendation system for daily news. At first, I evaluated all the recommender algorithms and their corresponding settings (e.g., similarities, factorizers, ...etc) implemented in Mahout. Since we want to recommend daily news for users, we use the reading behavior of each user collected two days ago as training set, data of the next day as the testing set. The evaluated RMSE is good, the best recommender is SVD+SGD, so we implemented the recommender on our system for several days of trial run.

However, the result, the actually recommended news, seems to be not very attractive for real users ("not attractive" here means, the users feel like "why you recommend this to me?"). So we decided another approach: use the tags and categories and their relationship to do the main job of recommendation, the result from CF is for just supporting.

This makes me wonder if CF if not appropriate for some kind of content. Because I also worked on movie and music recommendation, CF is a good tool. But for news, it seems not the case.

Can anyone explain why this happening, and also give some guideline about how to choose appropriate recommendation methods? Thanks:)
', 5184, '2015-01-16 05:22:00.583', '0c85dfff-9725-47a4-86b7-a16a6be2217a', 4883, 'Add more details about the original implementation.', 7963, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine learning is a good example of a problem type where Spark-based solutions are light-years ahead of mapreduce-based solutions, despite the young age of spark-on-yarn. ', 7786, '2015-01-16 05:30:35.787', '8cab737f-3e2f-4109-9693-a1085e12b23d', 4891, 7964, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can try RAKE(Rapid Automatic Keyword Extraction) and there is a python implementation here: https://github.com/aneesha/RAKE. RAKE is an document-oriented keyword extraction algorithm and also language-independent(theoretically, since RAKE use a generated stop word list to partition candidate keywords, and considering different languages, we need to find a better way to generated stop word list.). However, about English documents, RAKE can extract keywords(or tags) in a acceptable precision and recall. RAKE is also efficient, because to use it we don''t have to training a whole corpus, RAKE can generate a keyword list by calculating the word''s degree and frequency then comes up a score for every candidate keyword then pick the top N words.

Hope this answer helps you or lead you a direction for your next step investigation.', 1003, '2015-01-16 05:44:41.907', '55153a9b-a4fd-4a6a-bba9-c9368fca0a75', 4892, 7965, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('First we need to understand why we need Deep learning. To build models ML need Test Data with Labels (supervised or unsupervised). In many domains as the data grows maintaining the data with labels is hard. Deep learning networks doesn''t need labeled data. The Deep learning algorithms can figure out the labels. So this obviates the need for domain experts to come out with labels for the data which is very important in the areas of speech recognition, computer vision, and language understanding. Google Cat image recognition is a very interesting experiment. Also it is interesting to know "Geoffrey hinton" the professor who was hired by Google.

http://www.wired.com/2014/01/geoffrey-hinton-deep-learning/

You may get more insight as you explore in this framework.
', 6496, '2015-01-16 17:16:22.870', 'dcafff29-3091-4148-8f21-467dc1c773cc', 2311, 'grammar correction and duplicate words removal', 7969, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-16 17:16:22.870', 'dcafff29-3091-4148-8f21-467dc1c773cc', 2311, 'Proposed by 6496 approved by 2452, 21 edit id of 210', 7970, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-01-16 17:19:24.150', '36d1849c-c411-4ed7-88a8-125c408d2f90', 4884, '104', 7974, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('AdaBoost is a supervised learning method; it starts with a table of ''correct'' answers and generates a predictive model for a target, which is known. It is then possible to inspect this model to figure out how it works, what it judged was more important. With that in mind, here is my guess for what he did:

First, he created a training dataset. The dataset was created, according to the article, like this:


> Now hed do the same for love. First hed need data. While his
> dissertation work continued to run on the side, he set up 12 fake
> OkCupid accounts and wrote a Python script to manage them. The script
> would search his target demographic (heterosexual and bisexual women
> between the ages of 25 and 45), visit their pages, and scrape their
> profiles for every scrap of available information: ethnicity, height,
> smoker or nonsmoker, astrological signall that crap, he says.
>
> To find the survey answers, he had to do a bit of extra sleuthing.
> OkCupid lets users see the responses of others, but only to questions
> theyve answered themselves. McKinlay set up his bots to simply answer
> each question randomlyhe wasnt using the dummy profiles to attract
> any of the women, so the answers didnt mat­terthen scooped the
> womens answers into a database. "

So he used python scripts to collect lots of information! At the end of this, he had a table of data, where each row had three pieces of information:
- A bot''s answer to all the questions, and their weights
- A woman''s answer to all the questions
- Their match percentage.

On this, he could use AdaBoost to create a predictive model, predicting the match percentage from the available information. The weak learners were probably decision stumps, greedily choosing one variable at a time to split on, that''s the standard that people refer to when talking about AdaBoost.

Once the predictive model, was in place, it could be used for optimization - determining which weights to put on the questions when holding all other variables constant, maximizing the average match percentage to women in his target audience.

Of course, this is just a guess. The article doesn''t have much detail. But it''s a potential way to use AdaBoost for that purpose. ', 7786, '2015-01-17 04:29:24.393', '1146ef01-dc36-4869-8081-d9e9b83f6bbb', 4894, 7976, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Your results are reasonable. Your data brings several ideas to mind:

1) It is quite reasonable that as you change the available features, this will change the relative performance of machine learning methods. This happens quite a lot. Which machine learning method performs best often depends on the features, so as you change the features the best method changes.

2) It is reasonable that in some cases, disparate models will reach the exact same results. This is most likely in the case where the number of data points is low enough or the data is separable enough that both models reach the exact same conclusions for all test points. ', 7786, '2015-01-17 04:42:47.530', 'b955992e-b133-4e3e-bc5e-f4b62710999f', 4895, 7977, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A simple way would be to consider Laplace Smoothing (http://en.wikipedia.org/wiki/Additive_smoothing ) or something like it.

Basically, instead of calculating your response rate as (Clicks)/(Impressions) you calculate (Clicks + X)/(Impressions + Y), with X and Y chosen, for example, so that X/Y is the global average of clicks/impressions.

When Clicks and Impressions are both high, this smoothed response rate is basically equal to the true response rate (signal dominates the prior). When Clicks and Impressions are both low, the this smoothed response rate will be close to the global average response rate - a good guess when you have little data and don''t want to put much weight on it!

The absolute scale of X and Y will determine how many data points you consider "enough data". It''s been argued that the right thing to do is set X to 1, and Y appropriately given that. ', 7786, '2015-01-17 05:20:32.763', '820cdd89-810c-4804-bcfd-31143348d336', 4896, 7978, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You may want to consider gradient boosted trees rather than random forests. They''re also an ensemble tree-based method, but since this method doesn''t sample dimensions, it won''t run in to the problem of not having a useful predictor available to split on at any particular time.

Different implementations of GBDT have different ways of handling missing values, which will make a big difference in your case; I believe R does ternary splits which is likely to work fine. ', 7786, '2015-01-17 05:30:01.813', '6b5eca36-80e2-46f9-8b84-a9c8f3956d00', 4897, 7979, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Late answer, but here is an eclectic list of **[100+ Interesting Data Sets][1]**

The blog post is fun and easy to read through (I have no affiliation). It''s worth to scan through, and to scrape a few from the top:

+ Last words of every Texas inmate executed since 1984

+ 10,000 annotated images of cats

+ 2.2 million chess matches





  [1]: http://rs.io/100-interesting-data-sets-for-statistics/', 7842, '2015-01-17 16:53:08.033', '2ac3706b-4e46-48bd-9b73-93844a31b8ef', 4898, 7982, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('This question might sound silly. But I have been wondering why do we assume that there is
a hidden probability distribution between input-output pairs in machine learning setup ?

For example, if we want to learn a function $f: \mathcal{X} \rightarrow \mathcal{Y}$, we generally tend to assume a probability distribution $\rho(x,y)$ on $Z=\mathcal{X} \times \mathcal{Y} $ and try to minimize the error
$$
\mathcal{E}(f) = \int (f(x)-y)^2 \ d\rho(x,y)
$$

Is the probability distribtution $\rho$ inherent to the very nature of $Z$ or depends on $f$ ?

Can anyone please provide a good intuitive explanation for this ?', 7811, '2015-01-18 10:14:42.397', '0ff7bf4d-ac2a-43fe-8c05-db87fef8b5f2', 4881, 'added 379 characters in body', 7984, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I may still misunderstand what you mean, but the general simple formulation is to minimize sum of loss over all training examples. Converting to your formulation, that ''assumes'' the joint distribution of the input and output is just the empirical distribution found in the data. That''s the best assumption you can make without additional information. If you had reason to assume something else, you would. ', 21, '2015-01-18 10:22:35.777', 'f0544d7c-c4e9-42a8-8688-276cfad07b26', 4899, 7985, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I''d understood your question correctly, one easy solution would be to concatenate the bits together and make a 7-bit binary sequence then convert it to integer. So, for your sample dataset quoted in the question you would get:

<code>
CM
16
0
98
8
6
67
</code>
', 6496, '2015-01-18 19:14:41.420', 'aa36c548-0ac3-4a80-8a2b-d38d450bf853', 4900, 7989, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('If I''ve understood your question correctly, one easy solution would be to concatenate the bits together and make a 7-bit binary sequence then convert it to integer. So, for your sample dataset quoted in the question you would get:

<code>
CM
16
0
98
8
6
67
</code>
', 6496, '2015-01-19 08:13:58.220', '7858f3fe-e6f8-4885-b83b-a9d484330720', 4900, 'added 1 character in body', 7990, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a data set, which contains some features and a label from 0.0 to 1.0.

Now, I want to predict this label based on given features, it is a regression problem, so I use *gradient boosting regression tree* as model and run 10-fold cross validation.

However, it turns out that the average root-mean-square error of prediction is 0.383444, which is quite high. Because if I don''t use any complex models and just use the mean value of known labels as prediction values, then I can still have an RMSE of 0.3890, there is no obvious improvement.

In this case, I''m wondering:

1. Does this high error rate imply that the feature I using does not reflect the change of label?

2. Apart from feature problem, is there any other possibilities? if yes, how can I validate them?', 7867, '2015-01-19 09:37:56.340', '816c5881-13f2-41cc-af2e-9f84fd76bda1', 4901, 7991, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Does high error rate in regression imply that the feature I''m using could not reflect label change?', 7867, '2015-01-19 09:37:56.340', '816c5881-13f2-41cc-af2e-9f84fd76bda1', 4901, 7992, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<feature-selection><regression>', 7867, '2015-01-19 09:37:56.340', '816c5881-13f2-41cc-af2e-9f84fd76bda1', 4901, 7993, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It is always helpful to just Google the exact error that you are seeing, excluding specifics like the actual memory of the vector.  For me, the first hit was an interesting documentation called "[R: Memory limits of R][1]", where, under "Unix", one can read:

> The address-space limit is system-specific: 32-bit OSes imposes a limit of no more than 4Gb: it is often 3Gb. Running 32-bit executables on a 64-bit OS will have similar limits: 64-bit executables will have an essentially infinite system-specific limit (e.g., 128Tb for Linux on x86_64 cpus).

> See the OS/shell''s help on commands such as limit or ulimit for how to impose limitations on the resources available to a single process. For example a bash user could use

> `ulimit -t 600 -v 4000000`

> whereas a csh user might use

> `limit cputime 10m`
> `limit vmemoryuse 4096m`

> to limit a process to 10 minutes of CPU time and (around) 4Gb of virtual memory. (There are other options to set the RAM in use, but they are not generally honoured.)

So, you should check

1. [What type of OS are you running on your EC2 instance][2]
2. What type of R build are you running on that OS, and [make sure you run a 64bit version][3]
2. If both are already 64bit, then use `ulimit` to set memory to e.g. 8Gb: `ulimit -v 8000000`

[1]: http://stat.ethz.ch/R-manual/R-devel/library/base/html/Memory-limits.html
[2]: http://stackoverflow.com/questions/11565649/how-to-find-if-my-amazon-ec2-instance-is-32-bit-or-64-bit
[3]: http://randyzwitch.com/r-amazon-ec2/', 1367, '2015-01-19 10:07:11.270', '9a4d923d-77bf-4c2c-9878-2f768acef709', 4902, 7994, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a data set, which contains some features and a label from 0.0 to 1.0.

Now, I want to predict this label based on given features, it is a regression problem, so I use *gradient boosting regression tree* as model and run 10-fold cross validation.

However, it turns out that, the best root-mean-square error of prediction is 0.383444, which is quite high. Because if I don''t use any complex models and just use the mean value of known labels as prediction values, then I can still have an RMSE of 0.3890, there is no obvious improvement.

In this case, I''m wondering:

1. Does this high error rate imply that the feature I using does not reflect the change of label?

2. Apart from feature problem, is there any other possibilities? if yes, how can I validate them?', 7867, '2015-01-19 12:11:01.473', '35e0918b-fcea-4577-b4bc-5c52ed7c6527', 4901, 'deleted 2 characters in body; edited title', 7995, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Does high error rate in regression imply the data set is unpredicatable?', 7867, '2015-01-19 12:11:01.473', '35e0918b-fcea-4577-b4bc-5c52ed7c6527', 4901, 'deleted 2 characters in body; edited title', 7996, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can anybody tell me what is the purpose of feature generation? and why feature space enrichment is needed before classifying an image? Is it a necessary step?', 7873, '2015-01-19 14:26:57.117', 'a70b105f-ad6c-493e-b054-6e48c32f2fa9', 4903, 7997, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the difference between feature generation and feature extraction?', 7873, '2015-01-19 14:26:57.117', 'a70b105f-ad6c-493e-b054-6e48c32f2fa9', 4903, 7998, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification>', 7873, '2015-01-19 14:26:57.117', 'a70b105f-ad6c-493e-b054-6e48c32f2fa9', 4903, 7999, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Feature Generation** -- This is the process of taking raw, unstructured data and defining features (i.e. variables) for potential use in your statistical analysis. For instance, in the case of text mining you may begin with a raw log of thousands of text messages (e.g. SMS, email, social network messages, etc) and generate features by removing low-value words (i.e. stopwords), using certain size blocks of words (i.e. n-grams) or applying other rules.

**Feature Extraction** -- After generating features, it is often necessary to select a subset of the potential features for use in your model (i.e. feature extraction). Using too many features can result in multiply colinearity or otherwise confound statistical models, whereas extracting the minimum number of features to suit the purpose of your analysis follows the principal of parsimony.

Enhancing your feature space in this way is often a necessary step in classification of images or other data objects because the raw feature space is typically filled with an overwhelming amount of unstructured and irrelavent data that comprises what''s often referred to as "noise" in the paradigm of a "signal" and "noise" (which is to say that some data has predictive value and other data does not). By enhancing the feature space you can better identify the important data which has predictive or other value in your analysis (i.e. the "signal") while removing confounding information (i.e. "noise").

', 2723, '2015-01-19 14:55:25.580', '8dcfb17c-00ca-4fb0-b3b0-4d34b868edd0', 4904, 8001, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**Feature Generation** -- This is the process of taking raw, unstructured data and defining features (i.e. variables) for potential use in your statistical analysis. For instance, in the case of text mining you may begin with a raw log of thousands of text messages (e.g. SMS, email, social network messages, etc) and generate features by removing low-value words (i.e. stopwords), using certain size blocks of words (i.e. n-grams) or applying other rules.

**Feature Extraction** -- After generating features, it is often necessary to test transformations of the original features and select a subset of this pool of potential original and derived features for use in your model (i.e. feature extraction and selection). Testing derived values is a common step because the data may contain important information which has a non-linear pattern or relationship with your outcome, thus the importance of the data element may only be apparent in its transformed state (e.g. higher order derivatives). Using too many features can result in multiply colinearity or otherwise confound statistical models, whereas extracting the minimum number of features to suit the purpose of your analysis follows the principal of parsimony.

*Enhancing your feature space* in this way is often a necessary step in classification of images or other data objects because the raw feature space is typically filled with an overwhelming amount of unstructured and irrelevant data that comprises what''s often referred to as "noise" in the paradigm of a "signal" and "noise" (which is to say that some data has predictive value and other data does not). By enhancing the feature space you can better identify the important data which has predictive or other value in your analysis (i.e. the "signal") while removing confounding information (i.e. "noise").

', 2723, '2015-01-19 15:01:18.370', '035343f0-3a9c-419c-b768-7c60f93b076b', 4904, 'added 372 characters in body', 8002, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Can anybody tell me what is the purpose of feature generation? and why feature space enrichment is needed before classifying an image? Is it a necessary step?

Is there any method to enrich feature space?', 7873, '2015-01-19 17:48:27.370', 'db0e3249-637b-4eec-8dd6-956e7a734057', 4903, 'improved the question', 8003, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Some notes:

1. In the case of supervised learning you assume there is a function $f:\mathcal{X} \rightarrow \mathcal{Y}$, which means there is a connection somehow between inputs and outputs and you want to use it in order to predict. But how this function might look? Without considering the model itself, usually what happens is that there is some noise. And this noise can be in $Y$ and also can be in $X$. This noise gives use the probabilistic setup of the problem, because without it we would have only to solve some equations.

2. Now is important to understand that the noise defines the distribution. So a random variable can be imagined as a function having something fixed and well defined and something not fixed, but taking values according with a distribution. If the variable part does not exist, that you would not have a random variable, right, it would be a simple formula. But its not. Now the $P(X)$ incorporates what happens in $X$ alone, and $P(Y)$ what is in $Y$ alone. When you predict the decision theory says that you are interested in saying which is the most probable value $y_i$ given some input value $x_i$. So you are interested in finding $P(Y|X)$.

3. A joint probability is not always completely described by marginal probabilities. In fact, is completely described only when marginal probabilities are independent. Which means for r.v. $X, Y$ knowing $P(X)$ and $P(Y)$ does not put you in position to know the joint density $P(X, Y)$ (thought for independence you have  $P(X,Y)=P(X)P(Y)$).

4. From here you can go directly and try to estimate $P(Y|X)$. In fact if your only interest in only in prediction, this might be a fair bet. A lot of supervised learning algorithms tries to estimate directly this probability. They are called discriminant classifiers. The reason is because they are used to classify something to  the class with maximal conditional probability, you discriminate (choose) the maximum value.

5. Now arriving at your question. Notice the obvious $P(X,Y) = P(Y|X)P(X)$ than you see that by trying learning the joint probability, you also learn the conditional (what you need for prediction). This kind of approach is called generative, because knowing the joint probability you can not only predict, but you can generate new data for your problem. More than that, knowing the join probability can give you more insights related with how what are you model works. You can find such kind of information which is not contained only in marginal distributions.

Some final notes:

- Technically you do not minimize the error function, but it''s expectation. The error function remains as it is.
- $\mathcal{Z}$ is only a domain it''s impossible to describe a probability only by it''s domain.

It''s late, I hope I was not completely incoherent.', 108, '2015-01-19 18:03:17.467', '895590ba-f96e-4e38-bda5-4f24b1a3bbf5', 4905, 8004, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2015-01-19 19:11:09.507', 'e3ec8ed1-aa9e-45c8-b5b2-f7459fffdc06', 4906, 8005, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2015-01-19 19:11:09.507', 'fccfa721-14b7-4abb-8107-3f275812b600', 4907, 8006, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2015-01-19 19:17:36.820', '2890444e-356a-4af2-a355-b2e43377e09f', 4908, 8007, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2015-01-19 19:17:36.820', '21602f35-4e32-40de-a0bb-6649d54b0a1f', 4909, 8008, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2015-01-19 19:22:11.877', 'a293efc7-bf13-4317-aa98-c7fd5452a560', 4910, 8009, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('', -1, '2015-01-19 19:22:11.877', '3b318151-3fe6-4345-a2fb-b10e192ad62f', 4911, 8010, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It''s a little hasty to make too many conclusions about your data based on what you presented here. At the end of the day, all the information you have right now is that "GBT did not work well for this prediction problem and this metric", summed up by a single RMSE comparison. This isn''t very much information - it could be that this is a bad dataset for GBT and some
other model would work, it could be that the label can''t be predicted from these features with any model, or there could be some error in model setup/validation.

I''d recommend checking the following hypotheses:

1) Maybe, with your dataset size and the features you have, GBT isn''t a very high-performance model. Try something completely different - maybe just a simple linear regression! Or a random forest. Or GBDT with very different parameter settings. Or something else. This will help you diagnose whether it''s an issue with choice of models or with something else; if a few very different approaches give you roughly similar results, you''ll know that it''s not the model choice that is causing these results, and if one of those models behaves differently, then that gives you additional information to help diagnose the issue.

2) Maybe there''s some issue with model setup and validation? I would recommend doing some exploration to get some intuition as to whether the RMSE you''re getting is reasonable or whether you should expect better. Your post contained very little detail about what the data actually represents, what you know about the features and labels, etc. Perhaps you know those things but didn''t include them here, but if not, you should go back and try to get additional understanding of the data before continuing. Look at some random data points, plot the columns against the target, look at the histograms of your features and labels, that sort of thing. There''s no substitute for looking at the data.

3) Maybe there just aren''t enough data points to justify complex models. When you have low numbers of data points (< 100), a simpler parametric model built with domain expertise and knowledge of what the features are may very well outperform a nonparametric model.  ', 7786, '2015-01-19 21:18:54.347', '018e691e-26eb-460f-973f-83c79efd1545', 4912, 8011, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a data set, which contains some features and a label from 0.0 to 1.0.

Now, I want to predict this label based on given features, it is a regression problem, so I use *gradient boosting regression tree* as model and run 10-fold cross validation.

However, it turns out that, the best root-mean-square error of prediction is 0.383444, which is quite high. Because if I don''t use any complex models and just use the mean value of known labels as prediction values, then I can still have an RMSE of 0.3890, there is no obvious improvement.

In this case, I''m wondering:

1. Does this high error rate imply that the feature I am using does not reflect the change of label?

2. Apart from the feature problem, is there any other possibilities? If yes, how can I validate them?', 6496, '2015-01-20 02:29:26.153', '777596bb-06e0-4132-99ad-45a16ef958d4', 4901, 'fixed grammar', 8012, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-20 02:29:26.153', '777596bb-06e0-4132-99ad-45a16ef958d4', 4901, 'Proposed by 6496 approved by 2452, 7867 edit id of 211', 8013, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a data set of video watching records in a 3G network.

In this data set, 2 different kind of features are included:

 - user-side information, e.g., age, gender, data plan and etc;
 - Video watching record of these users, each of  which associated with a download ratio and some detailed network condition metrics, say, download speed, RTT, and something similar.

Now, Given this data set, I want to predict the download ratio of each video, it is a regression problem, so I use *gradient boosting regression tree* as model and run 10-fold cross validation.

However, I have tried different model parameter configurations and even different models (linear regression, decision regress tree), the best root-mean-square error I can get is 0.3790, which is quite high, because if I don''t use any complex models and just use the mean value of known labels as prediction values, then I can still have an RMSE of 0.3890. There is not obvious difference.

For this problem, I have some questions:

1. Does this high error rate imply that the label in data set is unpredictable?

2. Apart from the feature problem, is there any other possibilities? If yes, how can I validate them?', 7867, '2015-01-20 03:32:11.447', 'c32fdfdf-81af-4e84-879f-153c5be2b736', 4901, 'add details', 8014, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Does high error rate in regression imply the data set is unpredictable?', 7867, '2015-01-20 03:32:11.447', 'c32fdfdf-81af-4e84-879f-153c5be2b736', 4901, 'add details', 8015, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the methods that are present to perform spatial co-location pattern mining in case of generating Bag of visual phrases in image classification?

On what basis the threshold and window size for finding the frequently co-located pairs are determined is there any pre defined way to do that?

Thanks in advance', 7873, '2015-01-20 12:50:36.823', '9f3b3270-6535-41d9-8ec1-5601c438503d', 4913, 8016, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Spatial Co-location pattern in Bag of Phrases generation for image', 7873, '2015-01-20 12:50:36.823', '9f3b3270-6535-41d9-8ec1-5601c438503d', 4913, 8017, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining>', 7873, '2015-01-20 12:50:36.823', '9f3b3270-6535-41d9-8ec1-5601c438503d', 4913, 8018, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Recently in a Machine Learning class from professor Oriol Pujol at UPC/Barcelona he described the most common algorithms, principles and concepts to use for a wide range of machine learning related task. Here I share them with and you and ask you: are there other better approaches solve these problems? Is any of these not correct (or even used anymore)? (I understand the question is broad but the information seemed worthy to share).

**How do I learn a simple Gaussian?**
Probability, random variables, distributions; estimation, convergence and asymptotics, confidence interval.

**How do I learn a mixture of Gaussians (MoG)?** Likelihood, Expectation-Maximization (EM); generalization, model selection, cross-validation; k-means, hidden markov models (HMM)

**How do I learn any density?** Parametric vs. non-Parametric estimation, Sobolev and other functional spaces; l   2 error; Kernel density estimation (KDE), optimal kernel, KDE theory

**How do I predict a continuous variable (regression)?** Linear regression, regularization, ridge regression, and LASSO; local linear regression; conditional density estimation.

**How do I predict a discrete variable (classification)?** Bayes classifier, naive Bayes, generative vs. discriminative; perceptron, weight decay, linear support vector machine; nearest neighbor classifier and theory

**Which loss function should I use?** Maximum likelihood estimation theory; l -2 estimation; Bayessian estimation; minimax and decision theory, Bayesianism vs frequentism

**Which model should I use?** AIC and BIC; Vapnik-Chervonenskis theory; cross-validation theory; bootstrapping; Probably Approximately Correct (PAC) theory; Hoeffding-derived bounds

**How can I learn fancier (combined) models?** Ensemble learning theory; boosting; bagging; stacking

**How can I learn fancier (nonlinear) models?** Generalized linear models, logistic regression; Kolmogorov theorem, generalized additive models; kernelization, reproducing kernel Hilbert spaces, non-linear SVM, Gaussian process regression

**How can I learn fancier (compositional) models?** Recursive models, decision trees, hierarchical clustering; neural networks, back propagation, deep belief networks; graphical models, mixtures of HMMs, conditional random fields, max-margin Markov networks; log-linear models; grammars

**How do I reduce or relate features?** Feature selection vs dimensionality reduction, wrapper methods for feature selection; causality vs correlation, partial correlation, Bayes net structure learning

**How do I create new features?** principal component analysis (PCA), independent component analysis (ICA), multidimensional scaling, manifold learning, supervised dimensionality reduction, metric learning

**How do I reduce or relate the data?** Clustering, bi-clustering, constrained clustering; association rules and market basket analysis; ranking/ordinal regression; link analysis; relational data

**How do I treat time series?** ARMA; Kalman filter and stat-space models, particle filter; functional data analysis; change-point detection; cross-validation for time series

**How do I treat non-ideal data?** covariate shift; class imbalance; missing data, irregularly sampled data, measurement errors; anomaly detection, robustness

**How do I optimize the parameters?** Unconstrained vs constrained/Convex optimization, derivative-free methods, first- and second-order methods, backfitting; natural gradient; bound optimization and EM

**How do I optimize linear functions?** computational linear algebra, matrix inversion for regression, singular value decomposition (SVD) for dimensionality reduction

**How do I optimize with constraints?** Convexity, Lagrange multipliers, Karush-Kuhn-Tucker conditions, interior point methods, SMO algorithm for SVM

**How do I evaluate deeply-nested sums?** Exact graphical model inference, variational bounds on sums, approximate graphical model inference, expectation propagation

**How do I evaluate large sums and searches?** Generalized N-body problems (GNP), hierarchical data structures, nearest neighbor search, fast multiple method; Monte Carlo integration, Markov Chain Monte Carlo, Monte Carlo SVD

**How do I treat even larger problems?** Parallel/distributed EM, parallel/distributed GNP; stochastic subgradient methods, online learning

**How do I apply all this in the real world?** Overview of the parts of the ML, choosing between the methods to use for each task, prior knowledge and assumptions; exploratory data analysis and information visualization; evaluation and interpretation, using confidence intervals and hypothesis test, ROC curves; where the research problems in ML are', 5143, '2015-01-20 15:27:38.160', '1d05c5c9-0333-46e6-9766-044c50d6e862', 4914, 8019, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When to use what - Machine Learning', 5143, '2015-01-20 15:27:38.160', '1d05c5c9-0333-46e6-9766-044c50d6e862', 4914, 8020, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><algorithms>', 5143, '2015-01-20 15:27:38.160', '1d05c5c9-0333-46e6-9766-044c50d6e862', 4914, 8021, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('An instance of supervised learning that identifies the category or categories which a new instance of dataset belongs.', 6496, '2015-01-20 23:20:11.470', '965ffa64-ed6c-4891-9352-c5c62017ba67', 4911, 'added 118 characters in body', 8022, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-20 23:20:11.470', '965ffa64-ed6c-4891-9352-c5c62017ba67', 4911, 'Proposed by 6496 approved by 21 edit id of 217', 8023, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('That''s a good list covering a lot.  I''ve used some of these methods since before anything was called machine learning, and I think you will see some of the methods you list coming in and out of use over time.  If a method has been out of favour for too long, it might be time to a revisit.  Some methods can obfuscate behind different names resulting from different fields of study.

One of the main areas I have used these methods is in mineral potential modelling, which is geospatial and to support that you could add some additional categories relating to spatial and oriented data methods.

Taking your broad question to specific fields will probably be where you find more examples of methods not in your comprehensive list.  For example, two methods I''ve seen in mineral potential have been backward stepwise regression and weights of evidence modelling.  I''m not a statistician; perhaps these would be considered covered in the list under linear regression and Bayesian methods.', 6646, '2015-01-21 03:06:39.243', '5c1608d3-8f52-44ad-b3d0-cb5af3f755f7', 4915, 8026, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I agree with @geogaffer. This is a very good list, indeed. However, I see some *issues* with this list as it is currently formulated. For example, one issue is that suggested solutions are of different **granularity levels** - some of them represent *approaches*, some - *methods*, some - *algorithms*, and some other - just *concepts* (in other words, *terms* within a topic''s domain terminology). In addition, - and I believe that this is much more important than the above - I think that it would be much valuable, if all those solutions in the list were arranged within a **unified thematic statistical framework**. This idea was inspired by reading an excellent book by Lisa Harlow "The essence of multivariate thinking". Hence, recently I''ve initiated a corresponding, albeit currently somewhat limited, [discussion](http://stats.stackexchange.com/q/131320/31372) on the StackExchange''s *Cross Validated* site. Don''t let the title confuse you - my implied intention and hope is for building a *unified framework*, as mentioned above.', 2452, '2015-01-21 07:50:53.953', 'e5864143-e31a-461d-a8a9-cd7215995e76', 4916, 8027, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to visualize goal achievment progress.
This is my first idea:

- use area chart to show progress in current metric
- use horizontal band to show the goal value
- colorize areas under/above the band into "positive" and "negative" colors

![enter image description here][1]
![enter image description here][2]


  [1]: http://i.stack.imgur.com/h7fUa.png
  [2]: http://i.stack.imgur.com/EIhmz.png


**Is this approach informative enough? Are there better choises?**

Additional info:

- charts made in Tableau
- two data sources: metric progress & goals', 97, '2015-01-21 09:15:54.750', '5aed2bd3-43eb-4640-92f1-f9459356a7d7', 4917, 8028, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Visualize performance, % of goal implementation', 97, '2015-01-21 09:15:54.750', '5aed2bd3-43eb-4640-92f1-f9459356a7d7', 4917, 8029, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization><marketing><tableau>', 97, '2015-01-21 09:15:54.750', '5aed2bd3-43eb-4640-92f1-f9459356a7d7', 4917, 8030, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Firstly I''m creating a model for dependent variable y in [R] (e.g. via lm or glm) on a training set dataframe based on historic input variables (the x''s).

I then want to be able to predict future y using this model. The complexity I have is that I have additional models for some of these input variables that I used in training (in training they were static values), and these input vars can have dependency on an external variable z.

I want to run through an optimization routine to minimize y with respect to only changing variable z. I''m looking to build a solution that scales well with complexity of the given functions (e.g. with hundreds of input variables in a glm). The predict function scales (esp. in terms of code complexity) brilliantly with complexity on static datasets - but I don''t think I can tap into this in this instance.

Bonus if this can support other machine learning models rather than just regression based models.', 7896, '2015-01-21 13:30:45.307', 'd7381e36-11bc-4ae1-b5c3-fc861205f5a8', 4918, 8031, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Optimization using existing GLM. When predicting new output, need to switch input vars for models', 7896, '2015-01-21 13:30:45.307', 'd7381e36-11bc-4ae1-b5c3-fc861205f5a8', 4918, 8032, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><predictive-modeling><optimization><glm>', 7896, '2015-01-21 13:30:45.307', 'd7381e36-11bc-4ae1-b5c3-fc861205f5a8', 4918, 8033, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('May I know is there any method to automatically choose the window size to identify spatial co-location pairs that appear frequently. And how to choose the optimal threshold automatically? ', 7873, '2015-01-21 13:48:26.363', 'ffc78b72-1c01-4eab-85a6-b09624cdb471', 4919, 8034, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Spatial Co-location: Choosing optimal window size', 7873, '2015-01-21 13:48:26.363', 'ffc78b72-1c01-4eab-85a6-b09624cdb471', 4919, 8035, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 7873, '2015-01-21 13:48:26.363', 'ffc78b72-1c01-4eab-85a6-b09624cdb471', 4919, 8036, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think your approach is a little backwards.

"What is the mean of a Gaussian distribution fitted to this data?" is never the problem statement, so "how do I fit a Gaussian?" is never the problem you actually want to solve.

The difference is more than semantic. Consider the question of "how do I construct new features?" If your goal is to develop an index, you might use some type of factor analysis. If your goal is to simply reduce the feature space before fitting a linear model, you might skip the step entirely and use elastic net regression instead.

A better approach would be to compile a list of *actual data analysis tasks you would like to be able to tackle*. Questions like:

> How do I predict whether customers will return to my shopping website?
>
> How do I learn how many "major" consumer shopping patterns there are, and what are they?
>
> How do I construct an index of "volatility" for different items in my online store?

Also your list right now includes an enormous amount of material; far too much to "review" and gain more than a surface-level understanding. Having an actual purpose in mind can help you sort out your priorities.', 1156, '2015-01-21 14:17:17.970', '73f9728f-2d99-448c-bd95-a3836101b55d', 4920, 8037, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('That''s a good list covering a lot.  I''ve used some of these methods since before anything was called machine learning, and I think you will see some of the methods you list coming in and out of use over time.  If a method has been out of favour for too long, it might be time for a revisit.  Some methods can obfuscate behind different names resulting from different fields of study.

One of the main areas I have used these methods is in mineral potential modelling, which is geospatial and to support that you could add some additional categories relating to spatial and oriented data methods.

Taking your broad question to specific fields will probably be where you find more examples of methods not in your comprehensive list.  For example, two methods I''ve seen in mineral potential have been backward stepwise regression and weights of evidence modelling.  I''m not a statistician; perhaps these would be considered covered in the list under linear regression and Bayesian methods.', 6646, '2015-01-21 21:52:52.590', '0d69f3f7-9999-4c78-93e8-7d238446b9ff', 4915, 'added 1 character in body', 8038, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am COMPLETELY new to the field of Data Science, mainly because every employer I have worked for, simply COULDN''T sell any customers anything that would use techniques learned in this field.

Of particular interest to me is machine learning/Predictive Analysis.

I have attempted many "test projects" myself, but I seem to NEED some sort of outside "catalyst" to tell me a specific goal, and a specific set of guidelines, when I am trying to learn something.
Otherwise, I tend to lose focus, and jump from one interesting topic to the next, without ever gaining any experience.


Thank you!!', 7909, '2015-01-22 00:52:51.710', '254c3ce1-c8ea-4a20-86aa-3b0f9fa9fa05', 4921, 8039, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Could someone please offer me some guidance on some kind of particular, SPECIFIC project that I could attemp, to "get my feet wet, so to speak"', 7909, '2015-01-22 00:52:51.710', '254c3ce1-c8ea-4a20-86aa-3b0f9fa9fa05', 4921, 8040, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><bigdata><predictive-modeling>', 7909, '2015-01-22 00:52:51.710', '254c3ce1-c8ea-4a20-86aa-3b0f9fa9fa05', 4921, 8041, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would suggest Kaggle learning projects - http://www.kaggle.com/competitions

Look for the ones in the 101 section that offer knowledge. There''s many pre-made solutions ready, which you can ingest and try variations of.

Also, I have bookmarked a [Comprehensive learning path  Data Science in Python][1], which among other things gives a few answers to your specific question.


  [1]: http://www.analyticsvidhya.com/blog/learning-path-data-science-python/', 587, '2015-01-22 10:44:37.240', '4ba89f19-4673-4345-ac3b-74143cb00e6e', 4922, 8042, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In SIFT feature extraction how the key points will be generated and how the features will be stored in the database. In image will the bag of visual words be images or text words?', 7873, '2015-01-22 13:36:24.967', '45e4b71d-7b99-4e91-a78f-31212256820e', 4923, 8043, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Bag of Words creation in image', 7873, '2015-01-22 13:36:24.967', '45e4b71d-7b99-4e91-a78f-31212256820e', 4923, 8044, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<feature-extraction>', 7873, '2015-01-22 13:36:24.967', '45e4b71d-7b99-4e91-a78f-31212256820e', 4923, 8045, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can use some public data sets to play around with.

The iris flower data set is great for classification problems. As an example you can try to classify the flower species with k-nearest neighbor model or a decision tree (CART) model.

You can find this and other sample machine learning datasets here: http://archive.ics.uci.edu/ml/datasets.html

This list identifies what type of machine learning tasks can be performed with each dataset.

', 6648, '2015-01-22 17:25:40.690', 'ceb28a82-7f38-4890-b740-6b355484fed4', 4924, 8046, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As there are numerous tools available for data science tasks, and it''s cumbersome to install everything and build up a perfect system. Is there a linux/Mac OS image with Python, R and other open-source data science tools installed available for people to use? Ideally an Ubuntu or a light weight OS with latest version of python , R (IDEs too), and other opensource data visualization tools installed will be ideal.  I haven''t come across one in my quick search on google. Please let me know if there are any or if someone of you have created one for yourself? I assume some universities might have their own VM images. Please share the links. I thank everyone of you out there for your active support in this regard. ', 7923, '2015-01-22 21:34:57.443', 'be8e63ff-470f-40e6-8acc-8c619599a85d', 4925, 8047, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('VM image for data science projects', 7923, '2015-01-22 21:34:57.443', 'be8e63ff-470f-40e6-8acc-8c619599a85d', 4925, 8048, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><python><visualization><data-visualization>', 7923, '2015-01-22 21:34:57.443', 'be8e63ff-470f-40e6-8acc-8c619599a85d', 4925, 8049, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is another choice which popular recently: docker(https://www.docker.com). Docker is a container and let you create/maintain a working environment very easily and fast.

 - install essential tools for data science in python
  - https://registry.hub.docker.com/u/ceshine/python-datascience/
 - use r language to do data science
  - https://github.com/rocker-org/rocker

Hope that would help you.', 1003, '2015-01-23 02:08:59.950', '75b7bc66-fdf3-44fd-b17a-c0920f124ce4', 4926, 8050, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('While *Docker* images are now more trendy, I personally find *Docker* technology not user-friendly, even for advanced users. If you are OK with using *non-local* VM images and can use *Amazon Web Services (AWS) EC2*, consider R-focused images for data science projects, pre-built by Louis Aslett. The images contain very recent, if not the latest, versions of *Ubuntu LTS*, *R* and *RStudio Server*. You can access them [here](http://www.louisaslett.com/RStudio_AMI).', 2452, '2015-01-23 02:26:23.773', '7d11008d-a074-400c-b229-fcee73f38d33', 4927, 8051, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('While *Docker* images are now more trendy, I personally find *Docker* technology not user-friendly, even for advanced users. If you are OK with using *non-local* VM images and can use *Amazon Web Services (AWS) EC2*, consider R-focused images for data science projects, pre-built by Louis Aslett. The images contain very recent, if not the latest, versions of *Ubuntu LTS*, *R* and *RStudio Server*. You can access them [here](http://www.louisaslett.com/RStudio_AMI).

Besides main components I''ve listed above, the images contain many useful data science tools built-in as well. For example, the images support LaTeX, ODBC, OpenGL, Git, optimized numeric libraries and more.', 2452, '2015-01-23 02:32:18.343', '403f04cc-c4ad-4ea0-8217-cf3c5b45eefe', 4927, 'Added information on secondary tools.', 8052, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to calculate key points:

1. Take your image $I(x,y)$ and convolve it using different Gaussians $G(x,y,k\sigma$) at different scales $k \sigma$ to obtain $L(x,y,k\sigma)$. This produces several versions of your image $I(x,y)$ with different degrees of blurring.

2. Separate your blurred images according to octaves (an octave is usually taken as $2\sigma$). Within a given octave, take the difference between adjacent blurred images $L(x,y,k_{i}\sigma)$ and  $L(x,y,k_{j}\sigma)$. This difference is called a Difference of Gaussians (DoG) $D(x,y,\sigma) = L(x,y,k_{i}\sigma) - L(x,y,k_{j}\sigma)$. At this point, an image will be helpful (source: [opencv](http://docs.opencv.org/master/doc/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html)):

![enter image description here][1]

As you can see, you have the blurred/convolved/filtered images on the left and the differences between adjacent images on the right.

3. Then, take one pixel on a DoG and compare it to its 8 neighbors in the same DoG, as well as to the 9 equivalent pixels of the DoG located in the next scale  and to the other 9 pixels in the previous DoG. In the drawing, this corresponds to the DoG''s located above and below. If this pixel is a local extrema, then you have a candidate keypoint. Keep in mind that candidates can be discarded based on other criteria.

Once you have these keypoints, you proceed to generate descriptors. For each keypoint, calculate an image gradient. As you should know, gradients tell you the direction of maximum rate of change. Therefore, you can construct a grid around each keypoint that is oriented according to the dominant gradient around that point. This grid has subregions (usually 16 subregions) and for each subregion, calculate an 8-bin histogram. Finally, concatenate every histogram you obtained from every subregion in your grid and that is the feature vector for that keypoint (actually, the full feature vector also includes the location and rotation angle.) A helpful illustration (from Solem''s book Programming Computer Vision with Python):

![enter image description here][2]

At this point, you have a feature vector with 132 values for each keypoint. As for your second question, I''m not sure about the best approach to store an array in a database. Maybe others can expand on this point. Of course, there are several options:

1. Create an array datatype to store your data
2. Simply use a VARCHAR field
3. Store it as a binary file
4. Use a database specifically design to handle arrays.

  [1]: http://i.stack.imgur.com/yLVAi.jpg
  [2]: http://i.stack.imgur.com/Ws829.png', 4621, '2015-01-23 06:10:37.417', '0d304032-f529-4f64-a5ae-c33b8f83ae46', 4928, 8053, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to calculate key points:

1. Take your image $I(x,y)$ and convolve it using a Gaussian $G(x,y,k\sigma$) at different scales $k \sigma$ to obtain $L(x,y,k\sigma)$. This produces several versions (one for each scale $k\sigma$) of your image $I(x,y)$ with different degrees of blurring.

2. Separate your blurred images according to octaves (an octave is usually taken as $2\sigma$). Within a given octave, take the difference between adjacent blurred images $L(x,y,k_{i}\sigma)$ and  $L(x,y,k_{j}\sigma)$. This difference is called a Difference of Gaussians (DoG) $D(x,y,\sigma) = L(x,y,k_{i}\sigma) - L(x,y,k_{j}\sigma)$. At this point, an image will be helpful (source: [opencv](http://docs.opencv.org/master/doc/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html)):

![enter image description here][1]

As you can see, you have the blurred/convolved/filtered images on the left and the differences between adjacent images on the right.

3. Then, take one pixel on a DoG and compare it to its 8 neighbors in the same DoG, as well as to the 9 equivalent pixels of the DoG located in the next scale  and to the other 9 pixels in the previous DoG. In the drawing, this corresponds to the DoG''s located above and below. If this pixel is a local extrema, then you have a candidate keypoint. Keep in mind that candidates can be discarded based on other criteria.

Once you have these keypoints, you proceed to generate descriptors. For each keypoint, calculate an image gradient. As you should know, gradients tell you the direction of maximum rate of change. Therefore, you can construct a grid around each keypoint that is oriented according to the dominant gradient around that point. This grid has subregions (usually 16 subregions) and for each subregion, calculate an 8-bin histogram. Finally, concatenate every histogram you obtained from every subregion in your grid and that is the feature vector for that keypoint (actually, the full feature vector also includes the location and rotation angle.) A helpful illustration (from Solem''s book Programming Computer Vision with Python):

![enter image description here][2]

At this point, you have a feature vector with 132 values for each keypoint. As for your second question, I''m not sure about the best approach to store an array in a database. Maybe others can expand on this point. Of course, there are several options:

1. Create an array datatype to store your data
2. Simply use a VARCHAR field
3. Store it as a binary file
4. Use a database specifically designed to handle arrays.

  [1]: http://i.stack.imgur.com/yLVAi.jpg
  [2]: http://i.stack.imgur.com/Ws829.png', 4621, '2015-01-23 06:15:40.773', '315afed6-b57b-45ba-b310-24f4ebd1e3ad', 4928, 'typo and clarification', 8054, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you are looking for a VM with a bunch of tools preinstalled, try the [Data Science Toolbox][1].


  [1]: http://datasciencetoolbox.org/', 21, '2015-01-23 10:22:36.327', '9240cc45-9121-4787-8e9c-4fde49edc68a', 4929, 8055, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-01-23 10:23:39.330', 'cffbfc88-2c0c-45fe-901f-bef9bb200c36', 4921, '104', 8056, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-01-23 10:24:13.397', '02c8dfa7-c2b9-427e-b403-bd9ed8dc710e', 4882, '103', 8057, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Did you try Cloudera''s QuickStart VM?:

* http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.html

I found it very easy to run it and it includes open source software such as [Mahout][1] and [Spark][2].


  [1]: http://mahout.apache.org/
  [2]: https://spark.apache.org/', 7919, '2015-01-23 14:00:52.367', '1eb4a8d8-9563-4e6d-a38b-408e7ef391e9', 4930, 8058, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Today i used this repository from https://github.com/sequenceiq/docker-spark and build it with docker. it is a docker image building spark based on hadoop image of the same owner. if you to use spark, it has a python api called pyspark http://spark.apache.org/docs/latest/api/python/', 7926, '2015-01-23 15:34:07.570', 'ddcd1e70-e64f-4e1f-a843-20127548f8f0', 4931, 8061, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am starting to learn how R works and would like to figure out the best way to determine the direction, and the magnitude of the directional change on a series of data points in a data frame.

Are there any good libraries out there or recommended approaches?

Thanks!', 7938, '2015-01-23 19:18:21.597', 'fe6cdb8f-a7b3-4d24-8923-739d3e63f321', 4932, 8062, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Direction of a line in R', 7938, '2015-01-23 19:18:21.597', 'fe6cdb8f-a7b3-4d24-8923-739d3e63f321', 4932, 8063, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r>', 7938, '2015-01-23 19:18:21.597', 'fe6cdb8f-a7b3-4d24-8923-739d3e63f321', 4932, 8064, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try the Local Polynomial Regression Fitting function [loess](https://stat.ethz.ch/R-manual/R-patched/library/stats/html/loess.html), which is part of the standard stats package that comes with R.', 7940, '2015-01-23 20:31:34.690', '48394342-2a41-476d-af7f-b49653885fb6', 4933, 8065, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What information should be shown on the charts are based on what the viewer wants. From your sample charts, the x axis indicates ''date''. For time serial analysis, people usually want to know the trending of the metrics and to predict the performance in the future.

Here are 2 suggestions:

 1. People may want to know how it will perform in the future. Add a trendline. Trendline could help people to predict the performance in the future.
    http://onlinehelp.tableausoftware.com/current/pro/online/mac/en-us/trendlines.html
 2. People may want to know how far away from goal. When people mouseover on the points on the metric line, you can try to show how far from current metric to the goal.

Try to understand what information people want to know and visualise them. Hope my answer could help you.', 1003, '2015-01-23 23:16:54.220', 'c1f570fc-ad0d-4f49-b961-06f7b4b8926e', 4934, 8066, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As an example, say the input is an array of numbers representing an audio snippet and the output is a transformed/filtered version of it.

What would be the proper name for that? Which are examples of algorithms for the job?

**EDIT:**
More specifically, I want to train audio source separation. The input is a mixed sound (spectrogram) and the output is the sound with some energy removed in certain frequencies. The function needs to recognize some pattern in the input and decide what to remove.', 7806, '2015-01-24 14:09:47.810', '129d573b-013d-4ae5-a6d4-88e48bd3aed4', 4879, 'edited body', 8068, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In the big data world there is lot of talk about implementing an "active archive". I see cloudera talk about it a lot.

The value prop is that you move the low value (and less used) data from EDW to Hadoop and then save on expensive storage of Hadoop.

So in my company say we keep 10 years of data on EDW and say we don''t use anything below 2 years very actively. So I move 8 years of data from EDW to Hadoop.

I can setup an Impala (or equivalent) product to query the 8 years of data as well.

But the problem is how do you order, sort a query which requires some data form EDW and some data from Impala?

because this kind of grouping, sorting ordering etc will have to be done in memory and the apps which queried the EDW were not written for operations and don''t have the capacity as well to sort, group and process so much of data.

So how are people implementing the Active Archive?', 6644, '2015-01-24 15:15:27.777', 'ecfad6cb-fdea-4cd9-9834-a2f0f3683d25', 4935, 8069, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Questions on "Active Archive"', 6644, '2015-01-24 15:15:27.777', 'ecfad6cb-fdea-4cd9-9834-a2f0f3683d25', 4935, 8070, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop>', 6644, '2015-01-24 15:15:27.777', 'ecfad6cb-fdea-4cd9-9834-a2f0f3683d25', 4935, 8071, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In the big data world there is lot of talk about implementing an "active archive". I see cloudera talk about it a lot.

The value prop is that you move the low value (and less used) data from EDW to Hadoop and then save on expensive EDW storage by using Hadoop.

So in my company say we keep 10 years of data on EDW and say we don''t use anything below 2 years very actively. So I move 8 years of data from EDW to Hadoop.

I can setup an Impala (or equivalent) product to query the 8 years of data as well.

But the problem is how do you order, sort a query which requires some data form EDW and some data from Impala?

because this kind of grouping, sorting ordering etc will have to be done in memory and the apps which queried the EDW were not written for operations and don''t have the capacity as well to sort, group and process so much of data.

So how are people implementing the Active Archive?', 6644, '2015-01-24 15:22:51.663', '66b50d03-a539-40a2-889d-abf122790eb1', 4935, 'added 10 characters in body', 8072, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What happens when we train a basic support vector machine (linear kernel and no soft-margin) on non-linearly separable data? The optimisation problem is not feasible, so what does the minimisation algorithm return?', 7944, '2015-01-24 15:52:28.727', 'e7bdb70e-ee03-461b-a7d0-c7a48c30aaae', 4936, 8073, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What happens when we train a linear SVM on non-linearly separable data?', 7944, '2015-01-24 15:52:28.727', 'e7bdb70e-ee03-461b-a7d0-c7a48c30aaae', 4936, 8074, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 7944, '2015-01-24 15:52:28.727', 'e7bdb70e-ee03-461b-a7d0-c7a48c30aaae', 4936, 8075, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think the idea is that you have *all* data in HDFS, and query it with Impala, and also keep *some* small amount of data in your data warehouse. That is, keep all 10 years in Hadoop, and also 2 years in an EDW. The cost of also having the 2 years in Hadoop is small.', 21, '2015-01-24 16:51:33.300', 'eb9604fe-9c13-4955-888e-0647a07ced6e', 4937, 8076, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To your question, I think basic support vector machine means hard-margin SVM. Let''s review what is hard-margin SVM first.

**What is hard-margin SVM**:
In shortly, we want to find a hyperplane with the largest margin which be able to separate all observations correctly in our training sample space.

**The optimisation problem in hard-margin SVM**: Now, let''s look at the above definition again and what is the optimisation problem which we need to solve.

 1. the largest margin hyperplane: We want **`max(margin)`**
 2. be able to separate all observations correctly: We need to optimise **`margin`** and also satisfy a constrain: **No in-sample errors**

Back to your question, since you mentions the training data set is non-linearly separable, **we can NOT find any hyperplane which satisfy "No in-sample errors"**. Normally, we solve SVM optimisation problem by Quadratic Programming, because it can do optimise with constrains. So, if you use Gradient Decent or other optimisation algorithms which can not satisfy the constrains of hard-margin SVM, you should still get a result. However, that''s not a hard-margin SVM hyperplane.

By the way, with non-linearly separable data, usually we choose

 - hard-margin SVM + feature transformation
 - use soft-margin SVM directly(In practical, soft-margin SVM usually get good results)

Thank you for your question, and hope I do answer your question.', 1003, '2015-01-25 01:29:53.470', '367eecaf-b0ff-4fe5-91c8-ede4914175ec', 4938, 8077, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('About your question, I think basic support vector machine means hard-margin SVM. At first, let''s review what is hard-margin SVM.

**What is hard-margin SVM**:
In shortly, we want to find a hyperplane with the largest margin which be able to separate all observations correctly in our training sample space.

**The optimisation problem in hard-margin SVM**: Now, let''s look at the above definition again and what is the optimisation problem which we need to solve.

 1. the largest margin hyperplane: We want **`max(margin)`**
 2. be able to separate all observations correctly: We need to optimise **`margin`** and also satisfy the constrain: **No in-sample errors**

Back to your question, since you mentioned the training data set is non-linearly separable, by using hard-margin SVM without feature transformations, **it''s impossible to find any hyperplane which satisfy "No in-sample errors"**. Normally, we solve SVM optimisation problem by Quadratic Programming, because it can do optimisation tasks with constrains. If you use Gradient Descent or other optimisation algorithms which without satisfying the constrains of hard-margin SVM, you should still get a result, but that''s not a hard-margin SVM hyperplane.

By the way, with non-linearly separable data, usually we choose

 - hard-margin SVM + feature transformations
 - use soft-margin SVM directly(In practical, soft-margin SVM usually get good results)

Thank you for your question, and hope I do answer your question.', 1003, '2015-01-25 08:01:49.653', '363330d7-f557-4940-8ee7-086804c03402', 4938, 'corrected some typos.', 8079, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Generally, I agree with @StephanKolassa''s comment to this question. Having said that, the following are some advice and resources to get you started. For the simplest approach consider [linear regression](http://en.wikipedia.org/wiki/Linear_regression), for which in `R` you don''t need any extra libraries, just use the basic `lm()` function: http://data.princeton.edu/R/linearModels.html. Also see [this page](http://www.ats.ucla.edu/stat/dae) for tutorials on different types of regression analysis for `R` and other languages/systems. You can also find [this document](http://cran.r-project.org/doc/manuals/R-intro.pdf) (p. 57-) and [this document](http://cran.r-project.org/doc/contrib/usingR.pdf) (p. 35-) useful.', 2452, '2015-01-25 09:05:42.910', 'b1d74c42-7301-4fbf-87a8-91e2e2cca311', 4939, 8080, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><svm>', 1003, '2015-01-25 09:39:27.340', 'b7c582a7-1393-4344-82a4-f971b871892c', 4936, 'Add ''svm'' to tag list.', 8081, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-25 09:39:27.340', 'b7c582a7-1393-4344-82a4-f971b871892c', 4936, 'Proposed by 1003 approved by 2452, 7944 edit id of 219', 8082, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Recently in a Machine Learning class from professor Oriol Pujol at UPC/Barcelona he described the most common algorithms, principles and concepts to use for a wide range of machine learning related task. Here I share them with you and ask you:

- are there other, better approaches to solve these problems?
- is any of these not correct (or even not used anymore)?

I understand the question is broad but the information seemed worthy to share.

---
**How do I learn a simple Gaussian?**
Probability, random variables, distributions; estimation, convergence and asymptotics, confidence interval.

**How do I learn a mixture of Gaussians (MoG)?** Likelihood, Expectation-Maximization (EM); generalization, model selection, cross-validation; k-means, hidden markov models (HMM)

**How do I learn any density?** Parametric vs. non-Parametric estimation, Sobolev and other functional spaces; l   2 error; Kernel density estimation (KDE), optimal kernel, KDE theory

**How do I predict a continuous variable (regression)?** Linear regression, regularization, ridge regression, and LASSO; local linear regression; conditional density estimation.

**How do I predict a discrete variable (classification)?** Bayes classifier, naive Bayes, generative vs. discriminative; perceptron, weight decay, linear support vector machine; nearest neighbor classifier and theory

**Which loss function should I use?** Maximum likelihood estimation theory; l -2 estimation; Bayessian estimation; minimax and decision theory, Bayesianism vs frequentism

**Which model should I use?** AIC and BIC; Vapnik-Chervonenskis theory; cross-validation theory; bootstrapping; Probably Approximately Correct (PAC) theory; Hoeffding-derived bounds

**How can I learn fancier (combined) models?** Ensemble learning theory; boosting; bagging; stacking

**How can I learn fancier (nonlinear) models?** Generalized linear models, logistic regression; Kolmogorov theorem, generalized additive models; kernelization, reproducing kernel Hilbert spaces, non-linear SVM, Gaussian process regression

**How can I learn fancier (compositional) models?** Recursive models, decision trees, hierarchical clustering; neural networks, back propagation, deep belief networks; graphical models, mixtures of HMMs, conditional random fields, max-margin Markov networks; log-linear models; grammars

**How do I reduce or relate features?** Feature selection vs dimensionality reduction, wrapper methods for feature selection; causality vs correlation, partial correlation, Bayes net structure learning

**How do I create new features?** principal component analysis (PCA), independent component analysis (ICA), multidimensional scaling, manifold learning, supervised dimensionality reduction, metric learning

**How do I reduce or relate the data?** Clustering, bi-clustering, constrained clustering; association rules and market basket analysis; ranking/ordinal regression; link analysis; relational data

**How do I treat time series?** ARMA; Kalman filter and stat-space models, particle filter; functional data analysis; change-point detection; cross-validation for time series

**How do I treat non-ideal data?** covariate shift; class imbalance; missing data, irregularly sampled data, measurement errors; anomaly detection, robustness

**How do I optimize the parameters?** Unconstrained vs constrained/Convex optimization, derivative-free methods, first- and second-order methods, backfitting; natural gradient; bound optimization and EM

**How do I optimize linear functions?** computational linear algebra, matrix inversion for regression, singular value decomposition (SVD) for dimensionality reduction

**How do I optimize with constraints?** Convexity, Lagrange multipliers, Karush-Kuhn-Tucker conditions, interior point methods, SMO algorithm for SVM

**How do I evaluate deeply-nested sums?** Exact graphical model inference, variational bounds on sums, approximate graphical model inference, expectation propagation

**How do I evaluate large sums and searches?** Generalized N-body problems (GNP), hierarchical data structures, nearest neighbor search, fast multiple method; Monte Carlo integration, Markov Chain Monte Carlo, Monte Carlo SVD

**How do I treat even larger problems?** Parallel/distributed EM, parallel/distributed GNP; stochastic subgradient methods, online learning

**How do I apply all this in the real world?** Overview of the parts of the ML, choosing between the methods to use for each task, prior knowledge and assumptions; exploratory data analysis and information visualization; evaluation and interpretation, using confidence intervals and hypothesis test, ROC curves; where the research problems in ML are', 1367, '2015-01-25 10:37:44.347', 'c024e556-e9ea-476a-a7a8-8ecf1a7b5099', 4914, 'Emphasize questions and separate them from list', 8083, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-25 10:37:44.347', 'c024e556-e9ea-476a-a7a8-8ecf1a7b5099', 4914, 'Proposed by 1367 approved by 2452, 21 edit id of 218', 8084, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to correlate the visual words/phrases and text in cross domain correlated graph? How to construct the cross domain correlated graph?

Reference:[Web multimedia object classification using cross domain correlation knowledge][1]


  [1]: http://users.cis.fiu.edu/~taoli/pub/Lu-06589200-TMM.pdf  pg: 1924', 7873, '2015-01-25 14:30:31.187', 'b6e75fda-474b-4430-b23b-cb1112b3247f', 4940, 8085, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Cross Domain correlated graph- Comparison of visual words/phrases with text words', 7873, '2015-01-25 14:30:31.187', 'b6e75fda-474b-4430-b23b-cb1112b3247f', 4940, 8086, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 7873, '2015-01-25 14:30:31.187', 'b6e75fda-474b-4430-b23b-cb1112b3247f', 4940, 8087, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Neo4j and Spark GraphX are meant for solving problem at different level and they are complimentary to each other.

Check out this tutorial to get an idea on how to combine the two:
http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html', 7950, '2015-01-25 14:43:21.033', '28b5327b-6fdc-449e-811d-68c5b4fb95e7', 4941, 8088, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Due to various [curses of dimensionality][1], the accuracy and speed of many of the common predictive techniques degrade on high dimensional data. What are some of the most useful techniques/tricks/heuristics that help deal with high-dimensional data effectively? For example,

 - Do certain statistical/modeling methods perform well on high-dimensional datasets?
 - Can we improve the performance of our predictive models on high-dimensional data by using certain (that define alternative notions of distance) or [kernels][3] (that define alternative notions of dot product)?
 - What are the most useful techniques of dimensionality reduction for high-dimensional data?

  [1]: http://en.wikipedia.org/wiki/Curse_of_dimensionality
  [3]: http://en.wikipedia.org/wiki/Kernel_method', 5203, '2015-01-25 22:52:23.437', '9a139bd6-f45d-4c18-bd80-8bef429eb6d2', 4942, 8089, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('High-dimensional data: What are useful techniques to know?', 5203, '2015-01-25 22:52:23.437', '9a139bd6-f45d-4c18-bd80-8bef429eb6d2', 4942, 8090, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dimensionality-reduction>', 5203, '2015-01-25 22:52:23.437', '9a139bd6-f45d-4c18-bd80-8bef429eb6d2', 4942, 8091, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><statistics><dimensionality-reduction>', 5203, '2015-01-25 22:59:05.897', 'c74f4b68-4187-4574-b375-84fa35292d74', 4942, 'edited tags', 8092, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How does varying the regularization parameter in an SVM change the decision boundary for a non-separable dataset? A visual answer and/or some commentary on the limiting behaviors (for large and small regularization) would be very helpful.', 5203, '2015-01-25 23:03:02.297', '5f386b08-0501-4627-a76b-c165b21ef352', 4943, 8093, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Intuition for the regularization parameter in SVM', 5203, '2015-01-25 23:03:02.297', '5f386b08-0501-4627-a76b-c165b21ef352', 4943, 8094, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<svm>', 5203, '2015-01-25 23:03:02.297', '5f386b08-0501-4627-a76b-c165b21ef352', 4943, 8095, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the most useful techniques for learning a binary classifier from a dataset with a high degree of imbalance (i.e., a dataset with the "target" class being much rarer than the "background" class)? For example,

 - Should one first down-sample the majority/background class to reduce its frequency and then readjust the probabilities reported by the learning algorithm? How should one do the readjustment?
 - Should one use different approaches for different learning algorithms, i.e., are there different techniques for dealing with imbalance in SVM, random forests, logistic regression, etc.?', 5203, '2015-01-25 23:09:22.863', 'f48fc7f6-300b-4fb4-b1f4-ae858aaf3ede', 4944, 8096, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to learn a classifier from a dataset with high imbalance', 5203, '2015-01-25 23:09:22.863', 'f48fc7f6-300b-4fb4-b1f4-ae858aaf3ede', 4944, 8097, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification>', 5203, '2015-01-25 23:09:22.863', 'f48fc7f6-300b-4fb4-b1f4-ae858aaf3ede', 4944, 8098, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The regularization parameter (lambda) serves as a degree of importance that is given to miss-classifications. SVM pose a quadratic optimization problem that looks for maximizing the margin between both classes and minimizing the amount of miss-classifications. However, for non-separable problems, in order to find a solution, the miss-classification constraint must be relaxed, and this is done by setting the mentioned "regularization".

So, intuitively, as lambda grows larger they less the wrongly classified examples are allowed (or the highest the price the pay in the loss function). Then when lambda tends to infinite the solution tends to the hard-margin (allow no miss-classification). When lambda tends to 0 (without being 0) the more the miss-classifications are allowed.

There is definitely a tradeoff between this two and normally smaller lambdas but not too small use to generalize well. Below three examples for linear SVM classification (binary).

![Linear SVM Lambda = 0.1][1]
![Linear SVM Lambda = 1][2]
![enter image description here][3]

For non-linear-kernel SVM the idea is the similar.
Given this, for higher values of lambda there is a higher possibility of overfitting, while for lower values of lambda there is higher possibilities of underfitting.

The images below show the behavior for RBF Kernel, letting the sigma parameter fixed on 1 and trying lambda = 0.01 and lambda = 10

![RBF Kernel SVM lambda = 0.01][4]
![RBF Kernel SVM lambda = 10][5]

You can say the first figure where lambda is lower is more "relaxed" than the second figure where data is intended to be fitted more precisely.


  [1]: http://i.stack.imgur.com/NbNCS.png
  [2]: http://i.stack.imgur.com/xanGh.png
  [3]: http://i.stack.imgur.com/R6fHT.png
  [4]: http://i.stack.imgur.com/8KIpI.png
  [5]: http://i.stack.imgur.com/6Vmqt.png', 5143, '2015-01-26 01:37:05.640', '9d110f26-0fad-4c95-87d7-f08ee3b21345', 4945, 8099, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is very **broad** question, which I think it''s impossible to cover *comprehensively* in a single answer. Therefore, I think that it would be more beneficial to provide some pointers to relevant answers and/or resources. This is exactly what I will do by providing the following information and thoughts of mine.

First of all, I should mention the excellent and comprehensive [tutorial on dimensionality reduction](http://research.microsoft.com/pubs/150728/FnT_dimensionReduction.pdf) by Burges (2009) from Microsoft Research. He touches on *high-dimensional aspects* of data frequently throughout the monograph. This work, referring to *dimensionality reduction* as *dimension reduction*, presents a theoretical introduction into **the problem**, suggests a **taxonomy** of dimensionality reduction methods, consisting of *projective methods* and *manifold modeling methods*, as well as provides an **overview** of multiple methods in each category.

The "**projective** pursuit" methods reviewed include *independent component analysis (ICA)*, *principal component analysis (PCA)* and its variations, such as *kernel PCA* and *probabilistic PCA*, *canonical correlation analysis (CCA)* and its *kernel CCA* variation, *linear discriminant analysis (LDA)*, *kernel dimension reduction (KDR)* and some others. The **manifold** methods reviewed include *multidimensional scaling (MDS)* and its *landmark MDS* variation, *Isomap*, *Locally Linear Embedding* and graphical methods, such as *Laplacian eigenmaps* and *spectral clustering*. I''m listing the most of the reviewed methods here in case, if the original publication is inaccessible for you, either *online* (link above), or *offline* (References).

There is a **caveat** for the term "comprehensive" that I''ve applied to the above-mentioned work. While it is indeed rather comprehensive, this is relative, as some of the approaches to dimensionality reduction are not discussed in the monograph, in particular, the ones, focused on *unobservable (latent) variables*. Some of them are mentioned, though, with references to another source - a book on dimensionality reduction.

Now, I will briefly cover several narrower aspects of the topic in question by referring to my relevant or related answers. In regard to *nearest neighbors (NN)-type approaches* to high-dimensional data, please see my answers [here](http://datascience.stackexchange.com/a/975/2452) (I especially recommend to check the paper #4 in my list). One of the effects of the *curse of dimensionality* is that high-dimensional data is frequently **sparse**. Considering this fact, I believe that my relevant answers [here](http://datascience.stackexchange.com/a/918/2452) and [here](http://stats.stackexchange.com/a/130665/31372) on **regression** and **PCA** for *sparse and high-dimensional data* might be helpful.

**References**

Burges, C. J. C. (2010). Dimension reduction: A guided tour. *Foundations and Trends® in Machine Learning, 2*(4), 275-365. doi:10.1561/2200000002', 2452, '2015-01-26 08:00:14.630', '2d84f2b5-61c0-4cef-8c4c-96c490008392', 4946, 8101, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A common strategy for dealing with imbalance is to penalize harder the missclassifications that select the class with higher frequency.

In a binary classification problem you could penalize by dividing 1/n where n is the number of examples of the opposite class.

See the following from Prof. Jordi Vitriá

![enter image description here][1]


  [1]: http://i.stack.imgur.com/fTosz.png', 5143, '2015-01-26 09:39:41.527', '1b5bccf6-6e07-4c07-8b31-ea5bf469a41b', 4947, 8102, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A common strategy for dealing with imbalance is to penalize harder the missclassifications that select the class with higher frequency.

In a binary classification problem you could penalize by dividing 1/n where n is the number of examples of the opposite class.

See the following from Prof. Jordi Vitriá

![enter image description here][1]

This is the loss function for structured output SVM.

The problem you mention is common in object recognition and object classification in images where much more background images are used than images containing the object. A stronger case happens with exemplar SVM''s where just a single image of the object is used.

  [1]: http://i.stack.imgur.com/fTosz.png', 5143, '2015-01-26 09:52:10.093', 'd7496322-cb3a-4025-b6d1-3daee4d66e9b', 4947, 'added 321 characters in body', 8103, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Some good answers have already been posted at this site:

- http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets

And on Stats SE:

- http://stats.stackexchange.com/questions/81111/classification-problem-using-imbalanced-dataset
- http://stats.stackexchange.com/questions/16050/how-to-handle-data-imbalance-in-classification
- http://stats.stackexchange.com/questions/60180/testing-classification-on-oversampled-imbalance-data?rq=1', 97, '2015-01-26 14:45:43.053', 'ce3a051a-a488-4161-ab69-d7c891aa1dc9', 4948, 8104, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I hope this is a question appropriate for SO.

The article in question: http://www.nytimes.com/2015/01/25/opinion/sunday/seth-stephens-davidowitz-searching-for-sex.html

As far as I can tell, the only publicly available data from Google Search is through their Trends API.  This [help page](https://support.google.com/trends/answer/4355164?hl=en&rd=1) states that

> The numbers on the graph reflect how many searches have been done for a particular term, relative to the total number of searches done on Google over time. They don''t represent absolute search volume numbers, because the data is normalized and presented on a scale from 0-100.

However in the article, the author reports (absolute) "average monthly searches".  The source is stated as:

> All monthly search numbers are approximate and derived from anonymous and aggregate web activity.

> Source: analysis of Google data by (author)

So, how did he get this "anonymous and aggregate web activity"?', 7961, '2015-01-26 15:45:51.317', '71c17969-18f2-431e-9faa-74f42d775c4d', 4949, 8105, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where did this NY Times op-ed get his Google Search data?', 7961, '2015-01-26 15:45:51.317', '71c17969-18f2-431e-9faa-74f42d775c4d', 4949, 8106, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><search><google>', 7961, '2015-01-26 15:45:51.317', '71c17969-18f2-431e-9faa-74f42d775c4d', 4949, 8107, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-01-26 16:43:10.387', 'cc86db1d-c541-42f9-9a91-4f68c886950d', 4914, '104', 8108, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><classification><unbalanced-classes>', 97, '2015-01-26 16:43:42.273', 'f19d6e19-677f-4959-b6d0-f5e389713bd6', 4944, 'Add more relevant tags', 8109, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-26 16:43:42.273', 'f19d6e19-677f-4959-b6d0-f5e389713bd6', 4944, 'Proposed by 97 approved by 2452, 21 edit id of 220', 8110, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am looking for a method to parse semi-structured textual data, i.e. data poorly formatted but usually having a visual structure of a matrix which may vary a lot in content and number of items in it, which may have headers or not, which may be interpreted sometimes column-wise or row-wise, and so on.

I have read about the WHISK information extraction paper : https://homes.cs.washington.edu/~soderlan/soderland_ml99.pdf

but unfortunately, it is not very detailed and I have not been able to find a real-system implementing it, or even snippets of code.

Has anybody have an idea where I can find such help? Or suggest an alternative approach which may be suited to my problem?

Thank you in advance for your reply!', 7966, '2015-01-26 17:45:02.203', '563102e5-ea06-409d-bf31-0cab0e19499e', 4950, 8111, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('semi-structured text parsing using machine learning', 7966, '2015-01-26 17:45:02.203', '563102e5-ea06-409d-bf31-0cab0e19499e', 4950, 8112, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<text-mining><information-retrieval><parsing>', 7966, '2015-01-26 17:45:02.203', '563102e5-ea06-409d-bf31-0cab0e19499e', 4950, 8113, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I came across a predictive model where the author used the probabilistic distribution value of the target variable as a feature in the feature set. For example:

Target variable looks like rock, paper, scissors

The author built a model for each player to guess which gesture would be played next. Look over 1000 games played the distribution may look like (20%, 10%, 70%). These numbers were then used as feature variables to predict the target variable.

Is that legitimate? That seems like cheating. You could get a model that is right 70% of the time by always betting on scissors.

', 3430, '2015-01-26 18:20:58.203', 'edbc140a-23bf-4340-aed5-0ee89053258a', 4951, 8114, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can distribution values of a target variable be used as features?', 3430, '2015-01-26 18:20:58.203', 'edbc140a-23bf-4340-aed5-0ee89053258a', 4951, 8115, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<methods>', 3430, '2015-01-26 18:20:58.203', 'edbc140a-23bf-4340-aed5-0ee89053258a', 4951, 8116, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There is nothing necessarily wrong with this.  If you have no better information, then using past performance (i.e., prior probabilities) can work pretty well, particularly when your classes are very unevenly distributed.

Example methods using class priors are [Gaussian Maximum Likelihood](http://en.wikipedia.org/wiki/Maximum_likelihood) classification and [Naïve Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier).', 964, '2015-01-26 19:24:51.783', '5f4ad71f-63b5-43af-a930-612d6ca90c3f', 4952, 8117, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would also suggest you to try an idea of ''Anomaly detection'' using Gaussian distribution. In some cases it works really good - especially if you have a VERY skewed classes (say, among a million of examples only 10-20 are ''1'' (in class) and all the rest a 0''s).  You may look up it in this video by prof. Andrew Ng.

http://www.youtube.com/watch?v=h5iVXB9mczo

Or in text:
http://www.holehouse.org/mlclass/15_Anomaly_Detection.html

Notice, that this is not a classification problem, it is not using a classification algorithm.', 7969, '2015-01-26 21:09:16.850', '1a7022d4-98a6-4f41-8147-ea311f096324', 4953, 8118, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Google AdWords.  That has absolute search volumes. ', 7972, '2015-01-27 01:37:13.737', '0e08254e-1aa0-46e3-bf6d-9cd5654fe659', 4954, 8119, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My company provides managed services to a lot of its clients. Our customers typically uses following monitoring tools to monitor their servers/webapps:

 1. OpsView
 2. Nagios
 4. Pingdom
 5. Custom shell scripts

Whenever any issue is found, an alert mail comes to our Ops team so that they act upon rectifying the issue.

As we manage thousands of servers, our Ops teams'' inbox is flooded with email alerts all the time. Even a single issue which has a cascading effect, can trigger 20-30 emails.

Now, what I want to do is to implement a system which will be able to extract important features out of an alert email - like server IP address, type of problem, severity of problem etc. and also classify the emails into proper category, like `CPU-Load-Customer1-Server2, MySQL-Replication-Customer2-DBServer3` etc. We will then have a pre-defined set of debugging steps for each category, in order to help the Ops team to rectify the problem faster. Also, the feature extractor will provide input data to the team for a problem.

So far I have been able to train *NaiveBayesClassifier* with supervised learning techniques i.e. labeled training data(cluster data), and able to classify new unseen emails into its proper cluster/category. As the emails are based on certain templates, the accuracy of the classifier is very high. But we also get alert emails from custom scripts, which may not follow the templates. So, instead of doing supervised learning, I want to try out unsupervised learning for the same. I am looking into *KMeans clustering*. But again the problem is, we won''t know the number of clusters beforehand. **So, which algorithm will be best for this use case?** Right now I am using Python''s TextBlob library for classification.

Also, for feature extraction out of an alert email, I am looking into NLTK (http://www.nltk.org/book/ch07.html) library. I tried it out, but it seems to work on proper English paragraphs/texts well, however, for alert emails, it extracted a lot of unnecessary features. **Is there already any existing solution for the same? If not, what will be the best way to implement the same? Which library, which algorithm?**

**PS: I am not a Data Scientist.**



**Sample emails:**

    PROBLEM: CRITICAL - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.10.0.100  Host Group Hierarchy: Opsview > Customer1  - BIG C > Customer1_PROD  State: CRITICAL  Date & Time: Sat Oct 4 07:02:06 UTC 2014    Additional Information:     CRITICAL - load average: 41.46, 40.69, 37.91
    RECOVERY: OK - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.1.1.100  Host Group Hierarchy: Opsview > Customer1  - BIG C > Customer1_PROD  State: OK  Date & Time: Sat Oct 4 07:52:05 UTC 2014    Additional Information:     OK - load average: 0.36, 0.23, 4.83
    PROBLEM: CRITICAL - Customer1_PROD - Customer1_PROD_SLAVE_DB_01 -  CPU Load Avg     Service: CPU Load Avg  Host: Customer1_PROD_SLAVE_DB_01  Alias: Customer1_PROD_SLAVE_DB_01  Address: 10.100.10.10  Host Group Hierarchy: Opsview > Customer1  - BIG C > Customer1_PROD  State: CRITICAL  Date & Time: Sat Oct 4 09:29:05 UTC 2014    Additional Information:     CRITICAL - load average: 29.59, 26.50, 18.49

**Classifier code:(format of csv - email, <disk/cpu/memory/mysql>)**

    from textblob import TextBlob
    from textblob.classifiers import NaiveBayesClassifier
    import csv
    train = []
    with open(''cpu.txt'', ''r'') as csvfile:
     reader = csv.reader(csvfile, delimiter='','', quotechar=''"'')
     for row in reader:
      tup = unicode(row[0], "ISO-8859-1"), row[1]
      train.append(tup)
    // this can be done in a loop, but for the time being let it be
    with open(''memory.txt'', ''r'') as csvfile:
     reader = csv.reader(csvfile, delimiter='','', quotechar=''"'')
     for row in reader:
      tup = unicode(row[0], "ISO-8859-1"), row[1]
      train.append(tup)

    with open(''disk.txt'', ''r'') as csvfile:
     reader = csv.reader(csvfile, delimiter='','', quotechar=''"'')
     for row in reader:
      tup = unicode(row[0], "ISO-8859-1"), row[1]
      train.append(tup)

    with open(''mysql.txt'', ''r'') as csvfile:
     reader = csv.reader(csvfile, delimiter='','', quotechar=''"'')
     for row in reader:
      tup = unicode(row[0], "ISO-8859-1"), row[1]
      train.append(tup)

    cl = NaiveBayesClassifier(train)
    cl.classify(email)

**Feature extractor code taken from:** https://gist.github.com/shlomibabluki/5539628


Please let me know if any more information is required here.

Thanks in advance.', 7979, '2015-01-27 10:31:10.233', '5bad43ea-14e6-440e-bfe4-ee777fd3f806', 4955, 8120, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to extract features and classify alert emails coming from OpsView/Nagios into proper category?', 7979, '2015-01-27 10:31:10.233', '5bad43ea-14e6-440e-bfe4-ee777fd3f806', 4955, 8121, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><clustering><feature-extraction>', 7979, '2015-01-27 10:31:10.233', '5bad43ea-14e6-440e-bfe4-ee777fd3f806', 4955, 8122, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to extract features and classify alert emails coming from monitoring tools into proper category?', 7979, '2015-01-27 10:40:27.297', '6134b0dd-9d92-475e-8564-b6b219ee4802', 4955, 'edited title', 8123, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('To add to a possibly never ending list:

as mentioned by cyndd, there is [Wikidata][1],

and for curated structured knowledge, [Wolfram Alpha][2].


  [1]: http://www.wikidata.org/wiki/Wikidata:Main_Page
  [2]: http://www.wolframalpha.com/', 7980, '2015-01-27 11:15:04.250', 'fb5756f0-9774-4c99-ab3c-174e4fa92abf', 4956, 8124, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a data set of video watching records in a 3G network. In this data set, 2 different kind of features are included:

 - user-side information, e.g., age, gender, data plan and etc;
 - Video watching records of these users, each of  which associated with a download ratio and some detailed network condition metrics, say, download speed, RTT, and something similar.

Under the scenario of internet streaming, a video is divided into several chunks and downloaded to end device one by one, so we have download ratio = download bytes / file size in bytes

Now, Given this data set, I want to predict the download ratio of each video.

Since it is a regression problem, so I use *gradient boosting regression tree* as model and run 10-fold cross validation.

However, I have tried different model parameter configurations and even different models (linear regression, decision regress tree), the best root-mean-square error I can get is 0.3790, which is quite high, because if I don''t use any complex models and just use the mean value of known labels as prediction values, then I can still have an RMSE of 0.3890. There is not obvious difference.

For this problem, I have some questions:

1. Does this high error rate imply that the label in data set is unpredictable?

2. Apart from the feature problem, is there any other possibilities? If yes, how can I validate them?', 7867, '2015-01-27 11:58:33.430', '9735c145-2ad7-4974-bcab-2fb20bcd560f', 4901, 'add more explanation about context', 8125, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Do you know of any machine learning add-ins that I could use within Excel? For example I would like to be able to select a range of data and use that for training purposes and then use another sheet for getting the results of different learning algorithms.', 7982, '2015-01-27 15:13:09.157', '195ad8db-88b6-4eda-b5b0-dad1a0eaa4ea', 4957, 8126, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine learning toolkit for Excel', 7982, '2015-01-27 15:13:09.157', '195ad8db-88b6-4eda-b5b0-dad1a0eaa4ea', 4957, 8127, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork>', 7982, '2015-01-27 15:13:09.157', '195ad8db-88b6-4eda-b5b0-dad1a0eaa4ea', 4957, 8128, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I hope this is a question appropriate for SO.

The article in question: http://www.nytimes.com/2015/01/25/opinion/sunday/seth-stephens-davidowitz-searching-for-sex.html

As far as I can tell, the only publicly available data from Google Search is through their Trends API.  The help page states that

> The numbers on the graph reflect how many searches have been done for a particular term, relative to the total number of searches done on Google over time. They don''t represent absolute search volume numbers, because the data is normalized and presented on a scale from 0-100.

However in the article, the author reports (absolute) "average monthly searches".  The source is stated as:

> All monthly search numbers are approximate and derived from anonymous and aggregate web activity.

> Source: analysis of Google data by (author)

So, how did he get this "anonymous and aggregate web activity"?

EDIT: Good to hear from the author.  As he said, it appears to be possible through the Google Adwords [Keyword Planner](https://support.google.com/adwords/answer/2999770?hl=en).  Specifically, the "Search for new keyword and ad group ideas" option.

I am unable to replicate his results at the moment, but I imagine it is just a matter of picking the correct seed words.  Thanks!', 7961, '2015-01-27 18:23:45.747', 'f1c7a33a-c8db-486b-887e-09e938f0cf5c', 4949, 'added 323 characters in body', 8129, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I hope this is a question appropriate for SO.

The article in question: http://www.nytimes.com/2015/01/25/opinion/sunday/seth-stephens-davidowitz-searching-for-sex.html

As far as I can tell, the only publicly available data from Google Search is through their Trends API.  The help page states that

> The numbers on the graph reflect how many searches have been done for a particular term, relative to the total number of searches done on Google over time. They don''t represent absolute search volume numbers, because the data is normalized and presented on a scale from 0-100.

However in the article, the author reports (absolute) "average monthly searches".  The source is stated as:

> All monthly search numbers are approximate and derived from anonymous and aggregate web activity.

> Source: analysis of Google data by (author)

So, how did he get this "anonymous and aggregate web activity"?

EDIT: Good to hear from the author.  As he said, it appears to be possible through the Google Adwords [Keyword Planner](https://support.google.com/adwords/answer/2999770?hl=en).  Specifically, the "Search for new keyword and ad group ideas" option.

I am unable to get Google to actually generate his list, but I imagine it is just a matter of picking the correct seed words and settings.  I get the same order-of-magnitude results when I directly search his terms ("sexless marriage", "unhappy marriage", etc).

Another option for generating his list would be to search against a dictionary of negative terms ("<negative adjective> marriage") and then do some sorting and filtering.  You could probably automate this through Google''s API.

Anyway, thanks!', 7961, '2015-01-27 18:35:17.840', 'c09eedfa-10c5-4209-a583-cf873bcb6c97', 4949, 'added 383 characters in body', 8130, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Recently I am working at a similar analysis. I wrote some functions to test any possible combinations between variables, however it is specifically used for my own data set which definitely is different from your one.

This is a fairly small job so I can not say any package dealing with such tests. And you have already worked out some combinations. Just keep going for a ideal function, maybe will be done in a couple of days.', 7989, '2015-01-27 22:01:26.237', 'b644f44d-590d-44a1-8d8d-34a4cbeeaf04', 4958, 8131, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Recently I am working at a similar analysis. I wrote some functions to test any possible combinations between variables, however it is specifically used for my own data set which definitely is different from your one.

This is a fairly small job so I can not say any package dealing with such tests. And you have already worked out some combinations. Just keep going for a ideal function, maybe will be done in a couple of days.


I add a link here, which partially answers your question and code is included: http://stats.stackexchange.com/questions/4040/r-compute-correlation-by-group ', 7989, '2015-01-28 03:16:42.000', 'e83a8d4b-ba3a-4a57-a6e0-8f3fe5375cb0', 4958, 'added 162 characters in body', 8132, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The idea you have in mind is called "feature selection" or "attribute selection". The fact that you have a categorical dependent variable and continuous independent variables is mostly irrelevant because you''re expected to use an algorithm or statistical method that is suitable for your requirements.

As for feature selection methods, there are several options:

1. Find the subset of features that achieves better performance (usually in cross validation)

2. Find the subset of features that correlates highly with the target variable and low with each other (although other criteria can be used)

3. Use an algorithm that includes a built-in feature selection mechanism (e.g. decision trees, hierarchical bayesian methods)

Furthermore, there are several methods aimed at obtaining a good compromise between a thorough search and a reasonable time execution (e.g. best first, steepest ascent search, etc)

This [question](http://stats.stackexchange.com/questions/56092/feature-selection-packages-in-r) in particular provides very good suggestions for R packages. ', 4621, '2015-01-28 03:55:58.367', 'b531737e-04d0-471e-a99d-8e6c2131fbde', 4959, 8133, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Weka](http://www.cs.waikato.ac.nz/ml/weka/) can import CSV files, and allows you to choose which columns and rows you want to use in your analysis.  It''s not an "add-in" for Excel per-se, but it might work for you.', 7961, '2015-01-28 04:34:30.777', 'f4a66e2f-4ac2-47ef-b969-c58a3a9d0714', 4960, 8134, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First of all, let me tell you that Excel shouldn''t be used for machine learning or any data analysis complicated enough that you wouldn''t be comfortable doing it on paper. Why? [Here](http://lemire.me/blog/archives/2014/05/23/you-shouldnt-use-a-spreadsheet-for-important-work-i-mean-it/) is a small blog post that explains why.

Now, if you really really want to do heavy calculations without exporting your data, I suggest using [xlwings](http://xlwings.org/). Basically, this allows two-way communication between Excel and Python. Watch the video in the homepage for a quick introduction. In this way, you would be able to use numpy, pandas and scikit-learn (or other machine learning library that you may prefer) without exporting your data first.', 4621, '2015-01-28 04:42:19.780', '60f4666c-e2f2-43d4-ad6f-b30f6ee47e66', 4961, 8135, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Nobody does serious machine learning in Excel; that''s not what it''s for. Fortunately, you can directly import Excel files into better platforms like python. In particular, there''s a great package called `pandas`, which makes work very pleasant. [Here''s a demo](https://www.youtube.com/watch?v=_JZFSFR6Yeo).', 381, '2015-01-28 05:53:28.083', '9f2b8581-c6a4-4a2e-93f1-1f2fe68fcd4d', 4962, 8136, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I found that Apache-Spark very powerful in Big-Data processing. but I want to know about Dryad (Microsoft) benefits. Is there any advantage for this framework than Spark?
Why we must use Dryad instead of Spark?

Thanks', 7977, '2015-01-28 05:57:56.090', '75dc0502-7813-4909-8edc-c723e59a9c35', 4963, 8137, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is advantage of using Dryad istead of Spark?', 7977, '2015-01-28 05:57:56.090', '75dc0502-7813-4909-8edc-c723e59a9c35', 4963, 8138, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 7977, '2015-01-28 05:57:56.090', '75dc0502-7813-4909-8edc-c723e59a9c35', 4963, 8139, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Dryad is an academic project, whereas Spark is widely deployed in production, and now has a company behind it for support. Just focus on Spark.', 381, '2015-01-28 06:41:28.650', '948ceb4b-7568-42d9-8803-419064ecc1d1', 4964, 8140, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would suggest to consider using *latent variable modeling (LVM)* or similar *structural equation modeling (SEM)* as an approach to this problem. Using this approach is based on recognizing and analyzing **latent variables** - constructs (*factors*), measured not directly, but through sets of measured variables (*indicators*). Note that a closely related term *latent feature* is frequently used within the *machine learning* domain. It seems to me that latent variables resemble what you call "combinations/subsets/segments of the IVs".

By hypothesizing - usually, based on theory or domain knowledge - the **latent structure** of factors, LVM or SEM are able to automatically confirm or decline those hypotheses. This is done by using a combination of *exploratory factor analysis (EFA)* and *confirmatory factor analysis (CFA)* (see [my answer]() ). While EFA is frequently performed independently (and maybe that''s enough for your purposes), doing it along with CFA represents a large part of LVM/SEM methodology, which is usually completed by performing *path analysis*, which is concerned about relationships between latent variables.

The `R` *ecosystem* offers a variety of **packages** for performing LVM/SEM in its entirety or for performing EFA, CFA and path analysis. The most popular ones for EFA are `psych`, `GPArotation` and `Hmisc`. The most popular packages for CFA, path analysis and LVM are `sem` (the first R package for SEM), `lavaan`, `OpenMx`, `semPLS`, `plspm`. Various supplementary SEM-focused packages are [also available](http://pairach.com/2011/08/13/r-packages-for-structural-equation-model).', 2452, '2015-01-28 09:24:14.097', 'a6efc381-078e-4beb-97f8-14bce5f5a8ed', 4965, 8141, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As far as I know, currently there not that many projects and products that allow to perform serious *machine learning (ML)* work from within Excel. However, the situation seems to be changing rapidly due to active Microsoft''s efforts in popularizing its ML cloud platform *Azure ML* (along with *ML Studio*). The [recent acquisition](http://blogs.microsoft.com/blog/2015/01/23/microsoft-acquire-revolution-analytics-help-customers-find-big-data-value-advanced-statistical-analysis) of R-focused company Revolution Analytics by Microsoft (which appears to me as more of *acqui-hiring* to a large extent) is an example of the company''s aggressive data science **market strategy**.

In regard to *ML toolkits for Excel*, as a confirmation that we should expect most Excel-enabled ML projects and products to be **Azure ML-focused**, consider the following two *projects* (the latter is an open source):

 - **Excel DataScope** (Microsoft Research): http://research.microsoft.com/en-us/projects/exceldatascope
 - **Azure ML Excel Add-In** (seems to be Microsoft sponsored): https://azuremlexcel.codeplex.com', 2452, '2015-01-28 10:33:38.880', '01661756-1317-40ce-a096-15653a68b9bb', 4966, 8142, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Recently in a Machine Learning class from professor Oriol Pujol at UPC/Barcelona he described the most common algorithms, principles and concepts to use for a wide range of machine learning related task. Here I share them with you and ask you:

- is there any other comprehensive list as this one published in StackExchange, where machine learning algorithms are related to specific problems?

---
**How do I learn a simple Gaussian?**
Probability, random variables, distributions; estimation, convergence and asymptotics, confidence interval.

**How do I learn a mixture of Gaussians (MoG)?** Likelihood, Expectation-Maximization (EM); generalization, model selection, cross-validation; k-means, hidden markov models (HMM)

**How do I learn any density?** Parametric vs. non-Parametric estimation, Sobolev and other functional spaces; l   2 error; Kernel density estimation (KDE), optimal kernel, KDE theory

**How do I predict a continuous variable (regression)?** Linear regression, regularization, ridge regression, and LASSO; local linear regression; conditional density estimation.

**How do I predict a discrete variable (classification)?** Bayes classifier, naive Bayes, generative vs. discriminative; perceptron, weight decay, linear support vector machine; nearest neighbor classifier and theory

**Which loss function should I use?** Maximum likelihood estimation theory; l -2 estimation; Bayessian estimation; minimax and decision theory, Bayesianism vs frequentism

**Which model should I use?** AIC and BIC; Vapnik-Chervonenskis theory; cross-validation theory; bootstrapping; Probably Approximately Correct (PAC) theory; Hoeffding-derived bounds

**How can I learn fancier (combined) models?** Ensemble learning theory; boosting; bagging; stacking

**How can I learn fancier (nonlinear) models?** Generalized linear models, logistic regression; Kolmogorov theorem, generalized additive models; kernelization, reproducing kernel Hilbert spaces, non-linear SVM, Gaussian process regression

**How can I learn fancier (compositional) models?** Recursive models, decision trees, hierarchical clustering; neural networks, back propagation, deep belief networks; graphical models, mixtures of HMMs, conditional random fields, max-margin Markov networks; log-linear models; grammars

**How do I reduce or relate features?** Feature selection vs dimensionality reduction, wrapper methods for feature selection; causality vs correlation, partial correlation, Bayes net structure learning

**How do I create new features?** principal component analysis (PCA), independent component analysis (ICA), multidimensional scaling, manifold learning, supervised dimensionality reduction, metric learning

**How do I reduce or relate the data?** Clustering, bi-clustering, constrained clustering; association rules and market basket analysis; ranking/ordinal regression; link analysis; relational data

**How do I treat time series?** ARMA; Kalman filter and stat-space models, particle filter; functional data analysis; change-point detection; cross-validation for time series

**How do I treat non-ideal data?** covariate shift; class imbalance; missing data, irregularly sampled data, measurement errors; anomaly detection, robustness

**How do I optimize the parameters?** Unconstrained vs constrained/Convex optimization, derivative-free methods, first- and second-order methods, backfitting; natural gradient; bound optimization and EM

**How do I optimize linear functions?** computational linear algebra, matrix inversion for regression, singular value decomposition (SVD) for dimensionality reduction

**How do I optimize with constraints?** Convexity, Lagrange multipliers, Karush-Kuhn-Tucker conditions, interior point methods, SMO algorithm for SVM

**How do I evaluate deeply-nested sums?** Exact graphical model inference, variational bounds on sums, approximate graphical model inference, expectation propagation

**How do I evaluate large sums and searches?** Generalized N-body problems (GNP), hierarchical data structures, nearest neighbor search, fast multiple method; Monte Carlo integration, Markov Chain Monte Carlo, Monte Carlo SVD

**How do I treat even larger problems?** Parallel/distributed EM, parallel/distributed GNP; stochastic subgradient methods, online learning

**How do I apply all this in the real world?** Overview of the parts of the ML, choosing between the methods to use for each task, prior knowledge and assumptions; exploratory data analysis and information visualization; evaluation and interpretation, using confidence intervals and hypothesis test, ROC curves; where the research problems in ML are', 5143, '2015-01-28 12:38:53.280', 'ad103089-df21-4424-bb9d-3f3da6f41700', 4914, 'deleted 56 characters in body', 8143, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The regularization parameter (lambda) serves as a degree of importance that is given to miss-classifications. SVM pose a quadratic optimization problem that looks for maximizing the margin between both classes and minimizing the amount of miss-classifications. However, for non-separable problems, in order to find a solution, the miss-classification constraint must be relaxed, and this is done by setting the mentioned "regularization".

So, intuitively, as lambda grows larger they less the wrongly classified examples are allowed (or the highest the price the pay in the loss function). Then when lambda tends to infinite the solution tends to the hard-margin (allow no miss-classification). When lambda tends to 0 (without being 0) the more the miss-classifications are allowed.

There is definitely a tradeoff between this two and normally smaller lambdas but not too small use to generalize well. Below three examples for linear SVM classification (binary).

![Linear SVM Lambda = 0.1][1]
![Linear SVM Lambda = 1][2]
![enter image description here][3]

For non-linear-kernel SVM the idea is the similar.
Given this, for higher values of lambda there is a higher possibility of overfitting, while for lower values of lambda there is higher possibilities of underfitting.

The images below show the behavior for RBF Kernel, letting the sigma parameter fixed on 1 and trying lambda = 0.01 and lambda = 10

![RBF Kernel SVM lambda = 0.01][4]
![RBF Kernel SVM lambda = 10][5]

You can say the first figure where lambda is lower is more "relaxed" than the second figure where data is intended to be fitted more precisely.

(Slides from Prof. Oriol Pujol. Universitat de Barcelona)


  [1]: http://i.stack.imgur.com/NbNCS.png
  [2]: http://i.stack.imgur.com/xanGh.png
  [3]: http://i.stack.imgur.com/R6fHT.png
  [4]: http://i.stack.imgur.com/8KIpI.png
  [5]: http://i.stack.imgur.com/6Vmqt.png', 5143, '2015-01-28 12:40:09.577', 'c8bf8a8a-07ac-4b1e-871a-666a4c62ab1a', 4945, 'added 61 characters in body', 8144, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I came across an SVM predictive model where the author used the probabilistic distribution value of the target variable as a feature in the feature set. For example:

Target variable looks like rock, paper, scissors

The author built a model for each player to guess which gesture would be played next. Look over 1000 games played the distribution may look like (20%, 10%, 70%). These numbers were then used as feature variables to predict the target variable.

Is that legitimate? That seems like cheating. You could get a model that is right 70% of the time by always betting on scissors.

Even if the model is legitimate for now predicting the future, can you really say it has 96.7% accuracy?

', 3430, '2015-01-28 12:45:46.390', '98afb5e0-444c-4efe-8a2a-6ac35332dcac', 4951, 'elaboration for clarity sake', 8145, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Let''s say I''m trying to predict a person''s electricity consumption, using the time of day as a predictor (hours 00-23), and further assume I have a hefty but finite amount of historical measurements.

Now, I''m trying to set up a linear model akin to

power_used = hr_of_day * alpha_hr + temperature * beta

Problem: using the hr_of_day as a numerical value is a very bad idea for many reasons, the fact that 23 and 0 are actually quite close values is one problem that can be solved with a simple transformation [1]. The fact that electrical consumption is often bi-modal is another problem which isn''t solved by a simple transformation.

A possible solution that works rather well is to treat the time of day as a categorical variable. That does the trick, but it suffers from a significant drawback in that there''s no information sharing between neighbouring hours.

So what I''m asking is this: does anyone know of a "soft" version of categorical values? I''m suggesting something quite loosely defined: Ideally I would have some parameter alpha that reduces the regression to numerical regression where alpha = 1 and reduces to categorical regression where alpha = 0, and behaves "in between" if it''s some other number.

Right now the only answer I can think of is to alter the weights in the regression in such a way that they tend towards zero the further away the quasi-categorical value is from the desired value. Surely there are other approaches?


[1]
introduce the hour variable as two new variables: Cos(time_of_day/24) and Sin(time_of_day/24)
', 7999, '2015-01-28 13:42:15.430', '3197758a-73d0-48f2-ab2b-4391e161872c', 4967, 8146, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Quasi-categorical variables - any ideas?', 7999, '2015-01-28 13:42:15.430', '3197758a-73d0-48f2-ab2b-4391e161872c', 4967, 8147, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<regression><categorical-data>', 7999, '2015-01-28 13:42:15.430', '3197758a-73d0-48f2-ab2b-4391e161872c', 4967, 8148, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Recently in a Machine Learning class from professor Oriol Pujol at UPC/Barcelona he described the most common algorithms, principles and concepts to use for a wide range of machine learning related task. Here I share them with you and ask you:

- is there any comprehensive framework matching tasks with approaches or methods related to different types of machine learning related problems?

---
**How do I learn a simple Gaussian?**
Probability, random variables, distributions; estimation, convergence and asymptotics, confidence interval.

**How do I learn a mixture of Gaussians (MoG)?** Likelihood, Expectation-Maximization (EM); generalization, model selection, cross-validation; k-means, hidden markov models (HMM)

**How do I learn any density?** Parametric vs. non-Parametric estimation, Sobolev and other functional spaces; l   2 error; Kernel density estimation (KDE), optimal kernel, KDE theory

**How do I predict a continuous variable (regression)?** Linear regression, regularization, ridge regression, and LASSO; local linear regression; conditional density estimation.

**How do I predict a discrete variable (classification)?** Bayes classifier, naive Bayes, generative vs. discriminative; perceptron, weight decay, linear support vector machine; nearest neighbor classifier and theory

**Which loss function should I use?** Maximum likelihood estimation theory; l -2 estimation; Bayessian estimation; minimax and decision theory, Bayesianism vs frequentism

**Which model should I use?** AIC and BIC; Vapnik-Chervonenskis theory; cross-validation theory; bootstrapping; Probably Approximately Correct (PAC) theory; Hoeffding-derived bounds

**How can I learn fancier (combined) models?** Ensemble learning theory; boosting; bagging; stacking

**How can I learn fancier (nonlinear) models?** Generalized linear models, logistic regression; Kolmogorov theorem, generalized additive models; kernelization, reproducing kernel Hilbert spaces, non-linear SVM, Gaussian process regression

**How can I learn fancier (compositional) models?** Recursive models, decision trees, hierarchical clustering; neural networks, back propagation, deep belief networks; graphical models, mixtures of HMMs, conditional random fields, max-margin Markov networks; log-linear models; grammars

**How do I reduce or relate features?** Feature selection vs dimensionality reduction, wrapper methods for feature selection; causality vs correlation, partial correlation, Bayes net structure learning

**How do I create new features?** principal component analysis (PCA), independent component analysis (ICA), multidimensional scaling, manifold learning, supervised dimensionality reduction, metric learning

**How do I reduce or relate the data?** Clustering, bi-clustering, constrained clustering; association rules and market basket analysis; ranking/ordinal regression; link analysis; relational data

**How do I treat time series?** ARMA; Kalman filter and stat-space models, particle filter; functional data analysis; change-point detection; cross-validation for time series

**How do I treat non-ideal data?** covariate shift; class imbalance; missing data, irregularly sampled data, measurement errors; anomaly detection, robustness

**How do I optimize the parameters?** Unconstrained vs constrained/Convex optimization, derivative-free methods, first- and second-order methods, backfitting; natural gradient; bound optimization and EM

**How do I optimize linear functions?** computational linear algebra, matrix inversion for regression, singular value decomposition (SVD) for dimensionality reduction

**How do I optimize with constraints?** Convexity, Lagrange multipliers, Karush-Kuhn-Tucker conditions, interior point methods, SMO algorithm for SVM

**How do I evaluate deeply-nested sums?** Exact graphical model inference, variational bounds on sums, approximate graphical model inference, expectation propagation

**How do I evaluate large sums and searches?** Generalized N-body problems (GNP), hierarchical data structures, nearest neighbor search, fast multiple method; Monte Carlo integration, Markov Chain Monte Carlo, Monte Carlo SVD

**How do I treat even larger problems?** Parallel/distributed EM, parallel/distributed GNP; stochastic subgradient methods, online learning

**How do I apply all this in the real world?** Overview of the parts of the ML, choosing between the methods to use for each task, prior knowledge and assumptions; exploratory data analysis and information visualization; evaluation and interpretation, using confidence intervals and hypothesis test, ROC curves; where the research problems in ML are', 5143, '2015-01-28 14:23:32.613', '2c92195c-1b1c-4f40-b06b-d2ff0b6c58ec', 4914, 'deleted 2 characters in body', 8149, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('First of all, let me tell you that Excel shouldn''t be used for machine learning or any data analysis complicated enough that you wouldn''t be comfortable doing it on paper. Why? Here is a list of resources to tell you why:

1. [You shouldnt use a spreadsheet for important work (I mean it)](http://lemire.me/blog/archives/2014/05/23/you-shouldnt-use-a-spreadsheet-for-important-work-i-mean-it/)
2. [Destroy Your Data Using Excel With This One Weird Trick!](http://randyzwitch.com/excel-destroys-data/)
3. [Using Excel for Statistical Data Analysis - Caveats](http://people.umass.edu/evagold/excel.html)
4. [Problems with Excel](http://biostat.mc.vanderbilt.edu/wiki/Main/ExcelProblems)
5. [Spreadsheet Addiction](http://www.burns-stat.com/documents/tutorials/spreadsheet-addiction/)

Now, if you really really want to do heavy calculations without exporting your data, I suggest using [xlwings](http://xlwings.org/). Basically, this allows two-way communication between Excel and Python. Watch the video in the homepage for a quick introduction. In this way, you would be able to use numpy, pandas and scikit-learn (or other machine learning library that you may prefer) without exporting your data first.', 4621, '2015-01-28 17:00:58.453', '6c571b23-b298-43aa-a8ac-b60226f98716', 4961, 'added links', 8150, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Both Apache-Spark and Apache-Flink projects claim pretty much similar capabilities.

what is the difference between these projects. Is there any advantage in either Spark or Flink?

Thanks', 7977, '2015-01-28 17:40:32.450', '5d0de38b-4a76-4f49-947b-a4b3642f0399', 4968, 8151, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the differences between Apache Spark and Apache Flink?', 7977, '2015-01-28 17:40:32.450', '5d0de38b-4a76-4f49-947b-a4b3642f0399', 4968, 8152, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 7977, '2015-01-28 17:40:32.450', '5d0de38b-4a76-4f49-947b-a4b3642f0399', 4968, 8153, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Current size limit for Amazon Redshift is 128 nodes or 2 PBs of compressed data. Might be circa 6PB uncompressed though mileage varies for compression. You can always let us know if you need more. anurag@aws (I run Amazon Redshift and Amazon EMR)', 8003, '2015-01-28 18:42:06.763', '1ac4dd19-ad1b-41e6-84c6-fccd5f4335a6', 4969, 8154, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you really want to use an Excel add-in, the XLMiner package is a solid option, although expensive:
http://www.solver.com/xlminer-data-mining
', 8005, '2015-01-28 19:02:15.203', 'f3aa2636-7661-4e35-83ae-2dfe55b88b1c', 4970, 8155, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Twitter is a popular source of data for many applications, especially involving sentiment analysis and the like.  I have some things I''m interested in doing with Twitter data, but here''s the issue:  To get all Tweets, you have to get special permission from Twitter (which, as I understand it, is never granted) or pay big bucks to Gnip or the like.

OTOH, [Twitter''s API documentation][1] says:

*Few applications require this level of access. Creative use of a combination of other resources and various access levels can satisfy nearly every application use case.*

Using the filter api with keyword tracking seems like something that would be a big part of this, but you obviously can''t enumerate every keyword.  Using a User stream on many User accounts that follow a lot of people might be an option as well, and I''m not sure if it makes sense to think about using the search API in addition.

So here''s the question "What combination of other resources and access levels is the best way to get the maximum amount of data from Twitter"?


  [1]: https://dev.twitter.com/streaming/firehose', 6554, '2015-01-28 19:24:07.543', 'f044f332-6a7c-471b-bdfa-0d827021a695', 4971, 8156, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to access maximum volume of tweets using Twitter Streaming API, without firehose access?', 6554, '2015-01-28 19:24:07.543', 'f044f332-6a7c-471b-bdfa-0d827021a695', 4971, 8157, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<software-develpment>', 6554, '2015-01-28 19:24:07.543', 'f044f332-6a7c-471b-bdfa-0d827021a695', 4971, 8158, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"OriginalQuestionIds":[4919],"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-01-28 19:48:17.040', 'b09c1a62-e4d3-42dc-a2f8-e871f610cc3d', 4913, '101', 8159, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would suggest you to use the idea of so-called ''fuzzy clustering'', where you put each of your hours of the day value into several clusters at the same time. Details in paper: http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/cmeans.html

The idea is trivial:

You decide how many clusters you want to have. For example, 4 (so you divide your day hours into 4 cathegories). Instead of computing just 1 number (which defines cluster membership) for each of your day hours you compute 4 numbers which represent the degree of membership to each of 4 clusters. So for example if you 4 clusters will contain periods 12 AM-6 AM, 6 AM- 12 PM, 12 PM - 6 PM and 6 PM - 12 AM then you would replace 4 AM hour in original data with vector of 4 numbers, first one is the biggest, second is smaller, third one is the smallest one etc.

Then you use these 4 numbers in your model to fit a regression line.

Of course, if you want you could use 24 clusters and in such case each your day of hour would have a high ''relation'' with nearby hours and almost 0 with the distant hours.', 7969, '2015-01-28 21:56:23.980', '5abcefee-19dc-4aa5-a9d0-a8dda20a6db2', 4972, 8160, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would suggest you to use the idea of so-called ''fuzzy clustering'', where you put each of your hours of the day value into several clusters at the same time. Details in paper: http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/cmeans.html

The idea is trivial:

You decide how many clusters you want to have. For example, 4 (so you divide your day hours into 4 cathegories). Instead of computing just 1 number (which defines cluster membership) for each of your day hours you compute 4 numbers which represent the degree of membership to each of 4 clusters. So for example if you 4 clusters will contain periods 12 AM-6 AM, 6 AM- 12 PM, 12 PM - 6 PM and 6 PM - 12 AM then you would replace for example 4 AM hour in original data with vector of 4 numbers, first one is the biggest, second is smaller, third one is the smallest one etc.

Then you could use these 4 numbers in your model to fit a regression line.

Of course, if you want you could use 24 clusters and in such case each your day of hour would have a high ''relation'' with nearby hours and almost 0 with the distant hours.', 7969, '2015-01-28 22:02:09.333', '378c00c4-8a96-4056-80c4-aab709fc9615', 4972, 'added 18 characters in body', 8161, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Did you know about the PUMA Benchmarks and dataset downloads?
https://sites.google.com/site/farazahmad/pumadatasets

It does include the following:

 1. TeraSort
 2. Wikipedia
 3. List item
 4. Self-Join
 5. Adjacency-List
 6. Movies-database
 7. Ranked-Inverted-Index ', 2699, '2015-01-28 22:27:18.587', '5cde7222-0257-4499-ae4c-a54640c6e2ab', 4973, 8162, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have an array of edges and weights:

    [[''a'', ''b'', 4],
     [''a'', ''c'', 3],
     [''c'', ''a'', 2],
     ...]

I have about 100,000 edges and weights are between 1 and 700, most around 100.

I am thinking of using Markov Cluster Algorithm however wanted to reach out to see if this is the best to use. What about Affinity Propagation? In either case, what is the workflow? Do you typically have a way to measure how well clustered the results. Is there an equivalent to a silhouette score? Is there a way to visualize the clusters?


     ', 8009, '2015-01-28 23:19:37.187', '463aa88c-f939-4cbf-a9da-35389a7072f0', 4974, 8163, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Partitioning Weighted Undirected Graph', 8009, '2015-01-28 23:19:37.187', '463aa88c-f939-4cbf-a9da-35389a7072f0', 4974, 8164, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering><graphs>', 8009, '2015-01-28 23:19:37.187', '463aa88c-f939-4cbf-a9da-35389a7072f0', 4974, 8165, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Even a simple Internet search reveals numerous papers on *graph clustering* approaches and algorithms. [This paper](http://dollar.biz.uiowa.edu/~street/graphClustering.pdf) is most likely the best starting point, as it presents a rather **comprehensive overview** of the topic in terms of the problem as well as *approaches*, *methods* and *algorithms* for solutions. The rest you can find easily via online search. In regard to *graph clustering visualization*, I recommend you to check [my relevant answer](http://datascience.stackexchange.com/a/814/2452) - I''m pretty sure that the tools I reference there are able to visualize graph clusters as well.', 2452, '2015-01-29 00:57:39.603', '0dbc1968-1254-45b8-abcf-54f1a83d1f2e', 4975, 8166, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Flink is the Apache renaming of the [Stratosphere project from several universities in Berlin](http://stratosphere.eu/). It doesn''t have the same industrial foothold and momentum that the Spark project has, but it seems nice, and more mature than, say, Dryad. I''d say it''s worth investigating, at least for personal or academic use, but for industrial deployment I''d still prefer Spark, which at this point is battle tested. For a more technical discussion, see [this Quora post by committers on both projects](https://www.quora.com/Are-Spark-and-Stratosphere-competitors-Do-they-cover-the-same-set-of-use-cases).', 381, '2015-01-29 04:19:16.007', 'f087e262-0565-4350-8651-8e3a5c8f34eb', 4976, 8167, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I found that Apache-Storm, Apache-Spark, Apache-Flink and TIBCO StreamBase are some powerful frameworks for stream processing. but I don''t know which one of them has the best performance and capabilities.

I know Apache-Spark and Apache-Flink are two general purpose frameworks that support stream processing. but Apache-Storm and TIBCO StreamBase are built for stream processing specially. Is there any considerable advantage between these frameworks?

Thanks', 7977, '2015-01-29 07:32:19.207', '053fd377-0dfe-4c90-8189-b52fa0c26c1a', 4977, 8168, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is the best Big-Data framework for stream processing?', 7977, '2015-01-29 07:32:19.207', '053fd377-0dfe-4c90-8189-b52fa0c26c1a', 4977, 8169, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 7977, '2015-01-29 07:32:19.207', '053fd377-0dfe-4c90-8189-b52fa0c26c1a', 4977, 8170, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When considering Support Vector Machine, in an take in multiple inputs. Can each of these inputs be a vector??
What i am trying to say is, can the input be a 2 dimensional vector??', 8013, '2015-01-29 09:04:56.190', '962d769d-f7ad-47a6-a766-8002af03ab4f', 4978, 8171, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can 2 dimensional input be applied to SVM?', 8013, '2015-01-29 09:04:56.190', '962d769d-f7ad-47a6-a766-8002af03ab4f', 4978, 8172, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><svm>', 8013, '2015-01-29 09:04:56.190', '962d769d-f7ad-47a6-a766-8002af03ab4f', 4978, 8173, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I need to simulate for an academical project how the traffic fluxes (input/output with respect to a monitored area, measured in number of cars) of a city area evolves in correspondence of an event (i.e. the opening of a restricted traffic area to decongest the traffic).
I have some simulated sensors that provide the data: I was thinking to use a combination of a fuzzy system (to assign a membership function to each type of data, e.g. PM10 value and CO2 value) and a markov process: I would need to modify the probability to decrement the number of car in the monitored area (simulating that a car is going out the congested area, towards the new opened area) basing on decisions made by means of a fuzzy system.
So my questions are:

 - It is a good way to interpret the problem or there are better ideas that I have not taken into account yet?
 - How to implement such a combination of markov chain and fuzzy systems in matlab?

Thanks

', 6559, '2015-01-29 11:35:07.657', 'c64e8a85-adac-47b3-860c-215f2eb8563e', 4979, 8174, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Matlab simulation through FIS and Markov Process', 6559, '2015-01-29 11:35:07.657', 'c64e8a85-adac-47b3-860c-215f2eb8563e', 4979, 8175, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 6559, '2015-01-29 11:35:07.657', 'c64e8a85-adac-47b3-860c-215f2eb8563e', 4979, 8176, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have been working in NLTK for a while using Python. The problem I am facing is that their is no help available on training NER in NLTK with my custom data. They have used MaxEnt and trained it on ACE corpus. I have searched on the web a lot but I could not find any way that can be used to train NLTK''s NER.

If anyone can provide me with any link/article/blog etc which can direct me to Training Datasets Format used in training NLTK''s NER so I can prepare my Datasets on that particular format. And if I am directed to any link/article/blog etc which can help me TRAIN NLTK''s NER for my own data.

This is a question widely searched and least answered. Might be helpful for someone in the future whose working with NER.', 8016, '2015-01-29 12:13:01.677', 'fc375a58-ea50-4df4-951a-047f7f18ce45', 4980, 8177, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Help regarding NER in NLTK', 8016, '2015-01-29 12:13:01.677', 'fc375a58-ea50-4df4-951a-047f7f18ce45', 4980, 8178, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><nlp>', 8016, '2015-01-29 12:13:01.677', 'fc375a58-ea50-4df4-951a-047f7f18ce45', 4980, 8179, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I came across an SVM predictive model where the author used the probabilistic distribution value of the target variable as a feature in the feature set. For example:

Target variable looks like rock, paper, scissors

Feature variables look like last gesture played, 2nd to last gesture played, 3rd to last gesture played, judge commentary for the overall player tendencies for that game, handedness of player, height of the player,....

The author built a model for each gesture of each player to guess which gesture would be played next. Look over 1000 games played the distribution may look like (20%, 10%, 70%). These numbers were then used as feature variables to predict the target variable.

Is that legitimate? That seems like cheating. You could get a model that is right 70% of the time by always betting on scissors. I would think you would have to exclude your information in target variables for your test set in order to not "cheat".

Even if the model is legitimate for predicting future gestures, can you really say it has 96.7% accuracy without having a test set outside of the player distribution calculation? Can you predict the gesture if you don''t have notes from the game (they haven''t been written yet) to build the feature set?

', 3430, '2015-01-29 12:16:31.833', '34ae5ee7-fab2-47de-a115-825a6ea28e5b', 4951, 'clarification', 8180, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<accuracy><methods>', 3430, '2015-01-29 12:16:31.833', '34ae5ee7-fab2-47de-a115-825a6ea28e5b', 4951, 'clarification', 8181, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I understand your question correctly. Yes, SVM can take multiple inputs. My suggestion for handling a vector as a feature would be to expand it out. For example,

    x0 = (1,2)       x0 = 1
    x1 = .4  ----->  x1 = 2
    x2 = 0           x2 = .4
                     x3 = 0


If this does not capture all of the characteristics of the vector that are important, then you may want to add other features (like magnitude of the vector) as well. ', 3430, '2015-01-29 12:42:05.757', '0b83b4db-475f-40a2-a62e-a8c4bb97c034', 4981, 8182, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It really depends on what you are looking to do. I love Apache Spark, but Storm has some history. I am sure as the streaming capability in Spark is built out that it will become a competitive solution. However, until Spark has some heavy hitting users (for streaming) there will remain unknown bugs.

You can also consider the community. Spark has a great community. I am not sure the level of the Storm community as I am usually the one receiving the data not handling the ingest. I can say we have used Storm on projects and I have been impressed with the real-time analysis and volumes of streaming data.', 3430, '2015-01-29 12:48:41.620', '19e22842-e1b7-4683-ad40-33cbb372c4a4', 4982, 8183, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I agree that there is nothing wrong with using these type of features.  I have used for inter-arrival times for example in modeling work.  I have noticed however that many of these kind of features have "interesting" covariance relationships with each other, so you have to be really careful about using multiple distribution features in a model.', 8005, '2015-01-29 12:51:45.280', 'cdd85792-9548-4915-99e8-7c7c5f2c2ef2', 4983, 8185, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R is great for a lot of analysis. As mentioned about, there are newer adaptations for big data like MapR, RHadoop, and scalable versions of RStudio.

However, if your concern is libraries, keep your eye on Spark. Spark was created for big data and is MUCH faster than Hadoop alone. It has vastly growing machine learning, SQL, streaming, and graph libraries. Thus allowing much if not all of the analysis to be done within the framework (with multiple language APIs, I prefer Scala) without having to shuffle between languages/tools.', 3430, '2015-01-29 12:58:28.257', '96e5e016-e69c-4725-9478-297d29ab3acc', 4984, 8186, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am currently working on a multi-class classification problem with a large training set. However, it has some specific characteristics, which induced me to experiment with it, resulting in few versions of the training set (as a result of re-sampling, removing observations, etc).

I want to perform pre-processing of the data, that is to scale, center and impute (not much imputation though) values. This is the point where I''ve started to get confused.

I''ve been taught that you should always pre-process the test set in the same way you''ve pre-processed the training set, that is (for scaling and centering) to measure the mean and standard deviation on the training set and apply those values to the test set. This seems reasonably to me.

But what to do in case when you have shrinked/resampled training set? Should one focus on characteristics of the data that is actually feeding the model (that is what would ''train'' function in R''s caret package suggest, as you can put the pre-processing object in there directly) and apply these to the test set, or maybe one should capture the real characteristics of the data (from the whole untouched training set) and apply these? If the second option is better, maybe it would be worth it to capture the characteristics of the data by merging the training and test data together just for pre-processing step to get as accurate estimates as possible (I''ve actually never heard of anyone doing that though)?

I know I can simply test some of the approaches specified here, and I surely will, but are there any suggestions based on theory or your intuition/experience on how to tackle this problem?

I also have one additional and optional question. Does it make sense to center but NOT scale the data (or the other way around) in any case? Can anyone present any example where that approach would be reasonable?

Thank you very much in advance.', 8017, '2015-01-29 13:54:24.940', '55d990ac-849c-45b5-a959-d35a6263449f', 4985, 8187, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Pre-processing (center, scale, impute) among training sets (different forms) and the test set - what is a good approach?', 8017, '2015-01-29 13:54:24.940', '55d990ac-849c-45b5-a959-d35a6263449f', 4985, 8188, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><dataset><processing><feature-scaling>', 8017, '2015-01-29 13:54:24.940', '55d990ac-849c-45b5-a959-d35a6263449f', 4985, 8189, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a weekly dataset and I have to normalize this data.
Data is something like this :

    1. week   50
    2. week   51
    3. week   50
    4. week   54
    5. week   150
    6. week   155
    7. week   ...

The important thing is, the difference between week 3 and week 4 (50-54) is not same with week 5 and week 6. And also there is a huge different between week 4 and week 5.

My question is how can i handle all of this things ?

Is the standard normalization functions(for example scikit normalization) can do it for me and should I normalize this data 0-1 or -1 to 1 ?

[Sklearn normalization page][1]


**NOTE** I am working with python and generally scikit-learn library.

Any help is appreciated.




  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html', 8018, '2015-01-29 17:21:07.070', '0536fe9a-9189-464a-b8c2-5cb013ae700b', 4986, 8190, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Normalize weekly data - Python', 8018, '2015-01-29 17:21:07.070', '0536fe9a-9189-464a-b8c2-5cb013ae700b', 4986, 8191, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><sklearn><scikit>', 8018, '2015-01-29 17:21:07.070', '0536fe9a-9189-464a-b8c2-5cb013ae700b', 4986, 8192, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would find the unit variance of the all the weeks and then divide by that. Scikit can do this for you using [scale][1].


  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html', 3430, '2015-01-29 17:28:51.800', 'a6d5a9b9-601c-414b-acaf-d8cd36b2af2e', 4987, 8193, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This looks a well structured dataset. You can read more about database design in [this section of wikipedia][1]. Your data are well structured so querying is easy. As Jake C says, you''ll want to transform it for specific tasks. Packages like dplr and reshape2 are excellent for this. You could also consider writing your data to a specific database. This is particularly useful if your dataset is so large that R runs out of RAM. I''ve written an example with SQLite here: https://scottishsnow.wordpress.com/2014/08/14/writing-to-a-database-r-and-sqlite/


  [1]: http://en.wikipedia.org/wiki/Database_design#Normalization
', 8021, '2015-01-29 20:36:25.567', '226b7c1d-6f75-4446-a9c1-714f30ee3e90', 4988, 8194, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As other answers have noted, R can be used along with Hadoop and other distributed computing platforms to scale it up to the "Big Data" level.  However, if you''re not wedded to R specifically, but are willing to use an "R-like" environment, [Incanter][1] is a project that might work well for you, as it is native to the JVM (based on Clojure) and doesn''t have the "impedance mismatch" between itself and Hadop that R has.  That is to say, from Incanter, you can invoke Java native Hadoop / HDFS APIs without needing to go through a JNI bridge or anything.


  [1]: http://www.incanter.org', 6554, '2015-01-29 21:03:27.097', '4e455f54-eedb-42b5-b433-c75538948c5c', 4989, 8195, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As bogatron and Paul already said, there is nothing wrong with using the prediction from one classifier as a feature in another classifier. Actually, so-called "Cascading classifiers" work that way. From [Wikipedia](http://en.wikipedia.org/wiki/Cascading_classifiers):

>Cascading is a particular case of ensemble learning based on the concatenation of several classifiers, using all information collected from the output from a given classifier as additional information for the next classifier in the cascade.

This can be helpful not only to inform posterior classifiers using new features but also as an optimization measure. In the Viola-Jones object detection framework, a set of weak classifiers is used sequentially in order to reduce the amount of computation in the object recognition task. If one of the weak classifiers fails to recognize an object of interest, others classifiers don''t need to be computed.
', 4621, '2015-01-29 21:31:50.280', 'dda057e0-972f-4f33-92e0-98000dcdab35', 4990, 8196, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><python><nlp>', 8016, '2015-01-30 04:30:58.490', '27b0c22a-4c8a-429e-aaa2-9144b4245fdf', 4980, 'edited tags', 8197, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have data, which looks like this:

![Data table][1]

These data are only for one subject. I will have a lot more.
These data will be analyzed in R.

Now I''m storing them like this:

    subject <- rep(1, times = 24)
    measurement <- factor(x = rep(x = 1:3, each = 8),
                          labels = c("Distance", "Frequency", "Energy"))
    speed <- factor(x = rep(x = 1:2, each = 4, times = 3),
                    labels = c("speed1", "speed2"))
    condition <- factor(x = rep(x = 1:2, each = 2, times = 6),
                        labels = c("Control", "Experm"))
    Try <- factor(x = rep(x = 1:2, times = 12),
                  labels = c("Try1", "Try2"))
    result <- c(1:8,
                11:18,
                21:28)

    dt <- data.frame(subject, measurement, speed, condition, Try, result)

What is the appropriate way to store these data in R (in a data frame)?


  [1]: http://i.stack.imgur.com/EqH9I.jpg

', 8021, '2015-01-30 08:52:32.077', 'cd0c1c97-580d-4394-9405-fbd485112415', 4876, 'Improved the English to make the question understandable.', 8198, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-30 08:52:32.077', 'cd0c1c97-580d-4394-9405-fbd485112415', 4876, 'Proposed by 8021 approved by 3377 edit id of 221', 8199, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is this article good enough?
http://www.succeed-project.eu/wiki/index.php/NLTK#Input_format_for_training

There is explanation about how corpus should look like.

Your data needs to be in IOB format (word tag chunktag) to make it work.
Eric NNP B-PERSON
is VB O
the AT B-NP
CEO NN I-NP
of IN O
Google NNP B-ORGANIZATION ', 2750, '2015-01-30 10:44:58.467', 'bc9d7d1f-0fd8-402f-82ab-20ef48bc4fc5', 4991, 8200, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to build a cosine locality sensitive hash so I can find candidate similar pairs of items without having to compare every possible pair. I have it basically working, but most of the pairs in my data seem to have cosine similarity in the -0.2 to +0.2 range so I''m trying to dice it quite finely and pick things with cosine similarity 0.1 and above.

I''ve been reading Mining Massive Datasets chapter 3. This talks about increasing the accuracy of candidate pair selection by Amplifying a Locality-Sensitive Family. I think I just about understand the mathematical explanation, but I''m struggling to see how I implement this practically.

What I have so far is as follows

1. I have say 1000 movies each with ratings from some selection of 1M users. Each movie is represented by a sparse vector of user scores (row number = user ID, value = user''s score)
1. I build N random vectors. The vector length matches the length of the movie vectors (i.e. the number of users). The vector values are +1 or -1. I actually encode these vectors as binary to save space, with +1 mapped to 1 and -1 mapped to 0
1. I build sketch vectors for each movie by taking the dot product of the movie and each of the N random vectors (or rather, if I create a matrix R by laying the N random vectors horizontally and layering them on top of each other then the sketch for movie m is R*m), then taking the sign of each element in the resulting vector, so I end with a sketch vector for each movie of +1s and -1s, which again I encode as binary. Each vector is length N bits.
1. Next I look for similar sketches by doing the following
  1. I split the sketch vector into b bands of r bits
  1. Each band of r bits is a number. I combine that number with the band number and add the movie to a hash bucket under that number. Each movie can be added to more than one bucket.
  1. I then look in each bucket. Any movies that are in the same bucket are candidate pairs.

Comparing this to 3.6.3 of mmds, my AND step is when I look at bands of r bits - a pair of movies pass the AND step if the r bits have the same value. My OR step happens in the buckets: movies are candidate pairs if they are both in any of the buckets.

The book suggests I can "amplify" my results by adding more AND and OR steps, but I''m at a loss for how to do this practically as the explanation of the construction process for further layers is in terms of checking pairwise equality rather than coming up with bucket numbers.

Can anyone help me understand how to do this?', 8030, '2015-01-30 11:08:37.280', 'eddb51c7-74e5-495b-bc98-4584ef449043', 4992, 8201, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Amplifying a Locality Sensitive Hash', 8030, '2015-01-30 11:08:37.280', 'eddb51c7-74e5-495b-bc98-4584ef449043', 4992, 8202, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 8030, '2015-01-30 11:08:37.280', 'eddb51c7-74e5-495b-bc98-4584ef449043', 4992, 8203, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Is this article good enough?
http://www.succeed-project.eu/wiki/index.php/NLTK#Input_format_for_training

There is explanation about how corpus should look like.

Your data needs to be in IOB format (word tag chunktag) to make it work.<br/>
Eric NNP B-PERSON<br/>
is VB O<br/>
the AT B-NP<br/>
CEO NN I-NP<br/>
of IN O<br/>
Google NNP B-ORGANIZATION <br/>', 2750, '2015-01-30 11:57:37.427', 'e2a16e2a-d6bb-4b62-ba78-78652c3df99c', 4991, 'added 35 characters in body', 8204, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('After speaking with some experienced statisticians, this is what I got.

>As for technical issues regarding the paper, I''d be worried about data leakage or using future information in the current model. This can also occur in cross validation. You should make sure each model trains only on past data, and predicts on future data. I wasn''t sure exactly how they conducted CV, but it definitely matters. It''s also non-trivial to prevent all sources of leakage. They do claim unseen examples but it''s not explicit exactly what code they wrote here. I''m not saying they are leaking for sure, but I''m saying it could happen.
', 3430, '2015-01-30 12:11:41.293', '4acf2ba5-40cd-49d7-beca-bf3b1a4fb67b', 4993, 8205, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I came across an SVM predictive model where the author used the probabilistic distribution value of the target variable as a feature in the feature set. For example:

The author built a model for each gesture of each player to guess which gesture would be played next. Calculating over 1000 games played the distribution may look like (20%, 10%, 70%). These numbers were then used as feature variables to predict the target variable for cross-fold validation.

Is that legitimate? That seems like cheating. I would think you would have to exclude the target variables from your test set when calculating features in order to not "cheat".

', 3430, '2015-01-30 12:42:45.187', 'a7aec688-4091-4c09-be2a-313f43570d50', 4951, 'trying to clarify and be more concise.', 8206, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Can distribution values of a target variable be used as features in cross-validation?', 3430, '2015-01-30 12:42:45.187', 'a7aec688-4091-4c09-be2a-313f43570d50', 4951, 'trying to clarify and be more concise.', 8207, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There is nothing necessarily wrong with this.  If you have no better information, then using past performance (i.e., prior probabilities) can work pretty well, particularly when your classes are very unevenly distributed.

Example methods using class priors are [Gaussian Maximum Likelihood](http://en.wikipedia.org/wiki/Maximum_likelihood) classification and [Naïve Bayes](http://en.wikipedia.org/wiki/Naive_Bayes_classifier).

[UPDATE]

Since you''ve added additional details to the question...

Suppose you are doing 10-fold cross-validation (holding out 10% of the data for validating each of the 10 subsets). If you use the entire data set to establish the priors (including the 10% of validation data), then yes, it is "cheating" since each of the 10 subset models uses information from the corresponding validation set (i.e., it is not truly a blind test). However, if the priors are recomputed for each fold using only the 90% of data used for that fold, then it is a "fair" validation.

An example of the effect of this "cheating" is if you have a single, extreme outlier in your data. Normally, with k-fold cross-validation, there would be one fold where the outlier is in the validation data and not the training data. When applying the corresponding classifier to the outlier during validation, it would likely perform poorly. However, if the training data for that fold included global statistics (from the entire data set), then the outlier would influence the statistics (priors) for that fold, potentially resulting in artificially favorable performance.
', 964, '2015-01-30 13:52:46.097', '37b25f16-31cf-46a2-bee7-e06d4ae26e5f', 4952, 'Updated for new question details.', 8208, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would call a mapping between N dimensional input and N dimensional output a regression problem.

If you add more constraints about the relation between the input and output it might be called different names: linear filtering, nonlinear filtering, etc...

some examples on common techniques for that would be: neural networks, regression trees, regularised regressions...', 7999, '2015-01-30 16:33:23.683', '00c7680b-8d0e-4646-9889-0ad229048edf', 4994, 8209, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('We can access HDFS file system and YARN scheduler In the Apache-Hadoop. But Spark has a higher level of coding. Is it possible to access HDFS and YARN in Apache-Spark too?

Thanks', 7977, '2015-01-30 18:55:46.173', '0efb9a30-a584-423b-a73f-4fcb7afb77ef', 4995, 8210, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can we access HDFS file system and YARN scheduler in Apache Spark?', 7977, '2015-01-30 18:55:46.173', '0efb9a30-a584-423b-a73f-4fcb7afb77ef', 4995, 8211, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop>', 7977, '2015-01-30 18:55:46.173', '0efb9a30-a584-423b-a73f-4fcb7afb77ef', 4995, 8212, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Apache Storm and Apache Spark are more popular than the other ones, there are already many discussions on Quora([Storm vs Spark](http://www.quora.com/What-is-the-difference-between-Apache-Storm-and-Apache-Spark), [Use cases for comparison](http://www.quora.com/Are-there-any-use-cases-for-a-comparison-between-Storm-and-Spark-Streaming)).

Personally, I think Spark is a better choice.', 2522, '2015-01-30 20:15:09.003', '802f405a-0552-4745-acf6-2947d677e6c6', 4996, 8213, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[Data Sets](https://github.com/chenrui333/awesome-datascience#data-sets) From [awesome-datascience](https://github.com/okulbilisim/awesome-datascience)



', 2522, '2015-01-30 20:26:42.293', 'dec6e2ae-c452-4d9c-a953-81ef9402852e', 4997, 8214, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I found that Apache-Spark has pretty much simple interface and easy to use. But I want to know about other interfaces.

Can anyone give me a ranking of Big-Data frameworks in base of simplicity of their interfaces. also this is useful to express most simple and complex interfaces in base of your experiences.

Thanks', 7977, '2015-01-30 21:04:30.173', '2749ea1f-a857-4187-b4bd-8916755e3e85', 4998, 8215, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Which Big-Data Frameworks have most simple interfaces?', 7977, '2015-01-30 21:04:30.173', '2749ea1f-a857-4187-b4bd-8916755e3e85', 4998, 8216, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 7977, '2015-01-30 21:04:30.173', '2749ea1f-a857-4187-b4bd-8916755e3e85', 4998, 8217, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('N_dimension input - n_dimension output is a too general description. You could think of it as a regression problem where you predict multi-dimensional output.

But also it could be the case that you are solving multiclass-classification problem:

input: n features

output: vector which defines class membership - either 0''s and 1''s or the real value which defines degree of membership to the class

Or you could also think of it as of multilabel classification problem:

input: n features

output: vector of 0 and 1 which define which labels are associated with the input.

So in general multi-dimensional output is not telling anything about the matter of task.

You could try 2 approaches to solve the task which involves multi-dimensional output:

1) One-vs-rest or one-vs-one strategies (or their variations) where for each ''part'' (dimension) of the output you train separate classifier or separate regressor.

2) Neural network with multiple output neurons. I would suggest to try it after trying #1, neural networks are complicated, computing-expensive and maybe somewhat clumsy - so far, I wasn''t able to construct neural network which would outperform other models in specific tasks I tried to solve.  But of course, this is my personal opinion about NN. In your case they may really shine.', 7969, '2015-01-30 21:14:25.563', 'c6c3a93b-5fc2-4de3-84b1-9f3fcc1d5b16', 4999, 8218, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Maybe it is a bit general question. I am trying to solve various regression tasks and I try various algorithms for them. For example, multivariate linear regression or an SVR. I know that the output can''t be negative and I never have negative output values in my training set, though I could have 0''s in it (for example, I predict ''amount of cars on the road'' - it can''t be negative but can be 0). Rather often I face a problem that I am able to train relatively good algorithm (maybe fit a good regression line to my data) and I have relatively small average squared error on training set. But when I try to run my regression algorithm against new data I sometimes get a negative output. Obviously, I can''t accept negative output since it is not a valid value. The question is - what is the proper way of working with such output? Should I think of negative output as a 0 output? Is there any general advice for such cases?', 7969, '2015-01-30 21:30:18.077', '346e524c-9f28-4993-b288-e123b219119a', 5000, 8219, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Proper way of fighting negative outputs of a regression algorithms where output must be positive all the way', 7969, '2015-01-30 21:30:18.077', '346e524c-9f28-4993-b288-e123b219119a', 5000, 8220, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><regression>', 7969, '2015-01-30 21:30:18.077', '346e524c-9f28-4993-b288-e123b219119a', 5000, 8221, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m going to start my degree thesis and I want to do a fault detector system using machine learning techniques. I need datasets for my thesis but I don''t know where I can get that data. I''m looking for historical operation/maintenance/fault  datasets of any kind of machine in the oil&gas industry (Drills, steam injectors etc) or electrical companies (transformators, generators etc).

Thank you for your help.', 8037, '2015-01-30 23:39:04.687', '8e505342-2025-47c7-bdc8-646a22bea3d8', 5001, 8222, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data available from industry', 8037, '2015-01-30 23:39:04.687', '8e505342-2025-47c7-bdc8-646a22bea3d8', 5001, 8223, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><open-source><freebase>', 8037, '2015-01-30 23:39:04.687', '8e505342-2025-47c7-bdc8-646a22bea3d8', 5001, 8224, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think that it is impossible to answer this question *comprehensively*, at least for the following **reasons**:

 - big data frameworks have different **goals** and target different **knowledge domains**, so the comparison simply doesn''t make much sense;
 - most big data frameworks (and other programming frameworks, for that matter) have *multiple interfaces*, and frequently those **sets of interfaces** are significantly different (the *intersection* is small), so there is a risk of comparing apples and oranges;
 - trying to compare anything (in this case, interfaces), using *simplicity* as a criterion, involves a significant amount of **subjectivity** - what one person *perceive* as very simple, another person might find quite complex;
 - the **variety** and the **number** of *big data frameworks* is mind-boggling (for example, see https://github.com/onurakpolat/awesome-bigdata); the same applies to a related topic of *machine learning frameworks* (for example, see https://github.com/onurakpolat/awesome-bigdata);
 - corollary from the points above: a **comprehensive** comparison (considering all the above-mentioned issues) would go far beyond the **scope** of a single answer on this site, in volume and effort - it would be more like a long research paper, a book chapter or even a book.', 2452, '2015-01-30 23:44:42.127', 'f8b2fea8-21aa-421e-be92-8a5670c92b66', 5002, 8225, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A standard trick is to estimate the logarithm of the desired quantity, then take its exponent, which obviously is positive. The drawback is that the error is optimized for the log, which treats errors in order of magnitude as equal. Another option is to do your regression as usual then project onto the feasible set (use the positive part of the output; $min(0, \cdot)$)', 381, '2015-01-31 01:18:10.307', 'ff1475c1-3625-433c-82b8-d5672fdddbeb', 5004, 8228, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes.

There are examples on spark official document: https://spark.apache.org/examples.html
Just put your hdft file uri in your input file path as below(scala syntax).

> val file = spark.textFile("hdfs://train_data")', 1003, '2015-01-31 02:22:36.000', '6b2f3f3c-83a7-4a7e-b276-513bb78864c8', 5005, 8229, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Yes.

There are examples on spark official document: https://spark.apache.org/examples.html
Just put your HDFS file uri in your input file path as below (scala syntax).

    val file = spark.textFile("hdfs://train_data")', 1279, '2015-01-31 10:49:29.950', '84558b49-e538-418e-b608-4d992edf32b4', 5005, 'added 3 characters in body', 8230, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-01-31 11:32:43.967', 'a542a4c6-2a1a-453a-820f-53f178784e33', 4978, '103', 8231, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Actually this is coming around. In the book R in a Nutshell there is even a section on using R with Hadoop for big data processing. There are some work arounds that need to be done because R does all it''s work in memory, so you are basically limited to the amount of RAM you have available to you.

A mature project for R and Hadoop is [RHadoop](https://github.com/RevolutionAnalytics/RHadoop)

RHadoop has been divided into several sub-projects, rhdfs, rhbase, rmr2, plyrmr, and quickcheck ([wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki)).', 2522, '2015-01-31 11:34:03.700', '824606ba-8f2b-4f7b-8752-320afe31f197', 44, 'Update RHadoop project information. ', 8234, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-01-31 11:34:03.700', '824606ba-8f2b-4f7b-8752-320afe31f197', 44, 'Proposed by 2522 approved by 2452, 21 edit id of 222', 8235, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('## Data Sets

* [Academic Torrents](http://academictorrents.com/)
* [Quora](http://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)
* [hadoopilluminated.com](http://hadoopilluminated.com/hadoop_illuminated/Public_Bigdata_Sets.html)
* [data.gov](http://catalog.data.gov/dataset)
* [Quandl](http://www.quandl.com/)
* [freebase.com](https://www.freebase.com/)
* [usgovxml.com](http://usgovxml.com/)
* [enigma.io](http://enigma.io/)
* [datahub.io](http://datahub.io/)
* [aws.amazon.com/datasets](http://aws.amazon.com/datasets)
* [databib.org](http://databib.org/)
* [datacite.org](http://www.datacite.org)
* [quandl.com](https://www.quandl.com/)
* [figshare.com](http://figshare.com/)
* [GeoLite Legacy Downloadable Databases](http://dev.maxmind.com/geoip/legacy/geolite/)
* [Quora''s Big Datasets Answer](http://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)
* [Public Big Data Sets](http://hadoopilluminated.com/hadoop_illuminated/Public_Bigdata_Sets.html)
* [Houston Data Portal](http://data.ohouston.org/)
* [Kaggle Data Sources](https://www.kaggle.com/wiki/DataSources)
* [A Deep Catalog of Human Genetic Variation](http://www.1000genomes.org/data)
* [A community-curated database of well-known people, places, and things](https://www.freebase.com/)
* [Google Public Data](http://www.google.com/publicdata/directory)
* [World Bank Data](http://data.worldbank.org/)
* [NYC Taxi data](http://nyctaxi.herokuapp.com/)
* [Open Data Philly](http://www.opendataphilly.org/) Connecting people with data for Philadelphia
* [A list of useful sources](http://ahmetkurnaz.net/en/statistical-data-sources/) A blog post includes many data set databases

[Data Sets](https://github.com/okulbilisim/awesome-datascience#data-sets) From [awesome-datascience](https://github.com/okulbilisim/awesome-datascience)', 2522, '2015-01-31 11:34:10.343', '7721cc30-e5a4-4e08-b919-d2c5bc1fc0eb', 4997, 'Add links for data sets', 8236, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**HDFS**

Spark was built as an alternative to MapReduce and thus supports most of its functionality. In particular, it means that "Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc."[1]. For most common data sources (like HDFS or S3) Spark automatically recognizes schema, e.g.:

    val sc = SparkContext(...)
    val localRDD = sc.textFile("file://...")
    val hdfsRDD  = sc.textFile("hdfs://...")
    val s3RDD    = sc.textFile("s3://...")

For more complicated cases you may need to work with lower-level functions like `newAPIHadoopFile`:

    val hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat],
          classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
          classOf[org.apache.hadoop.hbase.client.Result])
    val customRDD = sc.newAPIHadoopRDD(conf, classOf[MyCustomInputFormat],
          classOf[MyCustomKeyClass],
          classOf[MyCustomValueClass])

But general rule is that if some data source is available for MapReduce, it can be easily reused in Spark.

**YARN**

Currently Spark supports 3 cluster managers / modes:

 * **Standalone**
 * **Mesos**
 * **YARN**

Standalone mode uses Spark''s own master server and works for Spark only, while YARN and Mesos modes aim to share same set of system resources between several frameworks (e.g. Spark, MapReduce, Impala, etc.). Comparison of YARN and Mesos may be found [here][2], and detailed description of Spark on YARN [here][3].

And, in best traditions of Spark, you can switch between different modes simply by changing [master URL][4].

[1]: http://spark.apache.org/docs/1.2.0/programming-guide.html#external-datasets
[2]: http://www.quora.com/How-does-YARN-compare-to-Mesos
[3]: http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/
[4]: https://spark.apache.org/docs/1.2.0/submitting-applications.html
', 1279, '2015-01-31 12:29:35.733', '778d3582-d299-421c-a656-0eb64f33b12f', 5006, 8237, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am wondering if there is a way to proceed 2 exectuions in 1 step in hive.
For example:



    SELECT * FROM TABLE1
    SELECT * FROM TABLE2
    ;
Do this in one window, and do not have to open 2 hive windows to execute each line separetly.', 5224, '2015-01-31 13:28:45.590', 'e97713e9-38fa-47ec-bf27-88da19ade507', 5007, 8238, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to proceed 2 executions in 1 step in hive?', 5224, '2015-01-31 13:28:45.590', 'e97713e9-38fa-47ec-bf27-88da19ade507', 5007, 8239, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hive>', 5224, '2015-01-31 13:28:45.590', 'e97713e9-38fa-47ec-bf27-88da19ade507', 5007, 8240, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A huge list of open data sets is listed here:

- http://datascience.stackexchange.com/questions/155/publicly-available-datasets

Including Amazon, KDnuggets, Stanford, Twitter, Freebase, Google Public and more.', 97, '2015-01-31 14:27:15.657', '1ec9dcfa-184e-481e-a69a-f2d155a9597f', 5008, 8242, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can use HiveCLI Tool to run HiveQL with a given sql file.

> $HIVE_HOME/bin/hive -f /home/my/hive-script.sql

Please see official document: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli

What you need to do is to

 1. Put your HiveQLs in a file as below
` SELECT * FROM TABLE1;
  SELECT * FROM TABLE2;`

 2. Use HiveCLI and run with above file', 1003, '2015-01-31 16:55:53.350', '9be5c4ac-a96d-4459-9735-7d07aad9aa5e', 5009, 8243, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can separate each query with a semi colon (;)

    select column1 from table1;
    select column2 from table2;

This command can be executed from command line via inline queries or a file. Usage of Hive CLI is not recommended. You must use beeline to execute queries configured via hive server 2 so that all/any underlying security control measures are honored.

you may invoke beeline with the command:

    beeline

 ', 7809, '2015-01-31 17:31:09.967', '19628024-ba4c-49cc-b9b6-b277a38c6ff5', 5010, 8244, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I thought about it this way: the training and test sets are both a sample of the unknown population. We assume that the training set is representative of the population we''re studying. That is, whatever transformations we make to the training set are what we would make to the overall population. In addition, whatever subset of the training data we use, we assume that this subset represents the training set, which represents the population.

So in response to your first question, it''s fine to use that shrinked/resmpled training as long as you feel it''s still representative of that population. That''s assuming your untouched training set captures the "real characteristics" in the first place :)

As for your second question, don''t merge the training and testing set. The testing set is there to act as future unknown observations. If you build these into the model then you won''t know if the model wrong or not, because you used up the data you were going to test it with.', 525, '2015-02-01 01:34:13.597', '58d19fba-2e02-4ff1-9e12-1bd5b71ebbb1', 5011, 8245, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to train ECG signal downloaded from [Physionet][1] using LIBSVM svmtrain, The code is as follows:

> model=svmtrain(labels,features_sparse,''-t 2 -s 0'');

I have created features_sparse, but I am confused in making/retrieving label vector from ''.atr'' file. The only thing I know is that this must be from ''.atr'' file. But I don''t know how to make this matrix/vector.

Any help is appreciated.

NOTE : I used [rddata.m][2] to extract ECG matrix from 100.dat, 100.atr and 100.hea


  [1]: http://www.physionet.org/physiobank/database/mitdb/
  [2]: http://www.physionet.org/physiotools/matlab/rddata.m', 8048, '2015-02-01 05:58:00.343', '10fc6322-7be5-4856-99ca-7169dba2a24c', 5012, 8246, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('physionet arrhythmia database (MIT-BIH) - Labels of an ECG signal', 8048, '2015-02-01 05:58:00.343', '10fc6322-7be5-4856-99ca-7169dba2a24c', 5012, 8247, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><libsvm>', 8048, '2015-02-01 05:58:00.343', '10fc6322-7be5-4856-99ca-7169dba2a24c', 5012, 8248, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I found that Apache-Spark has pretty much simple interface and easy to use. But I want to know about other interfaces.

Can anyone give me a ranking of Big-Data frameworks in base of simplicity of their interfaces. also this is useful to express most simple and complex interfaces in base of your experiences.

Definitely this question is about some frameworks with same tasks. For example a selection between Flink and Spark just in your opinion. Detailed comparison is so lengthy and this is not my purpose. Just a selection or ranking on your opinions is sufficient.

Thanks', 7977, '2015-02-01 07:13:30.427', 'ed64ad6d-e5cd-4bd7-91ca-c5aafb0de96e', 4998, 'added 262 characters in body', 8249, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Our main use case is object detection in 3d lidar point clouds i.e. data is not in RGB-D format. We are planning to use CNN for this purpose using theano. Hardware limitations are CPU: 32 GB RAM Intel 47XX 4th Gen core i7 and GPU: Nvidia quadro k1100M 2GB. Kindly help me with recommendation for architecture.

I am thinking in the lines of 27000 input neurons on basis of 30x30x30 voxel grid but can''t tell in advance if this is a good option.

Additional Note: Dataset has 4500 points on average per view per point cloud', 8051, '2015-02-01 09:35:16.113', '30359730-ab63-47eb-980b-dabb8c9d1a76', 5013, 8250, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine learning for Point Clouds Lidar data', 8051, '2015-02-01 09:35:16.113', '30359730-ab63-47eb-980b-dabb8c9d1a76', 5013, 8251, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><dataset><recommendation>', 8051, '2015-02-01 09:35:16.113', '30359730-ab63-47eb-980b-dabb8c9d1a76', 5013, 8252, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am wondering if there is a way to proceed 2 exectuions in 1 step in hive.
For example:



    SELECT * FROM TABLE1
    SELECT * FROM TABLE2
    ;
Do this in one window, and do not have to open 2 hive windows to execute each line separetly.


Can it be done on HUE?', 5224, '2015-02-01 18:37:12.500', 'c6d3c70d-7b9d-4dc5-8d8e-4a6108f76b46', 5007, 'added 25 characters in body', 8253, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('A standard trick is to estimate the logarithm of the desired quantity, then take its exponential, which is always positive. The drawback is that the error is optimized for the log, which treats differences in order of magnitude as equal. Another option is to do your regression as usual then project onto the feasible set (use the positive part of the output; $max(0, \cdot)$)', 381, '2015-02-01 19:45:23.910', 'c56b96a2-d0c9-46a5-a420-b4e1a1d002ac', 5004, 'correct typo', 8254, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Neo4j and Spark GraphX are meant for solving problem at different level and they are complimentary to each other.

They can be connected by Neo4j''s Mazerunner extension:

> Mazerunner is a Neo4j unmanaged extension and distributed graph
> processing platform that extends Neo4j to do big data graph processing
> jobs while persisting the results back to Neo4j.
>
> Mazerunner uses a message broker to distribute graph processing jobs
> to Apache Spark''s GraphX module. When an agent job is dispatched, a
> subgraph is exported from Neo4j and written to Apache Hadoop HDFS.
>
> After Neo4j exports a subgraph to HDFS, a separate Mazerunner service
> for Spark is notified to begin processing that data. The Mazerunner
> service will then start a distributed graph processing algorithm using
> Scala and Spark''s GraphX module. The GraphX algorithm is serialized
> and dispatched to Apache Spark for processing.
>
> Once the Apache Spark job completes, the results are written back to
> HDFS as a Key-Value list of property updates to be applied back to
> Neo4j.
>
> Neo4j is then notified that a property update list is available from
> Apache Spark on HDFS. Neo4j batch imports the results and applies the
> updates back to the original graph.

Check out this tutorial to get an idea on how to combine the two:
http://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html', 7950, '2015-02-02 07:31:51.050', '9f9e5d12-c003-44f7-bf12-21a49e10b7aa', 4941, 'added 1157 characters in body', 8255, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Recently I read about path ranking algorithm in a paper (source: [Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion][1].

In this paper was a table (Table 3) with facts and I tried to understand how they were calculated.

F1 (harmonic mean of precision and recall) = 0.04<br/>
P (precision) = 0.03<br/>
R (recall) = 0.33<br/>
W (weight given to this feature by logistic regression)

I found a formula for F1 via Google which is

$$ F1 = 2 * ((precision * recall)/(precision + recall)

The problem is that I get the result of 0.055 with this formula, but not the expected result of 0.04.
Can someone help me to get this part?
Also, does someone know how ''W'' can be calculated?
Thanks.


  [1]: https://www.cs.cmu.edu/~nlao/publication/2014.kdd.pdf', 8063, '2015-02-02 14:53:51.810', '48a76a76-d35f-48f5-a63b-ebf0ce9794aa', 5014, 8256, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to compute F1 score?', 8063, '2015-02-02 14:53:51.810', '48a76a76-d35f-48f5-a63b-ebf0ce9794aa', 5014, 8257, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 8063, '2015-02-02 14:53:51.810', '48a76a76-d35f-48f5-a63b-ebf0ce9794aa', 5014, 8258, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Recently I read about path ranking algorithm in a paper (source: [Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion][1]).

In this paper was a table (Table 3) with facts and I tried to understand how they were calculated.

F1 (harmonic mean of precision and recall) = 0.04<br/>
P (precision) = 0.03<br/>
R (recall) = 0.33<br/>
W (weight given to this feature by logistic regression)

I found a formula for F1 via Google which is

$F1 = 2 * \frac{precision * recall}{precision + recall}$

The problem is that I get the result of 0.055 with this formula, but not the expected result of 0.04.
Can someone help me to get this part?
Also, does someone know how ''W'' can be calculated?
Thanks.


  [1]: https://www.cs.cmu.edu/~nlao/publication/2014.kdd.pdf', 8063, '2015-02-02 14:59:43.687', '7a162335-9d9e-4b0f-be78-d51ef779d3ee', 5014, 'added 2 characters in body', 8259, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First you need to learn about Logistic Regression, it is an algorithm that will assign weights to different features given some training data. Read the wiki intro, is quite helpful, basically the Betas there are the same as the Ws in the paper.

The formula you have is correct, and those value do seem off. It also depends on the number of significant figures you have, perhaps they are making their calculations with more than the ones they are reporting.

But honestly, you can''t understand much of the paper unless you understand LR', 8065, '2015-02-02 15:55:49.363', '6c6f4f3b-e7ec-4524-895a-da0f56f097e7', 5015, 8260, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First, CNNs are great for image recognition, where you usually take sub sampled windows of about 80 by 80 pixels, 27,000 input neurons is too large and it will take you forever to train a CNN on that.

Furthermore, why did you choose CNN? Why don''t you try some more down to earth algorithms fisrst? Like SVMs, or Logistic regressions.

4500 Data points and 27000 features seems unrealistic to me, and very prone to over fitting.

Check this first.

http://scikit-learn.org/stable/tutorial/machine_learning_map/', 8065, '2015-02-02 15:59:40.643', '7f05a931-8e79-48ff-8249-5b0418c03a6b', 5016, 8261, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I don''t really get why would you mix Fuzziness and Probabilities. HMMs already can give you probabilities without the need of adding Fuzzy systems into the mix.

I would just do a random walk with probabilities of transitions defined by the state of the lights.

', 8065, '2015-02-02 16:08:50.580', 'c1984dc7-fb45-407c-8cc6-410a5f65fd1c', 5017, 8262, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes, you can execute multiple HQL''s using Hue as long as each individual HQL is separated by a semi colon (;)', 7809, '2015-02-02 17:00:51.307', 'b7198baa-a292-4320-9659-50067e499109', 5018, 8263, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The problem is your model choice, as you seem to recognize. In the case of linear regression, there is no restriction on your outputs.  Often this is fine when predictions need to be non-negative so long as they are far enough away from zero.  However, since many of your training examples are zero-valued, this isn''t the case.

If your data is non-negative and discrete (as in the case with number of cars on the road), you could model using a generalized linear model (GLM) with a log link function.  This is known as Poisson regression and is helpful for modeling discrete non-negative counts such as the problem you described. The Poisson distribution is parameterized by a single value $\lambda$, which describes both the expected value and the variance of the distribution.

This results in an approach similar to the one described by Emre in that you are attempting to fit a linear model to the log of your observations.
', 182, '2015-02-02 20:20:08.503', '7d003e56-0e07-4ad9-a99c-00e58975c361', 5019, 8264, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('> I want to try out unsupervised learning for the same. I am looking into KMeans clustering. But again the problem is, we won''t know the number of clusters beforehand. So, which algorithm will be best for this use case?

When you don''t know the number of clusters beforehand, it is still possible to do unsupervised learning using a [Dirichlet process](http://en.wikipedia.org/wiki/Dirichlet_process) to sample parameters associated to clusters/groups and then cluster your tokens according to those parameters. The general idea is to use a Dirichlet distribution to generate probabilities over words for each cluster and a Dirichlet process uses these probabilities to assign a cluster to each word in your vocabulary. If you want to share clusters between emails, then you use Hierarchical Dirichlet Processes. [Here](http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/) you can find a nice blog post of how this works.

The most popular library for clustering is [gensim](http://radimrehurek.com/gensim/tut2.html), but notice their warning regarding the Hierarchical Dirichlet Process implementation:

>gensim uses a fast, online implementation based on [3]. The HDP model is a new addition to gensim, and still rough around its academic edges  use with care.

As for feature extraction, your question doesn''t say exactly what kind of unnecessary features you are getting, but if that''s the case, you need to filter your tokens before or after processing them with NLTK. In general, you can''t expect excellent results for very specific applications.

', 4621, '2015-02-02 20:58:47.970', '8aa9ba29-0c3b-40c8-b7cb-04104a63abae', 5020, 8265, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve recently become interested in possibly of developing some sort of method for ranking athletes of sports such as American football and determining which players are better than others in terms of specific statistics.

My thoughts are that there are two ways to go about doing this. The first would be some sort of mathematical formula which would take in the statistics of a given player and provide some sort of standardized score which could be compared with other players to determine which is better.

My other idea would be to have some machine learning algorithm go through historical data and determine the patterns which indicate how well a certain combination of statistics would perform in the following week of play by using the patterns it recognizes over time.

I''m not sure which approach would be more effective and so I''m hoping that someone has an idea or any advice as to which would be best to look into. Thanks!', 8074, '2015-02-03 04:23:37.463', 'f96fa4b1-8913-4cdc-b534-418e635d92da', 5021, 8266, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How could I rank players of any given sport?', 8074, '2015-02-03 04:23:37.463', 'f96fa4b1-8913-4cdc-b534-418e635d92da', 5021, 8267, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><bigdata><algorithms>', 8074, '2015-02-03 04:23:37.463', 'f96fa4b1-8913-4cdc-b534-418e635d92da', 5021, 8268, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Prediction**

If the main goal is predicting anything, say, the statisitcs of the player in the next game, game result, then I would not recommend to do any scoring. Better way to go is using the pure statistics data as an input to the model. Any scoring/rankning - is information loss.

**Ranking**

If the goal is ranking itself, than you still need to have some target variable to predict. As you may want to check real predictive value of those ranks. That could be, again, playser stats in the *next* game or game result itself.


----------


*References*

[Sport scores prediction][1] and [RFM scoring][2] are probably the next directions for you to look at.


  [1]: http://datascience.stackexchange.com/questions/265/can-machine-learning-algorithms-predict-sports-scores-or-plays/269#269
  [2]: http://datascience.stackexchange.com/questions/1119/predictive-modeling-based-on-rfm-scoring-indicators', 97, '2015-02-03 08:09:44.953', 'ec329fac-c4a2-45d2-b706-5d7eeedf40e7', 5022, 8269, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('with respect to this link,<br>http://en.wikipedia.org/wiki/Receiver_operating_characteristic <br> can anyone please tell me what the phrase "discrimination threshold of binary classifier system" means?? I know what binary classifier is.', 8013, '2015-02-03 08:52:42.133', 'c40de879-74f2-435b-b82f-785a84c17afe', 5023, 8270, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('what is discrimination threshold of binary classifier?', 8013, '2015-02-03 08:52:42.133', 'c40de879-74f2-435b-b82f-785a84c17afe', 5023, 8271, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><graphs>', 8013, '2015-02-03 08:52:42.133', 'c40de879-74f2-435b-b82f-785a84c17afe', 5023, 8272, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am learning about Data Science and I love the Healthcare part. That''s why I have started a blog and my third entry is about using Genetic Algorithm for solving NP-Problems.
This post is https://datasciencecgp.wordpress.com/2015/01/31/the-amazing-genetic-algorithms/

Can you suggest me some deep learning in GA with R?

Thanks so much!', 8076, '2015-02-03 10:14:53.543', 'd0fde877-5848-49d4-9195-420f5509197b', 5024, 8273, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How you can do a deep learning in Genetic Algorithm with R?', 8076, '2015-02-03 10:14:53.543', 'd0fde877-5848-49d4-9195-420f5509197b', 5024, 8274, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<algorithms><genetic>', 8076, '2015-02-03 10:14:53.543', 'd0fde877-5848-49d4-9195-420f5509197b', 5024, 8275, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How can you do a deep learning in Genetic Algorithm with R?', 8076, '2015-02-03 10:53:27.957', '9f9e703a-bd51-4bad-a99e-c92bf755a18d', 5024, 'edited title', 8276, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('For such questions, I like to go to the [Task Views](http://cran.r-project.org/web/views/) on CRAN, since the packages noted there are, to a degree, pre-vetted by the R community. I''d trust those a tiny bit more than just googling myself.

The [Machine Learning Task View at CRAN](http://cran.r-project.org/web/views/MachineLearning.html) says:

> Packages [rgp](http://cran.r-project.org/web/packages/rgp/index.html)
> and
> [rgenoud](http://cran.r-project.org/web/packages/rgenoud/index.html)
> offer optimization routines based on genetic algorithms. The package
> [Rmalschains](http://cran.r-project.org/web/packages/Rmalschains/index.html)
> implements memetic algorithms with local search chains, which are a
> special type of evolutionary algorithms, combining a steady state
> genetic algorithm with local search for real-valued parameter
> optimization.', 2853, '2015-02-03 11:37:06.760', '9ec044c2-9124-4e82-b227-04d68d0a9a9f', 5025, 8277, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-03 11:49:22.750', '47d4afea-14ef-483e-ba86-064fe29ac89c', 5024, '104', 8278, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Classifiers often return probabilities of belonging to a class. For example in logistic regression the predicted values are the predicted probability of belonging to the non-reference class or $\text{Pr}(Y = 1)$. The discrimination threshold is just the cutoff imposed on the predicted probabilities for assigning observations to each class.', 5103, '2015-02-03 16:00:05.673', '0f9a6769-6856-4c2d-a995-f9810c934557', 5026, 8279, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Amazon Kinesis might be another choice for stream processing, if you don''t want to set up the clusters by yourself.', 8084, '2015-02-03 19:01:59.827', '064fdc57-ffe8-40cd-aa50-dd0cfa0faf01', 5027, 8280, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From what I understand of the objectives of the lambda architecture your point:

> Your batch layer is probably a map reduce job or a HIVE query.

Is not what was intended. The batch layer is not meant to be directly queried against, but rather feeds a serving layer, possibly a simple key-value store, for low latency queries.

![lambda architecture diagram][1]

Check out http://lambda-architecture.net/ for a more full explanation.


  [1]: http://i.stack.imgur.com/jtQ1b.png', 8085, '2015-02-03 19:53:47.233', 'b7735508-d570-4311-8e7a-2f1d7302ca85', 5028, 8281, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('One other data source I didn''t see listed is [The GDELT Project][1]. From the site:

> GDELT Project monitors the world''s broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, counts, themes, sources, and events driving our global society every second of every day, creating a free open platform for computing on the entire world.


  [1]: http://gdeltproject.org/', 8085, '2015-02-03 20:05:59.917', '20f74a4d-9916-4a5f-bb6d-c14b0a0b79e4', 5029, 8282, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As [@Sidhha][1] mentioned GridSearchCV is a great way to estimate parameters. Here is an [example][2] showing how to use it. Good luck.


  [1]: http://datascience.stackexchange.com/users/1131/sidhha
  [2]: http://scikit-learn.org/stable/auto_examples/grid_search_digits.html#example-grid-search-digits-py', 8085, '2015-02-03 20:24:04.800', '03ff0032-22f6-43f0-af9f-8b7a6d355854', 5030, 8283, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('With respect to this [link](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) can anyone please tell me what the phrase "discrimination threshold of binary classifier system" means? I know what a binary classifier is.', 5103, '2015-02-03 22:00:28.777', '57200cae-1286-4884-9131-ccccfd08fb81', 5023, 'improved formatting and grammer', 8284, '5');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-02-03 22:00:28.777', '57200cae-1286-4884-9131-ccccfd08fb81', 5023, 'Proposed by 5103 approved by -1 edit id of 226', 8285, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('With respect to [ROC](http://en.wikipedia.org/wiki/Receiver_operating_characteristic) can anyone please tell me what the phrase "discrimination threshold of binary classifier system" means? I know what a binary classifier is.', 2452, '2015-02-03 22:00:28.777', '313cd822-ce82-4dc6-b0d4-d038b7ab48d0', 5023, 'Improved grammar, wording and formatting.', 8286, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What is a discrimination threshold of binary classifier?', 2452, '2015-02-03 22:00:28.777', '313cd822-ce82-4dc6-b0d4-d038b7ab48d0', 5023, 'Improved grammar, wording and formatting.', 8287, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you have an imbalanced dataset you usually want to make it balanced to begin with, since that will artificially affect your scores.

Now, you want to be measuring precision and recall, since those can capture a bit better the imbalanced dataset biases.

L1 or L2 won''t perform particularly better in a balanced or unbalanced dataset, what you want to do is call elastic nets (which is a combination of the two) and do cross validation over the coefficients of each of the regularizers.

Also, doing grid search is very odd, you are better using just cross validation and see what parameters work better.

They even have ElasticNetCV, which does that part for you (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV)', 8065, '2015-02-03 22:51:57.083', '24cf0de4-5afc-48aa-9c8e-c2aca6fadc87', 5031, 8288, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Just to add a bit.

Like it was mentioned before, if you have a classifier (probabilistic) your output is a probability (a number between 0 and 1), ideally you want to say that everything larger than 0.5 is part of one class and anything less than 0.5 is the other class.

But if you are classifying cancer rates, you are deeply concerned with false negatives (telling some he does not have cancer, when he does) while a false positive (telling someone he does have cancer when he doesn''t) is not as critical. So you might artificially move that threshold from 0.5 to higher or lower values, to change the sensitivity of the model in general.

By doing this, you can generate the ROC plot for different thresholds.', 8065, '2015-02-03 22:56:04.747', 'bed6393a-1080-43c9-97b5-8e0113232d61', 5032, 8289, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''am trying t create a regression based prediction (like booking website): predict the number of clicks for each hotel.
I have to generate a .csv file containing two columns: hotel_id, predicted_number_of_clicks for all hotel_ids.
My first question question is : should I put the Id_hotel as feature in the predictive model. I think that I must to drop it no?
Second question is : how can write in the csv file only this 2 columns "hotel_id", "predicted_number_of_clicks" if I drop hotel_id from the model feature?
Thank you very much for your reply in advance


', 8088, '2015-02-03 23:49:30.013', '6aa91a12-9c81-4df1-8aac-be898e746abc', 5033, 8290, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Format of CSV file', 8088, '2015-02-03 23:49:30.013', '6aa91a12-9c81-4df1-8aac-be898e746abc', 5033, 8291, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<regression><predictive-modeling>', 8088, '2015-02-03 23:49:30.013', '6aa91a12-9c81-4df1-8aac-be898e746abc', 5033, 8292, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Rank players of any given sport', 97, '2015-02-03 23:58:36.380', 'cdf82280-41cb-421e-b7e2-17409c3c40bb', 5021, 'Retagging to relevant terms', 8293, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<predictive-modeling><scoring><ranking><sports>', 97, '2015-02-03 23:58:36.380', 'cdf82280-41cb-421e-b7e2-17409c3c40bb', 5021, 'Retagging to relevant terms', 8294, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-02-03 23:58:36.380', 'cdf82280-41cb-421e-b7e2-17409c3c40bb', 5021, 'Proposed by 97 approved by 2452, 8074 edit id of 223', 8295, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The hotel_id should not be a feature.

Let''s see if I understand you correctly.

At testing time you give your model a whole set of feature values for a particular hotel you are interested in. This hotel has an id, which is known to you.

Your model should be able to take both an id and a set of feature values as input, so it can print both to an output file.

Does that answer your question?', 8075, '2015-02-04 00:16:15.323', '641f0436-fd73-4af8-a7ab-1b4ab58cdc53', 5034, 8296, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''ve found an interesting project With tons of data available. It''s a real data benchmark executed over an industrial valve. This is the website.

[Industrial Actuator Real Data Benchmark Study.][1]


  [1]: http://diag.mchtr.pw.edu.pl/damadics/', 8037, '2015-02-04 02:54:52.773', '08ad6701-42cf-46e6-9964-a586d7cdd1cf', 5037, 8302, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m trying to use a particular cost function (based on doubling rate of wealth) for a classification problem, and the solution works well in MATLAB. See https://github.com/acmyers/compareCostFXs

When I try to do this in Python 2.7.6 I don''t get any errors, but it only returns zeros for the theta values.

Here is the cost function and optimization method I''ve used in Python:


    def costFunctionDRW(theta, X, y):

        # Initialize useful values
        m = len(y)
        # Marginal probability of acceptance
        marg_pA = sum(y)/m
        # Marginal probability of rejection
        marg_pR = 1 - marg_pA

        # =============================================================
        pred = sigmoid(np.dot(X,theta))
        final_wealth_individual = (pred/marg_pA)*y + ((1-pred)/marg_pR)*(1-y)
        final_wealth = np.prod(final_wealth_individual)
        final_wealth = -final_wealth

        return final_wealth

    result = scipy.optimize.fmin(costFunctionDRW, x0=initial_theta, \
                       args=(X_array, y_array), maxiter=1000, disp=False, full_output=True )

Any advice would be much appreciated!', 985, '2015-02-04 04:51:28.063', 'a8ee61ac-a846-47f7-9b11-d03d1338df9a', 5038, 8303, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('minimization with a negative cost function: works in MATLAB, not in Python', 985, '2015-02-04 04:51:28.063', 'a8ee61ac-a846-47f7-9b11-d03d1338df9a', 5038, 8304, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><python>', 985, '2015-02-04 04:51:28.063', 'a8ee61ac-a846-47f7-9b11-d03d1338df9a', 5038, 8305, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If your number of cases is large, you may be running into a problem of numerical underflow in the line

> final_wealth = np.prod(final_wealth_individual)

If each value of `final_wealth_individual` is between 0 and 1, multiplying them all together can lead to a result that is too small to represent as a floating point number, resulting in a value of 0.

To address this issue, take the log of `final_wealth_individual` and add them together instead of multiplying.  Note that this will cause `final_wealth` to be negative, so you will not need to multiply it by -1 as you are currently.

', 182, '2015-02-04 05:33:59.907', 'b2be39cf-2cdf-4146-aeb1-ea89eb64ecd7', 5039, 8306, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m using a set of features, says $X_1, X_2, ..., X_m $, to predict a target value $Y, y_i \in [0.0,1.0]$.

At first, I try to use a linear regression model to do the prediction, but it does not perform well. The root-mean-squared error is about 0.35, which is quite high for prediction of a value from 0 to 1.

Then, I have tried different models, *e.g.,* decision-tree-based regression, random-forest-based regression, gradient boosting tree regression and *etc.* However, all of these models also do not perform well. (RMSE $\approx $0.35, there is not significant difference with linear regression)

I understand there are many possible reasons for this problem, such as: feature selection or choice of model, but maybe more fundamentally, the quality of data set is not good.

My question is: **how can I examine whether it is caused by bad data quality?**

BTW, for the size of data set, there are more than 10K data points, which should be sufficient.', 7867, '2015-02-04 06:10:34.893', 'd55a052f-874d-4b73-8287-ad6b962fef87', 5040, 8307, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to determine whether a bad performance is caused by data quality?', 7867, '2015-02-04 06:10:34.893', 'd55a052f-874d-4b73-8287-ad6b962fef87', 5040, 8308, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 7867, '2015-02-04 06:10:34.893', 'd55a052f-874d-4b73-8287-ad6b962fef87', 5040, 8309, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How many features do you have?

Is quite unlikely that ALL the features are bad. So you could regress with a different number of features.

For example, do one pass with all the features, then take one out (usually X_m) so you have m-1 features. Keep doing this so you can take out uninformative features.

Also, I would recommend you calculating P-Values to see whether your regessors are significative are informative.', 8065, '2015-02-04 06:15:52.873', '9c3b22af-8501-41c1-83b7-b8f17d345ea1', 5041, 8310, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Trying to classify using hotel ID is the same as trying to determine if a student is going to perform well on a test based on their last name. You should get additional things, like number of rooms, amenities, location, staff, etc. Those are informative features that you can use in a classifier', 8065, '2015-02-04 06:18:04.207', 'dfc90fcd-8570-4e94-8ab8-5a936ffd6ddb', 5042, 8311, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First, it sounds like your choice of model selection is a problem here.  Your outputs are binary-valued, not continuous. Specifically you may have a classification problem on your hands rather than a traditional regression problem.  My first recommendation would be to try a simple classification approach such as logistic regression or linear discriminant analysis.

Regarding your suspicions of *bad data*, what would bad data look like in this situation? Do you have reason to suspect that your $X$ values are noisy or that your $y$ values are mislabeled? It is also possible that there is not a strong relationship between any of your features and your targets.  Since your targets are binary, you should look at histograms of each of your features to get a rough sense of the class conditional distributions, i.e. $p(X_1|y=1)$ vs $p(X_1|y=0)$. In general though, you will need to be more specific about what "bad data" means to you.', 182, '2015-02-04 06:22:23.377', '9346ce4c-0c2c-4ecd-95ff-7d3f0527b9bd', 5043, 8312, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m using a set of features, says $X_1, X_2, ..., X_m $, to predict a target value $Y$, which is a continuous value from zero to one.

At first, I try to use a linear regression model to do the prediction, but it does not perform well. The root-mean-squared error is about 0.35, which is quite high for prediction of a value from 0 to 1.

Then, I have tried different models, *e.g.,* decision-tree-based regression, random-forest-based regression, gradient boosting tree regression and *etc.* However, all of these models also do not perform well. (RMSE $\approx $0.35, there is not significant difference with linear regression)

I understand there are many possible reasons for this problem, such as: feature selection or choice of model, but maybe more fundamentally, the quality of data set is not good.

My question is: **how can I examine whether it is caused by bad data quality?**

BTW, for the size of data set, there are more than 10K data points, each of which associated with 105 features.', 7867, '2015-02-04 07:28:15.487', '57a44a34-40e3-4640-baf0-ebe4264f4efc', 5040, 'fix an ambiguous expression', 8313, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m using a set of features, says $X_1, X_2, ..., X_m $, to predict a target value $Y$, which is a continuous value from zero to one.

At first, I try to use a linear regression model to do the prediction, but it does not perform well. The root-mean-squared error is about 0.35, which is quite high for prediction of a value from 0 to 1.

Then, I have tried different models, *e.g.,* decision-tree-based regression, random-forest-based regression, gradient boosting tree regression and *etc.* However, all of these models also do not perform well. (RMSE $\approx $0.35, there is not significant difference with linear regression)

I understand there are many possible reasons for this problem, such as: feature selection or choice of model, but maybe more fundamentally, the quality of data set is not good.

My question is: **how can I examine whether it is caused by bad data quality?**

BTW, for the size of data set, there are more than 10K data points, each of which associated with 105 features.

I have also tried to investigate importance of each feature by using decision-tree-based regression, it turns out that, only one feature (which should not be the most outstanding feature in my knowledge to this problem) have an importance of 0.2, while the rest of them only have an importance less than 0.1.', 7867, '2015-02-04 07:37:14.677', '050ca1ce-88b2-4f45-bac8-f4cebca327dd', 5040, 'add more explanation ', 8314, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When training the model you don''t need to use the hotel id. The model needs to learn from examples. It only needs feature values and number of clicks, so it can learn the relationship between these.

Once you''ve trained your model, you can use it for unseen examples.
These would be hotels for which you have both an id and a set of feature values.
Your model should take the id and the feature values as input, but it should only use the feature values for the prediction. The id should just be kept on the side, so it can print it together with the prediction to the output csv file.

I hope this helps!', 8075, '2015-02-04 09:28:16.910', '7cfd364f-2107-418d-99ff-fb0c260b71fc', 5044, 8315, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Supervised learning should try to ''understand'' what makes a hotel to have more clicks than other. As a consequence learning tries to define which are the characteristics of some given hotels which make them attractive or not. So it uses some kind of similarities, because it is supposed that similar hotels behaves in a similar way.

Now if you restrict the similarity to identity than you learn nothing new because hotels are unique. In fact such kind of learner exists and is called Rote learner, and it consists of one-to-one mapping from inputs to outputs. It is also called memoisation. And this happens if you will add hotel_id in the features. However I think you hope to use that to predict the number of clicks for new hotels (which does have a different hotel_id than any from training set).

On the other hand, in order to use hotel_id to store prediction you only have to save a copy of the original data set. At learning time you have a train data set from which you remove hotel_id, and use that for learning.

At prediction time you make a copy of the data set for later use. From the original data set remove order_id, use that for prediction and get the results. Now the predicted results have the same order of instances as the copied data set. This happens for sure in python (scikit learn), java (weka), R. In fact I am not aware of a system which does not preserve positions.

Now using positions from the copy of the original and prediction you can associate each hotel_id to each prediction with no problem. ', 108, '2015-02-04 11:22:53.063', '128c3c84-5481-4344-9396-4358fc3cd0d6', 5045, 8316, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Your understanding is correct. The point is that equation (8)
$$y_i(<\textbf{w}, \phi_i> + b) - 1 = 0$$
is not exactly an equation, but a system of equations, one for each $i$ index of the support vectors (those for each $0<\alpha_i<C$.

The point is that you cannot compute $b$ during the optimization of the dual problem since it does not matter for optimization, you have to go back and compute $b$ from all the other equations you have (one possible way is (8)).

Vapnick suggestion is to not use only one of those equations, but two of them, specifically one support vector for a negative observation and one for a positive observation. In other words two support vectors which have opposite signs for $y_i$.

Let''s name $A$ the index of one support vector and $B$ the index of a suport vector of opposite side, baiscally you select from the system of equations at (8) only two of them. Evaluate both of them and take the mean.

From:
$$y_A(<\textbf{w},\phi_A>+b)=1$$
$$y_B(<\textbf{w}, \phi_B>+b)=1$$
We get:
$$b_A=\frac{1}{y_A}-<\textbf{w},\phi_A>$$
$$b_B=\frac{1}{y_B}-<\textbf{w},\phi_B>$$
Where $b_A$ and $b_B$ are two estimations, then the mean is
$$b = (b_A+b_B)/2 = -\frac{1}{2}(<\textbf{w},\phi_A>+<\textbf{w},\phi_B>)=-\frac{1}{2}\sum_{i=1}^{n}y_i\alpha_i(<\phi(x_i),\phi(x_A)>+<\phi(x_i),\phi(x_B)>)$$', 108, '2015-02-04 12:51:31.943', '438f5bcf-a53a-4a25-b5e0-f3fd774eb425', 5046, 8317, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<r><algorithms><genetic>', 2853, '2015-02-04 15:46:17.010', '2120cd14-0bc1-403a-957f-2d1f6e4966b3', 5024, 'added r tag', 8320, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-02-04 15:46:17.010', '2120cd14-0bc1-403a-957f-2d1f6e4966b3', 5024, 'Proposed by 2853 approved by 2452, 21 edit id of 224', 8321, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<markov-process><matlab><simulation>', 97, '2015-02-04 15:46:20.233', 'e35f5092-810d-4c94-924c-596fd47e1698', 4979, 'Add new relevant tags', 8322, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-02-04 15:46:20.233', 'e35f5092-810d-4c94-924c-596fd47e1698', 4979, 'Proposed by 97 approved by 2452, 21 edit id of 225', 8323, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am learning about Data Science and I love the Healthcare part. That''s why I have started a blog and my third entry is about using Genetic Algorithm for solving NP-Problems.
This post is https://datasciencecgp.wordpress.com/2015/01/31/the-amazing-genetic-algorithms/

I have some expertise with GA package solving problems like the TSP, but do you know any most powerful R package?

Thanks so much!', 8076, '2015-02-04 16:48:43.630', '1ad5904a-9782-46e5-a399-087c74159325', 5024, 'added 61 characters in body; edited title', 8324, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Is the GA R package the best Genetic Algorithm package?', 8076, '2015-02-04 16:48:43.630', '1ad5904a-9782-46e5-a399-087c74159325', 5024, 'added 61 characters in body; edited title', 8325, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am working on a research project that deals with American military casualties during WWII. Specifically, I am attempting to construct a count of casualties for each service at the county level. There are two sources of data here, each presenting their own challenges.

**1. Army and Air Force data.** The National Archives hosts lists of Army and Air Force servicemen killed in action by state and county. There are .gif images of the report available online. [Here][1] is a sample for several counties in Texas.

I DO NOT need to recover the names or any other information. I simply need to count the number of names (each on its own line, and listed in groups of five) under each County. There are hundreds of these images (50 states - 30-100 for each state).

I have been unable to find an OCR program that can tackle this problem adequately. How would you suggest I approach this challenge? (I have some programming expertise in Python and Java, but would prefer to use any off-the-shelf solutions that may exist).

**2. Navy and Marine Core data.** This data is organized differently. Each state has long lists of casualties with the address of their next of kin. [Here][2] is a sample for Texas again. For these images, I need to BOTH count the number of dead and recover their hometown, which is typically the last word in each entry. I can then match these hometowns to counties and merge with database 1.

Again, the usual OCR programs have proved inadequate. Any help on this (admittedly more difficult) problem would be very much appreciated.

Thank you in advance experts!

  [1]: http://media.nara.gov/media/images/29/19/29-1891a.gif
  [2]: http://media.nara.gov/media/images/27/31/27-3023a.gif', 8102, '2015-02-04 22:03:48.507', 'ae4afbeb-3698-45ad-8bfa-0203433c84a1', 5047, 8326, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('OCR / Text Recognition and Recovery Problem', 8102, '2015-02-04 22:03:48.507', 'ae4afbeb-3698-45ad-8bfa-0203433c84a1', 5047, 8327, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><text-mining><data-cleaning><processing>', 8102, '2015-02-04 22:03:48.507', 'ae4afbeb-3698-45ad-8bfa-0203433c84a1', 5047, 8328, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In what way have the usual OCR programs proved inadequate?
Do you have some example output that you find you can''t work with?

I can see how the columns complicate things.

I''d say for data set 1:
OCR the images, then read the files line by line and match on for instance a sequence of at least five numbers. So you get 0, 1, 2 or 3 per row.
You may miss a couple due to the OCR accidentally recognizing a number as a letter for instance, but I expect this to work reasonably well.
How precise do you have to be?

Data set 2 seems more difficult.
Maybe counting can be done by matching on capitalized sequences followed by a comma.
Placename... very tricky. Once again, do you have some OCR output we can look at?

', 8075, '2015-02-04 23:58:53.323', '260588f7-8324-4b41-b62d-4c3821335344', 5048, 8329, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m surprised one has mention this as it seems fairly obvious: kaggle.com consistently has new and very interesting datasets. Information is considered an asset, so often companies don''t want to release that data (plus privacy concerns). Kaggle gives you data and they hope you solve business problems in with it in exchange.', 8104, '2015-02-05 00:49:11.713', '493a6f2f-6c76-48f1-836d-57b35e25bfcc', 5049, 8330, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m doing some work trying to extract commonly occurring words from a set of human classified documents and had a couple questions for anyone who might know something about NLP or statistical analysis of text.

We have a set of a bunch of documents, and users have classified them as either good or bad. What I''d like to do is figure out what words are common to the good documents, but not necessarily the other ones.

I could, for example, use the (frequency within good documents / total frequency) which would essentially normalize the effect of a word being generally common. This, unfortunately, gives very high precedence to words that occur in only a few good documents & not at all in the other documents. I could add some kind of minimum threshold for # of occurrences in good docs before evaluating the total frequency, but it seems kind of hacky.

Does anyone know what the best practice equation or model to use in this case is? I''ve done a lot of searching and found a lot of references to TF-IDF but that seems more applicable for assessing the value of a term on a single document against the whole set of docs. Here I''m dealing with a set of docs that is a subset of the larger collection.

In other words, I''d like to identify which words are uniquely or more important to the class of good documents.', 8105, '2015-02-05 01:29:00.997', '8d34c433-74ca-4e85-acec-8b7d1982b789', 5050, 8331, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Word Frequency Analysis of Document Sets', 8105, '2015-02-05 01:29:00.997', '8d34c433-74ca-4e85-acec-8b7d1982b789', 5050, 8332, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><classification><statistics>', 8105, '2015-02-05 01:29:00.997', '8d34c433-74ca-4e85-acec-8b7d1982b789', 5050, 8333, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Your (frequency within good documents / total frequency) seems reasonable to me.
It could be that most words that occur in many good documents, simply also occur in many bad documents.

How about you make a list of all the words appearing in the good documents.
Then you count their appearance in the good documents and their appearance in the bad documents and compare those two numbers.
The words that appear more often in the good ones, with a difference higher than a certain threshold are the ones of interest to you (if they exist).', 8075, '2015-02-05 02:25:51.627', '2f9ee7e7-c89f-4645-9e52-1299c5a20768', 5051, 8334, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m new to NLP. I have few doubts about PCFG parser (NLTK).
My understanding is that PCFG parser will return most probable parse tree. So if I''m parsing one sentence with PCFG parser, I''ll be getting one parse tree. Is my assumption right?

Moving further, pcfg parser is trained using corpus consisting of 1000 sentences, will i be getting 1000 parse tree (1 per sentence) or only one parse tree? If it is different for each sentence, are the production rules for one parse tree independent of another parse tree?

Why NLTK PCFG parser excepts input in Penn treebank format? (I mean Penn treebank is also a parsed tree, isn''t it?)

Do we have to define production rules and assign its probability explicitly? If no, how to do it programmatically?
', 6465, '2015-02-05 05:44:05.497', '57f43a9f-caf8-4e97-86de-4b5f66344957', 5052, 8335, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How pcfg parser actually works?', 6465, '2015-02-05 05:44:05.497', '57f43a9f-caf8-4e97-86de-4b5f66344957', 5052, 8336, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><parsing>', 6465, '2015-02-05 05:44:05.497', '57f43a9f-caf8-4e97-86de-4b5f66344957', 5052, 8337, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<python><nlp><parsing>', 6465, '2015-02-05 06:17:44.940', 'dbd3a6a4-3fb9-4f19-84d2-959379ec3025', 5052, 'edited tags', 8338, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I guess what you are looking for is Differential Word Usage. This method takes two text corpora as input and you can get the list of words which are being used more in one text corpus over the other.

Basically what you need to do is to build a common Term Document Matrix for the corpora you are using and then divide this TDM into two TDMs such that all the document columns from corpus 1 fall in one TDM and all the documents columns form corpus 2 fall in the second TDM.  For example, you have 2 corpora, the first one containing 10 documents and the second one containing 15 documents. You first, combine both these corpora and form 25 document corpus and then form the TDM, where terms become the rows (let''s say there are 300 terms) and the 25 documents become the 25 columns. Here the first 10 columns represent the documents of first corpus and the remaining 15 belong to the second corpus. So, you divide this TDM of dimensions `300 x 25` to two TDMs of dimensions `300 x 10` and `300 x 15`. Then you can use `Chi-square difference` over these TDMs to determine which words are occurring more in one corpus than the other.

A wonderful example has been given regarding this approach by Vik in his blog using Wikileaks corpus and `R` here: http://www.vikparuchuri.com/blog/finding-word-use-patterns-in-wikileaks/', 8059, '2015-02-05 06:54:16.023', '189f4b7b-fb4f-4196-a6de-9f6326a9bec0', 5053, 8339, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are many algorithm to do classification: Naïve Bayes, logistic regression, SVM, decision tree..etc. My suggestion is to try Naïve Bayes first by calculating below probabilities which a new document belongs to $class_{good}$ or $class_{bad}$.

$$
P(Class_{good} \vert document_{new}) = \frac{P(document_{new} \vert Class_{good}) \cdot P(Class_{good}) }{P(document_{new})}
$$


$$
P(Class_{bad} \vert document_{new}) = \frac{P(document_{new} \vert Class_{bad}) \cdot P(Class_{bad}) }{P(document_{new})}
$$

And generally, when we are doing text mining questions, we will do several preprocess on one document:

 - Tokenization(1-gram/bigram/...etc)
 - remove stop words (''a'', ''the'' ''at'', ... etc)
 - Stemming: transforms a word into its root form. (studied => study)

My suggestion is to do above preprocesses and try more features not just the words in one document, if there are some metadataes.', 1003, '2015-02-05 07:26:04.347', 'dea06973-9c4f-4064-9b7a-5a450215b682', 5054, 8340, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There are many algorithm to do classification: Naïve Bayes, logistic regression, SVM, decision tree..etc. My suggestion is to try Naïve Bayes first by calculating below probabilities which a new document belongs to $class_{good}$ or $class_{bad}$. (https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)

$$
P(Class_{good} \vert document_{new}) = \frac{P(document_{new} \vert Class_{good}) \cdot P(Class_{good}) }{P(document_{new})}
$$


$$
P(Class_{bad} \vert document_{new}) = \frac{P(document_{new} \vert Class_{bad}) \cdot P(Class_{bad}) }{P(document_{new})}
$$

And generally, when we are doing text mining questions, we will do several preprocess on one document:

 - Tokenization(1-gram/bigram/...etc)
 - remove stop words (''a'', ''the'' ''at'', ... etc)
 - Stemming: transforms a word into its root form. (studied => study)

My suggestion is to do above preprocesses and try more features not just the words in one document, if there are some metadataes.', 1003, '2015-02-05 07:32:45.207', 'a9586316-f6e0-4c7e-b9ed-e39ad1dd968a', 5054, 'added 58 characters in body', 8341, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Example Data**

I have a dataset (in `R` as a `data frame`) of race results for athletes.

    athlete racedistance time    location tracktype       date    coach
    A          100       10.0       UK     typeA       2014-01-01  carlos
    A          200       20.0       US     typeB       2014-02-01  carla
    A          100        9.5      AUS     typeC       2014-03-01  chris
    B          100       11.0       UK     typeA       2014-01-01  carla
    B          200       21.0       US     typeB       2014-02-01  carlos
    B          400       61.0      AUS     typeC       2014-03-01  carla
    B          100       10.5      GER     typeA       2014-04-01  clive
    C          100        9.5       UK     typeA       2014-01-01  clive
    C          200       21.5       US     typeB       2014-02-01  chris


**Question**

Is there an appropriate machine learning algorithm or method that can use the previous results of each athlete as a feature, when trying to predict the `time` for an athlete in a future race?

For example, `athlete A` has three races, with one month rest between them. In the third race he performs slightly better than the first race over the same distance.

Can an algorithm learn that the second race had an effect on the athlete, which meant he performed better in the third race?

From what I''ve read on the subject and the training examples I''ve completed it would appear that each ''row'' of data should be independent, is this the case for all ML algorithms? Is there another prediction technique I should be considering to solve this type of problem?



', 6683, '2015-02-05 07:41:10.443', '95fcb1a3-13d8-4955-8a34-d47c444ab154', 5055, 8342, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('machine learning on athlete performances to predict the time in a future race', 6683, '2015-02-05 07:41:10.443', '95fcb1a3-13d8-4955-8a34-d47c444ab154', 5055, 8343, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><predictive-modeling>', 6683, '2015-02-05 07:41:10.443', '95fcb1a3-13d8-4955-8a34-d47c444ab154', 5055, 8344, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('By working with your features you could make the ML algorithm (maybe regression or SVR or whatever) to learn this fact (that sequental races are increasing the performance of athlete). To do this you may want to drop out the date column and introduce some new column, maybe ''race number'' with 0 for first race, 1 for second, 2 for third etc.

In such case regression model will be able to learn what you say ''that the second race had an effect on the athlete, which meant he performed better in the third race''. It is all about feature selection.', 7969, '2015-02-05 10:02:08.827', '6850255c-5145-40f9-98e3-7347003c4a60', 5056, 8345, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can anybody tell how the index table will be generated to place the patches from the source texture to perform texture synthesis?![Index table to paste the source patches from source texture into composition image][1]

How the index table is generated in the following image?


  [1]: http://i.stack.imgur.com/AtDT3.png', 7873, '2015-02-05 14:39:25.573', 'a7a1adf4-b678-4e6b-bff9-ceb22e21d7ec', 5057, 8346, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Image oriented index generation', 7873, '2015-02-05 14:39:25.573', 'a7a1adf4-b678-4e6b-bff9-ceb22e21d7ec', 5057, 8347, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 7873, '2015-02-05 14:39:25.573', 'a7a1adf4-b678-4e6b-bff9-ceb22e21d7ec', 5057, 8348, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Ok, in this case, time is your dependent variables, and all the other ones are your features.

You should use linear regression (since more complex stuff needs more expertise), any machine learning library has that implemented.

Do not use the date as a feature, those are usually lousy estimators.', 8065, '2015-02-05 17:22:04.123', '2bfa7aae-dd55-46a2-8688-09ba42b44f4e', 5058, 8349, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think I''ve worked something out. Basically I''m looking for an approach that works in a map/reduce type environment and I think this approach does it.

So,

 - suppose I have b bands of r rows and I want to add another AND stage, say another c ANDs.
 - so instead of b * r bits I need hashes of b * r * c bits
 - and I run my previous procedure c times, each time on b * r bits
 - If x and y are found to be a candidate pair by any of these procedures it emits a key value pair ((x, y), 1), with the tuple of IDs (x,y) as the key and the value 1
 - At the end of the c procedures I group these pairs by key and sum
 - Any pair (x,y) with a sum equal to c was a candidate pair in each of the c rounds, and so is a candidate pair of the entire procedure.

So now I have a workable solution, and all I need to do is work out whether using 3 steps like this will actually help me get a better result with fewer overall hash bits or better overall performance...', 8030, '2015-02-05 20:40:51.363', '02a2d16b-0655-4f93-ac66-b62f783f9610', 5060, 8353, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How much of a background in programming is necessary to become a data scientist? I once knew some Java, but that was close to 10 years ago.  Assuming I can learn a language to get into data analytics.... what language do you recommend?  ', 8123, '2015-02-05 21:18:01.527', 'a362456e-3dca-42e1-ba25-9578f5f6243c', 5061, 8354, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I know nothing about programming, can I become a data scientist?', 8123, '2015-02-05 21:18:01.527', 'a362456e-3dca-42e1-ba25-9578f5f6243c', 5061, 8355, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><beginner>', 8123, '2015-02-05 21:18:01.527', 'a362456e-3dca-42e1-ba25-9578f5f6243c', 5061, 8356, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Based on this infographic and other things I''ve read, it sounds like you need to know some coding to be a true data scientist. http://blog.datacamp.com/how-to-become-a-data-scientist/ But you could still be a data **analyst** without compsci - basically a statistician. ', 8126, '2015-02-05 23:22:08.800', '5960afb1-0865-402c-980c-b90f34d938e8', 5062, 8357, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The R language is the best place to start. Grab some open datasets, and start programming with r. R has many different analytical functions that you can learn a lot with it.', 8005, '2015-02-05 23:23:04.180', '3828588e-3004-45db-b3e4-4b1498c583bc', 5063, 8358, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am not certain, if this is the right place to ask the following question. I am looking for some practical scenarios in social networks where the following information propagation model can arise:

![A toy example of the desired information propagation model][1]


  [1]: http://i.stack.imgur.com/mERQh.jpg

Basically, I have a source node and some information propagating radially from it and each recipient receives the information from a single sender.', 8127, '2015-02-05 23:57:05.757', '2cac50cb-dd61-4042-88b8-43f120a2cc82', 5064, 8359, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where in practice can we see following information propagation model', 8127, '2015-02-05 23:57:05.757', '2cac50cb-dd61-4042-88b8-43f120a2cc82', 5064, 8360, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<social-network-analysis>', 8127, '2015-02-05 23:57:05.757', '2cac50cb-dd61-4042-88b8-43f120a2cc82', 5064, 8361, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data Scientists code every day. However, just because you don''t have background doesn''t mean you can''t pick it up! The level of programming you need to know to start doing Data Science isn''t very high, but you will at least need:

 1. the logical mindset to phrase the solution to your problem in
    procedural code
 2. to know the programming language, functions, and libraries needed in this field.

1st point is the most difficult of the two. Hopefully you have taken enough math and physics by now to wire your mind to think programmatically. If so then yes, you absolutely can learn a language! There are guides out there that teach out the syntax and functions. For example:

 - R - http://tryr.codeschool.com/
 - General Python - http://www.codecademy.com/en/tracks/python
 - DataSci Python - https://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-python

Personally I would recommend Python first. To me the language places more emphasis on readability and cleanliness, making it a great first language. It''s also a general purpose language so it''s good to know. I did start with R though and it''s also good, but is more function-over-form IMO. Try both out and see which feels best first, since you''ll likely have to pick up both if you delve into this field anyways.', 525, '2015-02-06 00:05:02.313', '48788086-b785-4e67-969e-e18f3207cf85', 5065, 8362, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It sounds to me like you have a binary classification problem (classifying good vs. bad documents for some definitions of good and bad) and the words are being used as features or "signals" for what predicts good vs. bad documents. One thing you might try is to measure some type of correlation statistic between unigrams and each class you''re interested in. This preserves your requirement of measuring occurrences of words given a target class over *groups* of documents.

So, to be a bit more concrete, you could split your documents into two sets (good and bad), and then tokenize your documents to obtain individual terms. From here you could really choose whichever term weighting scheme you like (TF, TF normalized against the length of the document, TF-IDF) and measure your correlation statistic between *all these unigrams* and the class of interest. You could then produce a ranking based on the correlation coefficients for each term, and take the top-*k* terms. Some correlation statistics you might try could be [Chi-squared][1] (which would measure "lack of independence" between terms and a class). There''s also a nice implementation of Chi-squared test for feature selection in [Python''s Scikit-Learn][2] machine learning library that may be a place to start for this task. Hopefully, that helps!


  [1]: http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test
  [2]: http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection', 4897, '2015-02-06 00:47:49.187', '4f3645f8-fd9d-4514-a6ee-fc55e2471d49', 5066, 8363, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I think that it is impossible to answer this question *comprehensively*, at least for the following **reasons**:

 - big data frameworks have different **goals** and target different **knowledge domains**, so the comparison simply doesn''t make much sense;
 - most big data frameworks (and other programming frameworks, for that matter) have *multiple interfaces*, and frequently those **sets of interfaces** are significantly different (the *intersection* is small), so there is a risk of comparing apples and oranges;
 - trying to compare anything (in this case, interfaces), using *simplicity* as a criterion, involves a significant amount of **subjectivity** - what one person *perceive* as very simple, another person might find quite complex;
 - the **variety** and the **number** of *big data frameworks* is mind-boggling (for example, see https://github.com/onurakpolat/awesome-bigdata); the same applies to a related topic of *machine learning frameworks* (for example, see https://github.com/josephmisiti/awesome-machine-learning);
 - corollary from the points above: a **comprehensive** comparison (considering all the above-mentioned issues) would go far beyond the **scope** of a single answer on this site, in volume and effort - it would be more like a long research paper, a book chapter or even a book.', 2452, '2015-02-06 01:08:32.770', '0b20e181-9b58-479f-9752-bea9d1e21314', 5002, 'Fixed an incorrect link.', 8364, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m new to NLP. I have a few doubts about PCFG parser (NLTK). My understanding is that PCFG parser will return most probable parse tree. So if I''m parsing one sentence with PCFG parser, I''ll be getting one parse tree. Is my assumption right?

Moving further, PCFG parser is trained using corpus consisting of 1000 sentences, will I be getting 1000 parse tree (1 per sentence) or only one parse tree? If it is different for each sentence, are the production rules for one parse tree independent of another parse tree?

Why does the NLTK PCFG parser expect input in Penn treebank format? (I mean Penn treebank is also a parsed tree, isn''t it?)

Do we have to define production rules and assign its probability explicitly? If no, how to do it programmatically?', 6465, '2015-02-06 04:59:47.690', 'dd4ac373-fa16-4331-b6a8-fe7ea1bdfd99', 5052, 'added 7 characters in body; edited title', 8365, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How do PCFG parsers actually work?', 6465, '2015-02-06 04:59:47.690', 'dd4ac373-fa16-4331-b6a8-fe7ea1bdfd99', 5052, 'added 7 characters in body; edited title', 8366, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Johns Hopkins University as a set of course on Coursea that is gear on Data Science. Here is the link to the classes [https://www.coursera.org/specialization/jhudatascience/1?utm_medium=courseDescripTop][1]. You can also take the classes for free.

This is a set of 9 classes that would give you a good foundation to build-on and to start a career.


  [1]: https://www.coursera.org/specialization/jhudatascience/1?utm_medium=courseDescripTop', 3588, '2015-02-06 05:51:56.090', '47654198-0b74-4c25-9bdb-05e816a53e46', 5067, 8367, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data science, being a new term, covers a broad spectrum of jobs. At one end you are expected to write production code. At the other end you do statistics in packaged software. They also call such people statisticians or analysts. So decide what you enjoy doing before you leap. If you just want to analyze data, you could definitely get by with R or python as long as you''re mathematically proficient. I find that in these kind of jobs, your communication and social skills matter too, since you have to explain the data to executives and the like.', 381, '2015-02-06 06:11:35.397', '377a3f54-ce4c-43ae-bbc4-5a34f9656765', 5068, 8368, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('First of all, the fact that you have known some Java, even ten years ago, already means that you don''t "know nothing about programming" (I suggest you update the title of your question to reflect that - change "nothing" to "a little"). I''d like to make several points, which I hope will be useful to you.

- In terms of the level of *programming proficiency*, which is expected (needed) for a data scientist, the following popular **definition** says it all:

> A *data scientist* is someone who is better at statistics than any
> software engineer and better at software engineering than any
> statistician.

- Another perspective on the role of programming abilities in a data scientist''s skill set can be found in a popular visual representation of data science, using Venn diagrams. The original data science Venn diagram was presented by Drew Conway (http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram):

![enter image description here][1]

- Since its original introduction, the original diagram was modified by various people for various reasons. The two interesting **adaptations** are for data science in the social sciences domain (http://www.datascienceassn.org/content/fourth-bubble-data-science-venn-diagram-social-sciences), as well as data science Venn diagram V2.0, where data science is represented not as an intersection of knowledge domains, but as their union (http://www.anlytcs.com/2014/01/data-science-venn-diagram-v20.html). Another very interesting and useful visual perspective of data science skill set, also based on Venn diagram, is the following Gartner''s diagram, mapping business intelligence/analytics'' knowledge domains to specific skills:

![enter image description here][2]

- An alternative perspective for a data scientist''s skill set and domain knowledge is a **taxonomy of data scientists**, such as [this taxonomy](http://www.datasciencecentral.com/profiles/blogs/six-categories-of-data-scientists), which classifies data scientists, according to their *focus* (or the *strongest skill set*): mathematics, data engineering, machine learning, business, software engineering, visualization, spacial data (GIS) or others.

- If you''re curious about the meaning of the **"Danger Zone"** in the original data science Venn diagram, [this Quora discussion](http://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), containing, among other nice answers, also an answer by the original diagram''s author, can be very helpful.

- If you''re interested in learning about a range of **skills** and **knowledge domains**, useful for a data scientist, check this open source *curriculum* for learning data science: http://datasciencemasters.org, or on GitHub: https://github.com/datasciencemasters/go. Of course, popular and research papers, lectures on YouTube, MOOC courses, online and offline bootcamps as well as a wealth of other resources is only an Internet search away.

- Finally, a note on **programming languages** for data science. I think that it is important to understand that this aspect is really of secondary importance. The **focus** should be on two words, which the term "data science" consists of: *data* and *science*. **Focus on data** means that it is important to think about data science (or BI, or analytics) tasks in terms of the corresponding *domain knowledge* as well as to pay attention to *data quality* and *representativeness*. **Focus on science** means adhering to *scientific approaches* to data collection and analysis, of which *reproducibility* plays an important role. A *programming language* for data science is just a **tool** and, therefore, should be chosen to match the task at hand. Python and R represent very good and the most popular programming languages and environments for a data scientist, however, you should be aware of other options (*tool set*).

  [1]: http://i.stack.imgur.com/ZwWt2.png
  [2]: http://i.stack.imgur.com/XBbDM.png', 2452, '2015-02-06 09:06:02.607', '0bbf3208-4686-4fd1-aed0-83088943827f', 5070, 8372, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('First of all, the fact that you have known some Java, even ten years ago, already means that you don''t "know nothing about programming" (I suggest you update the title of your question to reflect that - change "nothing" to "a little"). I''d like to make several points, which I hope will be useful to you.

- In terms of the level of *programming proficiency*, which is expected (needed) for a data scientist, the following popular **definition** says it all:

> A *data scientist* is someone who is better at statistics than any
> software engineer and better at software engineering than any
> statistician.

- Another perspective on the role of programming abilities in a data scientist''s skill set can be found in a popular visual representation of data science, using Venn diagrams. The original **data science Venn diagram** was presented by data scientist Drew Conway (see [this blog post](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram)):

![enter image description here][1]

- Since its original introduction, the original diagram was modified by various people for various reasons. The two interesting **adaptations** are for data science in the social sciences domain (http://www.datascienceassn.org/content/fourth-bubble-data-science-venn-diagram-social-sciences), as well as data science Venn diagram V2.0, where data science is represented not as an intersection of knowledge domains, but as their union (http://www.anlytcs.com/2014/01/data-science-venn-diagram-v20.html). Another very interesting and useful visual perspective of data science skill set, also based on Venn diagram, is the following Gartner''s diagram, **mapping** specific *skills* to business intelligence (BI) or business analytics *knowledge domains*:

![enter image description here][2]

- An alternative perspective for a data scientist''s skill set and domain knowledge is a **taxonomy of data scientists**, such as [this taxonomy](http://www.datasciencecentral.com/profiles/blogs/six-categories-of-data-scientists), which classifies data scientists, according to their *focus* (or the *strongest skill set*): mathematics, data engineering, machine learning, business, software engineering, visualization, spacial data (GIS) or others.

- If you''re curious about the meaning of the **"Danger Zone"** in the original data science Venn diagram, [this Quora discussion](http://www.quora.com/In-the-data-science-venn-diagram-why-is-the-common-region-of-Hacking-Skills-and-Substantive-Expertise-considered-as-danger-zone), containing, among other nice answers, also an answer by the original diagram''s author, can be very helpful.

- If you''re interested in learning about a range of **skills** and **knowledge domains**, useful for a data scientist, check this open source *curriculum* for learning data science: http://datasciencemasters.org, or on GitHub: https://github.com/datasciencemasters/go. Of course, popular and research papers, lectures on YouTube, MOOC courses, online and offline bootcamps as well as a wealth of other resources is only an Internet search away.

- Finally, a note on **programming languages** for data science. I think that it is important to understand that this aspect is really of secondary importance. The **focus** should be on two words, which the term "data science" consists of: *data* and *science*. **Focus on data** means that it is important to think about data science (or BI, or analytics) tasks in terms of the corresponding *domain knowledge* as well as to pay attention to *data quality* and *representativeness*. **Focus on science** means adhering to *scientific approaches* to data collection and analysis, of which *reproducibility* plays an important role. A *programming language* for data science is just a **tool** and, therefore, should be chosen to match the task at hand. Python and R represent very good and the most popular programming languages and environments for a data scientist, however, you should be aware of other options (*tool set*).

  [1]: http://i.stack.imgur.com/ZwWt2.png
  [2]: http://i.stack.imgur.com/XBbDM.png', 2452, '2015-02-06 09:22:15.570', 'd96e63e4-0991-4e8e-aef7-90650ddc3572', 5070, 'Improved wording and formatting.', 8373, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The problem is with division on python 2.x.  In python 2.x, division involving two integers produces an integer result.  So `1/2==0`.  Python 3.x does not have this problem, `1/2==.5`.

There are two ways to avoid this.  First, you can always convert one value you a float.  So `1./2==0.5` and `1/2.==0.5`.  However, you have to remember to do this everywhere, and if you forget it can lead to hard-to-find errors.

The more reliable method is to always put this at the top of your code: `from __future__ import division`.  This will switch python 2.x to the python 3 behavior, so `1/2==.5`.  In python 3.x it does nothing, so it also makes your code python 3.x compatible in this regard.', 8110, '2015-02-06 09:41:54.033', '42bd80ce-5464-46f3-aea0-ada5db8c4b8d', 5071, 8374, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to build a chatbot that serves as a first line customer support on a  retail website. I have a large log of chat sessions between customers and support professionals that I can use. I am wondering what is the best way to go about building this chatbot.

Here are some ideas I have looked into;

 - The first thing I did was to breakdown the chat logs into Q&A pairs. Treat each question as a document, compute a term-document matrix and use it to retrieve the most similar question when a customer asks sometihng. The idea was to then simply pick the answer given to the question by the human (with some modifications). However, this gives really abysmal results and does not match to previous questions very well. Even if I get this approach to work well, we would be limited to the questions that already been asked.
 - Another thing I thought about was to modify something like ALICE and write some AIML for customer support related QA. However, that would take a lot of very precise AIML writint to work well. This solution would not be able to scale to other languages, which is something I want to do.
 - Another idea I have is to try and cluster the answers given by the customer support staff (they are more likely to be aligned compared to the questions asked by customers). This would give me a sense of what questions are similar. Then I can use something like LSA on the questions and fall back to the first approach.
 - Yet another way is to build an ontology of the domain specific knowledge and given a question decide which category the question falls into. The questions from the chat logs can then be mapped to the ontology and I could train classifiers for mapping questions to different knowledge areas. Once I reach the leaf nodes, I can give back pre-defined answers. However, my reservation is that mapping chat logs to the ontology would be very tedious. Is there a good way to map the existing QA to the ontology?', 8145, '2015-02-06 17:52:05.363', 'd999c78e-4e00-44b6-ba9f-4804a0b6bd1d', 5073, 8379, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Question and Answer Chatbot for Customer Support', 8145, '2015-02-06 17:52:05.363', 'd999c78e-4e00-44b6-ba9f-4804a0b6bd1d', 5073, 8380, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><nlp>', 8145, '2015-02-06 17:52:05.363', 'd999c78e-4e00-44b6-ba9f-4804a0b6bd1d', 5073, 8381, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have 2 data files: the first one is a database, potentially very large; the second one contains queries I want to answer. My program pipeline is processing the database to get some information first, and then use that same information to answer the queries. Although the number of queries is not big, processing each query takes a long time. So I want to give each worker some queries to answer, then combine all the answers together into one.

This sounds like a MapReduce job. But to answer each query, the worker also needs to use the processed information from the database, and I''m not sure how to do this with MapReduce. I''m new to parallel programming and just heard of MPI and Spark.

Can you help me to choose an appropriate one?

Please ask if the description is not clear.', 8146, '2015-02-06 18:13:16.210', 'f60fb03e-4707-45de-9325-6f4fb74e72da', 5074, 8382, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('MPI, MapReduce, or Spark for complex datasets and processing', 8146, '2015-02-06 18:13:16.210', 'f60fb03e-4707-45de-9325-6f4fb74e72da', 5074, 8383, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<map-reduce><parallel>', 8146, '2015-02-06 18:13:16.210', 'f60fb03e-4707-45de-9325-6f4fb74e72da', 5074, 8384, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I once knew some Java, but that was close to 10 years ago.  Assuming I can learn a language to get into data analytics.... what language do you recommend?  ', 41, '2015-02-06 19:45:21.850', '9d97b66c-0b0d-4cbb-9970-9378bf86cfe0', 5061, 'made the title of the question slightly more objective', 8385, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How much of a background in programming is necessary to become a data scientist?', 41, '2015-02-06 19:45:21.850', '9d97b66c-0b0d-4cbb-9970-9378bf86cfe0', 5061, 'made the title of the question slightly more objective', 8386, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Slater''s condition states that there exists an x such that the constraints are strictly feasible. The KKT conditions are always sufficient, but are also necessary when Slate''s condition holds. Why does the condition hold for the SVM optimisation problem?', 8151, '2015-02-06 22:30:53.073', '39929c33-88fb-4032-a86f-0402f9b68c64', 5075, 8387, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why does Slater''s condition hold for the SVM quadratic programming problem?', 8151, '2015-02-06 22:30:53.073', '39929c33-88fb-4032-a86f-0402f9b68c64', 5075, 8388, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><svm>', 8151, '2015-02-06 22:30:53.073', '39929c33-88fb-4032-a86f-0402f9b68c64', 5075, 8389, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Slater''s condition is that there exists an x such that the optimisation constraints are strictly feasible. The KKT conditions are always sufficient, but are also necessary when Slate''s condition holds. Why does the condition hold for the SVM optimisation problem?', 8151, '2015-02-06 23:01:59.673', '78bc2931-d0ae-4a22-b80e-6e03700a2769', 5075, 'added 9 characters in body', 8390, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-07 00:38:20.700', 'e9a6ba8a-12e3-4992-94a6-5aca22498693', 5061, '103', 8394, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-07 00:39:09.683', 'e5303578-fb09-44c2-9002-05ab5f73c87a', 5024, 8395, '11');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I understand your question correctly. What you want to do is

 1. Write a sql query against to a database to get the processed/aggregated results first.
 2. Execute several queries in parallel.

The first thing first, if you want to run MapReduce programs on your data, you need to distribute your data. In addition, MapReduce doesn''t run your queries to different nodes separately. If your $query_{A}$ runs on $data_{A}$, what we do here is to divide $data_{A}$ to different nodes and run $query_{A}$ on these nodes on $PartOfData_{A}$ in parallel. By the way, you can use Sqoop to migrate the data from your DB to HDFS(it is used to store data separately).

Here are steps for you:

 1. migrate all/part of your data to HDFS
 2. transform your queries to MapReduce version program

', 1003, '2015-02-07 02:41:06.783', '63ee29ec-30e5-4544-ab56-9b0c6be4b46c', 5077, 8396, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The reason your OCR program is not recognizing anything is that the font size is very small. If you resize your image to increase its size, you will obtain better results. For example, using ImageMagick to apply a fixed threshold to remove the background on your first image and increase its size:

`convert -density 500 -threshold 40% 29-1891a.gif -resize 250% output.tiff`

After this, tesseract does a reasonable job:

`tesseract output.tiff output test.config`

I included a configuration file `test.config` to tesseract in order to restrict the permitted characters. In this way, we don''t obtain mistaken unicode characters in our text.

test.config

    tessedit_char_whitelist ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-

This is the result:

    HOWARD COUNTY 10011607

    NALL JOHN E 0-686196 1 LT F05 DALE MARVIN E 30037267 PVT DON SANSJNG DAVID 10505969 PFC
    NIXSON EONOND R 10077072 CPL KIA DANIEL NILBORN N 30637064 PVT FOB SAUNDER6 NEWTON L 0-401709 CAPT
    NOBLES STEPNEN E 30502791 PVT 0N0 DAVIS IR D 30433764 PVT 0N0 SNELTON CHARLES R 14001742 VT
    OSBURN SILLY F 02008518 2 LT DNB BAVIS JINKIE L 18214208 5 SC KIA SHELTON CHARLIE 33423953 7335
    PELTON J L 30105473 5 SC RNA 0 DEAN CHARLES 02066314 3 LT 0N0 SMERAN CARLTON A JR 0-510441 2 LT
    GUEZADA FRANK 5 10014939 PFC KIA IDIXON JOHN N 6202114 PVT 00R SIBLEV JESSE 30037241 PFC
    RAMIREZ JOSE T 30347914 PPC KIA DREADIN RAYMOND 6950964 PVT KIA SLATER JERONE E JR 0-605266 1 LT
    OSE THO S 0 30345100 PPC NTA - DUNN GARLAND J 30037202 PVT 0N0 SMITH 00 SE 30509104 PPC
    R035 ORREN C 16015376 PVT ONE 1 ELDER GERALD P 0-387706 2 LT KIA SNITN JASPER 7 33037355 7 SC
    RUTLEDCE CARL R 38107116 PVTELPNAA JEARHER MARVIN R 30279097 CPL DON SMITN JERALO D 10005000 CPL
    SCUDDAY BERNIE L 0-603906 1 LT KIA - FINDLEY PAUL A 50536010 PFC OMB SMITH NELDON A 5550735 PFC
    SNITH ROBERT L 10015524 507 ONE- ELINC ROY T I 0-724730 CAPT F05 S THEY 4295431 5 5
    SNITH TRAVIS L 30670530 PVT IDUN FORD HERRELL E 0-665672 CAPT E00 STEPN NSON JESSIE P 10006200 PVT
    SNEED ROY A 13076927 S 80 ONE GARRETT TOMMY 30220694 PFC KIA SULLIVAN 31 01525125 2 L
    SOUTN CARL 0 JR 30343379 PVT OMB CATLOR R T 20012665 SOT KIA SNINDELL VERL O 30037252 7005
    STEVENS JANESCO 10015557 PVT DNR CLASSCOCN CNARLES J 6571000 5 SO ONO TAYLOR ARTHUR V 33531479 PFC
    STENA D JO 0-723036 2 LT POL 7 00 A 10007027 PVT DOM ITOHN E N A JR 30035251 5 SO
    UTTON ARVI 6360431 AV C 0N6 COSSETT JANES N 30436055 CPL KIAI ALKER J NEE H 30117597 SOT
    IALBOTT CHARLES 9 38342083 PFC KIA GRESN J 18126923 5 SO KIA NALKER RALPH L 203126071PPC
    TUCKER JAMES 30848763 PVT MIA CRIFFIS WILLIAM J 6270119 PFC 0N0 NALLACE JESSE A 30043713 T SC
    -TUCKER STERLING P 33341143 RPCI KIA CROSS ELERY C 0-390713 3 LT DNR NARREN CHARLES D 30110253 5 50
    NAOSNORTN PAUL P 30345079 5 SC ONE HAKNONBS ROBERT M 38433940 3 SC KIA MASHINCTON HENRY 30299100 PFC
    NALNER JAN S H JR 0-696023 2 LT P0 HANDLSY JAMES J JR 0-417955 1 LT ONE EEMS NINPRED E 20017930 CPL
    NEBB GLEN 30343104 PVT -K1 7 NANET FARRELL 8 30012597 SCT KIA NNITE DE NIS 10124415 PFC
    NRAT JAMES H 30067743 TE05 KIA NARCIE PRANCNARD 6370053 1 SC NIA NMITE MARVIN J 30430974 PFC
    NRIOHT NAILAND 0 30609570 PFC KIA NARKEY VEWCEN 30424190 PVT KIA NOODARD BILLY E 510217472 T 5
    HARRIS PRBS 38111537 PFC OKIA NRICNT EILL 0-725355 CAPT
    I HARRISO DUKE N JR 18055432 AV 0 ONE YOST TNURNAN R 6273153 PVT
    HENDRIX JOHN N JR 10217563 PVT 0N8
    HICKERSDN JACK 04431540 1 LT 9N8
    HUDSPETH COUNTY 3536305354 A CHEW 55555 W
    9 0776 A -
    - JOHNSTON LONNIE O 0-754906 3 LT KIA H INSON COUNTY
    ONES GEORBE N 01173315 1 LT ONE UTCH
    JUMPER ISAAC H 30605960 PVT KIA I
    A9515 HAN C -7 7 3 LT N5 LONG OSCAR D 30812573 5 SC KIA ALEXANDER BOYD A 0-690739 2 LT
    EARDNERNE3SEPN H 3L5E3355 3 L7 335 LVTLE JOHN E 30433946 PVT KIA EALDNIN JANES H 0-407114 CAPT
    NORALES ALFREDO L 18015539 3 CG P05 MACK HULET 00057345 1 LT NIA EICCERSTAPP C N 34530001 PC
    54N1952 LOCAS N 39441440 PFC K14 NAJORS TRUSTT J 00410046 2 LT KIA BRITTON JAMES H 18036279 8 SC
    ROBLES VICENTE 30570733 PFC 0N5 MASON DICE 30203437 CPL NIA EULLARD CAR 5 30607623 PVT
    SANCNE2 ANGEL R 38310341 PVT ONE MASON HALTER P 0-671673 3 LT NIA C IN ROBE T E 33105799 TECS
    VALLES RENICIO 0 30441430 PIC DON NASSEV JESSE D 30435424 PVT KIA COHA K LL03 N 10077256 PVT
    NC CLENDON J H 30117530 PPC ONE EVANS L 0 37392406 TECS
    NC HNORTER C R 30300391 PVT KIA FIRLEY MARVIN L 38572402 PVT
    4 HILLNAN ODEAN R T-000345 FL 0 DNB IPIELDS JAKES 35711546 PVT
    HIL10N JACKSON 7 13030591 PFC 933 ORADDY ROY L 39112920 PFC
    HUNT COUNTY HOORE ARLON D 6295620 9 SC KIA GRANT 30572401 PVT
    MORRISON DURHARD D 0-519411 3 LT KIA HANNA EVERETT T 10104243 S SC
    - LNULLINS GERALD D 6295622 T 50 NIA HANSARD SAMUEL 2 04754619 1 LT
    NEAL HOMER M 30609200 PVT KIA THARVEY JOHN B 0-602116 2 LT
    ALANIS VICENTE 30894642 VT KIA MEAL RAVN 39 9 1759550 3 NECOAL MARTIN J 6396940 PFC
    ALLEN TRUHAN L 0-123936 1 LT KIA NELSON 1 0 30431073 PPET ONE HILL RAYMONO 8 30050142 PVT
    ALLEY NILBUR K 01703993 2 LT KIA NELSON TRAVIS C 38204649 PVT DOH HOP JACK H D 30342393 PFC
    BENCH CLARENCE A 30003600 PFC KIA INEWLAND OTIS T T-000346 FL 0 F051 HDFF ROBERT C 30345133 T 50
    BENNETT EVERETT N 0-692130 LT 0N5 NICNOLSON DALE- 10005042 PVT KIA JONES JANES 0 30343505 PVT
    I I
    A NIXON LOYD 5041674 TECS KIA KAPPELMN MC 0-360687 CAPT
    BLACKHELL E C 38357321 307 KIA PARKER N 33130N V 52959524 3 L7 KIA KECANS TIM JR 38342898 FC
    ERITT BASIL JR 36002307 PFC K14 PATTERSON THOMAS H 18136913 SCT 9N5 KENNINER EARL 0 30401408 SGT
    BROHN SHOE 8 38634396 PFC KIA PERRI JA 5 0-562333 1 LT KIA LANTRON EDWARD L 19190302 5 SO
    BURNS VIRCIL P 38035701 PRC KIA PETTICREH FRED 0 16215700 S 50 NIA LESNER ROLLIE H 30050735 AV C
    BUTLER GEORGE A 37259433 1 SC KIA PHILLIPS C L JR 0-431139 CAPT KIA NC CARTY TOURNAN P 30711042 PFC
    CAMERON VANDSLL C 30634570 PPCI KIA PILCRTN CLYDE 30115026 V NIA NC GLENDON JACK 30330395 PFC
    CARTER LEO RD K 30119959 AV 0 ONE PO D EUCENE N 4 30431074 PRC KIA NC NINNET K E 30340151 PVT
    CARTER OILLIAH F 38685535 PFC 0N8 PRESLEY NILLIAN H 30037547 PVT 0N5 NC QUEEN JAMES Y 02055059 2 LT
    CREEK LOTD 6379416 A SC DNB PRICE PREDRICK P 30409331 CPL 100W NOTEN GEORGE N 30304695 PFC
    CLARK JOHN 18317907 CPL KIA PURCELL SAMUEL N 30017905 PVT 0N5 PANNELL NOLENIA 6227623 1 SC
    COLLINS RAYMOND N 30137040 PFC OOH RAILIPF NARDEN A 30037006 CPL 00H PIERCE FELIX 0 30335390 5 SC
    CREAKER SAH BL N 18006101 PVT KIA HAYNES NILLIAN T 7-954739 PL 0 9N5 PIETZSCH JAI E 0-426961 2 LT
    CRIDER NARLAN 0 10154794 SCT KIA REED JDRN L 30529414 PVT POL RTER NALLACE N JR 38193560 CPL
    DALE CHARLEY 8 I 38049500 3 SC KIA ROI NILLIS 5 01393956 1 LT KIA PRESCOTT DAVID L JR 0-707191 2 LT
    05 231



    D KZXXN D
    NNNNH 2
    XXLA-A- Q

    2
    33

    UR R
    2055
    35

As you can see, there are many mistakes, but at least we are obtaining reasonable results. In any case, you don''t want to do this because even if you were able to get perfect recognition, some of your lines would be placed in the wrong county because tesseract is looking at your document as a big paragraph. I recommend you to use the vertical lines to segment your image into 3 parts and then preprocess each one of them. You can even try to concatenate the parts vertically and perform OCR in a single page. This also applies to the second image.

By the way, the resolution of your image is not great, so if you can get a better image, that is going to make a big difference.

', 4621, '2015-02-07 07:37:14.040', 'fa418716-f392-4969-bc48-7cf5624436d1', 5078, 8397, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a table with 3 columns: User; Arist; num. of times the User listened to the given Artist.

I want to construct Artist similarity matrix, with Artists as both rows and columns.
In the final step, I want to use SVD to put these artists in n-dimensional space, let''s say 50-dimensional space, so each artist is represented with a vector 50 numbers, and similarity between two artists is calculated by cosine similarity between two 50-dimensional vectors.

Is this a good idea? How is this procedure called? And can it be done in Mahout (or any other library / tool)?', 8158, '2015-02-07 14:21:42.543', '38c67e78-e8b4-4e53-8eb9-556ba4df0a66', 5079, 8398, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Item based recommender using SVD', 8158, '2015-02-07 14:21:42.543', '38c67e78-e8b4-4e53-8eb9-556ba4df0a66', 5079, 8399, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<recommendation><apache-mahout>', 8158, '2015-02-07 14:21:42.543', '38c67e78-e8b4-4e53-8eb9-556ba4df0a66', 5079, 8400, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a table with 3 columns: User; Artist; num. of times the User listened to the given Artist.

I want to construct Artist similarity matrix, with Artists as both rows and columns.
In the final step, I want to use SVD to put these artists in n-dimensional space, let''s say 50-dimensional space, so each artist is represented with a vector 50 numbers, and similarity between two artists is calculated by cosine similarity between two 50-dimensional vectors.

Is this a good idea? How is this procedure called? And can it be done in Mahout (or any other library / tool)?', 8158, '2015-02-07 14:32:45.203', '3b611055-51b5-4602-ba82-86b7d99dc711', 5079, 'added 1 character in body', 8401, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a table with 3 columns: User; Artist; num. of times the User listened to the given Artist.

I want to construct Artist similarity matrix, with Artists as both rows and columns.
In the final step, I want to use SVD to put these artists in n-dimensional space, let''s say 50-dimensional space, so each artist is represented with a vector 50 numbers, and similarity between two artists is calculated by cosine similarity between two 50-dimensional vectors.

Is this a good idea? What is this procedure called? And can it be done in Mahout (or any other library / tool)?', 8158, '2015-02-07 14:39:02.430', '9152b7a9-e78c-46f9-b462-bc224c12fbb5', 5079, 'added 1 character in body', 8402, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hello I work as data scientist for a private company.
I am interested in working for a no profit company, such as a research institute (public or private) or a company that takes care of  issues such as  environment, public health,  social improvements...
Even an internet company like Wikipedia can be interesting.
Does anybody know if no profit companies hire data scientists?', 3346, '2015-02-07 15:37:47.327', 'a1525b18-e0b3-43d9-a040-c2b001663160', 5080, 8403, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Working as data scientist for a no profit company', 3346, '2015-02-07 15:37:47.327', 'a1525b18-e0b3-43d9-a040-c2b001663160', 5080, 8404, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<career>', 3346, '2015-02-07 15:37:47.327', 'a1525b18-e0b3-43d9-a040-c2b001663160', 5080, 8405, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Hello I work as data scientist for a private company.
I am interested in working for a nonprofit company, such as a research institute (public or private) or a company that takes care of  issues such as  environment, public health,  social improvements. Even an internet company like Wikipedia can be interesting.
Does anybody know if nonprofit companies hire data scientists?', 21, '2015-02-07 18:37:01.737', '900a6a44-145e-491e-852d-5219941e76eb', 5080, 'deleted 3 characters in body; edited title', 8406, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Working as data scientist for a nonprofit company', 21, '2015-02-07 18:37:01.737', '900a6a44-145e-491e-852d-5219941e76eb', 5080, 'deleted 3 characters in body; edited title', 8407, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have an item-item similarity matrix.

I need to implement recommender which, for a set of items, recommends a new set of items.

I was thinking about using SVD to reduce the items to n-dimensional space, let''s say 50-dimensional space, so each item is represented with a vector 50 numbers, and similarity between two items is calculated by cosine similarity between two 50-dimensional vectors.

For a set of items, I hope I could calculate an average of their vectors, and use it for search.

Is this a good idea? What is this procedure called? And can it be done in Mahout?', 8158, '2015-02-08 01:31:11.663', 'af5368c5-99b4-409c-8b43-35b9ccc504b3', 5079, 'added 7 characters in body', 8408, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I see at least five ways to approach this problem of finding a data scientist position/work specifically at non-profit, non-governmental or similar organizations, as I describe below. I hope that this is helpful.

- First, and the most obvious, way is to search **major job portals**, such as indeed.com, dice.com, monster.com, CareerBuilder, Glassdoor and others, for *data scientist* or similar positions, such as *data analyst*, *data engineer*, *quantitative analyst*, *statistical modeler*, or even *market researcher*.

- The second and also obvious way is to tap your **professional social networks** and research or inquire your contacts for any potential data science work opportunities in the areas of your interests.

- The third way is to search web sites, **focused** on the non-profit and related topics (they usually have a job listings or similar section) as well as **specialized non-profit job portals**. For example: http://encore.org, http://idealist.org, http://bridgespan.org, http://cgcareers.org, http://opportunityknocks.org, http://foundationcenter.org, http://thenonprofittimes.com, http://philanthropy.com, http://ynpn.org, http://philanthropyjournal.org, http://nonprofit-jobs.org, [CoF Jobs](http://jobs.cof.org/home/index.cfm?site_id=11690) (section isn''t easy to find), http://careers.councilofnonprofits.org, http://nonprofittalentmatch.com.

- The fourth way is to perform **Internet search and research** on non-profit or similar organizations that you might be interested in working with (criteria might vary from organization''s size to industry focus or geographical locations). Based on the information presented  on their websites, make notes and then approach those organizations, **directly** inquiring or applying for positions of interest.

- The fifth way is to consider various non-profit, for-profit and otherwise social good-themed *data science-focused* **organizations, initiatives and Kaggle-like competitions**, such as [DataKind](http://www.datakind.org), [DrivenData](http://www.drivendata.org), [DataLook](http://datalook.io) and [Data Science for Social Good](http://dssg.io).', 2452, '2015-02-08 01:47:57.963', 'a0e33b04-4338-4a0f-8a87-af74c788e0e5', 5081, 8409, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('> I need to implement recommender which, for a set of items, recommends a new set of items.

> Is this a good idea?

Have you looked into [Association Rules Mining][1]? If you''re open to other procedures this one is first one that came to my mind for recommenders based on sets of items. For those not familiar, this is simple method for retail shops to determine "75% of customers who bought A, B also bought C". Within these algos, the [Apriori algo][2] is straight-forward, easy to implement, and may get you what you need.


  [1]: http://en.wikipedia.org/wiki/Association_rule_learning
  [2]: http://en.wikipedia.org/wiki/Apriori_algorithm', 525, '2015-02-08 03:29:06.623', 'b5b000d9-fd96-4574-b465-f92f1fc58d67', 5082, 8410, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I need to use a clustering algorithm that can handle mix data (numerical and strings). I read that k-prototype does exactly that.
Unfortunately, I can''t seem to find an implementation for k-prototype in any library.

I searched every library google has to offer (https://github.com/josephmisiti/awesome-machine-learning), and it seems k-prototype is not in any of them.

Also, i tried searching for "k-means mix of categorical data", which turns up quite a few papers... but none of them seem to have an implementation.

Does anyone have a clue where i could find implementations of such algorithms?
I would prefer Java, but at this point I accept anything!', 8167, '2015-02-08 10:58:01.273', '2b33988a-e9fc-461f-a339-08b76dd3603c', 5083, 8411, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where to find k-prototype implementation', 8167, '2015-02-08 10:58:01.273', '2b33988a-e9fc-461f-a339-08b76dd3603c', 5083, 8412, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering>', 8167, '2015-02-08 10:58:01.273', '2b33988a-e9fc-461f-a339-08b76dd3603c', 5083, 8413, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have an item-item similarity matrix.

I need to implement recommender which, for a set of items, recommends a new set of items.

I was thinking about using SVD to reduce the items to n-dimensional space, let''s say 50-dimensional space, so each item is represented with a vector 50 numbers, and similarity between two items is calculated by cosine similarity between two 50-dimensional vectors.

For a set of items, I hope I could calculate an average of their vectors, and use it for search.

Is this a good idea? What is this procedure called? And can it be done in Mahout?

----------------------
EDIT:

This is my code so far:

    ItemSimilarity similarity = new LogLikelihoodSimilarity(model);
    Matrix m = new DenseMatrix(NUM_ITEMS, NUM_ITEMS);
    // copy similarities to a matrix
    for (int i = 0; i < NUM_ITEMS; i++) {
            double[] similar = similarity.itemSimilarities(i, range(NUM_ITEMS));
            for (int j = 0; j < NUM_ITEMS; j++) {
                m.setQuick(i, j, similar[j]);
        }
    }
    Matrix v = new SingularValueDecomposition(m).getV();
    Matrix reduced = v.viewPart(0, NUM_ITEMS, 0, 50);

The problem is, SVD is taking forever for NUM_ITEMS > 30. I don''t know if there is an issue with data, or with SVD implementation I''m using. The matrix m is symmetrical, could that be an issue? I tried googling "demean matrix mahout" with no results. How should I preprocess it for SVD to work faster? I will need NUM_ITEMS to be about 20.000 - 40.000 in the future. Is this reasonable size for SVD?', 8158, '2015-02-08 13:53:51.660', '6324a9f0-a9d9-414d-a924-911b8c718c40', 5079, 'added 982 characters in body', 8414, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have an item-item similarity matrix. e.g. (the matrix is symmetric, and much bigger):

    0.999 0.887 0.969 0.990
    0.887 0.999 0.990 0.967
    0.969 0.990 0.999 0.865
    0.990 0.967 0.865 0.999

I need to implement recommender which, for a set of items, recommends a new set of items.

I was thinking about using SVD to reduce the items to n-dimensional space, let''s say 50-dimensional space, so each item is represented with a vector 50 numbers, and similarity between two items is calculated by cosine similarity between two 50-dimensional vectors.

For a set of items, I hope I could calculate an average of their vectors, and use it for search.

Is this a good idea? What is this procedure called? And can it be done in Mahout?

----------------------
EDIT:

This is my code so far:

    ItemSimilarity similarity = new LogLikelihoodSimilarity(model);
    Matrix m = new DenseMatrix(NUM_ITEMS, NUM_ITEMS);
    // copy similarities to a matrix
    for (int i = 0; i < NUM_ITEMS; i++) {
            double[] similar = similarity.itemSimilarities(i, range(NUM_ITEMS));
            for (int j = 0; j < NUM_ITEMS; j++) {
                m.setQuick(i, j, similar[j]);
        }
    }
    Matrix v = new SingularValueDecomposition(m).getV();
    Matrix reduced = v.viewPart(0, NUM_ITEMS, 0, 50);

The problem is, SVD is taking forever for NUM_ITEMS > 30. I don''t know if there is an issue with data, or with SVD implementation I''m using. The matrix m is symmetrical, could that be an issue? I tried googling "demean matrix mahout" with no results. How should I preprocess it for SVD to work faster? I will need NUM_ITEMS to be about 20.000 - 40.000 in the future. Is this reasonable size for SVD?', 8158, '2015-02-08 14:05:36.913', '997bf639-ad57-4f58-923b-467f698bf54d', 5079, 'added 154 characters in body', 8415, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have an item-item similarity matrix. e.g. (the matrix is symmetric, and much bigger):

    1.00 0.88 0.96 0.99
    0.88 1.00 0.99 0.96
    0.96 0.99 1.00 0.86
    0.99 0.96 0.86 1.00

I need to implement recommender which, for a set of items, recommends a new set of items.

I was thinking about using SVD to reduce the items to n-dimensional space, let''s say 50-dimensional space, so each item is represented with a vector 50 numbers, and similarity between two items is calculated by cosine similarity between two 50-dimensional vectors.

For a set of items, I hope I could calculate an average of their vectors, and use it for search.

Is this a good idea? What is this procedure called? And can it be done in Mahout?

----------------------
EDIT:

This is my code so far:

    ItemSimilarity similarity = new LogLikelihoodSimilarity(model);
    Matrix m = new DenseMatrix(NUM_ITEMS, NUM_ITEMS);
    // copy similarities to a matrix
    for (int i = 0; i < NUM_ITEMS; i++) {
            double[] similar = similarity.itemSimilarities(i, range(NUM_ITEMS));
            for (int j = 0; j < NUM_ITEMS; j++) {
                m.setQuick(i, j, similar[j]);
        }
    }
    Matrix v = new SingularValueDecomposition(m).getV();
    Matrix reduced = v.viewPart(0, NUM_ITEMS, 0, 50);

The problem is, SVD is taking forever for NUM_ITEMS > 30. I don''t know if there is an issue with data, or with SVD implementation I''m using. The matrix m is symmetrical, could that be an issue? I tried googling "demean matrix mahout" with no results. How should I preprocess it for SVD to work faster? I will need NUM_ITEMS to be about 20.000 - 40.000 in the future. Is this reasonable size for SVD?', 8158, '2015-02-08 14:10:50.710', '89caa354-bc92-48a1-b9ed-6dfd4a94760d', 5079, 'added 154 characters in body', 8416, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Assuming the central node is at level 0 (root), this graph becomes a tree. Now it is easy reason about the different types of trees that can be formed and different types of social networks that can be modelled.

In order for the graph to be a tree, communication between nodes has to be restricted, such that the tree structure is not violated (e.g. two nodes on the same level cannot communicate). I can''t think of any social network where communication is naturally restricted in this way, but such a social network can be created artificially.

Examples of such artificial social networks would commonly follow a chain-of-command structure. For example in a military setting, the official information flows would have a tree structure.
', 90, '2015-02-08 16:05:58.517', '98e6b46a-5a5f-439a-b011-64eb07699fb2', 5084, 8417, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have an item-item similarity matrix. e.g. (the matrix is symmetric, and much bigger):

    1.00 0.88 0.96 0.99
    0.88 1.00 0.99 0.96
    0.96 0.99 1.00 0.86
    0.99 0.96 0.86 1.00

I need to implement recommender which, for a set of items, recommends a new set of items.

I was thinking about using SVD to reduce the items to n-dimensional space, let''s say 50-dimensional space, so each item is represented with a vector 50 numbers, and similarity between two items is calculated by cosine similarity between two 50-dimensional vectors.

For a base set of items (which can get quite big), I hope I could calculate an average of their vectors, and use it for search.

Is this a good idea? What is this procedure called? And can it be done in Mahout?

----------------------
EDIT:

This is my code so far:

    ItemSimilarity similarity = new LogLikelihoodSimilarity(model);
    Matrix m = new DenseMatrix(NUM_ITEMS, NUM_ITEMS);
    // copy similarities to a matrix
    for (int i = 0; i < NUM_ITEMS; i++) {
            double[] similar = similarity.itemSimilarities(i, range(NUM_ITEMS));
            for (int j = 0; j < NUM_ITEMS; j++) {
                m.setQuick(i, j, similar[j]);
        }
    }
    Matrix v = new SingularValueDecomposition(m).getV();
    Matrix reduced = v.viewPart(0, NUM_ITEMS, 0, 50);

The problem is, SVD is taking forever for NUM_ITEMS > 30. I don''t know if there is an issue with data, or with SVD implementation I''m using. The matrix m is symmetrical, could that be an issue? I tried googling "demean matrix mahout" with no results. How should I preprocess it for SVD to work faster? I will need NUM_ITEMS to be about 20.000 - 40.000 in the future. Is this reasonable size for SVD?', 8158, '2015-02-08 16:46:11.643', 'c2088528-9c1e-41f8-bb81-159f4d4b5d55', 5079, 'added 31 characters in body', 8418, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If I have virtually endless training data (it''s synthesized) is there still purpose in having epochs? I.e. training on the same samples multiple times?', 7806, '2015-02-08 18:05:34.020', '89c16f62-ecf4-4850-b806-9089bb1edf9a', 5085, 8419, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Should I use epochs > 1 when training data is unlimited?', 7806, '2015-02-08 18:05:34.020', '89c16f62-ecf4-4850-b806-9089bb1edf9a', 5085, 8420, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork>', 7806, '2015-02-08 18:05:34.020', '89c16f62-ecf4-4850-b806-9089bb1edf9a', 5085, 8421, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If the data is unlimited, how would you have an epoch to begin with? For example, if you are analyzing tweets, you could never finish an epoch will *all* the tweets, since there will be an endless supply of new tweets. A much better approach will be to do some online or streaming learning.

Would it make sense to create a subset by ignoring new incoming tweets or data in general?

That really depends on your problem. For example if you have a datastream of tweets, or any other where new data is being produced in real time, you would miss out any trends or patterns that emerged after you sampled from the stream. Are those missed patterns relevant to your problem? They may or may not be.    ', 90, '2015-02-08 21:03:52.657', '740324c8-0450-4691-bf8f-e90f25fe8c38', 5086, 8422, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This was intended as a comment.

I think this question is not receiving enough attention because it is too difficult to answer without trying several approaches first.

Your third idea is a nice one, but LSA is most likely not going to be able to help you choose clusters to the necessary granularity. For example, you may get a cluster of "broken things" but not to the specificity that is required to be helpful to the customer. A similar approach would be using Hierarchical Dirichlet Processes, but again, you may not get the clusters that you want.

I wouldn''t be so eager to try your fourth option because it requires a lot of work and I''m not sure you''re going to get great results unless your customer support deals with very well-defined questions and in that case, you''re going to have to build your own ontology from scratch.

In your situation, I would be curious if I can get information about the inner workings of Watson. Here a descriptive image that could be helpful:

![enter image description here][1]


  [1]: http://i.stack.imgur.com/Anu0t.jpg', 4621, '2015-02-08 21:17:50.990', 'acb67ffb-827a-46a2-ada0-ee2e6d94e3d5', 5087, 8423, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have an item-item similarity matrix. e.g. (the matrix is symmetric, and much bigger):

    1.00 0.88 0.96 0.99
    0.88 1.00 0.99 0.96
    0.96 0.99 1.00 0.86
    0.99 0.96 0.86 1.00

I need to implement recommender which, for a set of items, recommends a new set of items.

I was thinking about using SVD to reduce the items to n-dimensional space, let''s say 50-dimensional space, so each item is represented with a vector 50 numbers, and similarity between two items is calculated by cosine similarity between two 50-dimensional vectors.

For a base set of items (which can get quite big), I hope I could calculate an average of their vectors, and use it for search.

Is this a good idea? What is this procedure called? And can it be done in Mahout?

----------------------
EDIT:

This is my code so far:

    ItemSimilarity similarity = new LogLikelihoodSimilarity(model);
    Matrix m = new DenseMatrix(NUM_ITEMS, NUM_ITEMS);
    // copy similarities to a matrix
    for (int i = 0; i < NUM_ITEMS; i++) {
            double[] similar = similarity.itemSimilarities(i, range(NUM_ITEMS));
            for (int j = 0; j < NUM_ITEMS; j++) {
                m.setQuick(i, j, similar[j]);
        }
    }
    Matrix v = new SingularValueDecomposition(m).getV();
    Matrix reduced = v.viewPart(0, NUM_ITEMS, 0, 50);

The problem is, SVD is taking forever for NUM_ITEMS > 30. I don''t know if there is an issue with data, or with SVD implementation I''m using. The matrix m is symmetrical, could that be an issue? I tried googling "demean matrix mahout" with no results. How should I preprocess it for SVD to work faster? I will need NUM_ITEMS to be about 20.000 - 40.000 in the future. Is this reasonable size for SVD?

-----------
EDIT 2:

The problem was the matrix contained a few NaN values, that''s why SVD was taking infinite time. After replacing these with 0.0 it works fine for 1000 x 1000 matrix. And my recommendations are working like a charm. I''ll still need compute SVD of 20x more rows and columns. If anyone knows what''s the easiest way to compute (approximate) SVD of 20.000 x 20.000 dense matrix, probably through some cloud parallel service (?), please let me know.

PS. Thanks for help!', 8158, '2015-02-08 23:14:53.843', '257723e9-f65b-42fd-b974-819f9dd9642d', 5079, 'added 518 characters in body', 8424, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am trying to calculate maximum values for different groups in a relation in Pig. The relation has three columns patientid, featureid and featurevalue (all int).
I group the relation based on featureid and want to calculate the max feature value of each group, heres the code:

    grpd = GROUP features BY featureid;
    DUMP grpd;
    temp = FOREACH grpd GENERATE $0 as featureid, MAX($1.featurevalue) as val;

Its  giving me  **Invalid scalar projection: grpd** Exception. I read on different forums that MAX takes in a "bag" format for such functions, but when I take the dump of grpd, it shows me a bag format. Here''s a small part of the output from the dump:



    (5662,{(22579,5662,1)})
    (5663,{(28331,5663,1),(2624,5663,1)})
    (5664,{(27591,5664,1)})
    (5665,{(30217,5665,1),(31526,5665,1)})
    (5666,{(27783,5666,1),(30983,5666,1),(32424,5666,1),(28064,5666,1),(28932,5666,1)})
    (5667,{(31257,5667,1),(27281,5667,1)})
    (5669,{(31041,5669,1)})

Whats the issue ?
', 8174, '2015-02-09 00:18:46.427', 'ce4bb09c-648b-44ae-9e93-48f8f6e748ff', 5088, 8425, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Error when using MAX in Apache Pig (Hadoop)', 8174, '2015-02-09 00:18:46.427', 'ce4bb09c-648b-44ae-9e93-48f8f6e748ff', 5088, 8426, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop><pig><apache-pig>', 8174, '2015-02-09 00:18:46.427', 'ce4bb09c-648b-44ae-9e93-48f8f6e748ff', 5088, 8427, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The issue was with column addressing, heres the correct working code:

    grpd = GROUP features BY featureid;
    temp = FOREACH grpd GENERATE group as featureid, MAX(features.featurevalue) as val;', 8174, '2015-02-09 00:34:26.520', '0da87895-36ce-4ad4-b7ad-1553855df003', 5089, 8428, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('This was intended as a comment.

I think this question is not receiving enough attention because it is too difficult to answer without trying several approaches first.

Your third idea is a nice one, but LSA is most likely not going to be able to help you choose clusters with the necessary granularity. For example, you may get a cluster of "broken things" but not to the specificity that is required to be helpful to the customer. A similar approach would be using Hierarchical Dirichlet Processes, but again, you may not get the clusters that you want.

I wouldn''t be so eager to try your fourth option because it requires a lot of work and I''m not sure you''re going to get great results unless your customer support deals with very well-defined questions and in that case, you''re going to have to build your own ontology from scratch.

In your situation, I would be curious if I can get information about the inner workings of Watson (obviously, the purpose is only to have a general idea of some steps that are sensible enough to try.) Here is a descriptive image that could be helpful from [Wikipedia](http://en.wikipedia.org/wiki/Watson_%28computer%29):

![enter image description here][1]

The main idea is to segment questions into its components (using a regular chunking), then come up with several possibilities (hypothesis generation) of what the question might refer to (maybe this step can be implemented as a clustering). The third stage is rating the previous hypothesis based on available information. Finally, you choose the more likely hypothesis. There is a feedback involved in this process using sample questions and their answers. It would be interesting to see if the rating stage can be modeled in terms of a Bayesian or an incremental approach to allow for a feedback.

  [1]: http://i.stack.imgur.com/Anu0t.jpg', 4621, '2015-02-09 07:42:44.700', '959f4cca-5e03-4cc0-b93c-e4f7d2387262', 5087, 'typo', 8429, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('

I have a complete Hadoop platform with *HDFS*, *MR*, *Hive*, *PIG*, *Hbase*, etc., *Python*, *R*, *Java*. All data sets have a large size.

The data set A, describing the jobs of people working in a company, is composed of the following fields:



 - Id Person: a unique alphanumeric identifier per person.
 - Start Date: a date format iso entry in the post
 - End Date: iso size release date of the position. If the date is not given, it is the current position

 - Job Title: a text field containing the title and the name of the company. The text is free, non-standardized, French and / or English and can contain typos. Ex: Director Big Data Analytics with Google, Commercial Manager at [missing text] , Manager at googole ...

My question is; how can I create a feature to easily process the name of company of the job (jobtitle)?

Thank you in advance
', 8088, '2015-02-09 17:28:54.390', 'e5e27415-0c16-4665-b2d5-26cc77118730', 5091, 8433, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('analyze data with hadoop', 8088, '2015-02-09 17:28:54.390', 'e5e27415-0c16-4665-b2d5-26cc77118730', 5091, 8434, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop><pig><hbase>', 8088, '2015-02-09 17:28:54.390', 'e5e27415-0c16-4665-b2d5-26cc77118730', 5091, 8435, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('
I have a complete Hadoop platform with *HDFS*, *MR*, *Hive*, *PIG*, *Hbase*, etc., *Python*, *R*, *Java*. All data sets have a large size.

The data set A, describing the jobs of people working in a company, is composed of the following fields:



 - Id Person: a unique alphanumeric identifier per person.
 - Start Date: a date format iso entry in the post
 - End Date: iso size release date of the position. If the date is not given, it is the current position

 - Job Title: a text field containing the title and the name of the company. The text is free, non-standardized, French and / or English and can contain typos. Ex: Director Big Data Analytics with **Google**, Commercial Manager at [missing text] , Manager at **googole** ...

My question is; how can I create a feature to easily process the name of company of the job (jobtitle)?

Thank you in advance
', 8088, '2015-02-09 20:05:49.627', '659e3a95-64ed-4c92-8e20-4e690738f121', 5091, 'added 6 characters in body', 8436, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am looking for a design pattern that is relevant to a module that extracts features.
I want to define a certain number of features over my data points, and then according to the performance and the feature selection, I may want to remove some of them and add others, and also I may want to consider any subsets of them to test.

What is a good design pattern to do that? Did I miss something obvious? I am neither an engineer nor a developer, so I never study such things but I understand that it could help me a lot!

Thanks for any help,', 7966, '2015-02-09 23:22:05.547', '5e3d2db9-ed01-4983-8a4e-8879bc61d93b', 5092, 8437, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('design pattern for extracting features', 7966, '2015-02-09 23:22:05.547', '5e3d2db9-ed01-4983-8a4e-8879bc61d93b', 5092, 8438, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<feature-selection>', 7966, '2015-02-09 23:22:05.547', '5e3d2db9-ed01-4983-8a4e-8879bc61d93b', 5092, 8439, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m very new to this community, so please overlook my noobness.

I have a data set with 2948 instances and I tried to remove outliers using InterquartileRange filter in Weka. The issue is that the number of ''YES'' instances in ExtremeValues and Outliers takes up to 2947 and 2946 respectively. In other words, all my data are considered outliers.

What does this say about my data set? Or am I not meant to perform IQR on this data, if so, is there other algorithms to identify outliers other than IQR?  And how would one perform regression on such a data set?


Thank you.', 4803, '2015-02-10 01:04:15.963', '18ed9f61-7652-4ea8-a4ee-1f8be00a188e', 5093, 8440, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('InterquartileRange takes up most instances in data set', 4803, '2015-02-10 01:04:15.963', '18ed9f61-7652-4ea8-a4ee-1f8be00a188e', 5093, 8441, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><data-cleaning>', 4803, '2015-02-10 01:04:15.963', '18ed9f61-7652-4ea8-a4ee-1f8be00a188e', 5093, 8442, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think what you want is to extract company names from "Job Title". In natural language process, we call this kind of research as "Name Entity Recognition(NER)". You can try to use  Stanford Named Entity Recognizer (NER)[http://nlp.stanford.edu/software/CRF-NER.shtml]. Stanford NER performs very well on English contents and there are lots packages for many programming language:

> UIMA: Florian Laws made a Stanford NER UIMA annotator using a modified version of Stanford NER, which is available on his homepage. [Old version.]


>Perl: Kieren Diment has written Text-NLP-Stanford-EntityExtract, a Perl module that provides an interface to Stanford NER running as a server.


> Ruby: tiendung has written a Ruby Binding for the Stanford POS tagger and Named Entity Recognizer.


> Python: Dat Hoang wrote pyner, a Python interface to Stanford NER. [Old version.] NLTK (2.0+) contains an interface to Stanford NER written by Nitin Madnani: documentation (note: set the character encoding or you get ASCII by default!), code, on Github.


> F#/C#/.NET: Sergey Tihon has ported Stanford NER to F# (and other .NET languages, such as C#), using IKVM. See also pages on: GitHub and NuGet.
PHP: PHP-Stanford-NLP. Supports POS Tagger, NER, Parser. By Anthony Gentile (agentile).

If you are not satisfied with the performance of Stanford NER, you can also train you own models to extract company names by crawl company names from several popular sites with company names, such as Linkedin/Facebook/Glassdoor...etc', 1003, '2015-02-10 02:06:19.957', '1f87458e-d54a-464c-b64b-dd16cf1443ef', 5094, 8443, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('While @Ben''s answer is nice and partially introduces what should be done first with a newly cleaned data set, I feel that the approach is important enough to have its name presented loud and clear: *exploratory data analysis (EDA)*. Therefore, the short answer to your question is that the **first step should be EDA**.

This suggestion is supported by most researchers, regardless of their knowledge domain or type of study. Here is how the father of EDA presents his thoughts on the subject (Tukey, 1977, p. 1-3; emphasis mine):

> Exploratory Data Analysis (EDA) is detective work  numerical
> detective work  or counting detective work  or graphical detective
> work ... unless exploratory data analysis uncovers indications,
> usually quantitative ones, there is likely to be nothing for
> confirmatory data analysis to consider ... [it] can never be the whole
> story, but **nothing else can serve as the foundation stone - as the
> first step**.

There is an enormous amount of information on approaches, guidelines and procedures for performing EDA. Potential starting points might include [EDA page](http://www.itl.nist.gov/div898/handbook/eda/eda.htm) on the NIST''s Engineering Statistics Handbook website, [EDA pages](http://www.epa.gov/caddis/da_exploratory_0.html) on the EPA''s website, [corresponding chapter](http://www.stat.cmu.edu/~hseltman/309/Book/chapter4.pdf)  from the book "Experimental Design for Behavioral and Social Sciences" and a [survey research paper on EDA](http://cll.stanford.edu/~langley/cogsys/behrens97pm.pdf) by Begrens (1997), among many others. It is interesting to note that some sources include less traditional methods into EDA toolset, such as dimensionality reduction and clustering (for example, see the description of [this research seminar](http://www.ischool.berkeley.edu/courses/i290-eda)). While some of the EDA approaches and methods are relatively simple, overall EDA is both **art and science**, as it [combines unstructured (creative) and structured approaches](http://community.lithium.com/t5/Science-of-Social-blog/Exploratory-Data-Analysis-Playing-with-Big-Data/ba-p/71632). This aspect is especially important to recognize, as *big data* exponentially increases complexity of data analyses, including EDA.

**References**

Behrens, J. T. (1997). Principles and procedures of exploratory data analysis. *Psychological Methods, 2*(2), 131-160.

Tukey, J. W. (1977). *Exploratory Data Analysis.* Addison-Wesley.

**NOTES:** For those interested in the Tukey''s classic, it is [available on Amazon](http://www.amazon.com/gp/product/0201076160). Various MOOCs on EDA are also available, for example [this one](https://www.coursera.org/course/exdata) and [this one](https://www.udacity.com/course/ud651) (both are R-focused).', 2452, '2015-02-10 05:36:05.493', 'c577b3ab-3994-44b8-898c-cb922ae7cf69', 5095, 8444, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When I think about how to implement a certain ML or data mining process in an OOP (like Java), I usually go to see what smarter people than me designed their system.
In this case, I''d see how Weka, RapidMiner, JAVA ML or others decided to tackle this problem.

In your case of feature/attribute selection, i''m adding a link to Weka''s API.
If you want to better understand how it''s done, you should download Weka and play with the source code.

http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/AttributeSelectedClassifier.html

Without getting into specific implementations, I think that a linked list of any sort would do the trick, since you want to select a subset of all features.
', 8113, '2015-02-10 07:09:28.237', 'a49cef5c-241f-4b99-82c6-f43ca00eb5c2', 5096, 8445, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I think what you want is to extract company names from "Job Title". In natural language process, we call this kind of research as "Name Entity Recognition(NER)". You can try to use  Stanford Named Entity Recognizer (NER)[http://nlp.stanford.edu/software/CRF-NER.shtml]. Stanford NER performs very well on English contents and there are lots packages for many programming language:

> UIMA: Florian Laws made a Stanford NER UIMA annotator using a modified version of Stanford NER, which is available on his homepage. [Old version.]


>Perl: Kieren Diment has written Text-NLP-Stanford-EntityExtract, a Perl module that provides an interface to Stanford NER running as a server.


> Ruby: tiendung has written a Ruby Binding for the Stanford POS tagger and Named Entity Recognizer.


> Python: Dat Hoang wrote pyner, a Python interface to Stanford NER. [Old version.] NLTK (2.0+) contains an interface to Stanford NER written by Nitin Madnani: documentation (note: set the character encoding or you get ASCII by default!), code, on Github.


> F#/C#/.NET: Sergey Tihon has ported Stanford NER to F# (and other .NET languages, such as C#), using IKVM. See also pages on: GitHub and NuGet.


> PHP: PHP-Stanford-NLP. Supports POS Tagger, NER, Parser. By Anthony Gentile (agentile).

If you are not satisfied with the performance of Stanford NER, you can also train you own models to extract company names by crawl company names from several popular sites with company names, such as Linkedin/Facebook/Glassdoor...etc', 1003, '2015-02-10 09:44:27.880', '829173a5-9e86-4991-93c0-a4e742847563', 5094, 'added 6 characters in body', 8446, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have recently learnt Python & done some projects including writing ETL script. I have learnt from many places that Python is widely used for data analytic. I was wondering how? Why it''s popular in field of data analytic? What unique feature(s) it provides for DA? How it''s different from other languages in field of DA? Are there any packages which help do DA in python?', 8195, '2015-02-10 12:36:48.250', '430c0883-c94c-4643-ad0a-a0165d762bf9', 5097, 8447, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Python for data analytics', 8195, '2015-02-10 12:36:48.250', '430c0883-c94c-4643-ad0a-a0165d762bf9', 5097, 8448, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python>', 8195, '2015-02-10 12:36:48.250', '430c0883-c94c-4643-ad0a-a0165d762bf9', 5097, 8449, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You''re looking for this answer: https://www.quora.com/Why-is-Python-a-language-of-choice-for-data-scientists', 525, '2015-02-10 15:59:19.010', '063ae672-152b-4b14-a595-c65fc6f9362c', 5098, 8450, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think the key here is what you believe to be dependent variables. You mentioned, for instance, a three month rest. Encoding the rest between races is likely to be a better idea than simply encoding the date of the rest as a lot of the date is redundant to what is actually being said. As with any machine learning algorithm, representation of the data is key and in many ways, the actual algorithm applied is less important.  ', 8200, '2015-02-10 16:26:19.457', '1ae155d6-fca4-455b-9266-2de9c516221c', 5099, 8451, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It might be a good idea to start with voluntary work and see if that leads to payed position. [DataKind][1], that has been mentioned above, is where I would start, especially if you live in England, since you can register your interest online and even do work with them as a data scientist for a weekend in a meetup event.

There are also various [meetups][2] in many cities where it''s likely to find people working on nonprofit companies and expand your network.


  [1]: http://www.datakind.org/howitworks/datachapters/datakind-uk/
  [2]: https://www.google.com/search?client=safari&rls=en&q=data%20science%20site:meetup.com&ie=UTF-8&oe=UTF-8&gfe_rd=cr&oq=data%20science&gs_l=navquery.3..0l4j0i5.1459.4969.0.5279.12.12.0.0.0.0.82.718.12.12.0....0....1.61.navquery..0.12.717.-odm8NeyBww&ved=0CCkQ2wE&bav=on.2,or.r_cp.r_qf.&bvm=bv.85464276,d.d2s&biw=1240&bih=658&ech=1&psi=yHDaVNOsH9LnasblgZAI.1423601865135.3&ei=yHDaVNOsH9LnasblgZAI&emsg=NCSR&noj=1', 7857, '2015-02-10 21:13:37.107', '20a8500c-01c5-4146-a72c-d746a12b40b3', 5101, 8455, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a complete Hadoop platform with HDFS, MR, Hive, PIG, Hbase, etc., Python, R, Java. All data sets have a large size.

The data set A, describing the jobs of people working in a company, is composed of the following fields:



 - Id Person: a unique alphanumeric identifier per person.
    Start Date: a date format iso entry in the post

 -  End Date: iso size release date of the position. If the date is not given, it is the current position

 -  Job Title: a text field containing the title and the name of the company. The text is free, non-standardized, French and / or English and can contain typos. Ex: Director Big Data Analytics with **Google**, Commercial Manager at ***[missing text]*** , Manager at **googole** ...

My question is how can I extract copany name from job Title using Pig Regex?
Thank you in advance
', 8088, '2015-02-10 21:20:38.437', 'f6c1fa94-b8e6-479f-872d-80d825607d86', 5102, 8457, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Extraction string using Pig Regex', 8088, '2015-02-10 21:20:38.437', 'f6c1fa94-b8e6-479f-872d-80d825607d86', 5102, 8458, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<pig><apache-pig>', 8088, '2015-02-10 21:20:38.437', 'f6c1fa94-b8e6-479f-872d-80d825607d86', 5102, 8459, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Both Storm and Spark are great tools. It depends on your use-case.

 - Do you want to quickly parse huge stream of data and store it into a database? Use Storm (e.g. counting tweets).
 - Training a classifier on a stream of data would be a task suitable for Spark. There''s a data window on which you''re working and it will take a while.

I haven''t tried Flink, but it looks more similar to Spark. Spark has general concept of RRD (Resilient Distributed Datasets) that can be used also for graphs, huge matrices etc.

If you want to write a word count, you any of them. But who wants a word count (except from hello world tutorials)? ', 8207, '2015-02-10 23:18:18.657', '71ab8d69-e2a5-404c-aba5-237a4f3c0e0d', 5103, 8460, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":471,"DisplayName":"Spacedman"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-11 02:06:50.447', '46a71721-841a-49da-b39b-2fef4148038f', 5080, '103', 8461, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a complete Hadoop platform with *HDFS*, *MR*, *Hive*, *PIG*, *Hbase*, etc., *Python*, *R*, *Java*. All data sets have a large size.

The data set A, describing the jobs of people working in a company, is composed of the following fields:



 - Id Person: a unique alphanumeric identifier per person.
 - Start Date: a date format iso entry in the post
 - End Date: iso size release date of the position. If the date is not given, it is the current position

 - Job Title: a text field containing the title and the name of the company. The text is free, non-standardized, French and / or English and can contain typos. Ex: Director Big Data Analytics with **Google**, Commercial Manager at [missing text] , Manager at **googole** ...

My question is; how can I create a feature to easily process the name of company of the job (jobtitle)?

Thank you in advance
', 471, '2015-02-11 02:06:56.973', 'a1af32b4-89be-4c7b-a5d6-d9e340a971a4', 5091, 'title was not relevant to actual task', 8462, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Extract company names/job titles from free text', 471, '2015-02-11 02:06:56.973', 'a1af32b4-89be-4c7b-a5d6-d9e340a971a4', 5091, 'title was not relevant to actual task', 8463, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-02-11 02:06:56.973', 'a1af32b4-89be-4c7b-a5d6-d9e340a971a4', 5091, 'Proposed by 471 approved by 21 edit id of 227', 8464, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-11 02:07:46.803', 'e64ffcbc-081a-41c6-ab8b-7aa3920e2747', 5097, '104', 8466, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Both Storm and Spark are great tools. It depends on your use-case.

 - Do you want to quickly parse huge stream of data and store it into a database? Use Storm (e.g. counting tweets).
 - Training a classifier on a stream of data would be a task suitable for Spark. There''s a data window on which you''re working and it will take a while.

I haven''t tried Flink, but it looks more similar to Spark. Spark has general concept of RDD (Resilient Distributed Datasets) that can be used also for graphs, huge matrices etc.

If you want to write a word count, you any of them. But who wants a word count (except from hello world tutorials)? ', 21, '2015-02-11 02:08:02.163', '07cc35b2-19e3-4f44-857b-2b3f1f46a09b', 5103, 'edited body', 8467, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Without a sample of your data, it''s unclear what''s the structure of your data and what tool is suitable to process it.

Here are some blind recommendations based on my experience:

 - If you just need some flexibilty parsing the text record, such as variable repeat number of certain field, or conditional parsing of fields, then you should check out this python library: http://construct.readthedocs.org/en/latest/
it allows you to first define a hirachcal structure of your data, and then apply this structure to parse information from a text file. It''s especial useful when parsing binary files.
 - If you''re looking for an algorithm that can actually "understand" your text data and "infer" the structure in a smart way. Then you might want to try graph based approach:
http://kavita-ganesan.com/opinosis', 7950, '2015-02-11 06:13:12.223', 'bb32009b-133b-431e-b105-49307a322bed', 5104, 8468, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What are some data analytic package & feature in python which helps do data analytic?', 8195, '2015-02-11 06:46:22.097', '28ece624-32e4-47b1-b97e-a06dd042f0b9', 5097, 'deleted 287 characters in body; edited tags', 8472, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-mining><python>', 8195, '2015-02-11 06:46:22.097', '28ece624-32e4-47b1-b97e-a06dd042f0b9', 5097, 'deleted 287 characters in body; edited tags', 8473, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am also a newbie to data analytic, though I have done some analytic with weka. I can suggest you some things.

1) Have you checked your data distribution for the field you are applying IQR for? If the distribution is normal then only IQR will be effective.

2) You haven''t explained about your data. Such as whether it''s labelled or unlabelled. If it''s unlabelled you can apply clustering & see what data lie outside.

I hope it helps :).', 8195, '2015-02-11 07:03:08.857', 'cd0479b6-3f02-4974-a917-340e6b183a4d', 5106, 8474, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('No, no purpose other than saving data. A fresh sample is always better than a used one.

The only situation I can think of in which having epochs would make sense would be if the synthesis process would be really time consuming per example.', 8210, '2015-02-11 07:56:48.717', 'da9d207e-c8ed-4449-af12-9fa5da2d4082', 5107, 8475, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was not sure about posting this question with mentioning the name of the company, which I quite respect and admire. However, I''ve figured that a wider exposure might help the team to fix this and similar problems faster as well as increase the quality of the *machine learning (ML)* engine of their website.

The problem exposes itself by too many occurrences of a quite trivial *misclassification error* on Amazon''s book categories **classification** (which I''m a frequent visitor of). In the following example, the underlying reason of such behavior is quite clear, but in other cases the reasons might be different. I am curious about what could be other potential *reasons* for misclassification and what are the *strategies/approaches* to avoiding such problems. Without much further ado, here''s how the problem appears in real life.

I was reviewing some books, related to transitioning from graduate programs (Ph.D., in particular) to work environment in academia. Among several other books, I ran across the following one:

![enter image description here][1]

So far, so good. However, let''s scroll down a bit further to see the the books ratings in relevant categories. We should expect Amazon to figure out *categories*, **relevant** to the book''s *discipline, topic and contents*. How surprised was I (and that''s an understatement!) to see the following result of Amazon.com''s sophisticated ML engine and algorithms:

![enter image description here][2]

Clearly, the only fuzzy fact that connects this book with the subject "Audiology and Speech Pathology" (!) is IMHO the author''s last name (Boice), which, is **close to** the word "voice". If my guess is correct, Amazon''s ML engine, for some reason, decided to take into account the book''s lexicographical attribute instead of the book''s most important and **most relevant attributes**, such as title, topic and contents. I''ve seen multiple occurrences of similar absolutely incorrect ML-based decision making on Amazon.com and some other websites. So, hopefully my question makes sense as well as interesting and important enough to spark a discussion: **What could be other potential reasons for misclassification and what are the strategies/approaches to avoiding such problems?** (Any related thoughts will also be appreciated.)

  [1]: http://i.stack.imgur.com/4LSzV.png
  [2]: http://i.stack.imgur.com/KXrFh.png', 2452, '2015-02-11 08:00:52.377', 'ac8168bb-4ea5-4ccf-97e1-7755d6f1c74d', 5108, 8476, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Reasons and prevention of trivial (and less trivial) misclassification errors?', 2452, '2015-02-11 08:00:52.377', 'ac8168bb-4ea5-4ccf-97e1-7755d6f1c74d', 5108, 8477, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><topic-model>', 2452, '2015-02-11 08:00:52.377', 'ac8168bb-4ea5-4ccf-97e1-7755d6f1c74d', 5108, 8478, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('When I think about how to implement a certain ML or data mining process in an OOP (like Java), I usually go to see how smarter people than me designed their system.
In this case, I''d see how Weka, RapidMiner, JAVA ML or others decided to tackle this problem.

In your case of feature/attribute selection, i''m adding a link to Weka''s API.
If you want to better understand how it''s done, you should download Weka and play with the source code.

http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/AttributeSelectedClassifier.html

Without getting into specific implementations, I think that a linked list of any sort would do the trick, since you want to select a subset of all features.
', 8113, '2015-02-11 08:06:24.537', '13ed0f11-4207-4b46-b3e9-fb15b1544161', 5096, 'deleted 1 character in body', 8479, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('_Disclaimer:_ although I know some things about big data and am currently learning some other things about machine learning, the specific area that I wish to study is vague, or at least appears vague to me now. I''ll do my best to describe it, but this question could still be categorised as too vague or not really a question. Hopefully, I''ll be able to reword it more precisely once I get a reaction.

So,

I have some experience with Hadoop and the Hadoop stack (gained via using CDH), and I''m reading a book about Mahout, which is a collection of machine learning libraries. I also think I know enough statistics to be able to comprehend the math behind the machine learning algorithms, and I have some experience with R.
My ultimate goal is making a setup that would make trading predictions and deal with financial data in real time.

I wonder if there''re any materials that I can further read to help me understand ways of managing that problem; books, video tutorials and exercises with example datasets are all welcome.', 8214, '2015-02-11 10:48:51.903', '19a866dd-0769-4959-b0b9-e88b5c11ca55', 5109, 8480, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Machine Learning on financial big data', 8214, '2015-02-11 10:48:51.903', '19a866dd-0769-4959-b0b9-e88b5c11ca55', 5109, 8481, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><bigdata>', 8214, '2015-02-11 10:48:51.903', '19a866dd-0769-4959-b0b9-e88b5c11ca55', 5109, 8482, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am looking for a suitable graph representation where the nodes/vertices are molecules with 2 main variables: structural characteristic and a real number property (for example biological potency). The network is build up by molecules by matching structural properties and it can be rather complex with many clusters of high degree vertices (ie molecules that are structurally similar to each other). However the representation should give a clear picture of potencies, although high detail accuracy is not required: some sort of classification is adequate. I played with JUNG with a graph having edge weights as the change in potency and the ISOM layout thinking that it reserves (as much as possible) the edge weights but with not good results. Something like radial layout could be ideal, as it gives a clear picture of potency distribution, but this obviously would suffer from crowding of vertices that are closer to the center.  As, obviously, my knowledge is restricted, is there some layout that you would suggest and is practical for a non expert to implement? ', 8218, '2015-02-11 11:53:30.890', 'b3e9d8ce-6ab4-4816-af36-7e4e584ed57e', 5110, 8483, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Graph layout for a network of molecules', 8218, '2015-02-11 11:53:30.890', 'b3e9d8ce-6ab4-4816-af36-7e4e584ed57e', 5110, 8484, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization><graphs>', 8218, '2015-02-11 11:53:30.890', 'b3e9d8ce-6ab4-4816-af36-7e4e584ed57e', 5110, 8485, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are tons of materials on financial (big) data analysis that you can read and peruse. I''m not an expert in finances, but am curious about the field, especially in the context of data science and R. Therefore, the following are several resource suggestions that I have for you in that regard. I hope that they will be useful.

**Books: Financial analysis (general / non-R)**

- [Statistics and Finance: An Introduction](http://www.springer.com/statistics/business,+economics+%26+finance/book/978-0-387-20270-9);

- [Statistical Models and Methods for Financial Markets](http://www.springer.com/economics/public+finance/book/978-0-387-77826-6).

**Books: Machine Learning in Finance**

- [Machine Learning for Financial Engineering](http://www.worldscientific.com/worldscibooks/10.1142/p818) (!) - seems to be an edited collection of papers;

- [Neural Networks in Finance: Gaining Predictive Edge in the Market](http://www.elsevier.com/books/neural-networks-in-finance/mcnelis/978-0-12-485967-8#description).

**Books: Financial analysis with R**

- [Statistical Analysis of Financial Data in R](http://www.springer.com/statistics/business,+economics+%26+finance/book/978-1-4614-8787-6);

- [Statistics and Data Analysis for Financial Engineering](http://www.springer.com/statistics/business,+economics+%26+finance/book/978-1-4419-7786-1);

- [Financial Risk Modelling and Portfolio Optimization with R](http://www.amazon.com/dp/0470978708)

- [Statistics of Financial Markets: An Introduction](http://www.springer.com/statistics/business%2C+economics+%26+finance/book/978-3-642-16520-7) (code in R and MATLAB).

**Academic Journals**

- [Algorithmic Finance](http://algorithmicfinance.org) (open access)

**Web sites**

- [RMetrics](https://www.rmetrics.org)

- [Quantitative Finance on StackExchange](http://quant.stackexchange.com)

**R Packages**

- the above-mentioned *RMetrics* site (see [this page](https://www.rmetrics.org/about) for general description);

- *CRAN Task Views*, including [Finance](http://cran.r-project.org/web/views/Finance.html), [Econometrics](http://cran.r-project.org/web/views/Econometrics.html) and several other Task Views.

**Competitions**

- [MODELOFF (The Financial Modeling World Championships)](http://www.modeloff.com)

**Educational Programs**

- [MS in Financial Engineering - Columbia University](http://ieor.columbia.edu/ms-financial-engineering);

- [Computational Finance - Hong Kong University](http://www3.cs.stonybrook.edu/~skiena/691).

**Blogs (Finance/R)**

- [Timely Portfolio](http://timelyportfolio.blogspot.com);

- [Systematic Investor](https://systematicinvestor.wordpress.com);

- [Money-making Mankind](https://moneymakingmankind.wordpress.com).', 2452, '2015-02-11 12:25:31.077', '67bbd4d4-3027-4f8d-9305-9f3ff381c193', 5111, 8486, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('By looking at the ratings of the book, it looks like they are doing some sort of hierarchical clustering.
First of all it''s a book, then a text book, then in medicine & health science.

This is a pretty difficult problem because:

1. You don''t know how many and which topics you have, and the topics are constantly changing.
2. Since this is a clustering problem (or a semi-supervised problem in the better case), a closed feedback loop for these errors cannot be applied. Clustering is always more difficult than a supervised learning problem.

Apparently they''re doing a pretty good job in general. This book in particular is a tough one...', 8113, '2015-02-11 13:25:28.473', '435aea84-e48c-4980-9954-31c062481011', 5112, 8487, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A book I''m now reading, "Apache Mahout Cookbook" by Pierro Giacomelli, states that

> To avoid [this], you need to divide the vector files into two sets called the 80-20 split <...>
> A good dividing percentage is shown to be 80% and 20%.

Is there a strict statistical proof of this being the best percentage, or is it a euristic result?', 8214, '2015-02-11 13:26:27.753', '4537d870-915f-430b-bfca-9ea1c77b691b', 5113, 8488, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Dividing percentage', 8214, '2015-02-11 13:26:27.753', '4537d870-915f-430b-bfca-9ea1c77b691b', 5113, 8489, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><statistics>', 8214, '2015-02-11 13:26:27.753', '4537d870-915f-430b-bfca-9ea1c77b691b', 5113, 8490, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If this is about splitting your data into training and testing data, then 80/20 is a common rule of thumb. An "optimal" split (which would need to be operationalized) would likely depend on your sample size, distributions and relationships between your variables.

It is also common to split your data *three* ways (e.g., 60/20/20 - again rules of thumb), into a training set that you train your models on and a test set which you test your model on. You will iterate training and testing until you like the result. Then, *and only then* you apply the final model (trained on both the training and test set) on the third validation set. This avoids "overfitting on the test set".

However, [cross-validation](http://en.wikipedia.org/wiki/Cross-validation_%28statistics%29) is much better than a simple data split. Your textbook should also cover cross-validation. If it doesn''t, get a better textbook.', 2853, '2015-02-11 13:56:30.847', 'f92251d1-c02f-4079-8efc-45d2d07beefd', 5114, 8491, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><bigdata><finance>', 97, '2015-02-11 16:12:47.457', 'e00a1214-829c-4b15-b551-2b24f4e0001a', 5109, 'additional tag', 8493, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-02-11 16:12:47.457', 'e00a1214-829c-4b15-b551-2b24f4e0001a', 5109, 'Proposed by 97 approved by 2452, 21 edit id of 228', 8494, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can you please show the step by step calculation of Entropy(Ssun)?  I do not understand how 0.918 is arrived at.

I tried but I get the values as 0.521089678, 0.528771238, 0.521089678 for Sunny, Windy, Rainy.

I was able to calculate the target entropy (Decision) correctly as = -(6/10)*log(6/10) + -(2/10)log(2/10) + -(1/10)log(1/10) + -(1/10)log(1/10) = 1.570950594

I am totally stuck at the next step.  Request your help.

Reference: http://www.doc.ic.ac.uk/~sgc/teaching/pre2012/v231/lecture11.html
Please search for "The first thing we need to do is work out which attribute will be put into the node at the top of our tree:" to reach the line I am referring to.


 ', 8221, '2015-02-11 16:46:38.800', 'cb23acaf-326c-4bcb-a8f2-e789f88bdd11', 5115, 8495, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Calculating entropies of attributes', 8221, '2015-02-11 16:46:38.800', 'cb23acaf-326c-4bcb-a8f2-e789f88bdd11', 5115, 8496, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining>', 8221, '2015-02-11 16:46:38.800', 'cb23acaf-326c-4bcb-a8f2-e789f88bdd11', 5115, 8497, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Context of question:

I want to find semantically similar documents in corpora. For that, I''m first trying LDA with measures (e.g. Hellinger) on the per document topic distributions. However, to find # of topics for my corpus ( a 948 document dataset, extracted from larger collection, where docs about the same story are humanly annotated), it was suggested to use HDP. Unfortunately, after MANY tries, I''m still unsure I''m using the packages correctly.

More detailed:

1) HDPFASTER continually increases the number of topics (in train.log file).  What is the stopping criterion (e.g. difference in avg. likelihood between successive iterations smaller than x? which x?) and/or how many cycles should I let it run? Afterwords, do I take as the correct # of topics the last line of "train.log" OR should I also test for minimum perplexity (It has also been suggested that BIC might be a better measure) and choose the smallest? If the latter, should I test for ALL iterations?!  Is HDPFASTER''s test feature the appropriate tool for this? Last, should I --sample_hyper? Do I just use a(alpha) as input to LDA''s prior alpha, perhaps divided by a number (e.g. # of topics)? What about LDA''s prior beta?

2) HCA continually decreases  the number of topics (in *.log file). When to stop? Do I accept the final # of topics in .log ( exp.ent =number0) OR do I search for the minimum perplexity of test set (as I assume it appears as number2 in "log_2(perp)=number1,number2" in .log throughout iterations), which ALWAYS appears VERY close to my initial max number of topics? Hyperparameters alpha,beta: do I sample using -D , -E? Which do I put as input to LDA''s priors? Is it generally worth it?

Geneally: Is it possible that my dataset is just too small and/or ''biased/fragmented/incomplete'', in the sense that each story has only 1-3 examples, while the diversity of the topics these stories discuss can be quite large? Should I just try and augment it with a larger ''homogenous'' dataset?

', 8224, '2015-02-11 18:31:31.193', '1fc8502b-524a-491f-9b82-534789d47d84', 5116, 8499, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How do I use HDP implementations (hdpfaster or hca) to discover number of topics?', 8224, '2015-02-11 18:31:31.193', '1fc8502b-524a-491f-9b82-534789d47d84', 5116, 8500, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><topic-model><lda>', 8224, '2015-02-11 18:31:31.193', '1fc8502b-524a-491f-9b82-534789d47d84', 5116, 8501, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Consider the formula for Entropy:

$E(S) = -p_{+}\log_2p_{+}-p_{-}\log_2p_{-}$

There are 3 observations where `Weather=Sunny`. One of those observations is positive and the other two are negative. So for `Weather=Sunny` you have $p_{+}=\frac{1}{3}$ and $p_{-}=\frac{2}{3}$. Plugging those values into the equation above gives

$E(S_{Sun}) = -\frac{1}{3}\log_2\left(\frac{1}{3}\right)-\frac{2}{3}\log_2\left(\frac{2}{3}\right) = -0.52832-0.38998 = 0.918$
', 964, '2015-02-11 18:32:14.230', 'f0f96abe-90e4-4e37-bf1d-08d7f6b65752', 5117, 8502, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Consider the formula for Entropy:

$E(S) = -p_{+}\log_2p_{+}-p_{-}\log_2p_{-}$

There are 3 observations where `Weather=Sunny`. One of those observations is positive and the other two are negative. So for `Weather=Sunny` you have $p_{+}=\frac{1}{3}$ and $p_{-}=\frac{2}{3}$. Plugging those values into the equation above gives

$E(S_{Sun}) = -\frac{1}{3}\log_2\left(\frac{1}{3}\right)-\frac{2}{3}\log_2\left(\frac{2}{3}\right) = 0.52832+0.38998 = 0.918$
', 964, '2015-02-11 20:02:26.547', 'd2210732-1e57-40c1-9100-712295e018c7', 5117, 'sign change', 8503, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Let me first explain the concept of entropy for decision trees:

Entropy is a so called impurity measure for a set of elements. Impurity - being the opposite of purity - is referring to the distribution of the decision categories (or class labels) within the set. Initially, each row in the table is one element and your set of elements are all rows of your table. This set is called pure if it just consists of the same class label and it is called impure if it contains all different class labels in the same proportion. Translating into the values of entropy (remember being an impurity measure, the set has in the latter case the highest possible value and in the former case (just the same class label) the lowest value. The lowest possible value for entropy is 0.

So let''s choose a visual respresentation to illustrate what really counts and what does not matter for the entropy.

1) You start off with the initial table which contains the following class labels:

![enter image description here][1]

Please note that for computing the entropy none of the remaining attributes, attribute values matter, also the order of the rows don''t matter (entropy is a measure for a set of elements)

2) You convert the different class labels to different colors

![enter image description here][2]

3) So instead of this table with the colored class labeled, you just draw a circle ("ball") for each class label (=element)

![enter image description here][3]

Now you throw all the balls in a bag. There you go! Your elements of a set are represented as balls in a bag.  If you open the bag and look into it, you might see

 1. just balls of the same color --> one pure color. The entropy says, it''s not impure at all, I give it a zero! entropy = 0
 2. the same amount of balls in different colors --> that doesn''t look pure at all, it''s impure! The entropy assigns the highest possible value to this set (bag). The highest possible entropy value depends on the number of class labels. If you have just two, the maximum entropy is 1. In your case, you have 4 different class labels, so the maximum entropy would be e.g. if you had 12 balls and 3 balls of each color. The maximum entropy of a 4 class set is 2.
 3. none of the above. Then your entropy is between the two values. If one color is dominant then the entropy will be close to 0, if the colors are very mixed up, then it is close to the maximum (2 in your case).

How does a decision tree use the entropy:
Well, first you calculate the entropy of the whole set. That impurity if your reference. What your decision tries to achieve is to reduce the impurity of the whole set. So the information Gain for a given attribute is computed by taking the entropy of the whole set and subtracting it with the entropies of sets that are obtained by breaking the whole set into one piece per attribute category. To make sure a set with just a few elements don''t get the same weight as a set with many elements the entropies of sets are multiplied by their relative frequency.

Coming back to the example, for computing the Gain (S, parents) you need to compute the weighted entropies of the two sets: set 1 = all rows with category(parent) = yes and set 2 = all rows with category(parent) = no).

Set 1 refers to five rows, all having the decision category = cinema(just 5 orange balls!)
So the weight is 5 rows out of 10 rows = 5/10 = 0.5. And the entropy is 0 (totally pure) or by applying the formula:


![enter image description here][4]

You have a sum over all class labels so n = 4. So you assign a class label to each of the i''s. E.g.

For i = 1 being cinema: - (5/5 * log(3/3) = - (1 * log(1)) = - (1*0) = 0

For i = 2 being tennis: - (0/5 * log(0/3) = - (0 * log(0)) = 0

For i = 3 being stay in: - (0/5 * log(0/3) = - (0 * log(0)) = 0

For i = 4 being shopping: - (0/5 * log(0/3) = - (0 * log(0)) = 0

So the sum of all four is 0. 0 times the weight 0.5 equals 0.

Set 2 also refers to five rows, having the decision categories = 1*cinema, 2*tennis, 1*stay in, 1*shopping
So the weight is also 5 rows out of 10 rows = 5/10 = 0.5. Applying the formula:

For i = 1 being cinema: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46

For i = 2 being tennis: - (2/5 * log(2/5) = - (0.4 * -1,32) = 0.53

For i = 3 being stay in: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46

For i = 4 being shopping: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46

So the sum of all four is 1.92. 1.92 times the weight 0.5 = 0.96.
Thus, the Gain for this attribute is:
Initial entropy - weighted entropies for all attribute categories:
1.57 - 0 - 0.96 = 0.61.

For the other attributes you do the same calculation.

  [1]: http://i.stack.imgur.com/CbjbT.png
  [2]: http://i.stack.imgur.com/A4mux.png
  [3]: http://i.stack.imgur.com/ufvDS.png
  [4]: http://i.stack.imgur.com/dDMBf.jpg', 8191, '2015-02-11 23:31:20.127', '41268b14-eb23-434e-85bd-cd9e9eb271bb', 5118, 8504, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('**Let me first explain the concept of entropy for decision trees:**

Entropy is a so called impurity measure for a set of elements. Impurity - being the opposite of purity - is referring to the distribution of the decision categories (or class labels) within the set. Initially, each row in the table is one element and your set of elements are all rows of your table. This set is called pure if it just consists of the same class label and it is called impure if it contains all different class labels in the same proportion. Translating into the values of entropy (remember being an impurity measure, the set has in the latter case the highest possible value and in the former case (just the same class label) the lowest value. The lowest possible value for entropy is 0.

So let''s choose a visual respresentation to illustrate what really counts and what does not matter for the entropy.

1) You start off with the initial table which contains the following class labels:

![enter image description here][1]

Please note that for computing the entropy none of the remaining attributes, attribute values matter, also the order of the rows don''t matter (entropy is a measure for a set of elements)

2) You convert the different class labels to different colors

![enter image description here][2]

3) So instead of this table with the colored class labeled, you just draw a circle ("ball") for each class label (=element)

![enter image description here][3]

Now you throw all the balls in a bag. There you go! Your elements of a set are represented as balls in a bag.  If you open the bag and look into it, you might see

 1. just balls of the same color --> one pure color. The entropy says, it''s not impure at all, I give it a zero! entropy = 0
 2. the same amount of balls in different colors --> that doesn''t look pure at all, it''s impure! The entropy assigns the highest possible value to this set (bag). The highest possible entropy value depends on the number of class labels. If you have just two, the maximum entropy is 1. In your case, you have 4 different class labels, so the maximum entropy would be e.g. if you had 12 balls and 3 balls of each color. The maximum entropy of a 4 class set is 2.
 3. none of the above. Then your entropy is between the two values. If one color is dominant then the entropy will be close to 0, if the colors are very mixed up, then it is close to the maximum (2 in your case).

**How does a decision tree use the entropy?**

Well, first you calculate the entropy of the whole set. That impurity is your reference. What your decision tree tries to achieve is to reduce the impurity of the whole set. So the information Gain for a given attribute is computed by taking the entropy of the whole set and subtracting it with the entropies of sets that are obtained by breaking the whole set into one piece per attribute category. To make sure a set with just a few elements don''t get the same weight as a set with many elements the entropies of sets are multiplied by their relative frequency.

Coming back to the example, for computing the Gain (S, parents) you need to compute the weighted entropies of the two sets: set 1 = all rows with category(parent) = yes and set 2 = all rows with category(parent) = no).

Set 1 refers to five rows, all having the decision category = cinema(just 5 orange balls!)
So the weight is 5 rows out of 10 rows = 5/10 = 0.5. And the entropy is 0 (totally pure) or by applying the formula:


![enter image description here][4]

You have a sum over all class labels so n = 4. So you assign a class label to each of the i''s. E.g.

For i = 1 being cinema: - (5/5 * log(3/3) = - (1 * log(1)) = - (1*0) = 0

For i = 2 being tennis: - (0/5 * log(0/3) = - (0 * log(0)) = 0

For i = 3 being stay in: - (0/5 * log(0/3) = - (0 * log(0)) = 0

For i = 4 being shopping: - (0/5 * log(0/3) = - (0 * log(0)) = 0

So the sum of all four is 0. 0 times the weight 0.5 equals 0.

Set 2 also refers to five rows, having the decision categories = 1*cinema, 2*tennis, 1*stay in, 1*shopping
So the weight is also 5 rows out of 10 rows = 5/10 = 0.5. Applying the formula:

For i = 1 being cinema: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46

For i = 2 being tennis: - (2/5 * log(2/5) = - (0.4 * -1,32) = 0.53

For i = 3 being stay in: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46

For i = 4 being shopping: - (1/5 * log(1/5) = - (0.2 * -2,32) = 0.46

So the sum of all four is 1.92. 1.92 times the weight 0.5 = 0.96.
Thus, the Gain for this attribute is:
Initial entropy - weighted entropies for all attribute categories:
1.57 - 0 - 0.96 = 0.61.

For the other attributes you do the same calculation.

  [1]: http://i.stack.imgur.com/CbjbT.png
  [2]: http://i.stack.imgur.com/A4mux.png
  [3]: http://i.stack.imgur.com/ufvDS.png
  [4]: http://i.stack.imgur.com/dDMBf.jpg', 8191, '2015-02-11 23:38:53.957', 'c1678c7f-21cb-427b-b3c2-2c4dfabfd7cf', 5118, 'added 15 characters in body', 8505, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('There are tons of materials on financial (big) data analysis that you can read and peruse. I''m not an expert in finance, but am curious about the field, especially in the context of data science and R. Therefore, the following are selected relevant resource suggestions that I have for you. I hope that they will be useful.

**Books: Financial analysis (general / non-R)**

- [Statistics and Finance: An Introduction](http://www.springer.com/statistics/business,+economics+%26+finance/book/978-0-387-20270-9);

- [Statistical Models and Methods for Financial Markets](http://www.springer.com/economics/public+finance/book/978-0-387-77826-6).

**Books: Machine Learning in Finance**

- [Machine Learning for Financial Engineering](http://www.worldscientific.com/worldscibooks/10.1142/p818) (!) - seems to be an edited collection of papers;

- [Neural Networks in Finance: Gaining Predictive Edge in the Market](http://www.elsevier.com/books/neural-networks-in-finance/mcnelis/978-0-12-485967-8#description).

**Books: Financial analysis with R**

- [Statistical Analysis of Financial Data in R](http://www.springer.com/statistics/business,+economics+%26+finance/book/978-1-4614-8787-6);

- [Statistics and Data Analysis for Financial Engineering](http://www.springer.com/statistics/business,+economics+%26+finance/book/978-1-4419-7786-1);

- [Financial Risk Modelling and Portfolio Optimization with R](http://www.amazon.com/dp/0470978708)

- [Statistics of Financial Markets: An Introduction](http://www.springer.com/statistics/business%2C+economics+%26+finance/book/978-3-642-16520-7) (code in R and MATLAB).

**Academic Journals**

- [Algorithmic Finance](http://algorithmicfinance.org) (open access)

**Web sites**

- [RMetrics](https://www.rmetrics.org)

- [Quantitative Finance on StackExchange](http://quant.stackexchange.com)

**R Packages**

- the above-mentioned *RMetrics* site (see [this page](https://www.rmetrics.org/about) for general description);

- *CRAN Task Views*, including [Finance](http://cran.r-project.org/web/views/Finance.html), [Econometrics](http://cran.r-project.org/web/views/Econometrics.html) and several other Task Views.

**Competitions**

- [MODELOFF (The Financial Modeling World Championships)](http://www.modeloff.com)

**Educational Programs**

- [MS in Financial Engineering - Columbia University](http://ieor.columbia.edu/ms-financial-engineering);

- [Computational Finance - Hong Kong University](http://www3.cs.stonybrook.edu/~skiena/691).

**Blogs (Finance/R)**

- [Timely Portfolio](http://timelyportfolio.blogspot.com);

- [Systematic Investor](https://systematicinvestor.wordpress.com);

- [Money-making Mankind](https://moneymakingmankind.wordpress.com).', 2452, '2015-02-12 03:33:47.147', '82536db9-6dcc-4c5b-a751-66b596fe0111', 5111, 'Improved wording.', 8506, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('is the survival table classification method on the Kaggle Titanic dataset an example of an implementation of Naive Bayes ? I am asking because I am reading up on Naive Bayes and the basic idea is as follows:
"Find out the probability of the previously unseen instance
belonging to each class, then simply pick the most probable class"

The survival table
(http://www.markhneedham.com/blog/tag/kaggle/)
seems like an evaluation of the possibilities of survival given possible combinations of values of the chosen features and I''m wondering if it could be an example of Naive Bayes in another name. Can someone shed light on this ?', 8234, '2015-02-12 07:00:38.160', '2629bd1f-414c-49b4-be80-6803b1b987d2', 5119, 8507, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Kaggle Titanic Survival Table an example of Naive Bayes?', 8234, '2015-02-12 07:00:38.160', '2629bd1f-414c-49b4-be80-6803b1b987d2', 5119, 8508, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification>', 8234, '2015-02-12 07:00:38.160', '2629bd1f-414c-49b4-be80-6803b1b987d2', 5119, 8509, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am working on a problem(non competition) from hacker rank https://www.hackerrank.com/challenges/predict-missing-grade

Basically you''re given test data of a bunch of students of their scores in other subjects but math and you are to predict their score in math based off all their other test scores. Say you were passed data of

{"SerialNumber":1,"English":1,"Physics":2,"Chemistry":3,"ComputerScience":2}

How would you go about generating that student''s score in mathematics or coming up with a prediction engine to generate the math score? I know that''s the whole point of this question but can someone give me a hint or a resource to go to so I can have a chance of figuring this out and actually get started? I really want to learn.

 ', 8235, '2015-02-12 07:07:56.257', 'd1c7b5c0-0b16-4282-aef9-44ff2a67f970', 5120, 8510, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to start the process of coming up with the predicted math score?', 8235, '2015-02-12 07:07:56.257', 'd1c7b5c0-0b16-4282-aef9-44ff2a67f970', 5120, 8511, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><statistics><predictive-modeling>', 8235, '2015-02-12 07:07:56.257', 'd1c7b5c0-0b16-4282-aef9-44ff2a67f970', 5120, 8512, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Jaccard similarity** and **cosine similarity** are two very common measurements while comparing item similarities. However, I am not very clear in what situation which one should be preferable than another.

Can somebody help clarify the differences of these two measurements (the difference in concept or principle, not the definition or computation) and their preferable applications?', 5184, '2015-02-12 07:08:16.537', '01e9d6c8-66a2-4699-938b-819c39a2d742', 5121, 8513, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Applications and differences for Jaccard similarity and Cosine Similarity', 5184, '2015-02-12 07:08:16.537', '01e9d6c8-66a2-4699-938b-819c39a2d742', 5121, 8514, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<similarity>', 5184, '2015-02-12 07:08:16.537', '01e9d6c8-66a2-4699-938b-819c39a2d742', 5121, 8515, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Overfitting is *empirically* bad.  Suppose you have a data set which you split in two, test and training.  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on additional (new) test datasets than models which are not overfitted.

One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).  The difference in performance between training and test data indicates how much noise the model picks up; and picking up noise directly translates into worse performance on test data (including future data).

Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal.

P.S. On the "ability to generalize" part of the question, it is very possible to have a model which has inherently limited ability to generalize due to the structure of the model (for example linear SVM, ...) but is still prone to overfitting.  In a sense overfitting is just one way that generalization may fail.', 26, '2015-02-12 07:08:27.463', '7c8aaa09-2a11-412f-a915-b6229ed86915', 62, 'added 15 characters in body', 8516, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m curious if anyone has Python library suggestions for inferential statistics. I''m currently reading [An Introduction to Statistical Learning][1], which uses R for the example code, but ideally I''d like to use Python as well.


Most of my data experience is with Pandas, Matplotlib, and Sklearn doing predictive modeling.

So far I''ve found [statsmodels][2]. Is this what is recommended or is there something else?

Thanks!


  [1]: http://www-bcf.usc.edu/~gareth/ISL/
  [2]: https://pypi.python.org/pypi/statsmodels', 8236, '2015-02-12 10:46:41.280', 'c8e83d9b-c6d4-4aae-819d-6c4cb75da561', 5122, 8517, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Best Python library for statistical inference', 8236, '2015-02-12 10:46:41.280', 'c8e83d9b-c6d4-4aae-819d-6c4cb75da561', 5122, 8518, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><statistics>', 8236, '2015-02-12 10:46:41.280', 'c8e83d9b-c6d4-4aae-819d-6c4cb75da561', 5122, 8519, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What you are looking for is a machine learning algorithm. Although the easiest way would be to take the average scores and use that, there are much more accurate ways to make predictive models.

This was the first data science tutorial I did. It''s perfect for getting started. Here is it in [R][1] and in [Python][2].

If you''re looking for a short answer, something you can just look up how to implement, I''d check out Random Forests.

  [1]: https://www.kaggle.com/c/titanic-gettingStarted/details/new-getting-started-with-r
  [2]: https://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-python-ii', 8236, '2015-02-12 11:10:53.967', '0ee9bcdc-f765-4ac8-ab50-91a4a3ff93a8', 5124, 8523, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[statsmodels](http://statsmodels.sourceforge.net/devel/) is a good, and fairly standard, package to statistics.

For Bayesian interference you can go with [PyMC](http://pymc-devs.github.io/pymc/) - see as in [Cam Davidson-Pilon, Probabilistic Programming & Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/).

', 289, '2015-02-12 11:23:23.487', '2dbf155c-ae43-4cbe-8d14-b145b02083b9', 5125, 8524, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Context of question:

I want to find semantically similar documents in corpora. For that, I''m first trying Latent Dirichlet Allocation (LDA) with divergences (Hellinger, Kullback-Leibler, Jensen-Shannon) on the per document topic distributions. However, to find # of topics for my corpus ( a 948 document dataset, extracted from larger collection, where docs about the same story are humanly annotated), it was suggested to use HDP. Unfortunately, after MANY tries, I''m still unsure I''m using the packages correctly.

More detailed:

1) HDPFASTER continually increases the number of topics (in train.log file).  What is the stopping criterion (e.g. difference in avg. likelihood between successive iterations smaller than x? which x?) and/or how many cycles should I let it run? Afterwords, do I take as the correct # of topics the last line of "train.log" OR should I also test for minimum perplexity (It has also been suggested that Bayesian Information Criterion (BIC) might be a better measure) and choose the smallest? If the latter, should I test for ALL iterations?!  Is HDPFASTER''s test feature the appropriate tool for this? Last, should I --sample_hyper? Do I just use a(alpha) as input to LDA''s prior alpha, perhaps divided by a number (e.g. # of topics)? What about LDA''s prior beta?

2) HCA continually decreases  the number of topics (in *.log file). When to stop? Do I accept the final # of topics in .log ( exp.ent =number0) OR do I search for the minimum perplexity of test set (as I assume it appears as number2 in "log_2(perp)=number1,number2" in .log throughout iterations), which ALWAYS appears VERY close to my initial max number of topics? Hyperparameters alpha,beta: do I sample using -D , -E? Which do I put as input to LDA''s priors? Is it generally worth it?

Generally: Is it possible that my dataset is just too small and/or ''biased/fragmented/incomplete'', in the sense that each story has only 1-3 examples, while the diversity of the topics these stories discuss can be quite large? Should I just try and augment it with a larger ''homogeneous'' dataset?

', 8227, '2015-02-12 13:07:17.367', 'af59f1d8-4839-4313-8b9e-a068df5c4145', 5116, 'Explanation of acronyms, corrected spelling', 8529, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How do I use Hierarchical Dirichlet Process (HDP) implementations (hdpfaster by C.Wang or hca by W.Buntine at mloss.org) to discover number of topics?', 8227, '2015-02-12 13:07:17.367', 'af59f1d8-4839-4313-8b9e-a068df5c4145', 5116, 'Explanation of acronyms, corrected spelling', 8530, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-02-12 13:07:17.367', 'af59f1d8-4839-4313-8b9e-a068df5c4145', 5116, 'Proposed by 8227 approved by 2452, 21 edit id of 229', 8531, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have two dataframes. One df contains an email and a timestamp of when someone opened an email. The other df contains a unique id and a timestamp of when someone clicked on the email. However, this data is captured in two different servers and their timestamps are off by a varied amount i.e (some click timestamps are possibly before the open timestamp and some possibly after). I want to either 1) run tests to see if these two datasets are off by some constant and calculate the match rates for each test or run statistical tests that calculate the probability that a unique id/timestamp combination belong to an email/timestamp combination and then the highest probability would create a match. The problem is that I have more click/timestamp observations than email/timestamp observations. Also, two different email addresses cannot have the same unique id but the same email address can have multiple timestamps. What methods do you advise?', 8243, '2015-02-12 14:40:43.000', 'f8318954-2af3-4027-93aa-61db195ba7c8', 5127, 8532, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Comparing two linked but varied observations', 8243, '2015-02-12 14:40:43.000', 'f8318954-2af3-4027-93aa-61db195ba7c8', 5127, 8533, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 8243, '2015-02-12 14:40:43.000', 'f8318954-2af3-4027-93aa-61db195ba7c8', 5127, 8534, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have two dataframes. One df contains an email and a timestamp of when someone opened an email. The other df contains a unique id and a timestamp of when someone clicked on the email. However, this data is captured in two different servers and their timestamps are off by a varied amount i.e (some click timestamps are possibly before the open timestamp and some possibly after). I want to either 1) run tests to see if these two datasets are off by some constant and calculate the match rates for each test or 2) run statistical tests that calculate the probability that a unique id/timestamp combination belong to an email/timestamp combination and then the highest probability would create a match. The problem is that I have more click/timestamp observations than email/timestamp observations. Also, two different email addresses cannot have the same unique id but the same email address can have multiple timestamps. What methods do you advise?', 8243, '2015-02-12 14:51:31.600', 'fb6ab179-9f30-44f4-bba2-2179cdde25a2', 5127, 'added 3 characters in body', 8535, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Consider the formula for Entropy:

$E(S) = \sum_\limits{i=1}^{n}-p_{i}\log_2p_{i}$

Expanding that summation for the four concept decision attributes for your problem gives

$E(S) = -p_{cinema}\log_2\left(p_{cinema}\right) - p_{tennis}\log_2\left(p_{tennis}\right) - p_{stayin}\log_2\left(p_{stayin}\right) - p_{shopping}\log_2\left(p_{shopping}\right)$

There are three observations where `Weather=Sunny`. Of those three, one has `Decision=Cinema` and two have `Decision=Tennis`. So for `Weather=Sunny` you have $p_{cinema}=\frac{1}{3}$, $p_{tennis}=\frac{2}{3}$, and other decision probabilities are zero. Plugging those values into the equation above gives

$
\begin{align}
E(S_{Sun}) &= -\frac{1}{3}\log_2\left(\frac{1}{3}\right)-\frac{2}{3}\log_2\left(\frac{2}{3}\right) - \left(0\right)\cdot\log_2\left(0\right) - \left(0\right)\cdot\log_2\left(0\right) \\
           &= 0.38998 + 0.52832 + 0 + 0 \\
           &= 0.918
\end{align}$
', 964, '2015-02-12 14:58:17.253', '2624293e-65ca-44f7-a372-bf4a42f8f6ac', 5117, 'values swapped.', 8536, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Jaccard Similarity is given by
$s_{ij} = \frac{p}{p+q+r}$

where,<br> p = # of attributes positive for both objects <br>
q = # of attributes 1 for i and 0 for j <br>
r = # of attributes 1 for i and 0 for j <br>

Whereas, cosine similarity = $\frac{A \cdot B}{\|A\|\|B\|}$ where A and B are object vectors.

Simply put, in cosine similarity, the number of common attributes is divided by the total number of possible attributes. Whereas in Jaccard Similarity, the number of common attributes is divided by the number of attributes that exists in at least one of the two objects.

And there are many other measures of similarity, each with its own eccentricities. When deciding which one to use, try to think of a few representative cases and work out which index would give the most usable results to achieve your objective.

For example, if you have two objects both with 10 attributes, out of a possible 100 attributes. Further they have all 10 attributes in common. In this case, the Jaccard index will be 1 and the cosine index will be 0.001. Now consider another scenario where object A has 10 attributes, and object B has 50 attributes, but B has all 10 attributes that A has. Here, Jaccard index will be 0.2 and cosine index will still be 0.001. So the key question is if that extra bit of information reflected, in this case, in the Jaccard index useful or does it hurt or does it not matter. Your choice will depend on which is the best solution for your problem.

The Cosine index could be used to identify plagiarism, but will not be a good index to identify mirror sites on the internet. Whereas the Jaccard index, will be a good index to identify mirror sites, but not so great at catching copy pasta plagiarism (within a larger document).

Of course, these are toy examples to illustrate a point. When applying these indices, you must think about your problem thoroughly and figure out how to define similarity. Once you have a definition in mind, you can go about shopping for an index.    ', 90, '2015-02-12 15:47:15.277', '58396ccc-3f66-4dce-a204-31c9e6d93cd8', 5128, 8537, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Matching two linked but varied observations using two unique identifiers', 8243, '2015-02-12 15:52:20.957', '120ee817-9646-45ad-84d7-06b4c74a43b3', 5127, 'edited title', 8538, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hi I am new to Data analytics.I am planning to learn R by doing some real time projects. How should I stream line (set goals) my time in learning R and also I have not learnt statistics till data. I am planning to learn both side by side. I am mid level data warehouse engineer who has experience in DBMS Data-Integration. I am planning to learn R so that I can bring out useful analysis from the Integrated data.

If I be specific, I am beginning in R, so what are the basic statistical concepts I should know and implement it in R. If I want to be an expert or above average person in R how should I plan strategically to become one. Say if I can spend 2 hrs a day for 1 year what level I should reach. FYI am working for a SaaS company. What are the way s in which I can utilize R knowledge in a SaaS environment ', 8244, '2015-02-12 16:11:30.087', '1a7d1af2-1be6-43af-a904-ef959080ef80', 5129, 8539, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R Programing beginner', 8244, '2015-02-12 16:11:30.087', '1a7d1af2-1be6-43af-a904-ef959080ef80', 5129, 8540, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><statistics>', 8244, '2015-02-12 16:11:30.087', '1a7d1af2-1be6-43af-a904-ef959080ef80', 5129, 8541, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would start off with [All Of Statistics by Larry Wasserman][1]. This quickly gets you upto speed with statistics assuming you have some mathematical background. I think all it needs is introductory calculus and linear algebra.

R is pretty straightforward to pick up and there are a number of resources you can use. The [R Programming][2] course on coursera is an excellent short course to get you familiarized with R. Besides that there are a number of books and tutorials on the subject.

I would recommend you start working through All of Statistics and start playing around in R along with that book.

If you are at a SaaS company, there are a number of data sciency roles and responsibilities that should be available. Most SaaS companies will have analytical tools to provide basic insight. Once you start learning statistics and data science, you will be able to identify areas where you can add value.


  [1]: http://www.amazon.ca/All-Statistics-Concise-Statistical-Inference/dp/0387402721
  [2]: https://www.coursera.org/course/rprog', 90, '2015-02-12 16:46:29.453', 'ea693a44-1ae1-4492-a1a5-fc2db13a0b22', 5130, 8542, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would start off with [All Of Statistics by Larry Wasserman][1]. This quickly gets you upto speed with statistics assuming you have some mathematical background. I think all it needs is introductory calculus and linear algebra.

R is pretty straightforward to pick up and there are a number of resources you can use. The [R Programming][2] course on coursera is an excellent short course to get you familiarized with R. Besides that there are a number of books and tutorials on the subject.

I would recommend you start working through All of Statistics and start playing around in R along with that book.

If you are at a SaaS company, there are a number of data sciency roles and responsibilities that should be available. Most SaaS companies will have analytical tools to provide basic insight. Once you start learning statistics and data science, you will be able to identify the gaps in the pre baked tools and ways to improve them. I would also read [The Field Guide to Data Science][3], it''s a short book that will give you a high level idea of data science and its utility.


  [1]: http://www.amazon.ca/All-Statistics-Concise-Statistical-Inference/dp/0387402721
  [2]: https://www.coursera.org/course/rprog
  [3]: http://www.boozallen.com/insights/2013/11/data-science-field-guide', 90, '2015-02-12 16:51:47.660', 'e54973d5-d86d-456b-a779-60c81445ef88', 5130, 'added 247 characters in body', 8543, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('No one seems to have posted the XKCD overfitting comic yet.

![enter image description here][1]


  [1]: http://i.stack.imgur.com/wSN2q.png', 3347, '2015-02-12 19:09:05.900', 'fc82438a-040b-4cc2-a3a4-f53a579bdcf7', 5131, 8544, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This issue is caused because the following environment variables are not set correctly
PKG_CONFIG_PATH and LD_LIBRARY_PATH
for my configuration, i should execute this:


export PKG_CONFIG_PATH = /usr/local/lib
export LD_LIBRARY_PATH = /usr/local/lib

also set this commands on R console:

Sys.setenv(HADOOP_HOME="/usr/local/hadoop/")
Sys.setenv(HADOOP_BIN="/usr/local/hadoop/bin")
Sys.setenv(HADOOP_CONF_DIR="/usr/local/hadoop/conf")', 4705, '2015-02-13 00:33:59.857', '3affb1d4-9609-4f35-89bb-1e599e3ce40d', 5132, 8545, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The answer to both data sets is an OCR application with some post-processing, but a more specialized program than a generic low-quality or an open source OCR. Essentially the harder the problem, the more capable and advanced tools need to be used to solve it.

There will be two major stages in this task: digitizing the data (image to text, i.e. OCR) and processing the data (performing the actual count). Look at them separately in order to select the best method for each stage.

The main challenges in these images and generic OCR are:

a) images have low resolution. For example the # 1 image has resolution of about 72 dpi. Suggested resolution for such text quality is to scan at 300 to 400 dpi, but it is clear that re-scanning or controlling scan resolution is not applicable now. Thats why one option is to clean and increase the size using image pre-processing tools. This is what the original #1 image snippet looks like after adaptive binarization and zoomed at 300%. It is clear that each character has too few pixels and characters can be easily misread.

![enter image description here][1]



b) GIF format in #1 is not supported by many OCR applications. Images need to be batch-converted to a different format, such as PNG or TIF.

c) in these scans the backgrounds and bleed-through (shadow from the text on the other side of the paper) is visible. Good binarization needs to be used to remove background and bleed-through, but not remove vital parts of actual characters.

After implementing specific pre-processing solutions for the items listed above, and then using a high quality OCR system, such as www.ocr-it.com API, highest possible results can be achieved.  Result is far from perfect, but it is as high accuracy as it could be achieved with a modern OCR engine on these images.

![enter image description here][2]



Luckily for this project, the data needs to be counted, so the second stage has all necessary data for reliable data post-processing analysis. Contrary to other basic OCR engines, the OCR provided by [www.ocr-it.com][3] API, which I used to produce the above recognition.  OCR-IT API is free to develop with, and very inexpensive to use per page, so that may be a very economical but powerful solution for this project.  It returns formatted text layout, including preserving line breaks and overall format structure.  This makes text post-processing easier.

A simple algorithm can be run to count the number of lines, resulting in the necessary for the research count.

The above describes a two-stage approach: getting best possible OCR result, and using an applicable method to process data for the required task

Bat wait, there is more

There is a second option to use an even more specialized OCR application called [FlexiCapture][4] with FlexiLayout technology.  This powerful and intelligent data capture technology has built-in high-accuracy OCR, and it has a powerful rules and data analytics engine to perform very specialized user-defined chains of actions and tasks.

The implementation of this method using FlexiCapture with FlexiLayout takes the following logical steps.

First, full page OCR is performed and all objects are extracted, including characters, noise, black horizontal and vertical lines, white gaps, and objects (which could be pictures, logos, handwriting, etc.).  This produces objects upon which we can apply our search criteria.

Next, the following constraints are applied to the post-OCR data analysis and search criteria: separate image into three vertical columns and run the following logic per column, use line-start as individual count, skip header/footer/indented lines (county names), assume each name to have at least three characters, find recursively every name starting from top to bottom in every column, exclude previously found lines.

While the above logic sounds complex to setup, the actual setup takes just a few minutes and requires minimal work through user interface (UI) environment.  No coding or programming is necessary.   The following search elements and criteria have been created.

![enter image description here][5]

RepeatingGroup consisting of a CharacterString search object.

This setup produces the following search result for the first column of data:

![enter image description here][6]

As the last step, FlexiCapture is instructed to return the number of total found elements that fit our search criteria, effectively producing the necessary data for the research task.

There are other logic alternatives that can be setup in FlexiCapture, such as finding the number of white spaces between lines, or searching for the fixed-length fixed-placement 3-letter combinations at the end of every column linen.

In conclusion, there are several options (which is always nice) how this task can be achieved with relative ease in effort and high quality, but the success depends on the quality of tools used and necessary knowledge how to use them.

If you believe some of these tools and processes can be beneficial to your project, please contact my directly.  I specialize in these workflows.  Ilya @ WiseTREND.  My company may be able to help with the setup or guidance.  We have participated in various research initiatives, some through donations to a good cause.


  [1]: http://i.stack.imgur.com/oNqPE.png
  [2]: http://i.stack.imgur.com/v5biL.png
  [3]: http://www.ocr-it.com
  [4]: http://www.wisetrend.com/abbyy-flexicapture/
  [5]: http://i.stack.imgur.com/MOPHu.png
  [6]: http://i.stack.imgur.com/KzpKF.png', 8255, '2015-02-13 05:57:36.840', 'ee2debe2-03f1-4f1f-8824-de695c98ae25', 5133, 8546, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The MOOCs like UDACITY, COURSERA, UDEMY and EDX  are a great place to learn high quality R programming courses free. You can also search on the MOOCTIVITY siter which is a MOOC aggregator.

I have personally found great courses on R programming at  UDACITY, COURSERA and UDEMY and also EDX.

Good Luck !!

Gopinath SUbbegowda', 8257, '2015-02-13 06:03:41.233', '38bc1388-6ac4-446f-be37-e0fe6ec704ac', 5134, 8547, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Naive Bayes is just one of the several approaches that you may apply in order to solve the Titanic''s problem. The aim of the Kaggle''s Titanic problem is to build a **classification** system that is able to predict one outcome (whether one person survived or not) given some input data. The survival table is a **training dataset**, that is, a table containing a set of examples to train your system with.

As I mentioned before, you could apply Naive Bayes to build your classification system to solve the Titanic problem. Naive Bayes is one of the simplest classification algorithms out there. It assumes that the data in your dataset has a very specific structure. Sometimes Naive Bayes can provide you with results that are good enough. Even if that is not the case, Naive Bayes may be useful as a first step; the information you obtain by analyzing Naive Bayes'' results, and by further data analysis, will help you to choose which classification algorithm you could try next. Other examples of classification methods are k-nearest neighbours, neural networks, and logistic regression, but this is just a short list.

If you are new to Machine Learning, I recommend you to take a look to this course from Stanford: https://www.coursera.org/course/ml', 2576, '2015-02-13 09:06:33.070', 'cc58efe8-d00d-4511-8950-ab075e13528b', 5135, 8548, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I second saq7 and Gopinath, the R courses on Coursera are excellent. I really rate the Johns Hopkins ones: https://www.coursera.org/specialization/jhudatascience/1/courses. You should also keep an eye on the [software carpentry][1] site for courses they run in your area. If you can''t wait, all the software carpentry [learning material is online][2] so you can follow it yourself.


  [1]: http://software-carpentry.org/
  [2]: http://swcarpentry.github.io/r-novice-inflammation/', 8021, '2015-02-13 15:04:24.963', 'cf973db8-d027-49e9-b3ed-f8ae448534f4', 5136, 8550, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('This issue is caused because the following environment variables are not set correctly
PKG_CONFIG_PATH and LD_LIBRARY_PATH
for my configuration, i should execute this:

export PKG_CONFIG_PATH = /usr/local/lib <br>
export LD_LIBRARY_PATH = /usr/local/lib

also set this commands on R console:

Sys.setenv(HADOOP_HOME="/usr/local/hadoop/")<br>
Sys.setenv(HADOOP_BIN="/usr/local/hadoop/bin")<br>
Sys.setenv(HADOOP_CONF_DIR="/usr/local/hadoop/conf")<br>', 4705, '2015-02-13 21:58:41.240', '109e9473-3876-4cad-ac55-c8ed65302dfd', 5132, 'added 15 characters in body', 8551, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to start with this video: https://www.youtube.com/watch?v=3Z73Wd2T1xE I''m studying this one right now

Do you think Ayasdi will decrease the data scientist job? and should I contunue studying Nano Degree in Data Analyst at Udacity.

What is your suggestion if I want to compete in Kaggle but do not have Ayasdi software. Because after I watched the Ayasdi video, it seems like many problem in Kaggle can be solve by their platform and don''t need much expert people. What do you think? Will this decrease Data Scientist job? can the Ayasdi platform solve many of the problem in data world?

should I post this on Cross validation?', 8268, '2015-02-14 07:09:01.527', '60a3485b-624f-4f68-92d5-76a1869826b5', 5137, 8553, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Will demand for data scientist decrease because of AYASDI?', 8268, '2015-02-14 07:09:01.527', '60a3485b-624f-4f68-92d5-76a1869826b5', 5137, 8554, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 8268, '2015-02-14 07:09:01.527', '60a3485b-624f-4f68-92d5-76a1869826b5', 5137, 8555, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I would like to start with this video: https://www.youtube.com/watch?v=3Z73Wd2T1xE I''m studying Data Analyst course at Udacity.

Do you think Ayasdi will decrease the data scientist job? and should I contunue studying Nano Degree in Data Analyst at Udacity.

What is your suggestion if I want to compete in Kaggle but do not have Ayasdi software. Because after I watched the Ayasdi video, it seems like many problem in Kaggle can be solve by their platform and don''t need much expert people. What do you think? Will this decrease Data Scientist job? can the Ayasdi platform solve many of the problem in data world?

should I post this on Cross validation?', 8268, '2015-02-14 07:19:10.877', '604b604e-0d1c-45f8-b40b-e2a43e446fcd', 5137, 'added 13 characters in body; edited tags', 8556, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<data-mining><bigdata>', 8268, '2015-02-14 07:19:10.877', '604b604e-0d1c-45f8-b40b-e2a43e446fcd', 5137, 'added 13 characters in body; edited tags', 8557, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am confused by the definition of the likelihood function in different contexts.

In the case of linear and logistic regression, it is defined as y given x
In the case naive bayes and LDA, it is defined over x and y
In the case of the EM algorithm (with observed variable x and unobserved variable z), it is defined over x

How do we know over what it should be defined?', 8273, '2015-02-14 17:13:31.473', '30717438-b071-439b-9755-ac58f14c3f88', 5138, 8559, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Definition of likelihood function', 8273, '2015-02-14 17:13:31.473', '30717438-b071-439b-9755-ac58f14c3f88', 5138, 8560, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><statistics>', 8273, '2015-02-14 17:13:31.473', '30717438-b071-439b-9755-ac58f14c3f88', 5138, 8561, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I also found confusing the definition of likelihood the first time I encountered it. The only thing you need to remember is that likelihood refers to the probability of your data given a parameter. That is,

$$L(\theta| X) = P(X|\theta)$$

As you can see, the likelihood focuses on the parameter, so you''re asking "What is the likelihood of this parameter given some observations?" In this case, the phrase "given some observation" assumes the observations $X$ are fixed and the parameter $\theta$ is not fixed. On the other hand, if you describe the same in terms of probability, you''re trying to measure the probability of some observations $X$ given a fixed parameter $\theta$. It is only a matter of usage.

In the case of logistic regression, the likelihood function is described as:

$$L(\beta| y) = \prod_{i}^{N} \frac{n_{i}}{n_{i}!(n_{i}-y_{i})!}\pi_{i}^{n_{i}}(1-\pi_{i})^{n_{i} - y_{i}}$$

where $\beta$ is the vector of parameters $\{\pi_{i}| i=1,...,N \}$and $y$ is the vector of observed counts. As you can see, this is the same (by definition) as $P(y|\beta)$ and that is what you have in the equation above: The product of the probabilities for each observation $y_{i}$ given its parameter $\pi_{i}$. As usual, each probability $\pi_{i}$ is equal to the logistic regression equation.

In Naive Bayes, you have the same basic idea. The likelihood is defined as $L(p_{k}|x) = \prod_{i}^{N}p_{ki}^{x_{i}}(1-p_{ki})^{(1-x_{i})}$ where $p_{k}$ is a vector of parameters describing the probabilities of a class $k$ generating a word $x_{i}$).

In the EM algorithm, the likelihood is again $L(\theta|X,Z)$, where $\theta$ are your parameters, $X$ your observations and $Z$ are latent variables. The fact that you have a latent variables shouldn''t confuse you, because this variable will be "integrated out"  in the expectation step of the algorithm.

In the previous equations, the calculation you need to do is self-evident if you think about it starting from your observations: First, I have some observations. Then I want to calculate the probability of obtaining these observation using some model. Most models require parameters to be fitted. So I want to calculate the probability of these observations given those parameters. That is, $p(X|\theta)$. Now, if you need to talk about likeliihood, then you simply write $L(\theta|X)$.', 4621, '2015-02-14 21:45:05.987', '633e8c90-f91b-4ae8-9e37-78ef21ef50cc', 5140, 8565, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-14 22:02:46.520', '683009ba-9a15-4b1b-81b3-575fb3156ef1', 5129, '102', 8568, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What got me to understand the problem about overfitting was by imagining what the most overfit model possible would be. Essentially, it would be a simple look-up table.

You tell the model what attributes each piece of data has and it simply remembers it and does nothing more with it. If you give it a piece of data that it''s seen before, it looks it up and simply regurgitates what you told it earlier. If you give it data it *hasn''t* seen before, the outcome is unpredictable or random. But the point of machine learning isn''t to tell you what happened, it''s to understand the patterns and use those patterns to predict what''s going on.

So think of a decision tree. If you keep growing your decision tree bigger and bigger, eventually you''ll wind up with a tree in which every leaf node is based on exactly one data point. You''ve just found a backdoor way of creating a look-up table.

In order to generalize your results to figure out what might happen in the future, you must create a model that generalizes what''s going on in your training set. Overfit models do a great job of describing the data you already have, but descriptive models are not necessarily predictive models.

The No Free Lunch Theorem says that no model can outperform any other model on the set of all *possible* instances. If you want to predict what will come next in the sequence of numbers "2, 4, 16, 32" you can''t build a model more accurate than any other if you don''t make the assumption that there''s an underlying pattern. A model that''s overfit isn''t really evaluating the patterns - it''s simply modeling what it knows is possible and giving you the observations. You get predictive power by assuming that there is some underlying function and that if you can determine what that function is, you can predict the outcome of events. But if there really is no pattern, then you''re out of luck and all you can hope for is a look-up table to tell you what you know is possible.', 8104, '2015-02-14 22:45:50.003', '7ac784a9-d8dd-4af9-8779-fe96f2e21e6f', 5141, 8569, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m wondering how other developers are setting up their local environments for working on Spark projects. Do you configure a ''local'' cluster using a tool like Vagrant? Or, is it most common to SSH into a cloud environment, such as a cluster on AWS? Perhaps there are many tasks where a single-node cluster is adequate, and can be run locally more easily.', 3466, '2015-02-15 04:51:21.167', 'a505ff45-489b-4216-b7b9-ac075d18761f', 5142, 8570, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Local Development for Apache Spark', 3466, '2015-02-15 04:51:21.167', 'a505ff45-489b-4216-b7b9-ac075d18761f', 5142, 8571, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<spark>', 3466, '2015-02-15 04:51:21.167', 'a505ff45-489b-4216-b7b9-ac075d18761f', 5142, 8572, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am confused by the definition of the likelihood function in different contexts.

In the case of linear and logistic regression, it is defined as y given x

In the case naive bayes and LDA, it is defined over x and y

In the case of the EM algorithm (with observed variable x and unobserved variable z), it is defined over x

How do we know over what it should be defined?', 8273, '2015-02-15 13:20:23.303', '007fc63b-a609-476a-92b2-7d263ad67aa8', 5138, 'added 4 characters in body', 8579, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hi I would appreciate it if someone can point me in the right direction.  I''m looking for an algorithm or mathematical theory which I would use to compute the similarity between two ordered lists, where each list element can have n sub-elements.  I will explain with an example:

Suppose I go to a baseball game and I record the sequence of strikes and balls for each of the first 30 players at bat.  My list looks like this, where P is a player, S is a strike and B is a ball. Order matters.

L1: {P1=(S,S,S)}, {P2=(B,B,S)}, {P3=(B,B,S,S)}, ...

My friend goes to a baseball game and does the same thing.  Later, we meet up and compare our lists.  We find that our lists are almost identical except that I recorded a strike for player 16 where my friend recorded a ball.  What are the chances we were at the same game and one of us made a mistake at player 16?

Thanks in advance...', 8281, '2015-02-15 14:00:31.457', '7de925e9-f64f-4ae8-b670-5cf36e0b2061', 5145, 8580, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Comparing two ordered lists', 8281, '2015-02-15 14:00:31.457', '7de925e9-f64f-4ae8-b670-5cf36e0b2061', 5145, 8581, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><statistics>', 8281, '2015-02-15 14:00:31.457', '7de925e9-f64f-4ae8-b670-5cf36e0b2061', 5145, 8582, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What about a model stating that you have two vectors of size N (where N is the total number of players, maybe unknown to us) where each element belongs to a space of {B,S} sequences, maybe empty. If you then define a distance function between two arbitrary sequences (say, normalized Levenshtein distance for two non-empty ones and some fixed cost when one is missing), you can define cosine similarity between the vectors.', 5249, '2015-02-15 14:14:47.110', '87f7f609-2395-4af0-8ef9-62e03d13a906', 5146, 8583, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What about a model stating that you have two vectors of size N (where N is the total number of players, maybe unknown to us) where each element belongs to a space of {B,S} sequences, maybe empty. If you then define a distance function between two arbitrary sequences (say, normalized Levenshtein distance for two non-empty ones and some fixed cost when one is missing), you can define cosine similarity between the vectors.

(Obviously, you just consider your sequence a compact representation of sparse vector in this case.)', 5249, '2015-02-15 14:20:40.370', '78d35328-8142-4a20-9742-136a09e799ae', 5146, 'added 104 characters in body', 8584, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the best known Data Science Methodologies today? A google search did not turn up much, but I find it hard to believe there is nothing out there. Any ideas? ', 7720, '2015-02-15 17:01:50.720', 'ef5d80e9-6135-444e-acbc-837b4bb65dd1', 5147, 8585, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data Science Methodologies', 7720, '2015-02-15 17:01:50.720', 'ef5d80e9-6135-444e-acbc-837b4bb65dd1', 5147, 8586, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<methods>', 7720, '2015-02-15 17:01:50.720', 'ef5d80e9-6135-444e-acbc-837b4bb65dd1', 5147, 8587, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Can you elaborate what you mean by ''methodologies''?

In the meantime, take a look at [The Field Guide To Data Science][1] by Booz Allen Hamilton. This guide talks about data science processes and frameworks.

[Data Science Design Patterns][2] by Mosaic talks about, you guessed it, data science design patterns. This is quite useful to get a sense of common design patterns. They are also working on releasing a book on the same subject.

Then there are several resources out there that will come up as results to more targeted searches, such as machine learning paradigms, recommender systems paradigms, etc. Data Science is a large and varied field, and you''ll find many resources out there for each subsection of it. As far as I know, there isn''t one book that covers it all.

  [1]: http://www.boozallen.com/insights/2013/11/data-science-field-guide
  [2]: http://www.mosaicdatascience.com/resources/data-science-design-patterns/', 90, '2015-02-15 17:35:11.930', 'afc98ae4-d3a2-405d-bff5-68b3ce28e8a5', 5148, 8588, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-15 17:36:08.860', '4c05e981-6a86-4ae9-a140-e1b4a2e06921', 5147, '103', 8592, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I also found confusing the definition of likelihood the first time I encountered it. The only thing you need to remember is that likelihood refers to the probability of your data given a parameter. That is,

$$L(\theta| X) = P(X|\theta)$$

As you can see, the likelihood focuses on the parameter, so you''re asking "What is the likelihood of this parameter given some observations?" In this case, the phrase "given some observation" assumes the observations $X$ are fixed and the parameter $\theta$ is not fixed. On the other hand, if you describe the same in terms of probability, you''re trying to measure the probability of some observations $X$ given a fixed parameter $\theta$. It is only a matter of usage.

In the case of logistic regression, the likelihood function is described as:

$$L(\beta| y) = \prod_{i}^{N} \frac{n_{i}}{n_{i}!(n_{i}-y_{i})!}\pi_{i}^{y_{i}}(1-\pi_{i})^{n_{i} - y_{i}}$$

where $\beta$ is the vector of parameters $\{\pi_{i}| i=1,...,N \}$and $y$ is the vector of observed counts. As you can see, this is the same (by definition) as $P(y|\beta)$ and that is what you have in the equation above: The product of the probabilities for each observation $y_{i}$ given its parameter $\pi_{i}$. As usual, each probability $\pi_{i}$ is equal to the logistic regression equation.

In Naive Bayes, you have the same basic idea. The likelihood is defined as $L(p_{k}|x) = \prod_{i}^{N}p_{ki}^{x_{i}}(1-p_{ki})^{(1-x_{i})}$ where $p_{k}$ is a vector of parameters describing the probabilities of a class $k$ generating a word $x_{i}$).

In the EM algorithm, the likelihood is again $L(\theta|X,Z)$, where $\theta$ are your parameters, $X$ your observations and $Z$ are latent variables. The fact that you have a latent variables shouldn''t confuse you, because this variable will be "integrated out"  in the expectation step of the algorithm.

In the previous equations, the calculation you need to do is self-evident if you think about it starting from your observations: First, I have some observations. Then I want to calculate the probability of obtaining these observation using some model. Most models require parameters to be fitted. So I want to calculate the probability of these observations given those parameters. That is, $p(X|\theta)$. Now, if you need to talk about likeliihood, then you simply write $L(\theta|X)$.', 4621, '2015-02-15 17:42:11.850', 'c1f2d345-e3be-4b73-b710-5e64102b66ef', 5140, 'fixed typo', 8594, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Alex''s answer is spot on in how you would go about figuring out similarity. One more step required, to answer your question, is to come up with a threshold of similarity. I.e. some similarity threshold beyond which you can say that the discrepancies are probably errors.

If you are looking for resources to learn more about this, I''d recommend [Data Mining Concepts and Techniques][1] by Han et all


  [1]: http://www.amazon.com/Data-Mining-Concepts-Techniques-Management/dp/0123814790', 90, '2015-02-15 17:46:34.887', '9fe231e8-f482-48a4-9c9f-54dd8add2f57', 5149, 8595, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What are the best known Data Science Methodologies today? By methodology I mean a step-by-step phased process that can be used for framing guidance, although I will be grateful for something close too.

A google search did not turn up much, but I find it hard to believe there is nothing out there. Any ideas? ', 7720, '2015-02-15 17:48:18.527', '080af7f0-d84e-4e75-a582-7e4033c0e691', 5147, 'added 147 characters in body', 8596, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I also found confusing the definition of likelihood the first time I encountered it. The only thing you need to remember is that likelihood refers to the probability of your data given a parameter. That is,

$$L(\theta| X) = P(X|\theta)$$

As you can see, the likelihood focuses on the parameter, so you''re asking "What is the likelihood of this parameter given some observations?" In this case, the phrase "given some observation" assumes the observations $X$ are fixed and the parameter $\theta$ is not fixed. On the other hand, if you describe the same in terms of probability, you''re trying to measure the probability of some observations $X$ given a fixed parameter $\theta$. It is only a matter of usage.

In the case of logistic regression, the likelihood function is described as:

$$L(\beta| y) = \prod_{i}^{N} \pi_{i}^{y_{i}}(1-\pi_{i})^{n_{i} - y_{i}}$$

where $\beta$ is the vector of parameters $\{\pi_{i}| i=1,...,N \}$and $y$ is the vector of observed counts. As you can see, this is the same (by definition) as $P(y|\beta)$ and that is what you have in the equation above: The product of the probabilities for each observation $y_{i}$ given its parameter $\pi_{i}$. As usual, each probability $\pi_{i}$ is equal to the inverse of the logit function.

In Naive Bayes, you have the same basic idea. The likelihood is defined as $L(p_{k}|x) = \prod_{i}^{N}p_{ki}^{x_{i}}(1-p_{ki})^{(1-x_{i})}$ where $p_{k}$ is a vector of parameters describing the probabilities of a class $k$ generating a word $x_{i}$).

In the EM algorithm, the likelihood is again $L(\theta|X,Z)$, where $\theta$ are your parameters, $X$ your observations and $Z$ are latent variables. The fact that you have a latent variables shouldn''t confuse you, because this variable will be "integrated out"  in the expectation step of the algorithm.

In the previous equations, the calculation you need to do is self-evident if you think about it starting from your observations: First, I have some observations. Then I want to calculate the probability of obtaining these observation using some model. Most models require parameters to be fitted. So I want to calculate the probability of these observations given those parameters. That is, $p(X|\theta)$. Now, if you need to talk about likeliihood, then you simply write $L(\theta|X)$.', 4621, '2015-02-15 17:57:09.287', 'e22f4962-512e-4b92-b9c1-37da47d4c124', 5140, 'changed binomial formulation to bernoulli', 8597, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('What are the best known Data Science Methodologies today? By methodology I mean a step-by-step phased process that can be used for framing guidance, although I will be grateful for something close too.

To help clarify, there are methodologies in the programming world, like Extreme Programming, Feature Driven Development, Unified Process, and many more. I am looking for their equivalents, if they exist.

A google search did not turn up much, but I find it hard to believe there is nothing out there. Any ideas? ', 7720, '2015-02-15 18:03:06.890', 'b74da321-7b08-4e70-b3cc-2e9fcb5b0102', 5147, 'added 207 characters in body', 8598, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-15 19:49:39.313', '217d8ee9-376c-45a1-be9e-aa6fe33a0a7e', 5147, 8599, '11');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m currently writing a book about Data Science in Higher Education, and the following methodologies are the ones I am including:

For *regression*, we have:

 - Simple Linear Regression
 - Multiple Linear Regression

For *classification*, we have:

 - Naive Bayes Classifier
 - Decision Tree Induction
 - K-Nearest Neighbor

These are some of the more elementary topics in statistical analysis (which you could argue is predictive analytics which you could argue is data science), and thus I would suspect they are also the more common.



', 3152, '2015-02-15 23:07:23.753', '8cd2d45a-ccc6-44c0-9b07-06bb2a9a6c47', 5150, 8602, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I will assume that you have not been working in statistics that long since you are talking about jumping ship because of a new software that''s coming out. Let me give you a bit of reassurance:

For the past fifty or so years computers have been solving complex mathematical problems for statisticians and mathematicians, and now with the type of computing power we have today we are able to process terabytes of data per second and develop extremely sophisticated predictive models in a matter of seconds. Software has constantly evolved and constantly made people''s lives easier. Has that changed the fact that we still need people to interpret the data? The model? The results? No, it hasn''t.

However, you should know that when software comes out that eliminates a lot of the grunt work, well, you no longer need to employ undergrads/post-baccs to complete those grunt tasks. Now you can use a software to do it, and higher people of a stronger caliber to perform more complex sets of analyses. The more software evolves, the more training a data scientist will require before breaking into the field.

So you see, it''s not that completing the nano degree alone will or will not give you a competitive edge in the data science job market, it''s that this nano degree is *the first step* in your journey toward becoming a data scientist.

If anything, software like this should encourage all of us to hit the books and ensure that we''re still up to date on our game. No amount of bangs and whistles will teach a computer judgement. Prediction? Yes. But not judgement. We don''t have Skynet yet. ', 3152, '2015-02-15 23:16:47.210', '2de8e877-5e7e-4646-8860-21e58d58ef8b', 5151, 8603, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Why restrict yourself to those two approaches? Because they''re cool? I would always start with a simple linear classifier \ regressor. So in this case a Linear SVM or Logistic Regression, preferably with an algorithm implementation that can take advantage of sparsity due to the size of the data. It will take a long time to run a DL algorithm on that dataset, and I would only normally try deep learning on specialist problems where there''s some hierarchical structure in the data, such as images or text. It''s overkill for a lot of simpler learning problems, and takes a lot of time and expertise to learn and also DL algorithms are very slow to train. Additionally, just because you have 50M rows, doesn''t mean you need to use the entire dataset to get good results. Depending on the data, you may get good results with a sample of a few 100,000 rows or a few million. I would start simple, with a small sample and a linear classifier, and get more complicated from there if the results are not satisfactory. At least that way you''ll get a baseline. We''ve often found simple linear models to out perform more sophisticated models on most tasks, so you want to always start there.', 1301, '2015-02-16 02:32:21.373', 'fc750fff-878e-47d3-9014-e89c00956ef0', 5152, 8607, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m doing some similar research, and have found PluralSight, http://pluralsight.com, to be an invaluable resource.  They have video courses on Machine Learning, AWS, Azure, Hadoop, Big Data, etc.  Personally, I find that these video courses allow me to learn the material much faster and more easily than books.', 8284, '2015-02-16 03:03:43.883', 'cc00da88-0358-4dd5-b5ef-32debb577da5', 5153, 8610, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Apologies if poorly worded:
I am a beginner to Hadoop and want to schedule various jobs in a sequene.
Multiple jobs include running 2 map reduce jobs one after another followed by a Pig script followed by a Python script for final processing.

Sequence is as follows
MapReduce1-->Mapreduce2-->Pig Script-->Python script

I have read about Oozie but still not clear about how to schedule the pig script and Python script using that.

Any help or pointers is highly appreciated.

Thank You', 8259, '2015-02-16 06:17:50.963', '2ed42cea-e127-4e40-98f4-4f489c3b2498', 5154, 8612, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Scheduling multiple jobs in Hadoop', 8259, '2015-02-16 06:17:50.963', '2ed42cea-e127-4e40-98f4-4f489c3b2498', 5154, 8613, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<hadoop><map-reduce>', 8259, '2015-02-16 06:17:50.963', '2ed42cea-e127-4e40-98f4-4f489c3b2498', 5154, 8614, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Given some clusters created from similarity measures between items, is there a recommended way to assign a new item to an existing cluster based on similarity alone? (i.e. avoiding re-clustering)

Measuring the similarity of a new item to all other items is fairly cheap, so I''m looking for a way of using this to assign it to the cluster it''s most likely to belong to. It''s also important for it to take cluster size into account (i.e. doesn''t unfairly weight towards or against larger clusters).

Basically, I''m trying to sacrifice some clustering accuracy in exchange for avoiding a complete re-clustering when the occasional new item is added.
', 474, '2015-02-16 12:16:26.923', 'a012e209-304c-4f82-b5c7-71d9c1b4d19b', 5155, 8615, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Assigning new items to existing similarity based clustering', 474, '2015-02-16 12:16:26.923', 'a012e209-304c-4f82-b5c7-71d9c1b4d19b', 5155, 8616, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering><similarity><online-learning>', 474, '2015-02-16 12:16:26.923', 'a012e209-304c-4f82-b5c7-71d9c1b4d19b', 5155, 8617, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I suggest you think about this in terms of "data set" and "training set" (technically, it is also recommended to have a separate test set). Once you have your clusters defined on the training set, your can start using them to classify any amount of new data without recalculating, by simply measuring similarity to cluster centroids, for example.

(This doesn''t prevent you from deciding to enlarge your training set and data set later, just try to not do that selectively to avoiding overfitting.)', 5249, '2015-02-16 13:50:32.200', '10dd41b1-275a-4dcc-b89b-942cd7916378', 5156, 8618, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I also found confusing the definition of likelihood the first time I encountered it. The only thing you need to remember is that likelihood refers to the probability of your data given a parameter. That is,

$$L(\theta| X) = P(X|\theta)$$

As you can see, the likelihood focuses on the parameter, so you''re asking "What is the likelihood of this parameter given some observations?" In this case, the phrase "given some observation" assumes the observations $X$ are fixed and the parameter $\theta$ is not fixed. On the other hand, if you describe the same in terms of probability, you''re trying to measure the probability of some observations $X$ given a fixed parameter $\theta$. It is only a matter of usage.

In the case of logistic regression, the likelihood function is described as:

$$L(\beta| y) = \prod_{i}^{N} \pi_{i}^{y_{i}}(1-\pi_{i})^{n_{i} - y_{i}}$$

where $\beta$ is the vector of parameters $\{\pi_{i}| i=1,...,N \}$and $y$ is the vector of Bernoulli variables. As you can see, this is the same (by definition) as $P(y|\beta)$ and that is what you have in the equation above: The product of the probabilities for each observation $y_{i}$ given its parameter $\pi_{i}$. As usual, each probability $\pi_{i}$ is equal to the inverse of the logit function.

In Naive Bayes, you have the same basic idea. The likelihood is defined as $L(p_{k}|x) = \prod_{i}^{N}p_{ki}^{x_{i}}(1-p_{ki})^{(1-x_{i})}$ where $p_{k}$ is a vector of parameters describing the probabilities of a class $k$ generating a word $x_{i}$).

In the EM algorithm, the likelihood is again $L(\theta|X,Z)$, where $\theta$ are your parameters, $X$ your observations and $Z$ are latent variables. The fact that you have a latent variables shouldn''t confuse you, because this variable will be "integrated out"  in the expectation step of the algorithm.

In the previous equations, the calculation you need to do is self-evident if you think about it starting from your observations: First, I have some observations. Then I want to calculate the probability of obtaining these observation using some model. Most models require parameters to be fitted. So I want to calculate the probability of these observations given those parameters. That is, $p(X|\theta)$. Now, if you need to talk about likeliihood, then you simply write $L(\theta|X)$.', 4621, '2015-02-16 20:03:48.880', '4407d2aa-af74-45e1-a8c4-dd4a3411dd8f', 5140, 'forgot to change a sentence to describe Bernoulli distribution', 8620, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('While I have only had a brief look at it I think [SageMathCloud][1] looks quite promising. I have recommended it to at least one person previously, and they seemed to be quite happy with it. Bayond R support you also get access to Python, SAGE (as the name indicates) and a few other things.

EDIT: Make sure to check the [documentation][2] on how to get an R (as opposed to a Python) session in a worksheet.

  [1]: https://cloud.sagemath.com/
  [2]: https://github.com/sagemath/cloud/wiki/FAQ#useR', 536, '2015-02-16 20:14:41.513', '15c03e84-3c25-440b-a78e-b186b9a9a9fa', 5158, 8621, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":-1,"DisplayName":"Community"}]}', -1, '2015-02-16 20:24:58.803', '1e65d255-12d4-487c-a12b-c757a196b649', 5154, 8624, '14');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-16 20:24:58.803', '52c983de-343b-4c60-b0ab-0e52e5e269ac', 5154, 'to http://stackoverflow.com/questions/28549723/scheduling-multiple-jobs-in-hadoop', 8625, '35');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-16 20:24:58.803', '9e9e91ba-6941-481c-aa8c-ed472ac43be3', 5154, '102', 8626, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have two time-dependent coupled equations. One of which is several orders of magnitude more computationally demanding than the other. I am trying to use machine learning to reproduce the behavior of the more expensive equation.

system 1

    input: c(t), a(t)
    output a(t+dt)

system 2

    input: a(t)
    output: c(t+dt)

So essentially I want to reconstruct the response of equation 2. Keep in mind that internally there are variables in equation 2 which retain ''memory'' of the previous states. So the response depends on the history of input.



some more details, this is a multiscale simulation

system 1 is a simple finite difference equation

$a(x,t+dt) = 2a(x,t) - a(x,t-dt) + \frac{dt^2}{dx^2} \left[ a(x+dx,t)-2a(x,t) + a(x-dx,t) \right] + dt^2 c(x,t)$


for the second part at each x I have a time-dependent set U(t) to propogate to U(t+dt). This propagation depends on an input a(x,t) and produces c(x,t+dt) to be fed back into the first equation. The details of this part are a bit convoluted/involved, but the essential point is that I want to avoid explicitly storing or propagating U (very very very expensive e.g. 10,000+ cpu cores needed)


Any advice on where to start or what methods have been developed for this type of system? OR if there is a more appropriate place to post this?


', 8302, '2015-02-16 21:10:06.893', 'aa70d90b-d87d-4a06-bae7-0989f9c20180', 5159, 8627, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Regression model of time dependent system response', 8302, '2015-02-16 21:10:06.893', 'aa70d90b-d87d-4a06-bae7-0989f9c20180', 5159, 8628, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><time-series><regression><performance>', 8302, '2015-02-16 21:10:06.893', 'aa70d90b-d87d-4a06-bae7-0989f9c20180', 5159, 8629, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><neuralnetwork><time-series><regression><performance>', 8302, '2015-02-16 21:19:01.190', '59050dcb-8399-49fb-8e71-b5499ce3223f', 5159, 'edited tags', 8630, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is in continuation to a previos question. I would like to know where in practice will the following M-ary tree-structured social network propagation model arise. I am looking for some concrete examples in social media (twitter and youtube etc.). Eventually, I want to do some statistical analysis on such social networks.

![Example of a binary tree-structured social network][1]


  [1]: http://i.stack.imgur.com/x14St.jpg', 8127, '2015-02-17 02:39:20.920', 'dfa1ef45-fd03-496b-b4b1-9961bee7fbfe', 5160, 8631, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where the following propagation model arise in practice', 8127, '2015-02-17 02:39:20.920', 'dfa1ef45-fd03-496b-b4b1-9961bee7fbfe', 5160, 8632, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics><social-network-analysis>', 8127, '2015-02-17 02:39:20.920', 'dfa1ef45-fd03-496b-b4b1-9961bee7fbfe', 5160, 8633, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('[RStudio Server](http://www.rstudio.com/products/rstudio/#Server) is definately one of the options, meant exactly for this. I''ve thought about using it with a cloud virtual machine, but haven''t had the need yet. But when I (probably) need to prepare an intro data analysis class for the fall semester, then Rstudio Server is the first option I''ll be trying out.', 587, '2015-02-17 06:43:03.820', '5e4702e5-7e39-4751-8e44-35d9ef2e2067', 5161, 8634, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In addition to other answers (and there''s some good link in the comments) it depends on what the problem is or what kinds of questions you want to answer. As I can only suggest based on my own experience, then in case of a classification task, the possible methods can be severely limited based on class balance in dataset.

Once you go to a larger than around 1:10 class imbalance, then most classification methods just stop working. You''ll be left with methods based on random forest and maybe neural nets (haven''t tried yet). I work with the class balance in the range of 1:500 to 1:1000 and have found that neither down- or upsampling works. Luckily my dataset is "only" 6mln observations by 200 variables and I''m able to run boosted trees on the whole set in reasonable time.

So to directly answer your question:

* you should come up with a bunch of questions you would want to answer and in case of classification then check the class balances of the target variables.

* you should check the distribution (not in mathematical sense) of missing values in all of your data and document what you find. Some ML methods are fine with missing values while others are not and you need to look into data imputation (which has its own set of rules and guidelines and problems).', 587, '2015-02-17 07:02:21.497', '6fae0f60-94f3-491d-97a6-729fbb2337c4', 5162, 8635, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''d like to see the top N results for a RandomForestClassifier prediction, ordered by descending probability.

The answer may be predict_proba, but I have no idea how to interpret the results.

Help appreciated!', 5233, '2015-02-17 18:07:28.333', 'e33031ca-ba26-476a-82ef-69e3f7a5d586', 5164, 8639, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Getting Scikit-Learn RandomForestClassifier to output Top N results', 5233, '2015-02-17 18:07:28.333', 'e33031ca-ba26-476a-82ef-69e3f7a5d586', 5164, 8640, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<random-forest><scikit>', 5233, '2015-02-17 18:07:28.333', 'e33031ca-ba26-476a-82ef-69e3f7a5d586', 5164, 8641, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('*Hadoop* is a buzzword now. A lot of start-ups use it (or just say, that they use it), a lot of widely known companies use it. But when and what is the border? When person can say: "Better to solve it without Hadoop"?
', 5255, '2015-02-17 19:10:05.547', '79154f01-7880-42bd-ad21-0cd6319f0373', 5165, 8642, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When it is time to use Hadoop?', 5255, '2015-02-17 19:10:05.547', '79154f01-7880-42bd-ad21-0cd6319f0373', 5165, 8643, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata><hadoop><efficiency><scalability><performance>', 5255, '2015-02-17 19:10:05.547', '79154f01-7880-42bd-ad21-0cd6319f0373', 5165, 8644, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have the following data

($x^1_i$, $y^1_i$) for $i=1,2,...N_1$

($x^2_i$, $y^2_i$) for $i=1,2,...N_2$

...

($x^m_i$, $y^m_i$) for $i=1,2,...N_m$

Is it possible to train a neural net to produce some $y_k$ where $k<=min(N)$ given a input ${x_1, x_2, ..., x_{k-1}}$?

If so any suggestion of documentation/ library I can look at (preferably python)?

', 8302, '2015-02-17 21:50:10.210', '98c2acc6-3b4f-43c5-9071-4f18ee7c1fc7', 5166, 8645, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('training neural net with multiple sets of time-series data', 8302, '2015-02-17 21:50:10.210', '98c2acc6-3b4f-43c5-9071-4f18ee7c1fc7', 5166, 8646, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><dataset><neuralnetwork><time-series><regression>', 8302, '2015-02-17 21:50:10.210', '98c2acc6-3b4f-43c5-9071-4f18ee7c1fc7', 5166, 8647, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('It''s an economic calculation, really.  When you have a computing "problem" (in the most general possible sense) that you can''t solve with one computer, it makes sense to use a cluster of commodity machines when doing so A. allows you to solve the problem, and B. is cheaper than a forklift upgrade to a bigger computer, or upgrading to specialized hardware.

When those things are true, and you are going the "commodity cluster" route, Hadoop makes a lot of sense, especially if the nature of the problem maps (no pun intended) well to MapReduce.  If it doesn''t, one shouldn''t be scared to consider "older" cluster approaches like a Beowulf cluster using MPI or OpenMP.

That said, the newer YARN based Hadoop does support a form of MPI, so those worlds are starting to move closer together.
', 6554, '2015-02-17 22:50:19.067', 'abc468e8-efeb-4d41-bd7c-fe08c476a66a', 5167, 8648, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have 22 classes of objects but they have very skewed distributions where max class has 100.000 images and the min class has 1600 images. In that setting I would like to hear some possible solutions to this balance problem.

I have tried followings so far;

1. multiply number of instances in the lower classes up to the max class by replicating the instances, possibly adding some noise as well.

2. changing the learning regarding the class distribution in the given minibatch of the next epoch. (no implemented but in my mind)

What are your suggestions?', 464, '2015-02-17 22:55:26.300', 'd20621f5-035f-4dfa-aa79-bf525bf6edf0', 5168, 8649, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What are the possible ways to handle class unbalance in a large scale image recognition problem with Deep Neural Nets?', 464, '2015-02-17 22:55:26.300', 'd20621f5-035f-4dfa-aa79-bf525bf6edf0', 5168, 8650, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><unbalanced-classes><deep-learning><object-recognition>', 464, '2015-02-17 22:55:26.300', 'd20621f5-035f-4dfa-aa79-bf525bf6edf0', 5168, 8651, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('From my perspective, for 5 million instances you need lots of trees to get a good generalization bound (a good model in the layman term). If this is not a problem then go for it,even the exact answer is relying on the nature of your problem. GBT is a good method especially if you have mixed feature types like categorical, numerical and such. In addition, compared to Neural Networks it has lower number of hyperparameters to be tuned. Therefore, it is faster to have a best setting model. One more thing is the alternative of parallel training. You can train multiple trees at the same time with a good CPU. If you are not satisfied with the results then go for Neural Nets since that means your model should be more extensive and should learn higher order information through your data. That is the due of NNs compared to other learning algorithms. ', 464, '2015-02-17 23:04:07.277', '58275e94-8283-425c-8a36-d9923cec5b3e', 5169, 8652, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have used the same methods/parameters to create two decision trees. The trees classify the presence or absence of a medical condition using the presence or absence of various symptoms. There is a tree for Medical Condition #1 and another tree for Medical Condition #2. Both trees are based on the same set of symptoms, rated by patients. If Medical Condition #1 resulted in a much simpler tree than Medical Condition #2, can that suggest Medical Condition #2 is a more complex disease? If so, can anyone point me to a reference that states the complexity/depth of a tree can be representative of a complex condition?', 8313, '2015-02-18 00:08:32.800', '24001fb8-a632-4b11-a1e9-fb3fe7600336', 5170, 8653, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('depth/complexity of decision trees', 8313, '2015-02-18 00:08:32.800', '24001fb8-a632-4b11-a1e9-fb3fe7600336', 5170, 8654, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<classification><statistics>', 8313, '2015-02-18 00:08:32.800', '24001fb8-a632-4b11-a1e9-fb3fe7600336', 5170, 8655, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m currently trying to predict a continuous variable using KNN. Instead of treating each neighbor equally I would like to use the weights to create a weighted average. The weights by themselves are not ideal, as the closer a neighbor the more I would like that neighbor to influence the final results.

This lead me to consider the inverse of each of the distances, but this doesn''t handle the case where an instance is the exact same -> with a distance of 0.

Any recommendations on how to properly set the weights of each neighbor relative to their distance? Similar to how the inverse would handle this, but one that allows for 0 values.', 8314, '2015-02-18 00:33:51.823', '468ff6b7-50c3-4e2e-be9d-8ea03d6dcfab', 5171, 8656, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to appropriately set weights for weighted KNN', 8314, '2015-02-18 00:33:51.823', '468ff6b7-50c3-4e2e-be9d-8ea03d6dcfab', 5171, 8657, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 8314, '2015-02-18 00:33:51.823', '468ff6b7-50c3-4e2e-be9d-8ea03d6dcfab', 5171, 8658, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have code to generate feature vectors in the style of the bag-of-words model, you can see it on my [github page](https://github.com/h1395010/perceptron/blob/master/src/file_dict_createur/FileDictCreateur.java).

It renders output of the form:

    /data/train/politics/p_0.txt, [0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
    /data/train/science/s_0.txt, [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0]
    /data/train/atheism/a_0.txt, [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    /data/train/sports/s_1.txt, [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]

I want to move on to the next step of the perceptron but I''m confused about what to do next. I guess the next thing I need to do is generate a feature vector for the ''test'' data, isn''t it?

For instance, in [this example](http://en.literateprograms.org/index.php?title=Special:DownloadCode/Perceptron_(Java)&oldid=19184) what is ''teachingOutput'' supposed to be, training data vectors? Such as the ones I have above?

I also tried poking around in [this implementation](http://stackoverflow.com/questions/21738277/perceptron-learning-most-important-feature) in an attempt to figure out what to do but, alas, to no avail. ', 8285, '2015-02-18 04:24:51.153', '75cbe047-1cf5-4c5e-b059-b1e65e66c5cc', 5172, 8659, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('perceptron implementation, preocedure for post-feature vecor: bag-of-words model', 8285, '2015-02-18 04:24:51.153', '75cbe047-1cf5-4c5e-b059-b1e65e66c5cc', 5172, 8660, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 8285, '2015-02-18 04:24:51.153', '75cbe047-1cf5-4c5e-b059-b1e65e66c5cc', 5172, 8661, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Technically speaking the inverse of the distance should not pose any problem. Considering the case when you have to classify an observation (instance) whih is identical with one observation from the ''learned'' data set, than the inverse distance is not defined by a direct formula since you have to divide by zero. But considering the math behind, inverse of the zero distance is positive infinity. Which means that, the weight for the identical point would dominate all other weights of the data instances. Thus the classification for that specific point could be taken as the class of the identical observation from the data set.

On the other hand the inverse of the euclidean distance is only one type of distance. You can always apply a kernel function on that to weight non-uniformly the contributions. For example you can choose a normal density with mean equals zero and variance equals 1. Than, your weight would be the value of density function for the euclidean distance. ', 108, '2015-02-18 09:04:27.770', '7d62d36b-5fde-4927-8092-9d63ca6276cb', 5173, 8662, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I often get the problem when this or that alias name is already used somewhere, and I can''t easily find that variable or aggregation to

![enter image description here][1]


  [1]: http://i.stack.imgur.com/kK9mM.png

Is there some place in Tableau where I can view/edit/reset full list of aliases?', 97, '2015-02-18 10:03:13.913', '4f89a81b-44ae-4544-befb-90d6bbac08c4', 5175, 8664, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('List of used aliases in Tableau', 97, '2015-02-18 10:03:13.913', '4f89a81b-44ae-4544-befb-90d6bbac08c4', 5175, 8665, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization><tools><tableau>', 97, '2015-02-18 10:03:13.913', '4f89a81b-44ae-4544-befb-90d6bbac08c4', 5175, 8666, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-18 11:13:49.617', 'bbaafc58-7221-49c8-9485-3800a6cd4361', 5165, '104', 8672, '10');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2015-02-18 12:37:46.950', 'ce686b41-d86a-4890-bc75-ebad4495b32d', 5178, 'from http://academia.stackexchange.com/questions/38813/how-to-deal-with-large-amounts-of-binary-data', 8677, '36');
INSERT INTO postHistory(userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES (-1, '2015-02-18 12:37:46.950', '4720a807-4635-4f8c-b439-58068810535d', 5179, 'from http://academia.stackexchange.com/questions/38813/how-to-deal-with-large-amounts-of-binary-data/38820#38820', 8678, '36');
INSERT INTO postHistory(userdisplayname, text, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('jakebeal', 'I have dealt with similar problems with very large synthetic biology datasets, where we have many, many GB of [flow cytometry][1] data spread across many, many thousands of files, and need to maintain them consistently between collaborating groups at (multiple) different institutions.

Typical version control like svn and git is not practical for this circumstance, because it''s just not designed for this type of dataset.  Instead, we have fallen to using "cloud storage" solutions, particularly [DropBox][2] and [Bittorrent Sync][3].  DropBox has the advantage that it does do at least some primitive logging and version control and manages the servers for you, but the disadvantage that it''s a commercial service, you have to pay for large storage, and you''re putting your unpublished data on a commercial storage; you don''t have to pay much, though, so it''s a viable option.  Bittorrent Sync has a very similar interface, but you run it yourself on your own storage servers and it doesn''t have any version control.  Both of them hurt my programmer soul, but they''re the best solutions my collaborators and I have found so far.


  [1]: http://en.wikipedia.org/wiki/Flow_cytometry
  [2]: https://www.dropbox.com/
  [3]: http://en.wikipedia.org/wiki/BitTorrent_Sync', '2015-02-13 13:38:56.797', '8827bfdd-ebd6-4ac2-bae5-3f99c515d06f', 5179, 8679, '2');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am a PHD student of Geophysics and work with large amounts of image data  (hundreds of GB, tens of thousands of files). I know svn and git fairly well and come to value a project history, combined with the ability to easily work together and have protection against disk corruption. I find git also extremely helpful for having consistent backups but I know that git cannot handle large amounts of binary data efficiently.

In my masters studies I worked on data sets of similar size (also images) and had a lot of problems keeping track of different version on different servers/devices. Diffing 100GB over the network really isn''t fun, and cost me a lot of time and effort.

I know that others in science seem to have similar problems, yet I couldn''t find a good solution.

I want to use the storage facilities of my institute, so I need something that can use a "dumb" server. I also would like to have an additional backup on a portable hard disk, because I would like to avoid transferring hundreds of GB over the network wherever possible. So I need a tool that can handle more than one remote location.

Lastly, I really need something that other researcher can use, so it does not need to be super simple, but should be learnable in a few hours.

If evaluated a lot of different solutions, but non seems to fit the bill:

- [svn][1] is somewhat inefficient and needs a smart server
- hg [bigfile][2]/[largefile][3] can only use one remote
- git [bigfile][4]/[media][5] can also use only one remote, but is also not very efficient
- [attic][6] doesn''t seem to have a log, or diffing capabilities
- [bup][7] looks really good, but needs a "smart" server to work

I''ve tried git annex which does everything I need it to do (and much much more) but is very difficult to use, and not well documented. I''ve used it for several days and couldn''t get my head around it, so I doubt any other coworker would be interested.

How do researchers deal with large datasets, and what are other research groups using?

To be clear, I am primarily interested in how other researchers deal with this situation, not just this specific dataset. It seems to me almost everyone should have this problem, yet I don''t know anyone who has solved it. Should I just keep a backup of the original data and forget all this version control stuff? Is that what everyone else is doing?

  [1]: http://subversion.apache.org/
  [2]: http://mercurial.selenic.com/wiki/BigfilesExtension
  [3]: http://mercurial.selenic.com/wiki/LargefilesExtension
  [4]: https://github.com/beenje/git-bigfile
  [5]: https://github.com/alebedev/git-media
  [6]: https://attic-backup.org/
  [7]: https://bup.github.io/', 'Johann', 8320, '2015-02-13 10:09:25.177', '1caf54cd-5f20-48b1-b38e-1092c589bacd', 5178, 8680, '2');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to deal with large amounts of (binary) data', 'Johann', 8320, '2015-02-13 10:09:25.177', '1caf54cd-5f20-48b1-b38e-1092c589bacd', 5178, 8681, '1');
INSERT INTO postHistory(text, userdisplayname, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<databases>', 'Johann', 8320, '2015-02-13 10:09:25.177', '1caf54cd-5f20-48b1-b38e-1092c589bacd', 5178, 8682, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I often get the problem when this or that alias name is already used somewhere, and I can''t easily find that variable or aggregation to release the name.

![enter image description here][1]


  [1]: http://i.stack.imgur.com/kK9mM.png

Is there some place in Tableau where I can view/edit/reset full list of aliases?', 97, '2015-02-18 13:07:25.567', 'e0edd481-f815-45e7-b811-191d3fefb3fe', 5175, 'cosmetic changes', 8684, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As there isn''t too much code/context you''re giving us I assume that you''re working with a binary classification problem:

    import numpy as np
    X = ... # input for classification, shape (n_samples, n_features)
    y_pred = rf.predict_proba(X)[:, 1] # index slicing to retrieve a 1d-array of probabilities
    y_pred
    # array([  0.18, 0.21, 0.1, 0.2, 0.3])

    # Now lets see what the 2 biggest probabilities are
    np.argsort(y_pred)
    # [2, 0, 3, 1, 4] => lowest probability is at index 2 (the 3rd probability as classified by the RandomForestClassifier)
    # Now lets see what the top 5 samples look like (remember, argsort sorts in ascending order, so we take the last 5 indices of argsort):
    X[np.argsort(y_pred)[-5:]]


Now you should see the rows in X with the 5 most confident predictions.

    ', 8319, '2015-02-18 14:07:39.487', '049b9ce3-7571-445d-b1aa-7a092a5f2fcf', 5180, 8685, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Imagine each node in your graph as a user, and each edge as an action such as ''share''. It may also be a bidirectional relationship ''share'' and ''view''.

Some social scientists and engineers estimate the probability that a message is ''shared'' and ''viewed'' given that a particular user decides to share it. This process is called "information diffusion", "information propagation", "cascading", etc.

If you are interested in the details of how these calculations are performed, check out these papers:

http://cs.stanford.edu/people/jure/pubs/netrate-netsci14.pdf

http://cs.stanford.edu/people/jure/pubs/cascades-www14.pdf

http://cs.stanford.edu/people/jure/pubs/infopath-wsdm13.pdf

Similar topics:
http://snap.stanford.edu/papers.html', 3466, '2015-02-18 23:14:02.563', 'a1cb6812-6a92-4622-a5d4-c5eedef3b690', 5181, 8689, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to deal with version controlling large amounts of (binary) data', 289, '2015-02-19 05:31:32.323', 'f6413d5a-b6d2-4057-81db-2c3737038bdc', 5178, 'title and tags', 8692, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<bigdata><databases><binary><version-control>', 289, '2015-02-19 05:31:32.323', 'f6413d5a-b6d2-4057-81db-2c3737038bdc', 5178, 'title and tags', 8693, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-02-19 05:31:32.323', 'f6413d5a-b6d2-4057-81db-2c3737038bdc', 5178, 'Proposed by 289 approved by -1 edit id of 231', 8694, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I am a PhD student of Geophysics and work with large amounts of image data  (hundreds of GB, tens of thousands of files). I know `svn` and `git` fairly well and come to value a project history, combined with the ability to easily work together and have protection against disk corruption. I find `git` also extremely helpful for having consistent backups but I know that git cannot handle large amounts of binary data efficiently.

In my masters studies I worked on data sets of similar size (also images) and had a lot of problems keeping track of different version on different servers/devices. Diffing 100GB over the network really isn''t fun, and cost me a lot of time and effort.

I know that others in science seem to have similar problems, yet I couldn''t find a good solution.

I want to use the storage facilities of my institute, so I need something that can use a "dumb" server. I also would like to have an additional backup on a portable hard disk, because I would like to avoid transferring hundreds of GB over the network wherever possible. So, I need a tool that can handle more than one remote location.

Lastly, I really need something that other researcher can use, so it does not need to be super simple, but should be learnable in a few hours.

I have evaluated a lot of different solutions, but none seem to fit the bill:

- [svn][1] is somewhat inefficient and needs a smart server
- hg [bigfile][2]/[largefile][3] can only use one remote
- git [bigfile][4]/[media][5] can also use only one remote, but is also not very efficient
- [attic][6] doesn''t seem to have a log, or diffing capabilities
- [bup][7] looks really good, but needs a "smart" server to work

I''ve tried `git-annex`, which does everything I need it to do (and much more), but it is very difficult to use and not well documented. I''ve used it for several days and couldn''t get my head around it, so I doubt any other coworker would be interested.

How do researchers deal with large datasets, and what are other research groups using?

To be clear, I am primarily interested in how other researchers deal with this situation, not just this specific dataset. It seems to me that almost everyone should have this problem, yet I don''t know anyone who has solved it. Should I just keep a backup of the original data and forget all this version control stuff? Is that what everyone else is doing?

  [1]: http://subversion.apache.org/
  [2]: http://mercurial.selenic.com/wiki/BigfilesExtension
  [3]: http://mercurial.selenic.com/wiki/LargefilesExtension
  [4]: https://github.com/beenje/git-bigfile
  [5]: https://github.com/alebedev/git-media
  [6]: https://attic-backup.org/
  [7]: https://bup.github.io/', 2452, '2015-02-19 05:31:32.323', '9fa5e378-1a9e-4046-b6f4-52e7bb176aa6', 5178, 'Improved title, grammar, wording and formatting.', 8695, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('How to deal with version control of large amounts of (binary) data', 2452, '2015-02-19 05:31:32.323', '9fa5e378-1a9e-4046-b6f4-52e7bb176aa6', 5178, 'Improved title, grammar, wording and formatting.', 8696, '4');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have used [Versioning on Amazon S3 buckets](http://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html) to manage 10-100GB in 10-100 files. Transfer can be slow, so it has helped to compress and transfer in parallel, or just run computations on EC2. The [boto](https://github.com/boto/boto) library provides a nice python interface.', 8335, '2015-02-19 06:30:45.190', '2389dc6f-fa86-4db1-9b94-56599ccba920', 5182, 8697, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":471,"DisplayName":"Spacedman"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-19 10:06:20.863', 'bcc458a5-d376-40ec-85cd-a5a4d1444865', 5109, '105', 8704, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Finally solved the issue.
You should go to:

    Data > your_data_source > Edit Aliases > Measure Names

And see full mapping between variables/aggregations and aliases.

![enter image description here][1]



  [1]: http://i.stack.imgur.com/uSoHC.png', 97, '2015-02-19 12:06:08.173', 'd56b6195-8566-4e6e-b3a3-00743d226686', 5184, 8707, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A decaying exponential of the form $e^{-\alpha x}$ (where $x$ is the distance from the observation) is convenient in this situation. It has the nice feature that the weight is equal to $1$ when the observation lies exactly at one of your training points and decays to zero as $x\rightarrow \infty$.

What you need to decide is the scaling factor, $\alpha$, which can greatly affect your results, since the decaying exponential is nonlinear. If $\alpha$ is too small or large, then all $k$ points will be weighted nearly equally (assuming none of the $k$ points have a distance very close to zero). One approach is to pick a global value of $\alpha$ that is suitable for your data set. Another approach is to compute a new value of $\alpha$ for each set of $k$ neighbors under consideration, which will guarantee variability in the weights (as long as all $k$ neighbors are not equidistant).', 964, '2015-02-19 14:59:05.007', '6e38f3d2-3cd6-4749-8dad-be255f90589b', 5185, 8708, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I need to draw a decision tree about this subject :

The research and development manager in an old oil company, which is considering making some changes, lists the following courses of action for the company:

(i) Adopt a process developed by another oil company. This would cost 7 million in royalties and yield a net 20 million profit (before paying the royalty).

(ii) Carry out one or two (not simultaneously) alternative research projects :

(R1) the more expensive one has a 0.8 chance of success; net profit 16 million
and a further 6 million in royalties. If it fails there will be a net loss of 10 million.

(R2) the alternative research programme is less expensive but only has a 0.7
chance of success with a net profit of 15 million and a further 5 million in royalties. If it fails a net loss of 6 million will be incurred.

(iii) Make no changes. After meeting current operating costs the company expects to make a net 15 million profit from its existing process.
Failure of one research program would still leave open all remaining courses of action (including the other research programme).

I need also to indicate the different payoffs. This is what I''ve done so far :
![enter image description here][1]

I would like to be sure that I''m going in the right direction since I''m a beginner with decision trees. And then I need to decide the best course of action using Bayes, Maximax and Maximin rules.

  [1]: http://i.stack.imgur.com/BLTJ6.png', 8343, '2015-02-19 16:23:15.093', 'a2621fcb-227c-4755-b25c-bc6df3b96761', 5186, 8710, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Decision Tree Bayes rules / Maximax / Maximin', 8343, '2015-02-19 16:23:15.093', 'a2621fcb-227c-4755-b25c-bc6df3b96761', 5186, 8711, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics>', 8343, '2015-02-19 16:23:15.093', 'a2621fcb-227c-4755-b25c-bc6df3b96761', 5186, 8712, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('No, you can''t infer that.
I assume you have the same training set with the same predictors (symptoms). The only difference in the training set is the binary class label for each patient.

The smaller tree just means:
- with the given symptoms, it might be easier to distinguish between the people who have the condition 1 and the ones not having it. (since you have the same certainty with fewer information)
This distinction seems harder with medical condition 2, that''s way you have to consider more symptoms to be pretty sure about your classification. So if condition 2 is a very mild condition where it is hard to diagnose even for an expert if someone has it then it would result in a deep tree.

', 8191, '2015-02-19 22:17:45.773', '084ed1bd-769e-443c-94e5-241e09af880f', 5188, 8718, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am working on a project in NLP area and in order to test the result I need to compare it against different data sources. I already have NYTimes data source. So,
I am looking for another news source data set easily available online. I have heard of Thomson Reuters and WSJs dataset but all through my search I have been unable to find one. Does anyone here know of some dataset?

Are there datasets for scientific journals too? JSTOR, Elsevier anything?

Your help is highly appreciated.

Thanks!', 1165, '2015-02-20 05:09:19.917', 'cb743e26-87d2-4625-b884-566721fbbe24', 5189, 8720, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('hunting for NLP datasets', 1165, '2015-02-20 05:09:19.917', 'cb743e26-87d2-4625-b884-566721fbbe24', 5189, 8721, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset><nlp>', 1165, '2015-02-20 05:09:19.917', 'cb743e26-87d2-4625-b884-566721fbbe24', 5189, 8722, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m quite new to Data Science, but I would like to do a project to learn more about it.
My subject will be Data Understanding in Public Health.
So I want to do some introductory research to public health.
I would like to visualize some data with the use of a tool like Tableau.
Which path would you take to develop a good understanding of Data Science? I imagine taking some online courses eg., Udacity courses on datascience, but which courses would you recommend?
Where can I get real data (secondary Dummy Data) to work with?
And is there some good resources on reasearch papers done in Data Science area with the subject of Public Health?


Any suggestions, comments is welcome.
', 7887, '2015-02-20 12:21:31.560', 'bda6ad33-3f6a-401d-8111-3bace82efd0c', 5192, 8728, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Where can I find resources and papers regarding Data Science in the area of Public Health', 7887, '2015-02-20 12:21:31.560', 'bda6ad33-3f6a-401d-8111-3bace82efd0c', 5192, 8729, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization><data-visualization><tableau><research>', 7887, '2015-02-20 12:21:31.560', 'bda6ad33-3f6a-401d-8111-3bace82efd0c', 5192, 8730, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was wondering which language can I use: R or Python, for my internship in fraud detection in an online banking system: I have to build machine learning algorithms (NN, etc.) that predict transaction frauds.
Thank you very much for your answer.', 8357, '2015-02-20 14:09:23.483', '9147ecb0-0efc-4d7a-8306-88afe28545fd', 5193, 8732, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Python or R for implementing machine learning algorithms for fraud detection', 8357, '2015-02-20 14:09:23.483', '9147ecb0-0efc-4d7a-8306-88afe28545fd', 5193, 8733, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><python>', 8357, '2015-02-20 14:09:23.483', '9147ecb0-0efc-4d7a-8306-88afe28545fd', 5193, 8734, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Given a set of temperatures of different cities for a month, which prediction model should I use for a two day look ahead prediction? Regression models or Time series?', 8360, '2015-02-20 15:40:49.800', '7a26c5a0-e38d-4342-9881-9fa33174ffdb', 5195, 8736, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Choosing the right model for prediction', 8360, '2015-02-20 15:40:49.800', '7a26c5a0-e38d-4342-9881-9fa33174ffdb', 5195, 8737, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<time-series><predictive-modeling><regression>', 8360, '2015-02-20 15:40:49.800', '7a26c5a0-e38d-4342-9881-9fa33174ffdb', 5195, 8738, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Hello and thanks in advance!  I''d like some advice on a scalability issue and the best way to resolve.  I''m writing an algorithm in R to produce forecasts for several thousand entities.  One entity takes about 43 seconds to generate a forecast and upload the data to my database.  That equates to about 80+ hours for the entire set of entities and that''s much too long.

I thought about running several R processes in parallel, possibly many on a few different servers, each performing forecasts for a portion of total entities.  Though that would work, is there a better way?  Can Hadoop help at all?  I have little experience with Hadoop so don''t really know if it can apply.  Thanks again!', 8358, '2015-02-20 21:05:16.223', 'fdaf6b29-b2f8-4718-a571-fdaa1bd655b3', 5196, 8740, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Scaling thousands of automated forecasts in R', 8358, '2015-02-20 21:05:16.223', 'fdaf6b29-b2f8-4718-a571-fdaa1bd655b3', 5196, 8741, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><scalability><forecast>', 8358, '2015-02-20 21:05:16.223', 'fdaf6b29-b2f8-4718-a571-fdaa1bd655b3', 5196, 8742, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('A usual way to find association rules in R is the "arules" package, which easily let''s use calculate some rules based on the apriori algorithm.

However, for the data i''m using, I have a lot of NULL cases (baskets where no product A o B is present). This means that I need to calculate some null-invariant measures (kulcynski, for instance).

Does anyone know of any package or workable code that let''s me implement this as opposed to writing from scratch the entire algorithm?', 8296, '2015-02-20 21:56:45.690', '33241c3c-23bd-4307-9c5c-13eb47c16408', 5197, 8743, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Null-invariant measures of association in R', 8296, '2015-02-20 21:56:45.690', '33241c3c-23bd-4307-9c5c-13eb47c16408', 5197, 8744, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><r>', 8296, '2015-02-20 21:56:45.690', '33241c3c-23bd-4307-9c5c-13eb47c16408', 5197, 8745, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Are there commonly accepted ways to visualize the results of a multivariate regression for a non-quantitative audience? In particular, I''m asking how one should present data on coefficients and T statistics (or p-values) for a regression with around 5 independent variables.', 6403, '2015-02-20 23:16:18.767', 'e74ae7e8-c092-4815-95a2-b593639c73cc', 5198, 8746, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to visualize multivariate regression results', 6403, '2015-02-20 23:16:18.767', 'e74ae7e8-c092-4815-95a2-b593639c73cc', 5198, 8747, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<visualization><regression><linear-regression>', 6403, '2015-02-20 23:16:18.767', 'e74ae7e8-c092-4815-95a2-b593639c73cc', 5198, 8748, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This question is likely somewhat naive.  I know I (and my colleagues) can install and use Python on local machines. But is that really a best practice?  I have no idea.

Is there value in setting up a Python "server"?  A box on the network where we develop our data science related Python code.  If so, what are the hardware requirements for such a box?  Do I need to be concerned about any specific packages or conflicts between projects?', 8368, '2015-02-21 01:16:34.387', 'ae3fdaed-6b2a-4b94-8f0a-b2fbdd6af694', 5199, 8749, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is a good hardware setup for using Python across multiple users', 8368, '2015-02-21 01:16:34.387', 'ae3fdaed-6b2a-4b94-8f0a-b2fbdd6af694', 5199, 8750, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python>', 8368, '2015-02-21 01:16:34.387', 'ae3fdaed-6b2a-4b94-8f0a-b2fbdd6af694', 5199, 8751, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would say that it is your call and purely depends on your comfort with (or desire to learn) the language. Both languages have extensive ecosystems of packages/libraries, including some, which could be used for fraud detection. I would consider anomaly detection as the main theme for the topic. Therefore, the following resources illustrate the variety of approaches, methods and tools for the task in each ecosystem.

**Python Ecosystem**

- `scikit-learn` library: for example, see [this page](http://scikit-learn.org/stable/modules/outlier_detection.html);
- `LSAnomaly`, a Python module, improving `OneClassSVM` (a drop-in replacement): see [this page](http://www.cit.mak.ac.ug/staff/jquinn/software/lsanomaly.html);
- `Skyline`: an open source example of implementation, see [its GitHub repo](https://github.com/etsy/skyline);
- A [relevant discussion](http://stackoverflow.com/q/18970171/2872891) on *StackOverflow*.

**R Ecosystem**

- [CRAN Task Views](http://cran.r-project.org/web/views), in particular, *Time Series*, *Multivariate* and *Machine Learning*;
- Twitter''s [Anomaly Detection package](https://github.com/twitter/AnomalyDetection);
- [Early Warning Signals (EWS) Toolbox](http://www.early-warning-signals.org), which includes `earlywarnings` [package](http://cran.r-project.org/web/packages/earlywarnings);
- [h2o](http://h2o.ai) machine learning platform (interfaces with R) uses [deep learning for anomaly detection](http://learn.h2o.ai/content/hands-on_training/anomaly_detection.html).

**Additional General Information**

- A [set of slides](http://www.autonlab.org/tutorials/biosurv01.pdf), mentioning a variety of methods for anomaly detection (AD);
- A [relevant discussion](http://stats.stackexchange.com/q/26688/31372) on *Cross Validated*;
- [ELKI Data Mining Framework](http://elki.dbs.ifi.lmu.de), implementing a large number of AD (and other) algorithms.', 2452, '2015-02-21 10:08:39.980', '6a753700-bb13-47f7-8db8-49edeab1dc6d', 5200, 8753, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('**Is installing Python locally a good practice?**  Yes, if you are going to develop in Python, it is always a good idea to have a local environment where you can break things safely.

**Is there value in setting up a Python "server"?**  Yes, but before doing so, be sure to be able to share your code with your colleagues using a [version control system](http://en.wikipedia.org/wiki/Revision_control).  My reasoning would be that, before you move things to a server, you can move a great deal forward by being able to test several different versions in the local environment mentioned above.  Examples of VCS are [git](http://git-scm.com), [svn](https://subversion.apache.org), and for the deep nerds, [darcs](http://darcs.net).

Furthermore, a "Python server" where you can deploy your software once it is integrated into a releasable version is something usually called "[staging server](http://en.wikipedia.org/wiki/Staging_site)".  There is a whole philosophy in software engineering &mdash; [Continuous Integration](http://en.wikipedia.org/wiki/Continuous_integration) &mdash; that advocates staging whatever you have in VCS daily or even on each change.  In the end, this means that some automated program, running on the staging server, checks out your code, sees that it compiles, runs all defined tests and maybe outputs a package with a version number.  Examples of such programs are [Jenkins](http://jenkins-ci.org), [Buildbot](http://buildbot.net) (this one is Python-specific), and [Travis](https://travis-ci.org/recent) (for cloud-hosted projects).

**What are the hardware requirements for such a box?** None, as far as I can tell.  Whenever it runs out of disk space, you will have to clean up.  Having more CPU speed and memory will make concurrent builds easier, but there is no real minimum.

**Do I need to be concerned about any specific packages or conflicts between projects?**  Yes, this has been identified as a problem, not only in Python, but in many other systems (see [Dependency hell](http://en.wikipedia.org/wiki/Dependency_hell)).  The established practice is to keep projects isolated from each other as far as their dependencies are concerned.  This means, avoid installing dependencies on the system Python interpreter, even locally; always define a [virtual environment](http://virtualenv.readthedocs.org/en/latest/) and install dependencies there.  Many of the aforementioned CI servers will do that for you anyway.', 1367, '2015-02-21 11:59:31.860', '866faf1e-bde9-44db-a1f7-ef90046989cb', 5201, 8754, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is largely a subjective question.  Trying to list some criteria that seem objective to me:

- the important advantage of Python is that it is a general-purpose language.  If you will need to do anything else than statistics with your program (generate a web interface, integrate it with a reporting system, pass it on to other developers for maintenance) you are far better off with Python.

- the important advantage of R is that it is a specialized language.  If you already know that there is a technique you want to use, and it is not a usual suspect (like NN), then you probably will find a library in CRAN that makes life easier for you.

And here is another, more subjective advice:

- both languages are not really performance-oriented.  If you need to process large quantities of data, or process very fast, or process in parallel, you will run into trouble ... but it is far easier to run into such trouble with R than with Python.  In my experience, you find the limits of R within some weeks, and the way to push them is quite arcane; while you can use Python for years, never really missing the speed that C developers always mention as the Holy Grail, and even when you do, you can use Cython to make up for the difference.  The only real trouble is concurrency, but for statistics, it is hardly ever an issue.', 1367, '2015-02-21 12:19:34.107', 'fa644287-b826-4f0c-b2cf-13213042cadc', 5202, 8755, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you''re working with R language, I would suggest first to try use **R ecosystem**''s abilities to *parallelize* the processing, if possible. For example, take a look at packages, mentioned in [this CRAN Task View](http://cran.r-project.org/web/views/HighPerformanceComputing.html).

**Alternatively**, if you''re not comfortable or satisfied with the approaches, implemented by the above-referred packages, you can try some other approaches, such as Hadoop or something else. I think that a *Hadoop* solution would be an **overkill** for such problem, considering the *learning curve*, associated with it, as well as the fact that, as far as I understand, Hadoop or other MapReduce frameworks/architectures target *long-running processes* (an average task is ~ 2 hours, I read somewhere recently). Hope this helps.', 2452, '2015-02-21 12:33:53.923', '5586bed6-e47c-48ea-80a7-0f548d42136a', 5203, 8756, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Most literature focus on either explicit rating data or implicit (like/unknown) data. Are there any good publications to handle like/dislike/unknown data? That is, in the data matrix there are three values, and I''d like to recommend from unknown entries.

And are there any good open source implementations on this?

Thanks.', 1376, '2015-02-21 14:02:39.170', 'a10f5c19-6890-4339-b554-1743fdeb9d17', 5204, 8757, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Matrix factorization for like/dislike/unknown data', 1376, '2015-02-21 14:02:39.170', 'a10f5c19-6890-4339-b554-1743fdeb9d17', 5204, 8758, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><recommendation>', 1376, '2015-02-21 14:02:39.170', 'a10f5c19-6890-4339-b554-1743fdeb9d17', 5204, 8759, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-21 23:22:32.677', '00215260-09b5-4f16-8382-60fd5c32c5c8', 5199, '104', 8760, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-21 23:22:54.040', '7360d6e1-658c-4b3c-9a70-32be9c9517c9', 5193, '103', 8761, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a pandas dataframe with a salary column which contains values like:

> £36,000 - £40,000 per year plus excellent bene...,
> £26,658 to £32,547 etc


I isolated this column and split it with the view to recombining into the data frame later via a column bind in pandas.

I now have an object with columns like the below. The columns I split the original data frame column I think are blank because I didn''t specify them (I called `df[''salary'']=df[''salary''].astype(str).str.split()`
)

So my new object contains this type of information:

[£26,658, to, £32,547],
[Competitive, with, Excellent, Benefits]

What I want to do is:


 1. Create three columns called minvalue and maxvalue and realvalue
 2. List items starting with £ (something to do with `"^£"`?
 3. Take till the end of the items found ignoring the £ (get the number out) (something to do with `(substr(x,2,nchar(x)))`?
 4. If there are two such items found, call the first number "minvalue" and call the second number "maxvalue" and put it below the right column. If there is only one value in the row, put it below the realvalue column.

I am very new to pandas and programming in general, but keen on learning, your help would be appreciated. I know it has something to do with finding ', 8375, '2015-02-22 11:57:57.840', 'c0bbe22c-855a-44f8-b5ff-368c71fc1463', 5205, 8762, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Split columns by finding "£" and then converting to minvalue, maxvalue', 8375, '2015-02-22 11:57:57.840', 'c0bbe22c-855a-44f8-b5ff-368c71fc1463', 5205, 8763, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<pandas>', 8375, '2015-02-22 11:57:57.840', 'c0bbe22c-855a-44f8-b5ff-368c71fc1463', 5205, 8764, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<dataset><statistics><pandas>', 8375, '2015-02-22 12:29:12.403', '664e696f-fca6-429f-8268-f7b07623e271', 5205, 'edited tags', 8765, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I have a pandas dataframe with a salary column which contains values like:

> £36,000 - £40,000 per year plus excellent bene...,
> £26,658 to £32,547 etc


I isolated this column and split it with the view to recombining into the data frame later via a column bind in pandas.

I now have an object with columns like the below. The columns I split the original data frame column I think are blank because I didn''t specify them (I called `df[''salary'']=df[''salary''].astype(str).str.split()`
)

So my new object contains this type of information:

[£26,658, to, £32,547],
[Competitive, with, Excellent, Benefits]

What I want to do is:


 1. Create three columns called minvalue and maxvalue and realvalue
 2. List items starting with £ (something to do with `"^£"`?
 3. Take till the end of the items found ignoring the £ (get the number out) (something to do with `(substr(x,2,nchar(x)))`?
 4. If there are two such items found, call the first number "minvalue" and call the second number "maxvalue" and put it below the right column. If there is only one value in the row, put it below the realvalue column.

I am very new to pandas and programming in general, but keen on learning, your help would be appreciated. ', 8375, '2015-02-22 12:57:21.040', '61e140c3-2d59-4f04-bdb5-110dee61852d', 5205, 'deleted 43 characters in body', 8766, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is more of a general regex question, rather than pandas specific.

I would first create a function that extracts the numbers you need from strings, and then use the `pandas.DataFrame.apply` function to apply it on the pandas column containing the strings. Here is what I would do:

    import re
    def parseNumbers(salary_txt):
        return [int(item.replace('','','''')) for item in re.findall(''£([\d,]+)'',salary_txt)]

    #testing if this works
    testcases = [''£23,000 to £100,000'',''£34,000'',''£10000'']
    for testcase in testcases:
        print testcase,parseNumbers(testcase)

Here, I just used `re.findall`, which finds all patterns that look like `£([\d,]+)`. This is anything that starts with £ and is followed by an arbitrary sequence of digits and commas. The parenthesis tells python to extract only the bit after the £ sign. The last thing I do is I remove commas, and parse the remaining string into an integer. You could be more elegant about this I guess, but it works.

#using this function in pandas

    df[''salary_list''] = df[''salary''].apply(parseNumbers)
    df[''minsalary''] = df[''salary''].apply(parseNumbers).apply(min)
    df[''maxsalary''] = df[''salary''].apply(parseNumbers).apply(max)

Checking if this all works:

    import pandas
    df = pandas.DataFrame(testcases,columns = [''salary''])
    df[''minsalary''] = df[''salary''].apply(parseNumbers).apply(min)
    df[''maxsalary''] = df[''salary''].apply(parseNumbers).apply(max)
    df

     salary minsalary maxsalary
    0 £23,000 to £100,000 23000 100000
    1 £34,000 34000 34000
    2 £10000 10000 10000

The advantages of moving the parsing logic to a separate function is that:

1. it may be reusable in other code
1. it is easier to read for others, even if they aren''t pandas experts
1. it''s easier to develop and test the parsing functionality in isolation', 8376, '2015-02-22 13:14:36.200', 'c72ccdae-c0c7-4982-82cc-c967c9fe3eec', 5206, 8767, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The goal of my analysis is separate from this question. I am trying to figure out all the conclusions I can draw or suggest from my analysis. Yes, I am interested in saying something about the complexity of condition A to B.
When is a condition complex ? When there are many symptoms as the disease is diagnosed by the symptoms.
If it''s hard to diagnose ? YES
if the symptoms are severe ? NO
can s.o. have both conditions ? NO', 8313, '2015-02-22 15:44:39.550', '3c873c68-d9fe-4d42-bf23-40628deb7834', 5207, 8768, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As an example. If you are tying to classify humans from dogs. Is it possible to approach this problem by classifying different kinds of animals (birds, fish, reptiles, mammals, ...) or even smaller subsets (dogs, cats, whales, lions, ...)

Then when you try to classify a new data set, anything that did not fall into one of those classes van be considered a human.

If this is possible, are there any benefits into breaking a binary class problem into several classes (or perhaps labels)?

Benefits I am looking into are: accuracy/precision of the classifier, parallel learning.
', 8381, '2015-02-22 20:30:30.630', '5b459ffe-ee26-44ba-9e21-bb6c6ff26f51', 5208, 8769, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Splitting binary classification into smaller susbsets', 8381, '2015-02-22 20:30:30.630', '5b459ffe-ee26-44ba-9e21-bb6c6ff26f51', 5208, 8770, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification>', 8381, '2015-02-22 20:30:30.630', '5b459ffe-ee26-44ba-9e21-bb6c6ff26f51', 5208, 8771, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('As an example. If you are tying to classify humans from dogs. Is it possible to approach this problem by classifying different kinds of animals (birds, fish, reptiles, mammals, ...) or even smaller subsets (dogs, cats, whales, lions, ...)

Then when you try to classify a new data set, anything that did not fall into one of those classes can be considered a human.

If this is possible, are there any benefits into breaking a binary class problem into several classes (or perhaps labels)?

Benefits I am looking into are: accuracy/precision of the classifier, parallel learning.
', 8381, '2015-02-23 00:19:21.277', '56b4f813-2835-42b5-9931-178346e99419', 5208, 'edited body', 8772, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am performing Named Entity Recognition using Stanford NER. I have successfully trained and tested my model. Now I want to know:

1) What is the general way of measuring accuracy of NER model ?? For example what techniques or approaches are used ??

2) Is there any built-in method in STANFORD NER for evaluating the accuracy ??

', 8016, '2015-02-23 08:00:03.360', 'bd46588b-66c3-41da-8a6e-1a73228d5ac9', 5209, 8773, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Accuracy of Stanford NER', 8016, '2015-02-23 08:00:03.360', 'bd46588b-66c3-41da-8a6e-1a73228d5ac9', 5209, 8774, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><performance>', 8016, '2015-02-23 08:00:03.360', 'bd46588b-66c3-41da-8a6e-1a73228d5ac9', 5209, 8775, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I personally like dotcharts of standardized regression coefficients, possibly with standard error bars to denote uncertainty. Make sure to standardize coefficients (and SEs!) appropriately so they "mean" something to your non-quantitative audience: "As you see, an increase of 1 unit in Z is associated with an increase of 0.3 units in X."

In R (without standardization):

    set.seed(1)
    foo <- data.frame(X=rnorm(30),Y=rnorm(30),Z=rnorm(30))
    model <- lm(X~Y+Z,foo)

    coefs <- coefficients(model)
    std.errs <- summary(model)$coefficients[,2]

    dotchart(coefs,pch=19,xlim=range(c(coefs+std.errs,coefs-std.errs)))
    lines(rbind(coefs+std.errs,coefs-std.errs,NA),rbind(1:3,1:3,NA))
    abline(v=0,lty=2)

![regression visualization][1]


  [1]: http://i.stack.imgur.com/0AwtB.png', 2853, '2015-02-23 08:17:30.120', '84f30247-c329-4ef5-a58f-f34ee888dc06', 5210, 8776, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you try to get the best accuracy, etc... for a given question you should always learn on a training set that is labeled exactly according to your questions. You shouldn''t expect to get better results if you are using more granular class labels. The classifier then would then try to pick up the differences in the classes and try to separate them apart. Since in practice your variables in the training set will not perfectly explain the more granular classification question you shouldn''t expect to get a better answer for your less granular classification problem.

If you are not happy with the accuracy of your model you try the following instead:

 1. review the explanatory variables. Think about what might influence the classification problem. Maybe there us a clever way to construct new variables (from your existing ones) that helps. It''s nowpossible to give a general advise on that since you have to consider the properties of your classifier
 2. if your class distribution is very skewed you might consider over/undersampling
 3. you might run more different classifiers and then classify based on the majority vote. Note that you will most likely sacrifice explainability of your model.

Also you seem to have some missunderstanding, when you write ''you would assign it to human if it doesn''t fall into any of the granular classes''. Note that you always try to pick class labels covering the whole universe (all possible classes). This can be always defined as the complement of the other classes. Also you will have to have instances for each class in your training set.', 8191, '2015-02-23 11:43:11.833', '1295918e-4f8c-4a6e-abba-a9ccaa5399ad', 5211, 8777, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('We are currently developing a customer relationship management software for SME''s. What I''d like to structure for our future CRM is developing CRM with a social-based approach (Social CRM). Therefore we will provide our users (SME''s) to integrate their CRM into their social network accounts. Also CRM will be enhance intercorporate communication of owner company.

All these processes I''ve just indicated above will certainly generate lots of unstructured data.

I am wondering how can we integrate <i>big data</i> and <i>data-mining</i> contepts for our project; especially for the datas generated by social network? I am not the expert of these topics but I really want to start from somewhere.

###Basic capabilities of CRM (Modules)###

-Contacts: People who you have a business relationship.

-Accounts: Clients who you''ve done a business before.

-Leads: Accounts who are your potential customers.

-Oppurtunites: Any business opportunity for an account or a lead.

-Sales Orders

-Calendar

-Tasks


What kind of unstructured data or the ways (ideas) could be useful for the modules I''ve just wrote above? If you need more specific information please write in comments.
', 8386, '2015-02-23 12:03:47.773', '8233b405-5830-4c1d-ab5b-e3ebb6f21208', 5212, 8778, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Big data and data mining for CRM?', 8386, '2015-02-23 12:03:47.773', '8233b405-5830-4c1d-ab5b-e3ebb6f21208', 5212, 8779, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><bigdata><software-develpment>', 8386, '2015-02-23 12:03:47.773', '8233b405-5830-4c1d-ab5b-e3ebb6f21208', 5212, 8780, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am using a two-convolution-layer neural networks to test some RNA sequencing data. What I did is first mapping all sequences into matrices as input of CNN, then to predict whether its expression is high or low. Basically I want to classify matrices into two classes. I am using "DeepLearnToolbox" for MATLAB.

I divided my data into training set and testing set in several ways. The strange thing is that I always got 20% training error but almost 100% (99.9%) prediction accuracy.

Even if I divide the entire 14000 data points into 500 points for training and rest for testing, I still get almost 100% right prediction.

I cannot interpret the result, could anyone give any clues to find out why the prediction is always right?

', 2435, '2015-02-23 15:22:56.400', '92f97981-db9c-46d9-9c77-26b72b2dd71f', 5213, 8781, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Convolutional neural network always gives nearly zero test error but higher training error', 2435, '2015-02-23 15:22:56.400', '92f97981-db9c-46d9-9c77-26b72b2dd71f', 5213, 8782, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><error-handling><matlab>', 2435, '2015-02-23 15:22:56.400', '92f97981-db9c-46d9-9c77-26b72b2dd71f', 5213, 8783, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am starting to play around in datamining / machine learning and I am stuck on a problem that''s probably easy.

So I have a report that lists the url and the number of visits a person did. So a combination of ip and url result in an amount of visits.

Now I want to run the k-means clustering algorithm on this so I thought I could approach it like this:

This is my data:

    url      ip    visits

    abc.be   123   5
    abc.be/a 123   2
    abc.be/b 123   2
    abc.be/b 321   4

And I would turn in into a feature vector/matrix like so:


    abc.be  abc.be/a   abc.be/b   impressions
       1       0          0          5
       0       1          0          2
       0       0          1          2
       0       0          1          4

But I am stuck on how to transform my data set to a feature matrix. Any help would be appreciated.', 8389, '2015-02-23 15:45:21.700', 'e3904320-74ea-4e36-8591-173977009cd1', 5214, 8784, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Going from report to feature matrix', 8389, '2015-02-23 15:45:21.700', 'e3904320-74ea-4e36-8591-173977009cd1', 5214, 8785, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<feature-extraction><k-means>', 8389, '2015-02-23 15:45:21.700', 'e3904320-74ea-4e36-8591-173977009cd1', 5214, 8786, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You can check the data types of your columns by doing df.dtypes, and if ''salary'' isn''t a string, you can convert it using df[''salary''] = df[''salary''].astype(str). This is what you were already doing before splitting. From there, Ferenc''s method should work!', 8391, '2015-02-23 21:57:39.677', 'd5f1fe6e-383a-4f35-a83c-be80952cdb6a', 5215, 8787, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I don''t understand what you mean by

> So I have a report that lists the url and the number of visits a person did. **So a combination of ip and url result in an amount of visits.**

Assuming that you equate an IP with a user, and you wish to cluster users by their URL visitation frequencies, your matrix, `M`, would have

 - One row per IP (user)
 - One column for each URL that you are tracking (your features)
 - and the entries in `M` would be "visits" of a given URL by a particular IP

Given these assumptions, and your report, `M` would be:

        abc.be  abc.be/a  abc.be/b
    123   5        2         2
    321   0        0         4', 8392, '2015-02-23 22:39:53.290', '664c766d-e852-4e59-a547-2ca4c36f7896', 5216, 8788, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('https://www.codeschool.com/ is very similar to https://www.datacamp.com/
when I tried it I fell in love with R and then found datacamp.
www.codecademy.com  is also console-based but R is not yet available.', 8394, '2015-02-24 02:11:45.787', 'aaac93a0-f9ea-41cf-aee8-a50aa057de22', 5217, 8789, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Suppose there is a warehouse with 5 dimensions and each of the dimensions has 5 levels not including the apex. How many cuboids would be there in the data cube, including apex and base?

T = Product(from i=1 to n) (Li + 1) - I''m not sure if this includes the apex.
', 2647, '2015-02-24 03:14:02.910', '8913f466-34b4-45e6-88f5-48fc28b38d58', 5219, 8792, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Number of cuboids in a data cube', 2647, '2015-02-24 03:14:02.910', '8913f466-34b4-45e6-88f5-48fc28b38d58', 5219, 8793, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><bigdata>', 2647, '2015-02-24 03:14:02.910', '8913f466-34b4-45e6-88f5-48fc28b38d58', 5219, 8794, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The two modules where you can really harness data mining and big data techniques are probably Leads and Opportunities. The reason is that, as you''ve written yourself, both contain ''potential'' information that you can harness (through predictive algorithms) to get more customers. Taking Leads as an example, you can use a variety of machine learning algorithms to assign a probability to each account, based on that account''s potential for becoming your customer in the near future. Since you already have an Accounts module which gives you information about your current customers, you can use this information to train your machine learning algorithms. This is all at a very high level but hopefully, you''re getting the gist of what I''m saying.', 8395, '2015-02-24 03:51:15.133', '07442fb4-7f66-4b92-ab1e-a89d539818c8', 5220, 8795, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('My advice is to begin with a thorough grounding in Statistics, for which a lot of classic and unambiguous material is available online. Topics that you should have a firm grasp on include regression, correlation, hypothesis testing and the bias-variance tradeoff. You don''t have to go into too much theoretical depth into any of these topics but you should know what these are before you start studying machine learning. Your study in machine learning should include developing an understanding of hypothesis spaces, Bayesian analysis (can you explain the difference between MLE, MAP and optimal Bayes?), Expectation Maximization, logistic regression, clustering (especially k-means), max-margin classifiers (SVMs), overfitting (can you explain what it is in terms of bias and variance?) and feature selection. Linear algebra is helpful but can be done without in many cases.

As someone who does research in data mining and has worked closely with several companies, I can tell you that if you seriously want to go into machine learning, it is NOT enough to simply know how to code something using Weka or R. Those are easy enough to use once you know the concepts. When companies hire data scientists, they want someone who can take the raw data and do something useful with it. A good grasp of fundamentals is obviously essential, since each company''s data has its own quirks (and will typically be too big for you to try ''everything''). Good luck!', 8395, '2015-02-24 04:07:29.860', 'ba6ad0d0-3ad4-44d5-a50c-b304f1793252', 5221, 8796, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('So far the answers have focused on learning particular methods. They are fine, but they won''t make you a Data Scientist. Being a Data Scientist is not solely or even primarily about having mastery of particular data analysis methods (ML or others).

Most fundamental is *problem solving* and *decision support*. What ever data you collect, what ever analysis methods you apply, and however you improve those methods over time, these must support the over-arching goals of solving problems or making better decisions.

You need to start getting first-hand experience with data in your field.  I don''t mean Kaggle data (i.e. already cleaned).  I mean raw data or nearly raw. A good 50% of a data scientist''s time is spent wrangling raw data and cleaning it to the point where it''s usable in analysis.  You need to learn how to deal with missing data, erroneous data, ambiguous data, misformatted data, and so on.

You should also get some experience with decisions that do not map neatly on to the data.  Recommender systems are easy in this regard.  For example, you might take on the challenge of evaluating software vulnerabilities to guide vulnerability management decisions.', 609, '2015-02-24 08:40:05.083', 'ce1faa33-eb8c-4bb6-bec6-ad8d20da3b2f', 5222, 8798, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes, this is a straightforward application for neural networks.  In this case yk are the outputs of the last layer ("classifier"); xk is a feature vector and yk is what it gets classified into.  For simplicity prepare your data so that N is the same for all.  The problem you have is perhaps that in the case of time series you won''t have *enough* data: you need (ideally) many 1000''s of examples to train a network, which in this case means time series, not points.  Look at the specialized literature on neural networks for time series prediction for ideas on network architecture.

Library: try Pylearn2 at http://deeplearning.net/software/pylearn2/  It''s not the only good option but it should serve you well.', 26, '2015-02-24 11:52:40.977', 'e0432f70-a563-4c30-8ae7-09ceb8a34cb4', 5223, 8799, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would like to use a neural network for image classification.  I''ll start with pre-trained CaffeNet and train it for my application.

My question is: how to prepare the input images?  In this case, all the images are of the same object but with variations (think: quality control).  They are at somewhat different scales/resolutions/distances/lighting conditions (and in many cases I don''t know the scale).  Also, in each image there is an area (known) around the object of interest that should be ignored by the network.

I could (for example) crop the center of each image, which is guaranteed to contain a portion of the object of interest and none of the ignored area; but that seems like it would throw away information, and also the results wouldn''t be really the same scale (maybe 1.5x variation).

Also: I''ve heard of creating more training data by random crop/mirror/etc, is there a standard method for this?  Any results on how much improvement it produces to classifier accuracy?', 26, '2015-02-24 11:59:36.033', '5a1f2470-7367-42a1-a2bf-e92b8d69e388', 5224, 8800, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to prepare images for neural network?', 26, '2015-02-24 11:59:36.033', '5a1f2470-7367-42a1-a2bf-e92b8d69e388', 5224, 8801, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork><convnet><image-classification>', 26, '2015-02-24 11:59:36.033', '5a1f2470-7367-42a1-a2bf-e92b8d69e388', 5224, 8802, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Data science is a new somewhat vague terminology.  You will find much more information by using keywords such as: epidemiology, population health, public health surveillance, public health, statistics, evidence based medicine, biostatistics, statistical epidemiology, clinical decision making, interventions etc.

The question as posted is overly broad. On the other hand you''re in luck since there is a substantial amount of information on this topic.  Many of the R/Data Science courses on Coursera from John Hopkins have a public health flavor. Also, R and S-PLUS before it were primarily used in health settings.', 6467, '2015-02-24 22:13:46.327', '95a526f6-dfb9-4961-9c52-b2d60489a95c', 5225, 8804, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am new to machine learning!

Right now I am doing some problems on application of decision tree/random forest. I am trying to fit a problem which has numbers as well as strings (such as country name) as features. Now the library, scikit-learn takes only numbers as parameters, but I want to inject the strings as well as they carry significant amount of knowledge.

How do I handle such scenario, I can convert string to numbers by some mechanism such as hashing in python. But I would like to know the best practice on how strings are handled in decision tree problems.

Thanks for your support!', 8409, '2015-02-25 01:07:14.717', '4ed99699-1ed3-494f-b590-89a3ae23717d', 5226, 8805, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('strings as features in decision tree/random forest', 8409, '2015-02-25 01:07:14.717', '4ed99699-1ed3-494f-b590-89a3ae23717d', 5226, 8806, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<python><random-forest><scikit>', 8409, '2015-02-25 01:07:14.717', '4ed99699-1ed3-494f-b590-89a3ae23717d', 5226, 8807, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am struggling to choose a right data prediction method for the following problem.
Essentially I am trying to model a scheduler operation, trying to predict its scheduling without knowing the scheduling mechanism and having incomplete data.

(1) There are M available resource blocks that can carry data, N data channels that must be scheduled every time instance i

(2) Inputs into the scheduler:

* Matrix $X_i$ size M by N, consisting of N column vectors from each data source.    Each of M elements is index from 1 to 32 carrying information about quality of data channel for particular resource block. 1 - really bad quality, 32 - excellent quality.

* Data which contains type of data to be carried (voice/internet etc)

Scheduler prioritizes number of resource blocks occupied by each channel every time instant i.

Given that

* I CAN see resource allocation map every time instant

* I DO have access to matrix $X_i$

* I DON''T know the algorithm of scheduler and

*  I dont have access to the type of data to be scheduled.

I want to have a best guess (prediction) how the data will be scheduled based on this incomplete information i.e, which resource block will be occupied by which data channel. What is the best choice of prediction/modelling algorithm?
Any help appreciated!
', 8410, '2015-02-25 03:24:20.450', '8f6ab2c1-c756-4000-808f-9fd489cd3ad1', 5227, 8810, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('problem of choosing right statistical method for scheduler prediction', 8410, '2015-02-25 03:24:20.450', '8f6ab2c1-c756-4000-808f-9fd489cd3ad1', 5227, 8811, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<predictive-modeling>', 8410, '2015-02-25 03:24:20.450', '8f6ab2c1-c756-4000-808f-9fd489cd3ad1', 5227, 8812, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('There are several metrics for the quality of a graph clustering, e.g. Newman modularity. These enable you to compare two candidate clusterings of the same graph.

Does anyone know a metric that will answer the question "how modular is this graph"? For example the first of these two graphs is more modular than the second:
    o===o-----o====o    o----o===o-----o

It would be possible to choose a clustering algorithm, run it, and compute your preferred modularity metric for the best clustering found. But this is only a lower bound, so it doesn''t seem very satisfactory.

The question matters. For example, the work of life scientists will be easier if the molecular organisation of life is modular than if it is not. It would be good to have a robust test - some of the discussion so far seems to involve wishful thinking.

My best attempt at this is:
- a tree is more modular if the edges near leaves are higher weight
- the modularity of a graph is the modularity of its min cut spanning tree
Does anyone know of an established answer to this question?', 8417, '2015-02-25 10:07:24.563', 'fa19f5de-f891-4570-aa8b-03c1785ecd24', 5228, 8813, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Graph modularity measure', 8417, '2015-02-25 10:07:24.563', 'fa19f5de-f891-4570-aa8b-03c1785ecd24', 5228, 8814, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<clustering>', 8417, '2015-02-25 10:07:24.563', 'fa19f5de-f891-4570-aa8b-03c1785ecd24', 5228, 8815, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In most of the well established machine learning systems categorical variables are handled naturally. For example in R you would use factors, in weka you would use nominal variables. This is not the care in scikit learn. The decision trees implemented in sklearn uses only numerical features and these features are interpreted always as continuous numeric variables.

Thus, simply replacing the strings with a hash code should be avoided, because being considered as a continuous numerical feature any coding you will use will induce an order which simply does not exists in your data.

One example is to code [''red'',''green'',''blue''] with [1,2,3], would produce weird things like ''red'' is lower than ''blue'', and if you average a ''red'' and a ''blue'' you will get a ''green''. Another more sublte example maigh happen when you code [''low'', ''medium'', ''high''] with [1,2,3]. In the latter case it might happen to have an ordering which makes sense, however some sublte inconsistencies might happen when ''medium'' in not in the middle of ''low'' and ''high''.

Finally the answer to your question lies in coding the categorical feature into multiple binary features. For example you might code [''red'',''green'',''blue''] with 3 columns, one for each category, having 1 when the category match and 0 otherwise. This is called one-hot-encoding, binary encoding, one-of-k-encoding or whatever. You can check documentation here for [encoding categorical features][1] and [feature extraction - hashing and dicts][2]. Obviously one-hot-encoding will expand your space requirements and sometimes it hurts the performance as well.


  [1]: http://scikit-learn.org/stable/modules/preprocessing.html
  [2]: http://scikit-learn.org/stable/modules/feature_extraction.html#dict-feature-extraction', 108, '2015-02-25 10:10:55.683', 'dbb0d023-8ec9-4991-98d9-f20993715a2d', 5229, 8816, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Have you heard about the [intersection graph][1]? You can try to draw players as points, connections (team mates) as edges and teams as transparent coloured blobs on top.

As for your original question, I cannot understand your goal. I think your formulation is not well defined / incomplete. Suppose you have teams A [1,2] B [2,3] and C [1,3]. What do you want to display? Do you want to list the parts of the Venn diagram? I think that for more than 3 sets this can become more cumbersome than the bipartite graph itself = simple listing of team compositions.

  [1]: http://en.wikipedia.org/wiki/Intersection_graph', 6550, '2015-02-25 11:18:26.313', '959bd46a-309b-4067-aea5-279531ca0734', 5230, 8817, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I produced association rules by using the arules package (apriori). I''m left with +/- 250 rules. I would like to test/validate the rules that I have, like answering the question: How do I know that these association rules are true? How can I validate them? What are common practice to test it?

I thought about cross validation (with training data and test data) as I read that it''s not impossible to use it on unsupervised learning methods..but I''m not sure if it makes sense since I don''t use labeled data.

If someone has a clue, even if it''s not specifically about association rules (but testing other unsupervised learning methods), that would also be helpful to me.

I uploaded an example of the data that I use here in case it''s relevant: https://www.mediafire.com/?4b1zqpkbjf15iuy
', 8422, '2015-02-25 14:10:04.777', '035ebea4-41d5-497e-827a-18aaf93421d6', 5231, 8818, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to test/validate unlabeled data in association rules in R?', 8422, '2015-02-25 14:10:04.777', '035ebea4-41d5-497e-827a-18aaf93421d6', 5231, 8819, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><cross-validation>', 8422, '2015-02-25 14:10:04.777', '035ebea4-41d5-497e-827a-18aaf93421d6', 5231, 8820, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('***First, some caveats***

I''m not sure why you can''t use your preferred programming (sub-)paradigm*, *Inductive Logic Programming (ILP)*, or what it is that you''re trying to classify. Giving more detail would probably lead to a much better answer; especially as it''s a little unusual to approach selection of classification algorithms on the basis of the programming paradigm with which they''re associated. If your real world example is confidential, then simply make up a fictional-but-analogous example.

***Big Data Classification without ILP***

Having said that, after ruling out ILP we have 4 other logic programming paradigms in our consideration set:

 1. Abductive
 2. Answer Set
 3. Constraint
 4. Functional

in addition to the dozens of paradigms and sub-paradigms outside of logic programming.

Within *Functional Logic Programming* for instance,  there exists extensions of ILP called *Inductive Functional Logic Programming*, which is based on inversion narrowing (i.e. inversion of the narrowing mechanism). This approach overcomes several limitations of ILP and ([according to some scholars, at least][1]) is as suitable for application in terms of representation and has the benefit of allowing problems to be expressed in a more natural way.

Without knowing more about the specifics of your database and the barriers you face to using ILP, I can''t know if this solves your problem or suffers from the same problems. As such, I''ll throw out a completely different approach as well.

[ILP is contrasted with "classical" or "propositional" approaches to data mining][2]. Those approaches include the meat and bones of Machine Learning like decision trees, neural networks, regression, bagging and other statistical methods. Rather than give up on these approaches due to the size of your data, you can join the ranks of many Data Scientists, Big Data engineers and statisticians who utilize High Performance Computing (HPC) to employ these methods on with massive data sets (there are also sampling and other statistical techniques you may choose to utilize to reduce the computational resources and time required to analyze the Big Data in your relational database).

HPC includes things like utilizing multiple CPU cores, scaling up your analysis with elastic use of servers with high memory and large numbers of fast CPU cores, using high-performance data warehouse appliances, employing clusters or other forms of parallel computing, etc. I''m not sure what language or statistical suite you''re analyzing your data with, but as an example this [CRAN Task View][3] lists many HPC resources for the R language which would allow you to scale up a propositional algorithm.


  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.225.4755&rep=rep1&type=pdf
  [2]: http://books.google.com/books?hl=en&lr=&id=xzVD8C2YpnQC&oi=fnd&pg=PR11&dq=Predictive%20Data-Mining.%20A%20Practical%20Guide.%20&ots=IN48sFFcSQ&sig=d6TOKBx_6qmitB8BSTk4_nLO9ak
  [3]: http://cran.r-project.org/web/views/HighPerformanceComputing.html', 2723, '2015-02-25 17:08:09.830', '1f4f543d-3a1a-48e2-994a-506dd2ac7efe', 5232, 8821, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Please I want to know if there is any SVM R package that can handle more than one response variable (y) at a time. that is to train one model for predicting more than one response variable. it could be regression or multi class classification problem.

Thanks for your help
', 8425, '2015-02-25 18:28:27.903', 'c43ca159-86e4-47e1-9592-fc2e5b653c36', 5233, 8822, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('training one SVM model for predicting more than one response variable', 8425, '2015-02-25 18:28:27.903', 'c43ca159-86e4-47e1-9592-fc2e5b653c36', 5233, 8823, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><algorithms>', 8425, '2015-02-25 18:28:27.903', 'c43ca159-86e4-47e1-9592-fc2e5b653c36', 5233, 8824, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m quite new to Data Science, but I would like to do a project to learn more about it.
My subject will be Data Understanding in Public Health.
So I want to do some introductory research to public health.
I would like to visualize some data with the use of a tool like Tableau.

Which path would you take to develop a good understanding of Data Science? I imagine taking some online courses, eg. Udacity courses on data science, but which courses would you recommend?
Where can I get real data (secondary Dummy Data) to work with?
And are there any good resources on research papers done in Data Science area with the subject of Public Health?

Any suggestions and comments are welcome.', 2452, '2015-02-25 21:49:01.210', '8b34bd8a-f935-4331-ad80-ef4e711d6146', 5192, 'Improved grammar and wording.', 8825, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I don''t think that you will learn much about *data science* (meaning, acquire understanding and skills) by using software tools like Tableau. Such tools are targeting mainly *advanced users* (not *data scientists*), for example analysts and other subject matter experts, who use *graphical user interface (GUI)* to *analyze* and (mostly) *visualize* data. Having said that, software tools like Tableau might be good enough to perform initial phase of data science **workflow**: *exploratory data analysis (EDA)*.

In terms of data science **self-education**, there are several popular online courses (MOOCs) that you can choose from (most come in both free and paid versions). In addition to the one on Udacity that you''ve mentioned (https://www.udacity.com/course/ud359), there are two data science courses on Coursera: *Introduction to Data Science* by University of Washington (https://www.coursera.org/course/datasci) and a set of courses from *Data Science specialization* by Johns Hopkins University (https://www.coursera.org/specialization/jhudatascience/1). Note that you can take specialization''s individual courses for free at your convenience. There are several other, albeit less popular, data science MOOCs.

In terms of **data sources**, I''m not sure what do you mean by "Dummy Data", but there is a wealth of *open data sets*, including many in the area of *public health*. You can review corresponding resources, listed on KDnuggets (http://www.kdnuggets.com/datasets/index.html) and choose ones that you''re interested in. For a country-level analysis, the fastest way to obtain data is finding and visiting corresponding *open data* government websites. For example, for public health data in US, I would go to http://www.healthdata.gov and http://www.data.gov (the latter - for corresponding non-medical data that you might want to include in your analysis).

In regard to **research papers** in the area of public health, I have two comments: 1) most *empirical research* in that (or any other) area IMHO can be considered a data science study/project; 2) you need to perform a *literature review* in the area or on the topic of your interest, so you''re on your own in that sense.

Finally, a note on **software tools**. If you''re serious about data science, I would suggest to invest some time in learning either R, or Python (if you don''t know them already), as those are two most popular open source tools among data scientists nowadays. Both have a variety of feature-rich *development environments* as well as large *ecosystems* of packages/libraries and users/developers all over the world.

You might also find useful some of my other **related answers** here on Data Science StackExchange site. For example, I recommend you to read [this answer](http://datascience.stackexchange.com/a/742/2452), [this answer](http://datascience.stackexchange.com/a/843/2452) and [this answer](http://datascience.stackexchange.com/a/759/2452). Good luck!', 2452, '2015-02-25 22:41:45.663', 'a7eca7a6-aadf-49e3-8b13-e2f8a7ec075c', 5234, 8826, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Do you know if the scheduler has a memory?

Let us assume for a moment that the scheduler has no memory.  This is a straightforward classification (supervised learning) problem: the inputs are X, the outputs are the schedules (N->M maps).  Actually, if *every* N gets scheduled and the only question is which M it gets, the outputs are lists which channel (or none) is scheduled to each block, and there is only a certain possible number of those, so you can model them as discrete outputs (classes) with their own probabilities.  Use whatever you like (AdaBoost, Naive Bayes, RBF SVM, Random Forest...) as a classifier.  I think you will quickly learn about the general behavior of the scheduler.

If the scheduler has a memory, then things get complicated.  I think you might approach that as a hidden Markov model: but the number of individual states may be quite large, and so it may be essentially impossible to build a complete map of transition probabilities.', 26, '2015-02-26 05:08:31.993', 'e6096378-6186-47e6-8e93-30ddfe87c393', 5235, 8827, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am not sure there is a clear answer to this, especially as the problem does not seem to be well-defined right now - your "figure" seems to indicate edge weights but you then mention node weights, something significantly different.

If the question is whether you can find a way to split a graph into two smaller modules, then you might want to look into applying [Sparsest Cut][1] techniques - a cut with low cost would imply (?) high modularity. I believe these can be easily modified to account for either unlabeled, edge-labeled or node-labeled graphs.

  [1]: http://en.wikipedia.org/wiki/Cut_%28graph_theory%29#Sparsest_cut', 8433, '2015-02-26 11:19:35.900', '9b3bd62d-26d7-4344-8051-af99e98b4baa', 5236, 8828, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This is very similar to the netflix problem, most matrix factorization methods can be adapted so that the error function is only evaluated at known points. For instance, you can take the gradient descent approach to SVD (minimizing the frobenius norm) but only evaluate the error and calculate the gradient at known points. I believe you can easily find code for this.

Another option would be exploiting the binary nature of your matrix and adapting binary matrix factorization tools in order to enforce binary factors (if you require them). I''m sure you can adapt one of the methods described [here][1] to work with unknown data using a similar trick as the one above.

  [1]: http://www.hongliangjie.com/2011/03/15/reviews-on-binary-matrix-decomposition/', 8433, '2015-02-26 11:31:04.870', 'ac4069a6-65a8-4564-a7e0-1377e4b4f61f', 5237, 8829, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think those two maybe help you:

[Introduction to Data Science][1]

[Dataquest.io][2]


  [1]: https://www.coursera.org/course/datasci?from_restricted_preview=1&course_id=346&r=https%3A%2F%2Fclass.coursera.org%2Fdatasci-001
  [2]: https://dataquest.io/', 8437, '2015-02-26 12:10:17.283', '89f54536-7cb3-4d3b-bae1-078025ee20e7', 5238, 8830, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You may want to consider using your own `APparameter` object to put "significance" constraints on the rules learned by Apriori. See page 13 of the [arules documentation][1]. This could reduce the number of uninteresting rules returned in your run.

In lieu of gold standard data for your domain, consider bootstrap resampling as a form of validation, as described [in this article][2].


  [1]: http://cran.r-project.org/web/packages/arules/arules.pdf
  [2]: http://eprints.pascal-network.org/archive/00003198/01/lal.pdf', 8392, '2015-02-27 00:45:39.740', '41c830d3-be72-4b3a-915e-adb9e4581fb3', 5239, 8831, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I recently started the Coursera Data Science Specialization and I was wondering if this would be a good start to learn and work as a data scientist. I have some experience as a java developer and I´m learning R now. Is these courses worth? ', 8356, '2015-02-27 02:21:06.713', '65650d50-0191-4bfd-bd75-057011a234b0', 5240, 8832, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Is Coursera Data Science Specialization worth?', 8356, '2015-02-27 02:21:06.713', '65650d50-0191-4bfd-bd75-057011a234b0', 5240, 8833, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 8356, '2015-02-27 02:21:06.713', '65650d50-0191-4bfd-bd75-057011a234b0', 5240, 8834, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I think this line is wrong:

  ytr = X(ii(1:N/2),:);

ytr should be the label of the training data. In this case, it should be

  ytr = y(ii(1:N/2),:);', 8415, '2015-02-27 05:28:25.457', '5a3f6db2-5c15-4540-984b-3e53dad3cedf', 5241, 8835, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Check out the e1071 package, here is the manual http://cran.r-project.org/web/packages/e1071/e1071.pdf
', 8310, '2015-02-27 08:13:53.847', '56e543ed-41f5-4566-b815-7d26f0453052', 5242, 8836, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have been working in the last years with statistics and have gone pretty deep in programming with R. I have however always felt that I wasn''t completely grasping what I was doing, still understanding all passages and procedures conceptually.

I wanted to get a bit deeper into the math behind it all. I''ve been looking online for texts and tips, but all texts start with a very high level. Any suggestions on where to start?

To be more precise, I''m not looking for an exaustive list of statistical models and how they work, I kind of get those. I was looking for something like "Basics of statistical modelling"', 8452, '2015-02-27 10:22:59.577', '1dc00386-1186-449e-8560-7f7ebdb99953', 5243, 8837, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Interested in Mathematical Statistics... where to start from?', 8452, '2015-02-27 10:22:59.577', '1dc00386-1186-449e-8560-7f7ebdb99953', 5243, 8838, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics><predictive-modeling>', 8452, '2015-02-27 10:22:59.577', '1dc00386-1186-449e-8560-7f7ebdb99953', 5243, 8839, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Many of us are very familiar with using R in reproducible, but very much targeted, ad-hoc analysis. Given that R is currently the best collection of cutting-edge scientific methods from world-class experts in each particular field, and given that plenty of libraries exist for data io in R, it seems very natural to extend its applications into production environments for live decision making.

Therefore my questions are:

 - did someone of you go into production with pure R (I know of shiny, yhat etc, but would be very interesting to hear of pure R);
 - is there a good book/guide/article on the topic of building R into some serious live decision-making pipelines (such as e.g. credit scoring);
 - I would like to hear also if you think it''s not a good idea at all;

', 8310, '2015-02-27 10:42:35.407', 'c8a5915f-9f57-4523-9e4c-8543cec1b2fa', 5244, 8840, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R in production', 8310, '2015-02-27 10:42:35.407', 'c8a5915f-9f57-4523-9e4c-8543cec1b2fa', 5244, 8841, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><predictive-modeling><scoring>', 8310, '2015-02-27 10:42:35.407', 'c8a5915f-9f57-4523-9e4c-8543cec1b2fa', 5244, 8842, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('the similar question has been asked here but i cant find any relevant answer of it so i am trying again. I am able to get NER and Dependency tree using the library. Now what i looking for is that i want to extract entities with the relationship between the entities. For example , "flipkart has invested in myntra" so i should be able to get entity1 as "flipkart " and entity2 as "myntra" and "investor" as the relation .or similar kind of structure.
Till now i am able to perform "entity linking" part. my code is as below.

    public static void main(String[] args) throws IOException, ClassNotFoundException {
    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution
    Properties props = new Properties();
    props.put("annotators", "tokenize, ssplit, pos, lemma, ner, parse, dcoref");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

    // read some text in the text variable
    //String text = "Mary has a little lamb. She is very cute."; // Add your text here!
    String text = "Matrix Partners along with existing investors Sequoia Capital and Nexus Venture Partners has invested R100 Cr in Mumbai based food ordering app, TinyOwl. The series B funding will be used by the company to expand its geographical presence to over 50 cities, upgrade technology and enhance user experience.";
    text+="In December last year, it raised $3 Mn from Sequoia Capital India and Nexus Venture Partners to deepen its presence in home market Mumbai. It was seeded by Deap Ubhi (who had earlier founded Burrp) and Sandeep Tandon.";


    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);

    // run all Annotators on this text
    pipeline.annotate(document);

    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List<CoreMap> sentences = document.get(SentencesAnnotation.class);

    for(CoreMap sentence: sentences) {
      // traversing the words in the current sentence
      // a CoreLabel is a CoreMap with additional token-specific methods
      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
        // this is the text of the token
        String word = token.get(TextAnnotation.class);
        //System.out.println(" word \n"+word);
        // this is the POS tag of the token
        String pos = token.get(PartOfSpeechAnnotation.class);
       // System.out.println(" pos \n"+pos);
        // this is the NER label of the token
        String ne = token.get(NamedEntityTagAnnotation.class);
        //System.out.println(" ne \n"+ne);
      }

      // this is the parse tree of the current sentence
      Tree tree = sentence.get(TreeAnnotation.class);
      System.out.println(" TREE \n"+tree);

      // this is the Stanford dependency graph of the current sentence
      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
      System.out.println(" dependencies \n"+dependencies);
    }

    // This is the coreference link graph
    // Each chain stores a set of mentions that link to each other,
    // along with a method for getting the most representative mention
    // Both sentence and token offsets start at 1!
    Map<Integer, CorefChain> graph = document.get(CorefChainAnnotation.class);
    System.out.println("graph \n "+graph);
  }

 I am not able to get correct tool for doing the same. I neee some guidelines guys, how to achieve this?. Thanks in advance', 8454, '2015-02-27 12:44:45.723', 'b5bfdd2b-430d-4d02-a04a-994a0d290454', 5245, 8843, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Extract Relationship Between two Entities using StanfordCoreNLP', 8454, '2015-02-27 12:44:45.723', 'b5bfdd2b-430d-4d02-a04a-994a0d290454', 5245, 8844, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<nlp><java>', 8454, '2015-02-27 12:44:45.723', 'b5bfdd2b-430d-4d02-a04a-994a0d290454', 5245, 8845, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Could somebody help me to create a find query?

If I got only the id of a document and would like to get only the values of the attributes a and b. There are of course a lot another attributes, but I don''t need everything.

And how would a query look like if I furthermore need to know the value of additional attribute c, which is an array and the value of c I''m looking for is c.mobile?

Would appreciate any help!

Thank you.', 4717, '2015-02-27 14:01:27.327', 'd6190be7-d8f1-4c48-93d6-b572a8579c56', 5246, 8846, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('MongoDB, find only some values of a document by id', 4717, '2015-02-27 14:01:27.327', 'd6190be7-d8f1-4c48-93d6-b572a8579c56', 5246, 8847, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<mongodb>', 4717, '2015-02-27 14:01:27.327', 'd6190be7-d8f1-4c48-93d6-b572a8579c56', 5246, 8848, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In Hive itself? Unfortunately, the answer is simply no -- as the language definition manual shows, that statistic is simply not built in. In addition to the language manual, you can get more information on statistics in development in Hive [here][1] and [here][2].

Having said that, there are plenty of ways to calculate Kendall''s W on data that''s in Hive.

You could write out the data to a file or query it into R or a statistical package such as  SAS, Stat, MATLAB, Excel, etc then run your calculation and, if necessary, write your results back to Hive.

In R, for instance, you could do something like this:

    install.packages("RODBC")
    require(RODBC)
    db   <- odbcConnect("Hive_DB")
    hql  <- "select * from table A"
    data <- sqlQuery(db , hql)
    kenw <- cor(x = data$a, y = data$b, method="kendall")
    sqlSave(db, kenw, tablename = "new_table_of_kendall_coef")

or (if using Linux or Unix) then you could use `RHive` without needing to use an ODBC name.

Another way to go about it would be to take the functions that do exist in Hive (which you linked to) and calculate Kendall''s coefficient yourself with a custom function.


  [1]: https://cwiki.apache.org/confluence/display/Hive/StatsDev
  [2]: https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining', 2723, '2015-02-27 19:38:22.287', '5717019a-874d-4fce-9b39-503ea10c590e', 5248, 8853, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('In Hive itself? Unfortunately, the answer is simply no -- as the language definition manual shows, that statistic is simply not built in. In addition to the language manual, you can get more information on statistics in development in Hive [here][1] and [here][2].

Having said that, there are plenty of ways to calculate Kendall''s W on data that''s in Hive.

You could write out the data to a file or query it into R or a statistical package such as  SAS, Stat, MATLAB, Excel, etc then run your calculation and, if necessary, write your results back to Hive.

In R, for instance, you could do something like this:

    install.packages("RODBC")
    require(RODBC)
    db   <- odbcConnect("Hive_DB")
    hql  <- "select * from table A"
    data <- sqlQuery(db , hql)
    kenw <- cor(x = data$a, y = data$b, method="kendall")
    sqlSave(db, kenw, tablename = "new_table_of_kendall_coef")

or (if using Linux or Unix) then you could use `RHive` without needing to use an ODBC name.

Another way to go about it would be to take the functions that do exist in Hive (which you linked to) and calculate Kendall''s coefficient yourself with a custom function. As to how to specifically implement that, well you''d probably want to post on Cross Validated (stats.stackexchange.com).


  [1]: https://cwiki.apache.org/confluence/display/Hive/StatsDev
  [2]: https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining', 2723, '2015-02-27 20:00:37.363', '52349303-e540-4865-95e1-ca47f20919e0', 5248, 'added 121 characters in body', 8854, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I start with a data.frame (or a data_frame) containing my dependent Y variable for analysis, my independent X variables, and some "Z" variables -- extra columns that I don''t need for my modeling exercise.

What I would like to do is:

 1. Create an analysis data set without the Z variables;
 2. Break this data set into random training and test sets;
 3. Find my best model;
 4. Predict on both the training and test sets using this model;
 5. Recombine the training and test sets by rows; and finally
 6. Recombine these data with the Z variables, by column.

It''s the last step, of course, that presents the problem -- how do I make sure that the rows in the recombined training and test sets match the rows in the original data set? We might try to use the row.names variable from the original set, but I agree with Hadley that this is an error-prone kludge (my words, not his) -- why have a special column that''s treated differently from all other data columns?

One alternative is to create an ID column that uniquely identifies each row, and then keep this column around when dividing into the train and test sets (but excluding it from all modeling formulas, of course). This seems clumsy as well, and would make all my formulas harder to read.

This must be a solved problem -- could people tell me how they deal with this? Especially using the plyr/dplyr/tidyr package framework?
', 3510, '2015-02-28 00:44:04.353', '7268f6a5-fa7d-40a8-ada9-28ac322eb9b4', 5249, 8855, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Combining data sets without using row.name', 3510, '2015-02-28 00:44:04.353', '7268f6a5-fa7d-40a8-ada9-28ac322eb9b4', 5249, 8856, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><r><predictive-modeling>', 3510, '2015-02-28 00:44:04.353', '7268f6a5-fa7d-40a8-ada9-28ac322eb9b4', 5249, 8857, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-28 04:09:35.097', '2ed6ad35-e0c8-40bb-a614-857fa7a4774c', 5240, '102', 8859, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":-1,"DisplayName":"Community"}]}', -1, '2015-02-28 04:17:25.890', 'fa13b94c-5e92-4ed8-a94b-63691f9326e1', 5189, 8860, '14');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":2452,"DisplayName":"Aleksandr Blekh"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-28 04:17:25.890', 'f7258e88-830a-402e-a322-cce0a0ece9e0', 5189, 'to http://opendata.stackexchange.com/questions/4634/hunting-for-nlp-datasets', 8861, '35');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":2452,"DisplayName":"Aleksandr Blekh"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-28 04:17:25.890', '148dbe20-521c-4b3a-9a41-d18858594839', 5189, '102', 8864, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":-1,"DisplayName":"Community"}]}', -1, '2015-02-28 04:17:47.083', '622ec7c3-5e7d-429f-95f7-7aa78b21db67', 5195, 8865, '14');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":471,"DisplayName":"Spacedman"},{"Id":2452,"DisplayName":"Aleksandr Blekh"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-28 04:17:47.083', '4b319ae5-6ee1-463f-a293-80d49ec80d57', 5195, 'to http://stats.stackexchange.com/questions/139711/choosing-the-right-model-for-prediction', 8866, '35');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":471,"DisplayName":"Spacedman"},{"Id":2452,"DisplayName":"Aleksandr Blekh"},{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-02-28 04:17:47.083', '54fad242-6317-4845-923c-a04b2ad9212c', 5195, '102', 8868, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m trying to set up a cluster (1 namenode, 1 datanode) on AWS.
I''m using free one year trial period of AWS, but the challenge is, instance is created with 1GB of RAM.

As I''m a student, I cannot afford much. Can anyone please suggest me some solution?

Also, it would be great if you could provide any links for setting up multi cluster hadoop with spark on AWS.

Note: I cannot try in GCE as my trial period is exhausted. ', 84, '2015-02-28 13:28:18.943', '084e698e-541f-4160-8147-4a3bd664ca9e', 2633, 'deleted 6 characters in body', 8872, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m surprised one has not mentioned this, as it seems fairly obvious: http://www.kaggle.com consistently has new and very interesting datasets. Information is considered an asset, so often companies don''t want to release that data (plus privacy concerns). Kaggle gives you data and they hope you solve business problems with it in exchange.', 84, '2015-02-28 13:36:02.240', '6a9b28a5-450f-4117-9e76-e923b835420e', 5049, 'Minor corrections.', 8873, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I think this line is wrong:

    ytr = X(ii(1:N/2),:);

ytr should be the label of the training data. In this case, it should be

    ytr = y(ii(1:N/2),:);', 84, '2015-02-28 13:38:20.850', '842cbce3-75c7-4a1e-8619-91aeb8b85080', 5241, 'Minor formatting adjustments.', 8874, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('R and most of its modules are licensed using the GPL.

In many companies, legal departments go crazy if you propose to use anything that is GPL in production...

Other than that, R is often really slooow. It''s okay when you are still trying to figure out what to do. But for production, you may want maximum performance, and full integration with your services.', 924, '2015-02-28 17:10:41.147', 'e61b490d-c806-46d4-89c4-e3466c44a9ec', 5251, 8875, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('As a fellow CS Ph.D. defending my dissertation in a Big Data-esque topic this year (I started in 2012), the best piece of material I can give you is in a link:
http://www.rpajournal.com/dev/wp-content/uploads/2014/10/A3.pdf

This is an article written by two Ph.D.s from MIT who have talked about Big Data and MOOCs. Probably, you will find this a good starting point. BTW, along this note, if you really want to come up with a valid topic (that a committee and your adviser will let you propose, research and defend) you need to read LOTS and LOTS of papers. The majority of Ph.D. students make the fatal error of thinking that some ''idea'' they have is new, when it''s not and has already been done. You''ll have to do something truly original to earn your Ph.D. Rather than actually focus on forming an idea right now, you should do a good literature survey and the ideas will ''suggest themselves''. Good luck! It''s an exciting time for you.', 8395, '2015-02-28 17:15:39.423', 'cebadd8f-6b4b-4198-ac37-308993a71b3b', 5252, 8876, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I would actually try out regression. Also, don''t make the mistake of using the serial number in your machine learning algorithms! The reason why I''m suggesting regression as opposed to ''better'' machine learning algorithms is because you said you wanted to learn, and it''s important to understand the algorithms (for the long run, and to truly be good at this stuff) that you''re using. Regression is the easiest tool in the book that works quite well! Weka is so easy to use that you''ll be able to plug and play different machine learning algorithms just for the sake of it. Another pointer that''s won me several competitions is to do some feature selection before using regression/machine learning. For example, in your case, it is reasonable to assume that a student who scores high in Physics probably has a better chance of scoring high in Math as opposed to someone who scores high in English (but not necessarily Physics). If you have enough data, the algorithm itself will be able to deduce these positive/negative correlations and train the model accordingly. Sometimes, there isn''t enough data, and you have to do some feature selection. Good luck! I''m a regular participant on Kaggle myself, and I think it''s great that you''re taking the ''hacker'' route to learn more. It''s the best way to get your hands dirty on real data and engineering problems. ', 8395, '2015-02-28 17:21:46.300', '81d9b4dc-a97d-4baf-a2e6-e3be309f2379', 5253, 8877, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('R and most of its modules are licensed using the GPL.

In many companies, legal departments go *crazy* if you propose to use anything that is GPL in production... It''s not reasonable, but you''ll see they love Apache, and hate GPL.

Other than that, R is often really slooow unless calling Fortran code hidden inside. It''s nice when you are still trying to figure out what to do. But for production, you may want maximum performance, and full integration with your services.', 924, '2015-02-28 19:38:10.580', '95dbba0f-f4dd-4957-8dfc-52e9b001df0c', 5251, 'added 116 characters in body', 8878, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('R and most of its CRAN modules are licensed using the GPL.

In many companies, legal departments go *crazy* if you propose to use anything that is GPL in production... It''s not reasonable, but you''ll see they love Apache, and hate GPL. **Before going into production, make sure it''s okay with the legal department.** (IMHO you are safe to *use* your modified code for *internal* products. Integrating R into your commercial product and handing this out to others is very different. But unfortunately, many legal departments try to ban all use of GPL whatsoever.)

Other than that, R is often really slooow unless calling Fortran code hidden inside. It''s nice when you are still trying to figure out what to do. But for production, you may want maximum performance, and full integration with your services. *Benchmark* yourself, if R is the best choice for your use case.

On the performance issues with R (I know R advocates are going to downvote me for saying so ...):

> Morandat, F., Hill, B., Osvald, L., & Vitek, J. (2012). Evaluating the design of the R language. In ECOOP 2012Object-Oriented Programming (pp. 104-131). Springer Berlin Heidelberg.

(by the TraceR/ProfileR/ReactoR people from purdue, who are now working on fastR which tries to execute R code on the JVM?) states:
> On those benchmarks, **R is on average 501 slower than C and 43 times slower Python.**

and:

> Observations. **R is clearly slow and memory inefficient.** Much more so than other dynamic languages. This is largely due to the combination of language features (call-by-value, extreme dynamism, lazy evaluation) and the lack of efficient built-in types. We believe that with some effort it should be possible to improve both time and space usage, but this would likely **require a full rewrite of the implementation**.

Sorry to break the news. It''s now my research, but it aligns with my observations.', 924, '2015-02-28 19:56:59.157', '3839662a-2a99-4220-887a-520f12db3e24', 5251, 'added 116 characters in body', 8879, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('If you know your approach is working, you can try to implement it more efficiently. Identify the crucial points, try to *vectorize* them better, for example.

The R interpreter isn''t the fastest. There are some efforts underway, but they are not yet ready. Vectorization (which means less interpreter, more low-level code) often yields a factor of 2x-5x, but you can sometimes get a factor of 100x by implementing it e.g. in C. (And the R advocates are going to hate me for saying this...)
Once you know that your approach is working, this may be worth the effort.', 924, '2015-02-28 21:42:31.383', '52114334-dd45-4fe8-83d7-935194d12308', 5254, 8880, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Terms like ''data science'' and ''data scientist'' are increasingly used these days.
Many companies are hiring ''data scientist''. But I don''t think it''s a completely new job.
Data have existed from the past and someone had to deal with data.
I guess the term ''data scientist'' becomes more popular because it sounds more fancy and ''sexy''
How were data scientists called in the past?', 8478, '2015-02-28 22:10:58.473', '92b2c035-cd68-4897-9c93-07d652b22c27', 5255, 8881, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('What is an ''old name'' of data scientist?', 8478, '2015-02-28 22:10:58.473', '92b2c035-cd68-4897-9c93-07d652b22c27', 5255, 8882, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<bigdata>', 8478, '2015-02-28 22:10:58.473', '92b2c035-cd68-4897-9c93-07d652b22c27', 5255, 8883, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('In reverse chronological order: data miner, statistician, (applied) mathematician.', 381, '2015-02-28 23:21:40.430', '77c85112-4eef-4100-9999-79997fec6a0d', 5256, 8884, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to turn reviews of up to 5 stars and the number of reviews into upvotes. Whats a good algorithm for doing this?

A venue with 10 reviews total with a 5 star average rating should obviously get more upvotes than a venue with 10 reviews total with a 3 star average rating. Also a venue with 60 ratings and a 4 star rating should probably get more upvotes than the one with 10 reviews and a 5 star rating.

I need this rating to be based off of the total number of reviews and the average star rating, but I would also like the number to stay below a variable number. (like say upvotes stay below 100 but I can also plugin 200 and it will stay below 200)', 8482, '2015-03-01 02:56:05.590', '6cb6cd23-b07a-44bd-91ce-77d0b0eea023', 5257, 8885, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Good formula for turning star reviews into upvotes', 8482, '2015-03-01 02:56:05.590', '6cb6cd23-b07a-44bd-91ce-77d0b0eea023', 5257, 8886, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<algorithms>', 8482, '2015-03-01 02:56:05.590', '6cb6cd23-b07a-44bd-91ce-77d0b0eea023', 5257, 8887, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You need neither to use the row names or to create an additonal ID column. Here is an approach based on the indices of the training set.

An example data set:

    set.seed(1)
    dat <- data.frame(Y = rnorm(10),
                      X1 = rnorm(10),
                      X2 = rnorm(10),
                      Z1 = rnorm(10),
                      Z2 = rnorm(10))

Now, your steps:

1. Create an analysis data set without the `Z` variables

        dat2 <- dat[grep("Z", names(dat), invert = TRUE)]
        dat2
        #             Y          X1          X2
        # 1  -0.6264538  1.51178117  0.91897737
        # 2   0.1836433  0.38984324  0.78213630
        # 3  -0.8356286 -0.62124058  0.07456498
        # 4   1.5952808 -2.21469989 -1.98935170
        # 5   0.3295078  1.12493092  0.61982575
        # 6  -0.8204684 -0.04493361 -0.05612874
        # 7   0.4874291 -0.01619026 -0.15579551
        # 8   0.7383247  0.94383621 -1.47075238
        # 9   0.5757814  0.82122120 -0.47815006
        # 10 -0.3053884  0.59390132  0.41794156

2. Break this data set into random training and test sets

        train_idx <- sample(nrow(dat2), 0.8 * nrow(dat2))
        train_idx
        # [1]  7  4  3 10  9  2  1  5

        train <- dat2[train_idx, ]
        train
        #             Y          X1          X2
        # 7   0.4874291 -0.01619026 -0.15579551
        # 4   1.5952808 -2.21469989 -1.98935170
        # 3  -0.8356286 -0.62124058  0.07456498
        # 10 -0.3053884  0.59390132  0.41794156
        # 9   0.5757814  0.82122120 -0.47815006
        # 2   0.1836433  0.38984324  0.78213630
        # 1  -0.6264538  1.51178117  0.91897737
        # 5   0.3295078  1.12493092  0.61982575

        test_idx <- setdiff(seq(nrow(dat2)), train_idx)
        test_idx
        # [1] 6 8

        test <- dat2[test_idx, ]
        test
        #            Y          X1          X2
        # 6 -0.8204684 -0.04493361 -0.05612874
        # 8  0.7383247  0.94383621 -1.47075238

3. Find my best model

    ...

4. Predict on both the training and test sets using this model

    ...

5. Recombine the training and test sets by rows

        idx <- order(c(train_idx, test_idx))
        dat3 <- rbind(train, test)[idx, ]
        identical(dat3, dat2)
        # [1] TRUE

6. Recombine these data with the `Z` variables, by column

        dat4 <- cbind(dat3, dat[grep("Z", names(dat))])
        identical(dat, dat4)
        # [1] TRUE

In summary, we can use the indices of the training and test data to combine the data in the rows in the original order.

', 106, '2015-03-01 07:16:51.440', 'd40707a9-1a9c-4e5d-81bb-e4a6815e810d', 5259, 8890, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to extract words from a comment and there by finding their ***Polarity*** for each word and award score according to it. This has to be done in sentimental analysis. ', 8484, '2015-03-01 07:23:57.447', '21d44d99-9f4b-4e98-83f7-620ac447dbc0', 5260, 8891, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Sentimental analysis', 8484, '2015-03-01 07:23:57.447', '21d44d99-9f4b-4e98-83f7-620ac447dbc0', 5260, 8892, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 8484, '2015-03-01 07:23:57.447', '21d44d99-9f4b-4e98-83f7-620ac447dbc0', 5260, 8893, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m struggling with for loop in R. I have a following data frame with sentences and two dictionaries with pos and neg words:

    library(stringr)
    library(plyr)
    library(dplyr)
    library(stringi)
    library(qdap)
    library(qdapRegex)
    library(reshape2)
    library(zoo)

    # Create data.frame with sentences
    sent <- data.frame(words = c("great just great right size and i love this notebook", "benefits great laptop at the top",
                             "wouldnt bad notebook and very good", "very good quality", "bad orgtop but great",
                             "great improvement for that great improvement bad product but overall is not good", "notebook is not good but i love batterytop"), user = c(1,2,3,4,5,6,7),
                   number = c(1,1,1,1,1,1,1), stringsAsFactors=F)

    # Create pos/negWords
    posWords <- c("great","improvement","love","great improvement","very good","good","right","very","benefits",
              "extra","benefit","top","extraordinarily","extraordinary","super","benefits super","good","benefits great",
              "wouldnt bad")
    negWords <- c("hate","bad","not good","horrible")

And now I''m gonna to create replication of origin data frame for big data simulation:

    # Replicate original data.frame - big data simulation (700.000 rows of sentences)
    df.expanded <- as.data.frame(replicate(100000,sent$words))
    sent <- coredata(sent)[rep(seq(nrow(sent)),100000),]
    sent$words <- paste(c(""), sent$words, c(""), collapse = NULL)
    rownames(sent) <- NULL

For my further approach, I''ll have to do descending ordering of words in dictionaries with their sentiment score (pos word = 1 and neg word = -1).

    # Ordering words in pos/negWords
    wordsDF <- data.frame(words = posWords, value = 1,stringsAsFactors=F)
    wordsDF <- rbind(wordsDF,data.frame(words = negWords, value = -1))
    wordsDF$lengths <- unlist(lapply(wordsDF$words, nchar))
    wordsDF <- wordsDF[order(-wordsDF[,3]),]
    wordsDF$words <- paste(c(""), wordsDF$words, c(""), collapse = NULL)
    rownames(wordsDF) <- NULL

Then I have a following function with for loop. 1) matching exact words 2) count them 3) compute score 4) remove matched words from sentence for another iteration:

    scoreSentence_new <- function(sentence){
      score <- 0
      for(x in 1:nrow(wordsDF)){
        sd <- function(text) {stri_count(text, regex=wordsDF[x,1])} # count matched words
        results <- sapply(sentence, sd, USE.NAMES=F) # count matched words
        score <- (score + (results * wordsDF[x,2])) # compute score
        sentence <- str_replace_all(sentence, wordsDF[x,1], " ") # remove matched words from sentence for next iteration
      }
      score
    }

When I call that function

    SentimentScore_new <- scoreSentence_new(sent$words)
    sent_new <- cbind(sent, SentimentScore_new)
    sent_new$words <- str_trim(sent_new$words, side = "both")

it resulted into desired output:

                                                                                 words user     SentimentScore_new
                                 great just great right size and i love this notebook    1                  4
                                                     benefits great laptop at the top    2                  2
                                                   wouldnt bad notebook and very good    3                  2
                                                                    very good quality    4                  1
                                                                 bad orgtop but great    5                  0
     great improvement for that great improvement bad product but overall is not good    6                  0
                                           notebook is not good but i love batterytop    7                  0

In real I''m using dictionaries with pos/neg words about 7.000 words and I have 200.000 sentences. When I used my approach for 1.000 sentences it takes 45 mins. Please, could you anyone help me with some faster approach using of vectorization or parallel solution. Because of my beginner R programming skills I''m in the end of my efforts :-( Thank you very much in advance for any of your advice or solution', 8488, '2015-03-01 14:28:22.123', 'd6748550-3dc6-4e47-8db3-5747f2a94d62', 5261, 8894, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Vectorization of for loop in sentiment analysis', 8488, '2015-03-01 14:28:22.123', 'd6748550-3dc6-4e47-8db3-5747f2a94d62', 5261, 8895, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r>', 8488, '2015-03-01 14:28:22.123', 'd6748550-3dc6-4e47-8db3-5747f2a94d62', 5261, 8896, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here is one of the simplest ways to do this using R for the language.

 1. remove punctuation from the comment with gsub:
    comment = gsub(''[[:punct:]]'', '''', comment)
 2. using str_split from stringr package, create a word list based on white spaces:
    WordsToEval.list = str_split(comment, ''\\\\s+'')
 3. use unlist to flatten your list of words:
    WordsToEval = unlist( WordsToEval.list)

Now your words are extracted and available in WordsToEval.

To establish the polarity of each word, you will need a positive and negative lexicon of words.  These can be found online or you could test with your own.
One excellent source is the opinion lexicon from Hu and Liu available here:
http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon


In R, use match to find the word matches in the positive and negative lists, non-matched words are neutral and marked as FALSE:
negative.matches = !is.na(match(WordsToEval, neglist))
positive.matches = !is.na(match(WordsToEval, poslist))

Then, Score the comment by summing the positive matches - negative matches:

score = sum(positive.matches) - sum(negative.matches)

The distance from 0 indicates the relative strength of the sentiment.


Again, this is one simple example.
You can build on this by weighting words, and other things.











', 8301, '2015-03-02 00:28:57.900', 'ceb10582-1710-4a46-81f4-f7b3c7ac3ea7', 5262, 8897, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Try using Earth Mover''s Distance (http://ai.stanford.edu/~rubner/papers/rubnerIjcv00.pdf). It measures how much it "costs" to optimally reshare one histogram into another, where an elementary transform (using the terminology of edit distance-like distance measures) is moving a unit of mass from a bin to a bin. If you have an implementation of EMD, then your distance is dist = EMD(H1, H2, D), where

     H1 = [ 14, 13, 40, 31 ]; -- histogram #1 (first image)
     H2 = [ 82, 8, 7 ]; -- histogram #2 (second image)
     D  = [ D11, D12, D13; ... ; D41, D42, D43 ]; -- (cross-bin) ground distance

Dij is a distance from the i''th bin of the first histogram to the j''th bin of the second histogram. For example, D21 is a distance between ''White'' and ''Pink''. It may be defined differently, based on which color space you want to use, and further, based on how you define what it means for two colors to be similar. If I use RGB, and I do not care much as to how human eye perceives colors, then D21 can be D21 can be the \ell_2 norm of the difference [1, 1, 1] - [1, 0.05, 0.7]. (As far as I remember, the paper I mentioned uses LAB instead of RGB.)

P.S. Besides color, you may include spatial information into comparison. EMD can handle it well, as long as you properly combine similarity of colors with spatial similarity in the definition of ground distance D.
', 8462, '2015-03-02 05:05:55.497', 'ede7f4a5-b05f-4625-9600-369784ded2e4', 5263, 8899, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Does reinforcement learning always need a grid world problem to be applied to?<br><br> Can anyone give me any other example of how reinforcement learning can be applied to something which does not have a grid world scenario?', 8013, '2015-03-02 05:10:01.897', '2edb3875-302d-49ea-bbc2-5817f940e7ef', 5264, 8900, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Does reinforcement learning always work on grid world?', 8013, '2015-03-02 05:10:01.897', '2edb3875-302d-49ea-bbc2-5817f940e7ef', 5264, 8901, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 8013, '2015-03-02 05:10:01.897', '2edb3875-302d-49ea-bbc2-5817f940e7ef', 5264, 8902, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m struggling with for loop in R. I have a following data frame with sentences and two dictionaries with pos and neg words:

    library(stringr)
    library(plyr)
    library(dplyr)
    library(stringi)
    library(qdap)
    library(qdapRegex)
    library(reshape2)
    library(zoo)

    # Create data.frame with sentences
    sent <- data.frame(words = c("great just great right size and i love this notebook", "benefits great laptop at the top",
                             "wouldnt bad notebook and very good", "very good quality", "bad orgtop but great",
                             "great improvement for that great improvement bad product but overall is not good", "notebook is not good but i love batterytop"), user = c(1,2,3,4,5,6,7),
                   number = c(1,1,1,1,1,1,1), stringsAsFactors=F)

    # Create pos/negWords
    posWords <- c("great","improvement","love","great improvement","very good","good","right","very","benefits",
              "extra","benefit","top","extraordinarily","extraordinary","super","benefits super","good","benefits great",
              "wouldnt bad")
    negWords <- c("hate","bad","not good","horrible")

And now I''m gonna to create replication of origin data frame for big data simulation:

    # Replicate original data.frame - big data simulation (700.000 rows of sentences)
    df.expanded <- as.data.frame(replicate(100000,sent$words))
    sent <- coredata(sent)[rep(seq(nrow(sent)),100000),]
    sent$words <- paste(c(""), sent$words, c(""), collapse = NULL)
    rownames(sent) <- NULL

For my further approach, I''ll have to do descending ordering of words in dictionaries with their sentiment score (pos word = 1 and neg word = -1).

    # Ordering words in pos/negWords
    wordsDF <- data.frame(words = posWords, value = 1,stringsAsFactors=F)
    wordsDF <- rbind(wordsDF,data.frame(words = negWords, value = -1))
    wordsDF$lengths <- unlist(lapply(wordsDF$words, nchar))
    wordsDF <- wordsDF[order(-wordsDF[,3]),]
    wordsDF$words <- paste(c(""), wordsDF$words, c(""), collapse = NULL)
    rownames(wordsDF) <- NULL

Then I have a following function with for loop. 1) matching exact words 2) count them 3) compute score 4) remove matched words from sentence for another iteration:

    scoreSentence_new <- function(sentence){
      score <- 0
      for(x in 1:nrow(wordsDF)){
        sd <- function(text) {stri_count(text, regex=wordsDF[x,1])} # count matched words
        results <- sapply(sentence, sd, USE.NAMES=F) # count matched words
        score <- (score + (results * wordsDF[x,2])) # compute score
        sentence <- str_replace_all(sentence, wordsDF[x,1], " ") # remove matched words from sentence for next iteration
      }
      score
    }

When I call that function

    SentimentScore_new <- scoreSentence_new(sent$words)
    sent_new <- cbind(sent, SentimentScore_new)
    sent_new$words <- str_trim(sent_new$words, side = "both")

it resulted into desired output:

                                                                                 words user     SentimentScore_new
                                 great just great right size and i love this notebook    1                  4
                                                     benefits great laptop at the top    2                  2
                                                   wouldnt bad notebook and very good    3                  2
                                                                    very good quality    4                  1
                                                                 bad orgtop but great    5                  0
     great improvement for that great improvement bad product but overall is not good    6                  0
                                           notebook is not good but i love batterytop    7                  0

In real I''m using dictionaries with pos/neg words about 7.000 words and I have 200.000 sentences. When I used my approach for 1.000 sentences it takes 45 mins. Please, could you anyone help me with some faster approach using of vectorization or parallel solution. Because of my beginner R programming skills I''m in the end of my efforts :-( Thank you very much in advance for any of your advice or solution

I was wondering about something like that:

    n <- 1:nrow(wordsDF)
    score <- 0

    try_1 <- function(ttt) {
    sd <- function(text) {stri_count(text, regex=wordsDF[ttt,1])}
    results <- sapply(sent$words, sd, USE.NAMES=F)
    score <- (score + (results * wordsDF[ttt,2])) # compute score (count * sentValue)
    sent$words <- str_replace_all(sent$words, wordsDF[ttt,1], " ")
    score
    }

    a <- unlist(sapply(n, try_1))
    apply(a,1,sum)

But doesn''t work :-(', 8488, '2015-03-02 08:20:08.023', '08c69f5e-4c4f-4612-a726-ef67bbf3e23e', 5261, 'Try own solution...', 8903, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-03-02 08:53:29.090', '858d189e-f7ad-42f1-802e-b9245869eba8', 5260, '103', 8906, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Reinforcement learning does not depend on a grid world.  It can be applied to any space of possibilities where there is a "fitness function" that maps between points in the space to a fitness metric.

Topological spaces have a formally-defined "neighborhoods" but do not necessarily conform to a grid or any dimensional representation.  In a topological space, the only way to get from "here" to "there" is via some "paths" which are sets of contiguous neighborhoods. Continuous fitness functions can be defined over topological spaces.

For what it is worth, reinforcement learning is not the be-all-end-all (family of) learning algorithms in fitness landscapes.  In a sufficiently rugged fitness landscape, other learning algorithms can perform better.  Also, if there are regions of the space where there are no well-defined fitness function at a given point in time, it may be indeterminate as to what learning algorithms are optimal, if any.', 609, '2015-03-02 10:06:27.483', '409013e7-13aa-415a-93c0-cb68a621ca3a', 5265, 8907, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><sports>', 553, '2015-03-02 12:33:11.007', '7202ac34-ee3f-43bb-bbf7-ce07a2d9ee97', 265, 'adding a tag', 8912, '6');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-03-02 12:33:11.007', '7202ac34-ee3f-43bb-bbf7-ce07a2d9ee97', 265, 'Proposed by 553 approved by 84 edit id of 232', 8913, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I took on a project to predict the outcome of soccer matches but it turned out to be a very challenging task.
I tried out different models but I only got 50-54% accuracy on my test dataset. Some of the models were created in such
a way that a certain model would predict if a team will win, draw, or loose a match. That same model would also predict if
the opponent of that team will win, draw, or loose the match. Each model predicting with an accuracy of about 50% on each team distinctively. The second set of models I tried, takes the combination of data from both teams and predicts which class the match belongs to (home win, away win, draw). In the system,
only 10 matches are given everyday to be predicted. Meaning, if I predict the 10 matches using the second model, I have a chance of predicting 5
correctly. In this project, I only need to predict 3 matches correctly out of the 10 matches given in a day. Is there a system
of knowing the 3 matches which my models have the best chance of predicting correctly? I only need to get 3 correct, I usually
get 5 correctly but I don''t know how to select my 3 best matches.

**Note:** The first type of models use about 50 features for prediction while the second uses 101. I''ve tried ensembles, they still give
me ~50% accuracy. I''m still about to setup a system that selects matches where the prediction for the home team does not
contradict the prediction for the away team using the first type of model.', 4811, '2015-03-02 21:15:34.480', 'b48b8b87-c416-4d68-beb7-db8dde9981d5', 5267, 8916, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Predicting Soccer: guessing which matches a model will predict correcly', 4811, '2015-03-02 21:15:34.480', 'b48b8b87-c416-4d68-beb7-db8dde9981d5', 5267, 8917, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><predictive-modeling>', 4811, '2015-03-02 21:15:34.480', 'b48b8b87-c416-4d68-beb7-db8dde9981d5', 5267, 8918, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The project I am working on allows users to create Stock Screeners based on both technical and fundamental criteria.  Stock Screeners are then "backtested" by simulating the results of applying in over the last 10 years using Point-in-Time data. I get back the list of trades and overall graph of performance.
(If that is unclear, I have an overview [here](https://www.equitieslab.com/features/stock-screener/) and [there](https://www.equitieslab.com/wiki/QuickStart/StockScreener) with more details).

Now a common problem is that users create overfitted stock screeners. I would love to give them a warning when the screen is likely to be over-fitted.


Fields I have to work with

  + All trades made by the Stock Screener
    + Stock, Start Date, Start Price, End Date, End Price
  + S&P 500 performance for the same time frame
  + Market Cap, Sector, and Industry of each Stock
  ', 8344, '2015-03-02 23:02:45.583', 'a9576600-eac8-4b1f-b62b-8b9d6e60a955', 5268, 8919, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to detect overfitting of a stock screener', 8344, '2015-03-02 23:02:45.583', 'a9576600-eac8-4b1f-b62b-8b9d6e60a955', 5268, 8920, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><classification><bigdata><statistics>', 8344, '2015-03-02 23:02:45.583', 'a9576600-eac8-4b1f-b62b-8b9d6e60a955', 5268, 8921, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The short answer is no! Reinforcement Learning is not limited to discrete spaces. But most of the introductory literature does deal with discrete spaces.

As you might know by now that there are three important components in any Reinforcement Learning problem: Rewards, States and Actions. The first is a scalar quantity and theoretically the latter two can either be discrete or continuous. The convergence proofs and analyses of the various algorithms are easier to understand for the discrete case and also the corresponding algorithms are easier to code. That is one of the reasons, most introductory material focuses on them.

Having said that, it should be interesting to note that the early research on Reinforcement Learning actually focussed on continuous state representations. It was only in the the 90s since the literature started representing all the standard algorithms for discrete spaces as we had a lot of proofs for them.

Finally, if you noticed carefully, I said continuous states only. Mapping continuous states and continuous actions is hard. Nevertheless, we do have some solutions for now. But it is an active area of Research in RL.

This [paper by Sutton][1] from ''98 should be a good start for your exploration!


  [1]: http://webdocs.cs.ualberta.ca/~sutton/papers/SSR-98.pdf', 8491, '2015-03-03 01:35:31.727', '74e3375c-2917-4e90-bf59-2cbe9f7fc2bf', 5269, 8922, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Try using Earth Mover''s Distance (http://ai.stanford.edu/~rubner/papers/rubnerIjcv00.pdf). It measures how much it "costs" to optimally reshape one histogram into another, where an elementary transform (using the terminology of edit distance-like distance measures) is moving a unit of mass from a bin to a bin. If you have an implementation of EMD, then your distance is dist = EMD(H1, H2, D), where

     H1 = [ 14, 13, 40, 31 ]; -- histogram #1 (first image)
     H2 = [ 82, 8, 7 ]; -- histogram #2 (second image)
     D  = [ D11, D12, D13; ... ; D41, D42, D43 ]; -- (cross-bin) ground distance

Dij is a distance from the i''th bin of the first histogram to the j''th bin of the second histogram. For example, D21 is a distance between ''White'' and ''Pink''. It may be defined differently, based on which color space you want to use, and further, based on how you define what it means for two colors to be similar. If I use RGB, and I do not care much as to how human eye perceives colors, then D21 can be the \ell_2 norm of the difference [1, 1, 1] - [1, 0.05, 0.7]. (As far as I remember, the paper I mentioned uses LAB instead of RGB.)

P.S. Besides color, you may include spatial information into comparison. EMD can handle it well, as long as you properly combine similarity of colors with spatial similarity in the definition of ground distance D.
', 8462, '2015-03-03 04:49:41.447', 'd5db518f-051c-4d41-8bcb-a20b3719fa0f', 5263, 'typo (reshare -> reshape)', 8923, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Training a model, related to *information extraction*, in general, and *named entity recognition/resolution (NER)*, in particular, is described in detail in **Chapter 7** of the [NLTK Book](http://www.nltk.org/book), available online at this URL: http://www.nltk.org/book/ch07.html.

Additionally, I think that you might find useful my [related answer](http://stats.stackexchange.com/a/136760/31372) on *Cross Validated* site. It has a lot of references to relevant *sources* on NER and related topics as well as to various related *software* tools.', 2452, '2015-03-03 05:59:08.820', '11d89f1a-25ce-4f91-84c5-dc9f675a34cb', 5270, 8924, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was going through this paper, and came across the term task based regularization approach:<br>
Multitask learning for hostpathogen protein interactions
Meghana Kshirsagar1, Jaime Carbonell1 and Judith Klein-Seetharaman1,2,3,*<br><br>
1. What is task based regularization approach?<br>
2. What does this phrase mean?<br>
3. what is its utility?<br><br>
I have Googled the term, what I get are different methods of regularization but none describing what regularization originally means. Please help', 8013, '2015-03-03 07:14:02.420', '75b52c32-2c7c-4464-a0df-ff6a79ea11d0', 5271, 8925, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('task based regularization approach', 8013, '2015-03-03 07:14:02.420', '75b52c32-2c7c-4464-a0df-ff6a79ea11d0', 5271, 8926, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 8013, '2015-03-03 07:14:02.420', '75b52c32-2c7c-4464-a0df-ff6a79ea11d0', 5271, 8927, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I was going through this paper, and came across the term task based regularization approach:<br>
Multitask learning for hostpathogen protein interactions
Meghana Kshirsagar1, Jaime Carbonell1 and Judith Klein-Seetharaman1,2,3,*<br><br>
1. What is task based regularization approach?<br>
2. What does this phrase mean?<br>
3. what is its utility?<br>
4. What is a regularizer? What does it do?<br><br>
I have Googled the term, what I get are different methods of regularization but none describing what regularization originally means. Please help', 8013, '2015-03-03 08:21:33.067', 'cdb33178-c39a-4421-8f95-37e55d5f9664', 5271, 'added 48 characters in body', 8928, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Yes, I do realise that this is probably suited to some "health" stackexchange but this was the closest stackexchange website I found to ask my question as I need some data.

My question is, where can I get structured data with symptoms and diagnosis results of H1N1 influenza. I need some data for a computer program.

Probably like the following:
https://sites.google.com/a/googlesciencefair.com/science-fair-2012-project-64a91af142a459cfb486ed5cb05f803b2eb41354-1333130785-87/data

(In the above data each line is a diagnosis, where the each digit before the space corresponds to a symptom and its value from 0-A (ten) represents a value for the symptom like maybe if a digit is fever, 0 could be mild, 1 could be moderate and 2 could be high (just an example). The number after the space is a boolean that represents if the diagnosis was +ve or -ve.



I''m aware that these kind of questions are not encouraged by the stackexchange community, but there''s none other that could help me.', 8502, '2015-03-03 15:19:41.647', 'b04c5fa5-9704-4b63-b2c0-2cb83f25b24d', 5272, 8929, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Datasets for training H1N1 diagnosis AI program', 8502, '2015-03-03 15:19:41.647', 'b04c5fa5-9704-4b63-b2c0-2cb83f25b24d', 5272, 8930, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<dataset>', 8502, '2015-03-03 15:19:41.647', 'b04c5fa5-9704-4b63-b2c0-2cb83f25b24d', 5272, 8931, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Yes, I do realise that this is probably suited to some "health" stackexchange but this was the closest stackexchange website I found to ask my question as I need some data.

My question is, where can I get structured data with symptoms and diagnosis results of H1N1 influenza. I need some data for a computer program.

Probably like the following:
https://sites.google.com/a/googlesciencefair.com/science-fair-2012-project-64a91af142a459cfb486ed5cb05f803b2eb41354-1333130785-87/data

(In the above data each line is a diagnosis, where the each digit before the space corresponds to a symptom and its value from 0-9 and A (ten) represents a value for the symptom like maybe if a digit is fever, 0 could be mild, 1 could be moderate and 2 could be high (just an example). The number after the space is a boolean that represents if the diagnosis was +ve or -ve.



I''m aware that these kind of questions are not encouraged by the stackexchange community, but there''s none other that could help me.', 8502, '2015-03-03 15:25:49.633', '773e323c-8d0e-41aa-8afe-42bb0897347f', 5272, 'added 6 characters in body', 8932, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('training set contains **p** number of papers. each paper is annotated has research or non-research. To develop the research paper filter, we consider the **W** most frequent phrases in a paper. the research paper filter will use the presence or absence of these **W** phrases to decide if the paper is indeed a research paper or not

1. How many possible hypothesis are there?
I have 2^W possible hypothesis, but if we are including conjuctive hypothesis symbols then I would say 4^W.

is my logic correct?

2. How many of them are consistent?
For this question, I have no idea where to begin. any insight?', 8505, '2015-03-03 16:12:13.860', '59879c6c-d614-4ec8-aeee-f4698687fc4d', 5273, 8933, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How many possible hypothesis', 8505, '2015-03-03 16:12:13.860', '59879c6c-d614-4ec8-aeee-f4698687fc4d', 5273, 8934, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning>', 8505, '2015-03-03 16:12:13.860', '59879c6c-d614-4ec8-aeee-f4698687fc4d', 5273, 8935, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a 3gpp protocol stack system that sends out software traces from different layers of the stack. The traces are in plain text. When an error is occurred, the trace file is analyzed to find the module and procedure that caused the error. I would like to build a system to automatically analyze the trace file and predict the position in the trace file where there is a chance of error. ( And then find the module/procedure that caused the error ).

1. I would like to know if any such systems are built for 3GPP protocol stacks (or similar systems ). Please give pointers.

2. What kind of a machine learning system is useful for this? I have lot of trace files. But there are not much labeled data.

Thanks for your help.

ps: I posted the question in CV; But there was a comment to move the question to Data Science. ( http://stats.stackexchange.com/questions/140232/error-position-in-software-trace-file )', 8516, '2015-03-04 03:49:37.190', 'c6a8e0fc-1f34-4e71-9a54-3cb404dde862', 5274, 8936, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Error position in software trace file', 8516, '2015-03-04 03:49:37.190', 'c6a8e0fc-1f34-4e71-9a54-3cb404dde862', 5274, 8937, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><recommendation>', 8516, '2015-03-04 03:49:37.190', 'c6a8e0fc-1f34-4e71-9a54-3cb404dde862', 5274, 8938, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I do think it is new job, basically data scientist has to apply mathematical algorithms on data with considerable constraint in terms 1) Run time of the application 2) Resource use of the application. If these constraints are not present, I would not call the job data science. Moreover, these algorithms are often need to be ran on distributed systems, which is another dimension of the problem.

Of course, this has been done before, in some combination of statistics, mathematics and programming, but it was not wide spread to give rise to the new term. The real rise of data science is from the ability to gather large amounts of data, thus need to need to process it.

', 3070, '2015-03-04 04:39:35.380', '85a16041-bc30-48cd-9099-a9ab3618654d', 5275, 8939, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Terms that covered more or less the same topics that Data Science covers today:

 - Pattern Recognition
 - Machine Learning
 - Data Mining
 - Quantitative methods', 8517, '2015-03-04 06:05:05.320', '361afd69-3b7c-46a8-a7d8-47421cc492f7', 5276, 8940, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('So, our data set this week has 14 attributes and each column has very different values. One column has values below 1 while another column has values that go from three to four whole digits.

We learned normalization last week and it seems like you''re supposed to normalize data when they have very different values. For decision trees, is the case the same?

I''m not sure about this but would normalization affect the resulting decision tree from the same data set?', 8318, '2015-03-04 08:05:45.003', '631aec33-cf8e-47b3-b875-86fd7ce33c63', 5277, 8941, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Do you have to normalize data when building decision trees using R?', 8318, '2015-03-04 08:05:45.003', '631aec33-cf8e-47b3-b875-86fd7ce33c63', 5277, 8942, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<r><beginner>', 8318, '2015-03-04 08:05:45.003', '631aec33-cf8e-47b3-b875-86fd7ce33c63', 5277, 8943, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('So, our data set this week has 14 attributes and each column has very different values. One column has values below 1 while another column has values that go from three to four whole digits.

We learned normalization last week and it seems like you''re supposed to normalize data when they have very different values. For decision trees, is the case the same?

I''m not sure about this but would normalization affect the resulting decision tree from the same data set? It doesn''t seem like it should but...', 8318, '2015-03-04 08:15:57.703', 'e28cd1af-263c-4645-9b62-6e98df774787', 5277, 'added 38 characters in body', 8944, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":-1,"DisplayName":"Community"}]}', -1, '2015-03-04 11:30:18.767', '78b317e9-c685-41c8-8893-2c89f24bcc91', 5272, 8945, '14');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":84,"DisplayName":"Rubens"}]}', 84, '2015-03-04 11:30:18.767', '6541d3a1-bbdc-4f64-be7d-6f582329c7f9', 5272, 'to http://opendata.stackexchange.com/questions/4660/datasets-for-training-h1n1-diagnosis-ai-program', 8946, '35');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":84,"DisplayName":"Rubens"}]}', 84, '2015-03-04 11:30:18.767', '8fdbf271-68e6-47b6-ba7e-01a6b79c7757', 5272, '102', 8947, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Training set contains $p$ papers. Each paper is annotated as *research* or *non-research*. To develop the research paper filter, we consider the $W$ most frequent phrases in a paper. The research paper filter will use the presence/absence of these $W$ phrases to decide if the paper is indeed a research paper or not.

1. How many possible hypothesis are there?

 I have $2^W$ possible hypothesis, but if we are including conjunctive hypothesis symbols, then I would say $4^W$. Is my logic correct?

2. How many of them are consistent?

 For this question, I have no idea where to begin. Any insight?', 84, '2015-03-04 11:36:52.963', '273ac1bf-5b21-44a2-9128-22622afc5235', 5273, 'Improving writing.', 8948, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('<machine-learning><classification>', 84, '2015-03-04 11:36:52.963', '273ac1bf-5b21-44a2-9128-22622afc5235', 5273, 'Improving writing.', 8949, '6');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Most common types of decision trees you encounter are not affected by any monotonic transformation. So, as long as you preserve orde, the decision trees are the same (obviously by the same tree here I understand the same decision structure, not the same values for each test in each node of the tree).

The reason why it happens is because how usual impurity functions works. In order to find the best split it searches on each dimension (attribute) a split point which is basically an if clause which groups target values corresponding to instances which has test value less than split value, and on the right the values greater than equal. This happens for numerical attributes (which I think is your case because I do not know how to normalize a nominal attribute). Now you might note that the criteria is less than or greater than. Which means that the real information from the attributes in order to find the split (and the whole tree) is only the order of the values. Which means that as long as you transform your attributes in such a way that the original ordering is reserved, you will get the same tree.

Not all models are insensitive to such kind of transformation. For example linear regression models give the same results if you multiply an attribute with something different than zero. You will get different regression coefficients, but the predicted value will be the same. This is not the case when you take a log of that transformation. So for linear regression, for example, normalizing is useless since it will provide the same result.

However this is not the case with a penalized linear regression, like ridge regression. In penalized linear regressions a constraint is applied to coefficients. The idea is that the constraint is applied to the sum  of a function of coefficients. Now if you inflate an attribute, the coefficient will be deflated, which means that in the end the penalization for that coefficient it will be artificially modified. In such kind of situation, you normalize attributes in order that each coefficient to be constraint ''fairly''.

Hope it helps', 108, '2015-03-04 12:15:10.117', '1f31afce-00c1-41a8-b4b6-bff9d559f6d1', 5278, 8952, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I was given a target function to design neural network and train: (y = (x1  x2)  (x3  x4))

The number of input and number of output seems obvious (4 and 1). And the training data can use truth table.

However, in order to train as a multilayer artificial neural network, I need to choose number of hidden units. May I know where can I find some general guideline for this?

Thank you!', 8524, '2015-03-04 14:19:00.713', '7d385661-a1d0-4fd0-8dac-435492c4d0bb', 5279, 8953, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to select topology for neural network?', 8524, '2015-03-04 14:19:00.713', '7d385661-a1d0-4fd0-8dac-435492c4d0bb', 5279, 8954, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<neuralnetwork>', 8524, '2015-03-04 14:19:00.713', '7d385661-a1d0-4fd0-8dac-435492c4d0bb', 5279, 8955, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I want to generate an $N\times N$ matrix $A$ so as to target an $N$ vector of row sums and simultaneously all column sums should sum to 1. In addition to this, I have a prefixed number of elements which are set to zero.
For example, beginning with:
$$
  \left[\begin{array}{rrr}
    0 & 1 & 1 \\
    1 & 0 & 0 \\
    1 & 1 & 0
  \end{array}\right]
$$
and the row sum vector $[1.5, 0.25, 1]^{T}$, I want to end up with
$$
  \left[\begin{array}{rrr}
    0 & a_{12} & a_{13} \\
    a_{21} & 0 & 0 \\
    a_{31} & a_{32} & 0
  \end{array}\right]
$$
under the following conditions:

$a_{12} + a_{13}  = 1.5$

$ a_{21} = 0.25$

$a_{31}+a_{32} = 1$

$a_{21}+a_{31} = 1$

$a_{12}+a_{32} = 1$

$a_{13} = 1$

While this is simplistic, in general, I have $2N$ equations in $N^{2}-Z$ unknowns, where $Z$ is the number of elements fixed to zero. So, this system of equations could be overdetermined or underdetermined, but I would like to be able to generate matrices like this such that all nonzero elements lie in $(0,1]$.
', 8526, '2015-03-04 14:34:51.013', '9a841ab9-91e1-4e8d-8e2e-99ec264a9f1b', 5280, 8956, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Statistical Procedure to generate column stochastic matrix with target row sums', 8526, '2015-03-04 14:34:51.013', '9a841ab9-91e1-4e8d-8e2e-99ec264a9f1b', 5280, 8957, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<statistics><algorithms><graphs><social-network-analysis>', 8526, '2015-03-04 14:34:51.013', '9a841ab9-91e1-4e8d-8e2e-99ec264a9f1b', 5280, 8958, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Speed of code execution is rarely an issue.  The important speed in business is almost always the speed of designing, deploying, and maintaining the application.  An experienced programmer can optimize where necessary to get code execution fast enough.  In these cases, R can make a lot of sense in production.

In cases where speed of execution IS an issue, you are already going to find an optimized C++ or some such real-time decision engine.  So your choices are integrate an R process, or add the bits you need to the engine.  The latter is probably the only option, not because of the speed of R, but because you don''t have the time to incorporate any external process.  If the company has nothing to start with, I can''t imagine everyone saying "let''s build our time critical real-time engine in R because of the great statistical libraries".

I''ll give a few examples from my corporate experiences, where I use R in production:

 - Delivering Shiny applications dealing with data that is not/ not yet institutionalized.  I will generally load already-processed data frames and use Shiny to display different graphs and charts.  Computation is minimal.
 - Decision making analysis that requires heavy use of advanced libraries (mcclust, machine learning) but done on a daily or longer time-scale.  In this case there is no reason to use any other language.  I''ve already done the prototyping in R, so my fastest and best option is to keep things there.

I did not use R for production when integrating with a real-time C++ decision engine.  Issues:

 - An additional layer of complication to spawn R processes and integrate the results
 - A suitable machine-learning library (Waffles) was available in C++

The caveat in the latter case: I still use R to generate the training files.







 ', 1077, '2015-03-04 15:53:46.530', '050b4d25-f662-45e6-9345-5136b95facfd', 5281, 8959, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('You problem is linearly separable so you can use a single layer perceptron. ', 8517, '2015-03-04 21:47:35.453', '4981a438-479d-4ae2-8965-f79e3160530b', 5282, 8960, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('You problem is linearly separable so you can use a single layer perceptron.
Hidden units are necessary only for non linear problems (xor is a classic example).

As for general guidelines, I don''t think anyone has ever needed more than one hidden layer. Also the number of neurons in the hidden layer doesn''t need to exceed the number of inputs. ', 8517, '2015-03-05 03:17:42.347', 'f79d4fdd-700d-421c-a6d1-07b53966084d', 5282, 'added 86 characters in body', 8961, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Consider that each item has title, description,  tag, category, sub category n location', 8537, '2015-03-05 07:22:57.690', '59a2e634-bc60-460c-bf10-e5b8b3014100', 5283, 8962, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How to store n search location based items which has category hierarchy and tags', 8537, '2015-03-05 07:22:57.690', '59a2e634-bc60-460c-bf10-e5b8b3014100', 5283, 8963, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<categorical-data><hierarchical-data-format>', 8537, '2015-03-05 07:22:57.690', '59a2e634-bc60-460c-bf10-e5b8b3014100', 5283, 8964, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Consider that each item has title, description,  tag, category, sub category n location. Each of above properties has a weight and search results are those on top', 8537, '2015-03-05 08:31:19.290', '37ca088d-5f39-4e94-bcb1-a454f43c06b5', 5283, 'added 75 characters in body', 8965, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('{"Voters":[{"Id":21,"DisplayName":"Sean Owen"}]}', 21, '2015-03-05 12:24:33.010', '7e9905dc-b3c8-45df-9a01-fc71788169ba', 5283, '103', 8966, '10');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I''m a Newbie to Hadoop!

By web search and after going through hadoop guides, it is clear that hdfs doesn''t allow us to edit the file but to append some data to existing file, it will be creating a new instance temporarily and append the new data to it.

That being said,

1.would like to know whether the new file or temp is created in the same block or in a different block?

2.What will happen if the file exceeds the previous  allocated block size is exceeded?

Any help would be greatly appreciated!', 8540, '2015-03-05 18:28:43.367', 'ec7c4094-8977-4ae5-a93c-9441d75d4aca', 5284, 8967, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('How append works in hdfs? Where the newly created instance of file is placed?', 8540, '2015-03-05 18:28:43.367', 'ec7c4094-8977-4ae5-a93c-9441d75d4aca', 5284, 8968, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><bigdata><hadoop>', 8540, '2015-03-05 18:28:43.367', 'ec7c4094-8977-4ae5-a93c-9441d75d4aca', 5284, 8969, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Here are two ideas that I was thinking about.

1. Bug Prediction

Based on the previous activity future bugs can be predicted. There are a number of papers available on the net. I remember there was a paper about bug prediction from SVN data. ( Just google for software bug prediction )

2. Error position from software traces

Errors may be like rare events or anomalies in the trace output. May be you can classify the error and pinpoint the procedure that caused the error. I am currently thinking about a system like that now ( Not sure about its success though ). I asked a question here: http://stats.stackexchange.com/questions/140232/error-position-in-software-trace-file

 ', 8516, '2015-03-06 02:43:38.727', 'e34acd44-8058-4ef1-bbc2-c408832b51fb', 5285, 8970, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('When looking for texts to learn advanced topics, I start with a web search for relevant grad courses and textbooks, or background tech/math books like those from Dover.

To wit, Theoretical Statistics by Keener looks relevant:
http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-93838-7

And this:
"Looking for a good Mathematical Statistics self-study book (I''m a physics student and my class & current book are useless to me)"
http://www.reddit.com/r/statistics/comments/1n6o19/looking_for_a_good_mathematical_statistics/


', 3556, '2015-03-06 03:46:41.693', 'ecb5b773-5a2b-4c1f-af35-31725447c41a', 5286, 8971, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Any suggestion on what kind of dataset lets say nXd (n rows, d columns) would give me same eigenvectors?.

I believe it should be the one with like same absolute values in each cell. Like alternating +1 and -1. But it seems to work otherwise.

Any pointers?', 8547, '2015-03-06 04:09:56.030', '4a1e4f21-74ea-428d-b430-6d6b9bd1be3a', 5287, 8972, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Dataset to give same eigenvectors?', 8547, '2015-03-06 04:09:56.030', '4a1e4f21-74ea-428d-b430-6d6b9bd1be3a', 5287, 8973, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><data-mining><python>', 8547, '2015-03-06 04:09:56.030', '4a1e4f21-74ea-428d-b430-6d6b9bd1be3a', 5287, 8974, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('This problem, being more of a combinatorial rather than statistical nature, can be though of as a *max-flow problem in a bipartite network*. Each column of your matrix corresponds to a "source node" in the left part of the bipartite graph $BP(L, R)$; each row of the matrix corresponds to a "destination node" in the right part $R$. Capacity of each source node is the corresponding column''s sum; similarly, capacities are defined for the destinations via row sums. Value $a_{ij}$ in the matrix describes flow of mass from $j$''th source to $i$''th destination along an existing edge $(j \to i)$ of $BP$. Your initially given 0-1 matrix describes which edges are present (e.g., the matrix in your example prescribes the edge $(3, 2)$ to be absent from the network, along with all self-loops). If you denote $F$ to be the sum of all column sums (or, alternatively, the total capacity of all sources), then your problem narrows down to finding a max-flow from $L$ to $R$ in $BP$ and checking whether the volume of the discovered flow equals $F$.

*Note 1 (Feasibility):* Before plunging into finding best flows, you may want to check your row sums: a necessary condition for your problem to have a solution is that the sum of all column sums (which is $N$ in your particular case of a column-stochastic matrix) should be equal the sum of row sums, as both numbers describe the same -- the sum of all elements of the target matrix. (For example, for $N = 3$ and the vector of row sums from your example, the problem is unfeasible.)

*Note 2 (Upper bound):* Since $a_{ij}$ describes the flow from $j$''th source, and the capacities of all sources are 1 in your setting, then each source cannot send more than 1 unit of mass to a single destination. Thus, each $a_{ij}$ will not exceed 1.

*Note 3 (Lower bound):* To make $a_{ij}$ for each existing edge $(j \to i)$ strictly positive, you can augment the max-flow problem with lower bounds on edge capacities. In a general setting, edge capacity bounds are $0$ and $+\infty$. You may want to replace the lower bound with some positive number, small enough not to interfere with the discovery of the best flow and large enough to satisfy your edge saturation needs.', 8462, '2015-03-06 05:45:03.720', '0cd5bbf4-ec35-413a-82a0-1c8b114a20f6', 5288, 8975, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('The only such existing "dataset" is a 1-by-1 matrix with an arbitrary value of its sole item.', 8462, '2015-03-06 06:11:10.997', 'c274a20f-3478-4ca7-bdb3-215dc578a214', 5289, 8976, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I will restrict myself to the finite-dimensional situation.

First, if we are talking about *normal* eigenvalues and not generalized ones, we need a square matrix. If $M\in\mathbb{R}^{n\times n}$ (or $\mathbb{C}^{n\times n}$) is a [diagonizable matrix][1], it has an [eigendecomposition][2]
\begin{equation}
M = V\Delta V^{-1}
\textrm{,}
\end{equation}
with $V:=\begin{bmatrix}v_{1},v_{2},\ldots,v_{n}\end{bmatrix}$ the matrix with its $n$ eigenvectors $v_{i}$ as columns and $\Delta:=\operatorname{diag}\left(\lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right)$ the diagonal matrix with the corresponding eigenvalues on its main diagonal.

As you can see from this equation if you want to get matrices with the same eigenvectors, i.e. $V$ fixed, you can only change the eigenvalues $\lambda_{i}$. In general the relationship between the eigenvalues and the entries of the original matrix $M$ is non-trivial.

The easiest situation arises if the eigenvectors $v_{i}$ form the standard basis, i.e. only $1$ at the $i$th position, $0$ otherwise. Then $M=I\Delta I^{-1}=\Delta$ ($I$ the identity matrix) and you can change every entry on the main diagonal without changing any eigenvector.

  [1]: http://en.wikipedia.org/wiki/Diagonalizable_matrix
  [2]: http://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Eigendecomposition_of_a_matrix', 8249, '2015-03-06 07:56:19.903', '3d3c2bb4-7fce-4b81-9678-102239ce4b6c', 5290, 8977, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Computation of a column-stochastic matrix with target row sums', 8462, '2015-03-06 09:35:47.037', '7b4ad1f8-578c-4f8d-a6df-e065d1bc6b8d', 5280, 'This question has nothing to do with statistics (at least as it has been posed and how it is supposed to be resolved)', 8978, '4');
INSERT INTO postHistory(creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('2015-03-06 09:35:47.037', '7b4ad1f8-578c-4f8d-a6df-e065d1bc6b8d', 5280, 'Proposed by 8462 approved by 84 edit id of 233', 8979, '24');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('Any suggestion on what kind of dataset lets say $n \times d$ ($n$ rows, $d$ columns) would give me same eigenvectors?.

I believe it should be the one with same absolute values in each cell. Like alternating +1 and -1. But it seems to work otherwise.

Any pointers?', 84, '2015-03-06 09:38:33.380', '33d63f23-3b1a-4c6f-8c3c-8a3b49a69f1a', 5287, 'added 7 characters in body', 8980, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('The only such existing "dataset" is a 1-by-1 matrix with an arbitrary value of its sole item.

If the eigenvectors, being the columns of matrix $V$, are "the same" (precisely up to a scalar constant factor), then matrix $V$ is singular (its columns are linearly dependent; its determinant is zero), and, hence, non-invertible $\Rightarrow$ such an eigendecomposition $V \cdot \Lambda \cdot V^{-1}$ cannot exist.', 8462, '2015-03-06 09:45:35.087', '975413aa-5368-4e02-a728-58376ea15f89', 5289, 'explanation', 8981, '5');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I am using twitteR package to retrievie timeline data. My request looks as follows:

`tweets <- try(userTimeline(user , n=50),silent=TRUE)`

and this worked quite well for a time, but now I receive this error message:

    Error in function (type, msg, asError = TRUE)  : easy handle already used in multi handle

In a related question on Stackoverflow one answer is to use Rcurl directly but this does not seem to work with twitteR package. Anybody got an idea on this?', 8549, '2015-03-06 10:11:02.133', '66d3bf6f-4532-4755-b1cf-e3132078831a', 5291, 8982, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Error using twitter R package''s userTimlien', 8549, '2015-03-06 10:11:02.133', '66d3bf6f-4532-4755-b1cf-e3132078831a', 5291, 8983, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<data-mining><r>', 8549, '2015-03-06 10:11:02.133', '66d3bf6f-4532-4755-b1cf-e3132078831a', 5291, 8984, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('I have a huge collection of objects from which only a tiny fraction are in a class of interest. The collection is initially unlabelled, but labels can be added using an expensive operation (for example, by human).

Currently I use the simple generic machine learning strategy:

 1. Use hand-crafted rules to select a smaller subset of objects (thus leaving out a fraction of interesting ones).

 2. Label part of the smaller subset, and use these for training and choosing a classification algorithm and its parameters.

 3. Classify the remaining objects in the smaller set (and also perhaps in the big set).

This has two drawbacks:

 1. The labeller still needs to see a huge number of uninteresting objects, and therefore is able to label only a very small fraction of interesting ones.

 2. The objects not in the smaller set are completely ignored in the learning phase, resulting in a loss of some information (the classification algorithm might not work well on this complement).

It seems that it would be better to use online learning: i.e., select the objects to show to the labeller based on the previous labels. But then it becomes no longer obvious that the result of classification algorithm retains the nice theoretical properties (i.e., statistical consistency).

Is there a general framework for active object detection which works either theoretically or practically (or both)? I could not get the complete picture from the Wikipedia article [active learning][1].


  [1]: http://en.wikipedia.org/wiki/Active_learning_%28machine_learning%29', 6550, '2015-03-06 10:28:04.483', '59fafbc8-332f-4eee-aa22-a510212681a2', 5292, 8985, '2');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('Generic strategy for object detection', 6550, '2015-03-06 10:28:04.483', '59fafbc8-332f-4eee-aa22-a510212681a2', 5292, 8986, '1');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, id, posthistorytypeid) VALUES ('<machine-learning><classification><object-recognition>', 6550, '2015-03-06 10:28:04.483', '59fafbc8-332f-4eee-aa22-a510212681a2', 5292, 8987, '3');
INSERT INTO postHistory(text, userid, creationdate, revisionguid, postid, comment, id, posthistorytypeid) VALUES ('I''m a Newbie to Hadoop!

By web search and after going through hadoop guides, it is clear that hdfs doesn''t allow us to edit the file but to append some data to existing file, it will be creating a new instance temporarily and append the new data to it.

That being said,

1.would like to know whether the *new file* or *temp file* is created in the *same block* or in a *different block*?

2.What will happen if the revised file **exceeds** the previous  allocated block size?

Any help would be greatly appreciated!', 8540, '2015-03-06 16:51:02.073', 'fd909838-4a9b-48ed-8abf-62c1a45805bd', 5284, 'edited body', 8988, '5');
